Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.231    |
| time/              |          |
|    fps             | 1068     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.20 +/- 0.60
Episode length: 275.40 +/- 65.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009582016 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.211      |
|    learning_rate        | 0.01        |
|    loss                 | 0.00485     |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00118     |
|    value_loss           | 0.721       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.192    |
| time/              |          |
|    fps             | 737      |
|    iterations      | 2        |
|    time_elapsed    | 11       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-1.00 +/- 0.00
Episode length: 307.20 +/- 46.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 307         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.007553236 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0261      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.000541   |
|    value_loss           | 0.0462      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 671      |
|    iterations      | 3        |
|    time_elapsed    | 18       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-1.00 +/- 0.00
Episode length: 286.00 +/- 49.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 286          |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0057763457 |
|    clip_fraction        | 0.038        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0158       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 0.0537       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.173    |
| time/              |          |
|    fps             | 644      |
|    iterations      | 4        |
|    time_elapsed    | 25       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-1.00 +/- 0.00
Episode length: 275.00 +/- 33.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.007883012 |
|    clip_fraction        | 0.0484      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.01        |
|    loss                 | 0.0103      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 0.0543      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 0.169    |
| time/              |          |
|    fps             | 636      |
|    iterations      | 5        |
|    time_elapsed    | 32       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 318         |
|    ep_rew_mean          | 0.169       |
| time/                   |             |
|    fps                  | 657         |
|    iterations           | 6           |
|    time_elapsed         | 37          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009091426 |
|    clip_fraction        | 0.00671     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0412      |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.000614    |
|    value_loss           | 0.0541      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-1.00 +/- 0.00
Episode length: 285.60 +/- 42.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 286        |
|    mean_reward          | -1         |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01015342 |
|    clip_fraction        | 0.0432     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0          |
|    learning_rate        | 0.01       |
|    loss                 | 0.00184    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00165   |
|    value_loss           | 0.053      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 0.167    |
| time/              |          |
|    fps             | 644      |
|    iterations      | 7        |
|    time_elapsed    | 44       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=0.20 +/- 0.40
Episode length: 276.20 +/- 49.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.011790307 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.00777     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 0.0524      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 638      |
|    iterations      | 8        |
|    time_elapsed    | 51       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=0.30 +/- 0.64
Episode length: 305.20 +/- 53.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.014188856 |
|    clip_fraction        | 0.0842      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0394      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 0.0627      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 320      |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 629      |
|    iterations      | 9        |
|    time_elapsed    | 58       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=0.60 +/- 1.20
Episode length: 326.00 +/- 65.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 326         |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.009928556 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.00237     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 0.0539      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 325      |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 622      |
|    iterations      | 10       |
|    time_elapsed    | 65       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=0.30 +/- 0.46
Episode length: 292.60 +/- 36.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0035957065 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0269       |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.000794    |
|    value_loss           | 0.0609       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 324      |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 619      |
|    iterations      | 11       |
|    time_elapsed    | 72       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 317         |
|    ep_rew_mean          | 0.24        |
| time/                   |             |
|    fps                  | 631         |
|    iterations           | 12          |
|    time_elapsed         | 77          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.007644603 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.01        |
|    loss                 | 0.00552     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 0.0606      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=0.70 +/- 0.64
Episode length: 329.40 +/- 54.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 329          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0046882657 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.0233       |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.000635     |
|    value_loss           | 0.0543       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 0.25     |
| time/              |          |
|    fps             | 623      |
|    iterations      | 13       |
|    time_elapsed    | 85       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=0.20 +/- 0.40
Episode length: 304.00 +/- 36.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 304        |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.01519074 |
|    clip_fraction        | 0.0291     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0          |
|    learning_rate        | 0.01       |
|    loss                 | -0.00793   |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.000787  |
|    value_loss           | 0.054      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 621      |
|    iterations      | 14       |
|    time_elapsed    | 92       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=0.50 +/- 0.50
Episode length: 300.20 +/- 37.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0102424435 |
|    clip_fraction        | 0.0466       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.012        |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00168     |
|    value_loss           | 0.0605       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 617      |
|    iterations      | 15       |
|    time_elapsed    | 99       |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=65000, episode_reward=0.60 +/- 0.80
Episode length: 329.00 +/- 46.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 329         |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 65000       |
| train/                  |             |
|    approx_kl            | 0.011616479 |
|    clip_fraction        | 0.0537      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0122      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 0.0648      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 319      |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 613      |
|    iterations      | 16       |
|    time_elapsed    | 106      |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 320         |
|    ep_rew_mean          | 0.31        |
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 17          |
|    time_elapsed         | 111         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.006710824 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.01        |
|    loss                 | 0.0418      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00315    |
|    value_loss           | 0.0563      |
-----------------------------------------
Eval num_timesteps=70000, episode_reward=0.20 +/- 0.40
Episode length: 280.20 +/- 65.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.013542369 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0256      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00441    |
|    value_loss           | 0.0668      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 320      |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 621      |
|    iterations      | 18       |
|    time_elapsed    | 118      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=75000, episode_reward=0.20 +/- 0.60
Episode length: 295.20 +/- 43.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 75000        |
| train/                  |              |
|    approx_kl            | 0.0038194004 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.945       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0314       |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 0.0592       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 320      |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 618      |
|    iterations      | 19       |
|    time_elapsed    | 125      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=80000, episode_reward=0.40 +/- 0.66
Episode length: 287.00 +/- 34.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0043358235 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.94        |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0342       |
|    n_updates            | 190          |
|    policy_gradient_loss | -6.69e-05    |
|    value_loss           | 0.0619       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 320      |
|    ep_rew_mean     | 0.32     |
| time/              |          |
|    fps             | 617      |
|    iterations      | 20       |
|    time_elapsed    | 132      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=85000, episode_reward=0.50 +/- 0.67
Episode length: 298.00 +/- 36.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 85000        |
| train/                  |              |
|    approx_kl            | 0.0030815657 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.903       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0143       |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 0.0564       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 615      |
|    iterations      | 21       |
|    time_elapsed    | 139      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=90000, episode_reward=0.60 +/- 0.80
Episode length: 287.60 +/- 55.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 90000        |
| train/                  |              |
|    approx_kl            | 0.0049760556 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.889       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0384       |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 0.0681       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 613      |
|    iterations      | 22       |
|    time_elapsed    | 146      |
|    total_timesteps | 90112    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 317          |
|    ep_rew_mean          | 0.31         |
| time/                   |              |
|    fps                  | 620          |
|    iterations           | 23           |
|    time_elapsed         | 151          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0038614734 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.844       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0345       |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00124     |
|    value_loss           | 0.0567       |
------------------------------------------
Eval num_timesteps=95000, episode_reward=0.30 +/- 0.64
Episode length: 288.60 +/- 33.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 289         |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.003032369 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.874      |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0279      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.000428   |
|    value_loss           | 0.0616      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 618      |
|    iterations      | 24       |
|    time_elapsed    | 159      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=100000, episode_reward=0.60 +/- 0.66
Episode length: 284.40 +/- 50.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0061165835 |
|    clip_fraction        | 0.0549       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.781       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0276       |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00273     |
|    value_loss           | 0.0667       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 616      |
|    iterations      | 25       |
|    time_elapsed    | 166      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=105000, episode_reward=0.60 +/- 0.66
Episode length: 304.40 +/- 40.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 304         |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 105000      |
| train/                  |             |
|    approx_kl            | 0.004181123 |
|    clip_fraction        | 0.0584      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.793      |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0243      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 0.063       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 615      |
|    iterations      | 26       |
|    time_elapsed    | 173      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=110000, episode_reward=0.50 +/- 0.50
Episode length: 302.80 +/- 52.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 303          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 110000       |
| train/                  |              |
|    approx_kl            | 0.0030103538 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.846       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0422       |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00122     |
|    value_loss           | 0.0536       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 613      |
|    iterations      | 27       |
|    time_elapsed    | 180      |
|    total_timesteps | 110592   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 305          |
|    ep_rew_mean          | 0.19         |
| time/                   |              |
|    fps                  | 618          |
|    iterations           | 28           |
|    time_elapsed         | 185          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0037952764 |
|    clip_fraction        | 0.0441       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.86        |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.03         |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 0.059        |
------------------------------------------
Eval num_timesteps=115000, episode_reward=0.50 +/- 0.67
Episode length: 307.60 +/- 54.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 308         |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.004937561 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0398      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00077    |
|    value_loss           | 0.056       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 617      |
|    iterations      | 29       |
|    time_elapsed    | 192      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=120000, episode_reward=0.40 +/- 0.66
Episode length: 291.60 +/- 61.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.004099801 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.71       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0401      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.000915   |
|    value_loss           | 0.0604      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 615      |
|    iterations      | 30       |
|    time_elapsed    | 199      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=125000, episode_reward=0.90 +/- 0.54
Episode length: 338.80 +/- 42.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 339          |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0040781796 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.651       |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.0404       |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 0.0636       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 613      |
|    iterations      | 31       |
|    time_elapsed    | 206      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=130000, episode_reward=0.20 +/- 0.40
Episode length: 302.20 +/- 36.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 302          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0030631768 |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.572       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.013        |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 0.056        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 612      |
|    iterations      | 32       |
|    time_elapsed    | 213      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=135000, episode_reward=0.90 +/- 0.70
Episode length: 341.40 +/- 51.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 341          |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 135000       |
| train/                  |              |
|    approx_kl            | 0.0011102702 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.581       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0435       |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000167    |
|    value_loss           | 0.0617       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 610      |
|    iterations      | 33       |
|    time_elapsed    | 221      |
|    total_timesteps | 135168   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 305         |
|    ep_rew_mean          | 0.27        |
| time/                   |             |
|    fps                  | 614         |
|    iterations           | 34          |
|    time_elapsed         | 226         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.002800296 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.54       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0293      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 0.062       |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=0.40 +/- 0.49
Episode length: 315.40 +/- 40.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 315          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0022142143 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.513       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0192       |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 0.0631       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 613      |
|    iterations      | 35       |
|    time_elapsed    | 233      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=145000, episode_reward=0.20 +/- 0.40
Episode length: 271.20 +/- 47.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 271          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 145000       |
| train/                  |              |
|    approx_kl            | 0.0019297057 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0375       |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.000736    |
|    value_loss           | 0.0688       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 612      |
|    iterations      | 36       |
|    time_elapsed    | 240      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=150000, episode_reward=0.10 +/- 0.30
Episode length: 282.80 +/- 34.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0018921547 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.482       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0258       |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000598    |
|    value_loss           | 0.0683       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 611      |
|    iterations      | 37       |
|    time_elapsed    | 247      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=155000, episode_reward=0.20 +/- 0.40
Episode length: 284.60 +/- 47.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 155000       |
| train/                  |              |
|    approx_kl            | 0.0020769183 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.489       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.033        |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.000875    |
|    value_loss           | 0.0567       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 611      |
|    iterations      | 38       |
|    time_elapsed    | 254      |
|    total_timesteps | 155648   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 314          |
|    ep_rew_mean          | 0.5          |
| time/                   |              |
|    fps                  | 615          |
|    iterations           | 39           |
|    time_elapsed         | 259          |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 0.0008416314 |
|    clip_fraction        | 0.00432      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.455       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0341       |
|    n_updates            | 380          |
|    policy_gradient_loss | 0.000165     |
|    value_loss           | 0.0714       |
------------------------------------------
Eval num_timesteps=160000, episode_reward=0.80 +/- 0.60
Episode length: 320.20 +/- 49.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 320          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0021180762 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.439       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0475       |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.000737    |
|    value_loss           | 0.0702       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 613      |
|    iterations      | 40       |
|    time_elapsed    | 267      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=165000, episode_reward=0.60 +/- 0.49
Episode length: 305.00 +/- 55.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 165000       |
| train/                  |              |
|    approx_kl            | 0.0019725736 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.441       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0333       |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 0.0648       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 612      |
|    iterations      | 41       |
|    time_elapsed    | 273      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=170000, episode_reward=0.50 +/- 0.50
Episode length: 283.20 +/- 43.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0019818963 |
|    clip_fraction        | 0.0311       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.466       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0355       |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 0.0712       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 612      |
|    iterations      | 42       |
|    time_elapsed    | 281      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=175000, episode_reward=0.60 +/- 0.49
Episode length: 322.40 +/- 34.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 175000       |
| train/                  |              |
|    approx_kl            | 0.0018750846 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0406       |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 0.0675       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 610      |
|    iterations      | 43       |
|    time_elapsed    | 288      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=180000, episode_reward=0.50 +/- 0.67
Episode length: 303.60 +/- 57.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 304          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 180000       |
| train/                  |              |
|    approx_kl            | 0.0024408507 |
|    clip_fraction        | 0.0374       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.399       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0102       |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 0.0638       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 609      |
|    iterations      | 44       |
|    time_elapsed    | 295      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 304         |
|    ep_rew_mean          | 0.43        |
| time/                   |             |
|    fps                  | 613         |
|    iterations           | 45          |
|    time_elapsed         | 300         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.001959277 |
|    clip_fraction        | 0.0239      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.366      |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0355      |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.000724   |
|    value_loss           | 0.0663      |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=0.70 +/- 1.19
Episode length: 317.80 +/- 71.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 318          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 185000       |
| train/                  |              |
|    approx_kl            | 0.0016211534 |
|    clip_fraction        | 0.00688      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.324       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0415       |
|    n_updates            | 450          |
|    policy_gradient_loss | -8.95e-05    |
|    value_loss           | 0.0657       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 612      |
|    iterations      | 46       |
|    time_elapsed    | 307      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=190000, episode_reward=0.90 +/- 0.70
Episode length: 331.00 +/- 59.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 331          |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0007838481 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.352       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0125       |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.000751    |
|    value_loss           | 0.0602       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 610      |
|    iterations      | 47       |
|    time_elapsed    | 315      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=195000, episode_reward=0.40 +/- 0.66
Episode length: 309.60 +/- 52.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0012616927 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.352       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0273       |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.000564    |
|    value_loss           | 0.0635       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 610      |
|    iterations      | 48       |
|    time_elapsed    | 322      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=200000, episode_reward=0.50 +/- 0.67
Episode length: 284.80 +/- 46.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 285          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0014287215 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.318       |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0141       |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00129     |
|    value_loss           | 0.0644       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 609      |
|    iterations      | 49       |
|    time_elapsed    | 329      |
|    total_timesteps | 200704   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 308          |
|    ep_rew_mean          | 0.42         |
| time/                   |              |
|    fps                  | 612          |
|    iterations           | 50           |
|    time_elapsed         | 334          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0012710514 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.302       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0164       |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 0.0667       |
------------------------------------------
Eval num_timesteps=205000, episode_reward=0.40 +/- 0.49
Episode length: 292.80 +/- 40.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0012700321 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.265       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0397       |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000492    |
|    value_loss           | 0.0597       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 611      |
|    iterations      | 51       |
|    time_elapsed    | 341      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=210000, episode_reward=0.40 +/- 0.49
Episode length: 287.40 +/- 48.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0007482998 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.248       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0662       |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.000364    |
|    value_loss           | 0.0753       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 611      |
|    iterations      | 52       |
|    time_elapsed    | 348      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=215000, episode_reward=0.30 +/- 0.46
Episode length: 301.20 +/- 54.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 301          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 215000       |
| train/                  |              |
|    approx_kl            | 0.0008550687 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.243       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0212       |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.000815    |
|    value_loss           | 0.0662       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 610      |
|    iterations      | 53       |
|    time_elapsed    | 355      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=220000, episode_reward=0.70 +/- 0.64
Episode length: 307.20 +/- 56.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 220000       |
| train/                  |              |
|    approx_kl            | 0.0009028527 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.228       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0483       |
|    n_updates            | 530          |
|    policy_gradient_loss | 7.04e-05     |
|    value_loss           | 0.0662       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 54       |
|    time_elapsed    | 362      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=225000, episode_reward=0.20 +/- 0.40
Episode length: 290.20 +/- 46.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 290          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 225000       |
| train/                  |              |
|    approx_kl            | 0.0009527713 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.211       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0246       |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.000365    |
|    value_loss           | 0.0579       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 55       |
|    time_elapsed    | 369      |
|    total_timesteps | 225280   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 298           |
|    ep_rew_mean          | 0.4           |
| time/                   |               |
|    fps                  | 611           |
|    iterations           | 56            |
|    time_elapsed         | 374           |
|    total_timesteps      | 229376        |
| train/                  |               |
|    approx_kl            | 0.00044525845 |
|    clip_fraction        | 0.00937       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.222        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0177        |
|    n_updates            | 550           |
|    policy_gradient_loss | -0.000182     |
|    value_loss           | 0.0572        |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=0.50 +/- 0.67
Episode length: 267.20 +/- 60.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 267           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 230000        |
| train/                  |               |
|    approx_kl            | 0.00093862036 |
|    clip_fraction        | 0.0197        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.198        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0363        |
|    n_updates            | 560           |
|    policy_gradient_loss | -0.000676     |
|    value_loss           | 0.0662        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 611      |
|    iterations      | 57       |
|    time_elapsed    | 381      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=235000, episode_reward=0.50 +/- 0.67
Episode length: 308.20 +/- 69.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 235000       |
| train/                  |              |
|    approx_kl            | 0.0006017578 |
|    clip_fraction        | 0.00894      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.185       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0326       |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.000241    |
|    value_loss           | 0.0663       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 610      |
|    iterations      | 58       |
|    time_elapsed    | 389      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=240000, episode_reward=0.60 +/- 0.66
Episode length: 314.00 +/- 63.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 314          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0007298143 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0248       |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.000781    |
|    value_loss           | 0.0653       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 59       |
|    time_elapsed    | 396      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=245000, episode_reward=0.50 +/- 0.67
Episode length: 314.60 +/- 67.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 315           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 245000        |
| train/                  |               |
|    approx_kl            | 0.00043052842 |
|    clip_fraction        | 0.00679       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.168        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0356        |
|    n_updates            | 590           |
|    policy_gradient_loss | -0.000151     |
|    value_loss           | 0.0648        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 60       |
|    time_elapsed    | 403      |
|    total_timesteps | 245760   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 303         |
|    ep_rew_mean          | 0.37        |
| time/                   |             |
|    fps                  | 611         |
|    iterations           | 61          |
|    time_elapsed         | 408         |
|    total_timesteps      | 249856      |
| train/                  |             |
|    approx_kl            | 0.000715596 |
|    clip_fraction        | 0.0149      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.037       |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.000666   |
|    value_loss           | 0.0563      |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=0.40 +/- 0.66
Episode length: 297.40 +/- 45.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0007351152 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.157       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0282       |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00109     |
|    value_loss           | 0.0566       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 610      |
|    iterations      | 62       |
|    time_elapsed    | 416      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=255000, episode_reward=0.70 +/- 0.78
Episode length: 322.60 +/- 72.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 323        |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 255000     |
| train/                  |            |
|    approx_kl            | 0.00047403 |
|    clip_fraction        | 0.00957    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0          |
|    learning_rate        | 0.01       |
|    loss                 | 0.0241     |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.000362  |
|    value_loss           | 0.0652     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 63       |
|    time_elapsed    | 423      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=260000, episode_reward=0.40 +/- 0.66
Episode length: 294.40 +/- 49.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 294          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 260000       |
| train/                  |              |
|    approx_kl            | 0.0004997655 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0287       |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000161    |
|    value_loss           | 0.0605       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 64       |
|    time_elapsed    | 430      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=265000, episode_reward=0.50 +/- 0.50
Episode length: 329.40 +/- 43.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 329          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 265000       |
| train/                  |              |
|    approx_kl            | 0.0006117387 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.153       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0383       |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.000525    |
|    value_loss           | 0.0594       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 65       |
|    time_elapsed    | 437      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=270000, episode_reward=0.70 +/- 0.64
Episode length: 323.60 +/- 50.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 324          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 270000       |
| train/                  |              |
|    approx_kl            | 0.0002113326 |
|    clip_fraction        | 0.00496      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 0.01         |
|    loss                 | 0.024        |
|    n_updates            | 650          |
|    policy_gradient_loss | -4.93e-05    |
|    value_loss           | 0.0675       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 66       |
|    time_elapsed    | 444      |
|    total_timesteps | 270336   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 291         |
|    ep_rew_mean          | 0.36        |
| time/                   |             |
|    fps                  | 609         |
|    iterations           | 67          |
|    time_elapsed         | 449         |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.000521663 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0302      |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.000923   |
|    value_loss           | 0.0603      |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=0.40 +/- 0.49
Episode length: 291.00 +/- 40.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 275000        |
| train/                  |               |
|    approx_kl            | 0.00022678908 |
|    clip_fraction        | 0.00378       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.154        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0538        |
|    n_updates            | 670           |
|    policy_gradient_loss | 0.000204      |
|    value_loss           | 0.0656        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 68       |
|    time_elapsed    | 457      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=280000, episode_reward=0.40 +/- 0.49
Episode length: 331.60 +/- 65.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 332           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 0.00059846736 |
|    clip_fraction        | 0.0108        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.162        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0171        |
|    n_updates            | 680           |
|    policy_gradient_loss | -0.00036      |
|    value_loss           | 0.0518        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 69       |
|    time_elapsed    | 464      |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=285000, episode_reward=0.80 +/- 0.75
Episode length: 290.80 +/- 52.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | 0.00022241555 |
|    clip_fraction        | 0.00237       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.147        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0265        |
|    n_updates            | 690           |
|    policy_gradient_loss | 7.57e-05      |
|    value_loss           | 0.0587        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 70       |
|    time_elapsed    | 471      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=290000, episode_reward=0.80 +/- 0.98
Episode length: 301.80 +/- 50.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 302           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 290000        |
| train/                  |               |
|    approx_kl            | 0.00049885607 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.139        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0537        |
|    n_updates            | 700           |
|    policy_gradient_loss | -0.000264     |
|    value_loss           | 0.0629        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 71       |
|    time_elapsed    | 478      |
|    total_timesteps | 290816   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 292           |
|    ep_rew_mean          | 0.34          |
| time/                   |               |
|    fps                  | 610           |
|    iterations           | 72            |
|    time_elapsed         | 483           |
|    total_timesteps      | 294912        |
| train/                  |               |
|    approx_kl            | 0.00036736362 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.122        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0402        |
|    n_updates            | 710           |
|    policy_gradient_loss | -0.000149     |
|    value_loss           | 0.058         |
-------------------------------------------
Eval num_timesteps=295000, episode_reward=0.40 +/- 0.66
Episode length: 293.40 +/- 66.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 293          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0005962703 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.11        |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0273       |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.000764    |
|    value_loss           | 0.0593       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 73       |
|    time_elapsed    | 490      |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=300000, episode_reward=0.30 +/- 0.64
Episode length: 301.80 +/- 36.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 302           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 300000        |
| train/                  |               |
|    approx_kl            | 0.00043447368 |
|    clip_fraction        | 0.00786       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.106        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0184        |
|    n_updates            | 730           |
|    policy_gradient_loss | -0.000266     |
|    value_loss           | 0.0609        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 74       |
|    time_elapsed    | 497      |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=305000, episode_reward=0.70 +/- 0.64
Episode length: 310.40 +/- 54.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 305000       |
| train/                  |              |
|    approx_kl            | 0.0003288434 |
|    clip_fraction        | 0.00667      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0364       |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.000384    |
|    value_loss           | 0.0674       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 75       |
|    time_elapsed    | 504      |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=310000, episode_reward=0.30 +/- 0.46
Episode length: 301.60 +/- 50.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 302           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 310000        |
| train/                  |               |
|    approx_kl            | 0.00048386896 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.106        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0255        |
|    n_updates            | 750           |
|    policy_gradient_loss | -0.000596     |
|    value_loss           | 0.0573        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 76       |
|    time_elapsed    | 511      |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=315000, episode_reward=0.40 +/- 0.66
Episode length: 305.40 +/- 66.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 315000        |
| train/                  |               |
|    approx_kl            | 0.00014024774 |
|    clip_fraction        | 0.002         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0975       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0307        |
|    n_updates            | 760           |
|    policy_gradient_loss | 5.86e-05      |
|    value_loss           | 0.0681        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 77       |
|    time_elapsed    | 519      |
|    total_timesteps | 315392   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 303           |
|    ep_rew_mean          | 0.49          |
| time/                   |               |
|    fps                  | 609           |
|    iterations           | 78            |
|    time_elapsed         | 523           |
|    total_timesteps      | 319488        |
| train/                  |               |
|    approx_kl            | 0.00022600773 |
|    clip_fraction        | 0.00425       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0998       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0292        |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.000137     |
|    value_loss           | 0.061         |
-------------------------------------------
Eval num_timesteps=320000, episode_reward=0.60 +/- 0.66
Episode length: 322.60 +/- 73.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 323          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0003451907 |
|    clip_fraction        | 0.00457      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.105       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0493       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.000324    |
|    value_loss           | 0.0678       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 79       |
|    time_elapsed    | 531      |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=325000, episode_reward=0.50 +/- 0.81
Episode length: 288.20 +/- 29.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 288           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 325000        |
| train/                  |               |
|    approx_kl            | 0.00038605492 |
|    clip_fraction        | 0.00601       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.107        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0211        |
|    n_updates            | 790           |
|    policy_gradient_loss | -0.000139     |
|    value_loss           | 0.0628        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 80       |
|    time_elapsed    | 538      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=330000, episode_reward=0.20 +/- 0.40
Episode length: 305.20 +/- 48.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 330000        |
| train/                  |               |
|    approx_kl            | 0.00020125287 |
|    clip_fraction        | 0.00225       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.102        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.037         |
|    n_updates            | 800           |
|    policy_gradient_loss | -7.47e-05     |
|    value_loss           | 0.0597        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 81       |
|    time_elapsed    | 545      |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=335000, episode_reward=0.50 +/- 0.67
Episode length: 331.40 +/- 53.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 331          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 335000       |
| train/                  |              |
|    approx_kl            | 0.0006039337 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0938      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0603       |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000327    |
|    value_loss           | 0.0662       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 82       |
|    time_elapsed    | 552      |
|    total_timesteps | 335872   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 302           |
|    ep_rew_mean          | 0.51          |
| time/                   |               |
|    fps                  | 609           |
|    iterations           | 83            |
|    time_elapsed         | 557           |
|    total_timesteps      | 339968        |
| train/                  |               |
|    approx_kl            | 0.00037281428 |
|    clip_fraction        | 0.00728       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0731       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0236        |
|    n_updates            | 820           |
|    policy_gradient_loss | -0.000385     |
|    value_loss           | 0.0678        |
-------------------------------------------
Eval num_timesteps=340000, episode_reward=0.40 +/- 0.49
Episode length: 292.20 +/- 41.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 292           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 340000        |
| train/                  |               |
|    approx_kl            | 0.00022044136 |
|    clip_fraction        | 0.00344       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0816       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0418        |
|    n_updates            | 830           |
|    policy_gradient_loss | -8.23e-05     |
|    value_loss           | 0.063         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 609      |
|    iterations      | 84       |
|    time_elapsed    | 564      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=345000, episode_reward=0.30 +/- 0.46
Episode length: 295.40 +/- 57.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 295           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 345000        |
| train/                  |               |
|    approx_kl            | 0.00034292115 |
|    clip_fraction        | 0.00562       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.076        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0294        |
|    n_updates            | 840           |
|    policy_gradient_loss | -0.000305     |
|    value_loss           | 0.0589        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 85       |
|    time_elapsed    | 572      |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=350000, episode_reward=0.10 +/- 0.30
Episode length: 264.20 +/- 41.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 264           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 350000        |
| train/                  |               |
|    approx_kl            | 0.00032703835 |
|    clip_fraction        | 0.00828       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0692       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.00438       |
|    n_updates            | 850           |
|    policy_gradient_loss | -0.000616     |
|    value_loss           | 0.054         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 86       |
|    time_elapsed    | 578      |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=355000, episode_reward=0.20 +/- 0.40
Episode length: 300.80 +/- 63.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 301           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 355000        |
| train/                  |               |
|    approx_kl            | 0.00019868379 |
|    clip_fraction        | 0.00474       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0567       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0563        |
|    n_updates            | 860           |
|    policy_gradient_loss | -0.000233     |
|    value_loss           | 0.0591        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 87       |
|    time_elapsed    | 586      |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=360000, episode_reward=0.40 +/- 0.49
Episode length: 310.20 +/- 48.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 310           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 360000        |
| train/                  |               |
|    approx_kl            | 0.00012329574 |
|    clip_fraction        | 0.0032        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0503       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.025         |
|    n_updates            | 870           |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 0.0644        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 88       |
|    time_elapsed    | 593      |
|    total_timesteps | 360448   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 294           |
|    ep_rew_mean          | 0.33          |
| time/                   |               |
|    fps                  | 609           |
|    iterations           | 89            |
|    time_elapsed         | 598           |
|    total_timesteps      | 364544        |
| train/                  |               |
|    approx_kl            | 0.00014938545 |
|    clip_fraction        | 0.00356       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0402       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0298        |
|    n_updates            | 880           |
|    policy_gradient_loss | -0.000313     |
|    value_loss           | 0.0532        |
-------------------------------------------
Eval num_timesteps=365000, episode_reward=0.40 +/- 0.49
Episode length: 314.40 +/- 32.85
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 314            |
|    mean_reward          | 0.4            |
| time/                   |                |
|    total_timesteps      | 365000         |
| train/                  |                |
|    approx_kl            | 0.000110401175 |
|    clip_fraction        | 0.0021         |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0394        |
|    explained_variance   | 5.96e-08       |
|    learning_rate        | 0.01           |
|    loss                 | 0.0273         |
|    n_updates            | 890            |
|    policy_gradient_loss | -0.000114      |
|    value_loss           | 0.0583         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 90       |
|    time_elapsed    | 605      |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=370000, episode_reward=0.40 +/- 0.49
Episode length: 305.20 +/- 42.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 370000        |
| train/                  |               |
|    approx_kl            | 6.1036786e-05 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0403       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0403        |
|    n_updates            | 900           |
|    policy_gradient_loss | -4.59e-05     |
|    value_loss           | 0.0612        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 91       |
|    time_elapsed    | 612      |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=375000, episode_reward=0.50 +/- 0.81
Episode length: 288.40 +/- 56.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0001658613 |
|    clip_fraction        | 0.00349      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0397      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0264       |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.000152    |
|    value_loss           | 0.0571       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 92       |
|    time_elapsed    | 619      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=380000, episode_reward=0.30 +/- 0.64
Episode length: 304.00 +/- 43.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 380000        |
| train/                  |               |
|    approx_kl            | 0.00012619763 |
|    clip_fraction        | 0.00349       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0308       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0291        |
|    n_updates            | 920           |
|    policy_gradient_loss | -0.000208     |
|    value_loss           | 0.0639        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 93       |
|    time_elapsed    | 626      |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=385000, episode_reward=0.30 +/- 0.46
Episode length: 293.00 +/- 61.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 293           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 385000        |
| train/                  |               |
|    approx_kl            | 0.00013929006 |
|    clip_fraction        | 0.00266       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0254       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0191        |
|    n_updates            | 930           |
|    policy_gradient_loss | -0.000132     |
|    value_loss           | 0.0531        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 94       |
|    time_elapsed    | 633      |
|    total_timesteps | 385024   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 296           |
|    ep_rew_mean          | 0.4           |
| time/                   |               |
|    fps                  | 609           |
|    iterations           | 95            |
|    time_elapsed         | 638           |
|    total_timesteps      | 389120        |
| train/                  |               |
|    approx_kl            | 5.6243225e-05 |
|    clip_fraction        | 0.000269      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0229       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0261        |
|    n_updates            | 940           |
|    policy_gradient_loss | -2.38e-05     |
|    value_loss           | 0.062         |
-------------------------------------------
Eval num_timesteps=390000, episode_reward=0.40 +/- 0.66
Episode length: 319.40 +/- 76.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 319          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 6.738436e-05 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0234      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.034        |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.000207    |
|    value_loss           | 0.0632       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 609      |
|    iterations      | 96       |
|    time_elapsed    | 645      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=395000, episode_reward=0.40 +/- 0.66
Episode length: 312.20 +/- 43.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 395000       |
| train/                  |              |
|    approx_kl            | 8.270495e-05 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0259      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0192       |
|    n_updates            | 960          |
|    policy_gradient_loss | -8.8e-05     |
|    value_loss           | 0.064        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 97       |
|    time_elapsed    | 652      |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=400000, episode_reward=0.70 +/- 0.78
Episode length: 300.20 +/- 57.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 7.538633e-05 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0224      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0149       |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.000317    |
|    value_loss           | 0.0573       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 98       |
|    time_elapsed    | 660      |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=405000, episode_reward=0.40 +/- 0.66
Episode length: 312.40 +/- 48.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 405000       |
| train/                  |              |
|    approx_kl            | 6.908267e-05 |
|    clip_fraction        | 0.00212      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0279      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0169       |
|    n_updates            | 980          |
|    policy_gradient_loss | -5.55e-05    |
|    value_loss           | 0.0556       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 99       |
|    time_elapsed    | 667      |
|    total_timesteps | 405504   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 300           |
|    ep_rew_mean          | 0.46          |
| time/                   |               |
|    fps                  | 609           |
|    iterations           | 100           |
|    time_elapsed         | 672           |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 1.6193066e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0294       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0199        |
|    n_updates            | 990           |
|    policy_gradient_loss | -5.48e-07     |
|    value_loss           | 0.0585        |
-------------------------------------------
Eval num_timesteps=410000, episode_reward=0.50 +/- 0.67
Episode length: 288.80 +/- 60.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 289           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 410000        |
| train/                  |               |
|    approx_kl            | 0.00027668453 |
|    clip_fraction        | 0.00325       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0311       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0345        |
|    n_updates            | 1000          |
|    policy_gradient_loss | -0.000187     |
|    value_loss           | 0.0613        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 608      |
|    iterations      | 101      |
|    time_elapsed    | 679      |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=415000, episode_reward=0.20 +/- 0.60
Episode length: 289.80 +/- 55.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 290           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 415000        |
| train/                  |               |
|    approx_kl            | 0.00014841466 |
|    clip_fraction        | 0.00449       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.021        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0297        |
|    n_updates            | 1010          |
|    policy_gradient_loss | -0.000363     |
|    value_loss           | 0.0614        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 102      |
|    time_elapsed    | 686      |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=420000, episode_reward=0.80 +/- 0.75
Episode length: 325.00 +/- 59.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 6.138273e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.02        |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0212       |
|    n_updates            | 1020         |
|    policy_gradient_loss | -4.87e-05    |
|    value_loss           | 0.0555       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 103      |
|    time_elapsed    | 693      |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=425000, episode_reward=0.60 +/- 0.66
Episode length: 307.00 +/- 66.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 307           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 425000        |
| train/                  |               |
|    approx_kl            | 6.4934575e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0208       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0221        |
|    n_updates            | 1030          |
|    policy_gradient_loss | -7.55e-07     |
|    value_loss           | 0.0549        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 104      |
|    time_elapsed    | 700      |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=430000, episode_reward=0.70 +/- 0.78
Episode length: 311.40 +/- 40.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 311           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 430000        |
| train/                  |               |
|    approx_kl            | 6.9500995e-05 |
|    clip_fraction        | 0.00173       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0288       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0209        |
|    n_updates            | 1040          |
|    policy_gradient_loss | -9.49e-05     |
|    value_loss           | 0.0529        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 105      |
|    time_elapsed    | 708      |
|    total_timesteps | 430080   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 306          |
|    ep_rew_mean          | 0.41         |
| time/                   |              |
|    fps                  | 608          |
|    iterations           | 106          |
|    time_elapsed         | 713          |
|    total_timesteps      | 434176       |
| train/                  |              |
|    approx_kl            | 7.024099e-05 |
|    clip_fraction        | 0.001        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0243      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0338       |
|    n_updates            | 1050         |
|    policy_gradient_loss | 6.24e-05     |
|    value_loss           | 0.0607       |
------------------------------------------
Eval num_timesteps=435000, episode_reward=0.30 +/- 0.64
Episode length: 290.40 +/- 48.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 290          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 8.453583e-05 |
|    clip_fraction        | 0.00139      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0209      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0257       |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.00015     |
|    value_loss           | 0.0568       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 107      |
|    time_elapsed    | 720      |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=440000, episode_reward=0.80 +/- 0.87
Episode length: 298.00 +/- 57.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 298           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 440000        |
| train/                  |               |
|    approx_kl            | 0.00011618306 |
|    clip_fraction        | 0.00139       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0183       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.045         |
|    n_updates            | 1070          |
|    policy_gradient_loss | 2.54e-05      |
|    value_loss           | 0.0594        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 108      |
|    time_elapsed    | 727      |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=445000, episode_reward=0.20 +/- 0.40
Episode length: 301.00 +/- 52.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 301          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 445000       |
| train/                  |              |
|    approx_kl            | 7.348777e-05 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0161      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0168       |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.000165    |
|    value_loss           | 0.0663       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 608      |
|    iterations      | 109      |
|    time_elapsed    | 734      |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=450000, episode_reward=0.10 +/- 0.30
Episode length: 277.40 +/- 51.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 277         |
|    mean_reward          | 0.1         |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 9.16656e-05 |
|    clip_fraction        | 0.00178     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0129     |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0572      |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.000252   |
|    value_loss           | 0.0606      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 110      |
|    time_elapsed    | 741      |
|    total_timesteps | 450560   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 300          |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 609          |
|    iterations           | 111          |
|    time_elapsed         | 746          |
|    total_timesteps      | 454656       |
| train/                  |              |
|    approx_kl            | 7.505296e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0128      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0217       |
|    n_updates            | 1100         |
|    policy_gradient_loss | -2.11e-07    |
|    value_loss           | 0.0615       |
------------------------------------------
Eval num_timesteps=455000, episode_reward=0.30 +/- 0.46
Episode length: 292.20 +/- 55.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 292          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 9.485608e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0129      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0195       |
|    n_updates            | 1110         |
|    policy_gradient_loss | -4.74e-06    |
|    value_loss           | 0.0534       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 112      |
|    time_elapsed    | 753      |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=460000, episode_reward=0.30 +/- 0.46
Episode length: 308.80 +/- 45.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 309           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 460000        |
| train/                  |               |
|    approx_kl            | 3.0928975e-05 |
|    clip_fraction        | 0.00061       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0165       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0134        |
|    n_updates            | 1120          |
|    policy_gradient_loss | -6.78e-05     |
|    value_loss           | 0.0558        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 113      |
|    time_elapsed    | 760      |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=465000, episode_reward=1.10 +/- 0.94
Episode length: 321.80 +/- 68.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | 1.1          |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0002078163 |
|    clip_fraction        | 0.00217      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0116      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0271       |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.000239    |
|    value_loss           | 0.0637       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 114      |
|    time_elapsed    | 767      |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=470000, episode_reward=0.40 +/- 0.49
Episode length: 332.40 +/- 47.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 332           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 470000        |
| train/                  |               |
|    approx_kl            | 5.3647207e-05 |
|    clip_fraction        | 0.00105       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0143       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0167        |
|    n_updates            | 1140          |
|    policy_gradient_loss | -0.000152     |
|    value_loss           | 0.0539        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 115      |
|    time_elapsed    | 774      |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=475000, episode_reward=0.40 +/- 0.66
Episode length: 295.20 +/- 64.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 475000       |
| train/                  |              |
|    approx_kl            | 6.736495e-05 |
|    clip_fraction        | 0.0012       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0321       |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.000104    |
|    value_loss           | 0.0614       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 116      |
|    time_elapsed    | 782      |
|    total_timesteps | 475136   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 305           |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 608           |
|    iterations           | 117           |
|    time_elapsed         | 786           |
|    total_timesteps      | 479232        |
| train/                  |               |
|    approx_kl            | 0.00023069327 |
|    clip_fraction        | 0.00168       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0171       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0323        |
|    n_updates            | 1160          |
|    policy_gradient_loss | -0.000117     |
|    value_loss           | 0.0627        |
-------------------------------------------
Eval num_timesteps=480000, episode_reward=0.60 +/- 0.80
Episode length: 311.00 +/- 52.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 311           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 480000        |
| train/                  |               |
|    approx_kl            | 2.8647948e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0212        |
|    n_updates            | 1170          |
|    policy_gradient_loss | 3.11e-05      |
|    value_loss           | 0.0615        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 118      |
|    time_elapsed    | 794      |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=485000, episode_reward=0.70 +/- 0.64
Episode length: 317.80 +/- 43.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 485000        |
| train/                  |               |
|    approx_kl            | 4.6493587e-05 |
|    clip_fraction        | 0.00107       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0145       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.023         |
|    n_updates            | 1180          |
|    policy_gradient_loss | -3.97e-05     |
|    value_loss           | 0.0618        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 119      |
|    time_elapsed    | 801      |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=490000, episode_reward=0.20 +/- 0.40
Episode length: 278.00 +/- 42.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 278          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 9.314147e-05 |
|    clip_fraction        | 0.00125      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0183      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0337       |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.000103    |
|    value_loss           | 0.0633       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 608      |
|    iterations      | 120      |
|    time_elapsed    | 808      |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=495000, episode_reward=0.80 +/- 0.75
Episode length: 330.40 +/- 48.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 330          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 495000       |
| train/                  |              |
|    approx_kl            | 9.812138e-06 |
|    clip_fraction        | 0.000562     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0158      |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.0312       |
|    n_updates            | 1200         |
|    policy_gradient_loss | 1.65e-05     |
|    value_loss           | 0.0603       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 121      |
|    time_elapsed    | 815      |
|    total_timesteps | 495616   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 305          |
|    ep_rew_mean          | 0.53         |
| time/                   |              |
|    fps                  | 608          |
|    iterations           | 122          |
|    time_elapsed         | 820          |
|    total_timesteps      | 499712       |
| train/                  |              |
|    approx_kl            | 4.644315e-05 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0134      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0351       |
|    n_updates            | 1210         |
|    policy_gradient_loss | -3.66e-05    |
|    value_loss           | 0.0673       |
------------------------------------------
Eval num_timesteps=500000, episode_reward=0.50 +/- 0.50
Episode length: 296.00 +/- 40.60
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 296            |
|    mean_reward          | 0.5            |
| time/                   |                |
|    total_timesteps      | 500000         |
| train/                  |                |
|    approx_kl            | 0.000115241884 |
|    clip_fraction        | 0.00132        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0126        |
|    explained_variance   | 0              |
|    learning_rate        | 0.01           |
|    loss                 | 0.0219         |
|    n_updates            | 1220           |
|    policy_gradient_loss | -0.000216      |
|    value_loss           | 0.0585         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 608      |
|    iterations      | 123      |
|    time_elapsed    | 827      |
|    total_timesteps | 503808   |
---------------------------------
Eval num_timesteps=505000, episode_reward=0.40 +/- 0.92
Episode length: 307.00 +/- 41.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 505000       |
| train/                  |              |
|    approx_kl            | 3.551398e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0329       |
|    n_updates            | 1230         |
|    policy_gradient_loss | -3.8e-06     |
|    value_loss           | 0.058        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 124      |
|    time_elapsed    | 835      |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=510000, episode_reward=0.30 +/- 0.46
Episode length: 286.20 +/- 39.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 286           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 510000        |
| train/                  |               |
|    approx_kl            | 0.00019345141 |
|    clip_fraction        | 0.00217       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0194       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.026         |
|    n_updates            | 1240          |
|    policy_gradient_loss | -0.000116     |
|    value_loss           | 0.0609        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 125      |
|    time_elapsed    | 842      |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=515000, episode_reward=0.50 +/- 1.02
Episode length: 302.00 +/- 59.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 302          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 515000       |
| train/                  |              |
|    approx_kl            | 6.079038e-05 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0267      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0185       |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.000198    |
|    value_loss           | 0.0567       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 126      |
|    time_elapsed    | 848      |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=520000, episode_reward=0.30 +/- 0.46
Episode length: 296.00 +/- 36.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 296           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 520000        |
| train/                  |               |
|    approx_kl            | 6.8561814e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.023        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0341        |
|    n_updates            | 1260          |
|    policy_gradient_loss | -0.000104     |
|    value_loss           | 0.0556        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 127      |
|    time_elapsed    | 856      |
|    total_timesteps | 520192   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 301           |
|    ep_rew_mean          | 0.4           |
| time/                   |               |
|    fps                  | 608           |
|    iterations           | 128           |
|    time_elapsed         | 861           |
|    total_timesteps      | 524288        |
| train/                  |               |
|    approx_kl            | 0.00019343501 |
|    clip_fraction        | 0.00288       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0286       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0283        |
|    n_updates            | 1270          |
|    policy_gradient_loss | -0.000151     |
|    value_loss           | 0.0571        |
-------------------------------------------
Eval num_timesteps=525000, episode_reward=0.20 +/- 0.40
Episode length: 276.20 +/- 37.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 525000       |
| train/                  |              |
|    approx_kl            | 0.0001633551 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0265      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0166       |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.000126    |
|    value_loss           | 0.0638       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 129      |
|    time_elapsed    | 868      |
|    total_timesteps | 528384   |
---------------------------------
Eval num_timesteps=530000, episode_reward=0.90 +/- 0.94
Episode length: 310.80 +/- 64.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 311           |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 530000        |
| train/                  |               |
|    approx_kl            | 0.00014179113 |
|    clip_fraction        | 0.00278       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.027        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0407        |
|    n_updates            | 1290          |
|    policy_gradient_loss | -0.000251     |
|    value_loss           | 0.056         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 130      |
|    time_elapsed    | 875      |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=535000, episode_reward=0.70 +/- 0.78
Episode length: 318.80 +/- 57.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 319           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 535000        |
| train/                  |               |
|    approx_kl            | 5.8542937e-05 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0229       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0388        |
|    n_updates            | 1300          |
|    policy_gradient_loss | -0.000163     |
|    value_loss           | 0.0511        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 131      |
|    time_elapsed    | 882      |
|    total_timesteps | 536576   |
---------------------------------
Eval num_timesteps=540000, episode_reward=0.90 +/- 0.54
Episode length: 291.60 +/- 51.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 292           |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 540000        |
| train/                  |               |
|    approx_kl            | 0.00012270581 |
|    clip_fraction        | 0.00171       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0267       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0209        |
|    n_updates            | 1310          |
|    policy_gradient_loss | -0.000225     |
|    value_loss           | 0.0586        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 132      |
|    time_elapsed    | 889      |
|    total_timesteps | 540672   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 307          |
|    ep_rew_mean          | 0.44         |
| time/                   |              |
|    fps                  | 608          |
|    iterations           | 133          |
|    time_elapsed         | 894          |
|    total_timesteps      | 544768       |
| train/                  |              |
|    approx_kl            | 0.0002182941 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0245      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0415       |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.000288    |
|    value_loss           | 0.0624       |
------------------------------------------
Eval num_timesteps=545000, episode_reward=0.50 +/- 0.67
Episode length: 311.60 +/- 56.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 9.774385e-05 |
|    clip_fraction        | 0.00168      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0312      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0404       |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.000239    |
|    value_loss           | 0.0652       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 134      |
|    time_elapsed    | 902      |
|    total_timesteps | 548864   |
---------------------------------
Eval num_timesteps=550000, episode_reward=0.40 +/- 0.49
Episode length: 321.80 +/- 41.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 550000       |
| train/                  |              |
|    approx_kl            | 9.001435e-05 |
|    clip_fraction        | 0.00149      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0333      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0412       |
|    n_updates            | 1340         |
|    policy_gradient_loss | -3.86e-05    |
|    value_loss           | 0.0655       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 135      |
|    time_elapsed    | 909      |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=555000, episode_reward=0.50 +/- 0.67
Episode length: 325.00 +/- 54.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 325           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 555000        |
| train/                  |               |
|    approx_kl            | 9.2087765e-05 |
|    clip_fraction        | 0.00247       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0375       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0364        |
|    n_updates            | 1350          |
|    policy_gradient_loss | -0.000109     |
|    value_loss           | 0.0606        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 136      |
|    time_elapsed    | 916      |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=560000, episode_reward=0.40 +/- 0.66
Episode length: 301.40 +/- 72.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 301           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 560000        |
| train/                  |               |
|    approx_kl            | 0.00019960606 |
|    clip_fraction        | 0.00293       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0412       |
|    explained_variance   | 2.38e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0263        |
|    n_updates            | 1360          |
|    policy_gradient_loss | -0.000266     |
|    value_loss           | 0.0649        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 137      |
|    time_elapsed    | 923      |
|    total_timesteps | 561152   |
---------------------------------
Eval num_timesteps=565000, episode_reward=0.70 +/- 0.78
Episode length: 325.00 +/- 55.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 325          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 565000       |
| train/                  |              |
|    approx_kl            | 6.910569e-05 |
|    clip_fraction        | 0.00168      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0428      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0332       |
|    n_updates            | 1370         |
|    policy_gradient_loss | -2.25e-05    |
|    value_loss           | 0.0587       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 138      |
|    time_elapsed    | 930      |
|    total_timesteps | 565248   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 311          |
|    ep_rew_mean          | 0.57         |
| time/                   |              |
|    fps                  | 608          |
|    iterations           | 139          |
|    time_elapsed         | 935          |
|    total_timesteps      | 569344       |
| train/                  |              |
|    approx_kl            | 0.0003744371 |
|    clip_fraction        | 0.00559      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.035       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0247       |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.000243    |
|    value_loss           | 0.0574       |
------------------------------------------
Eval num_timesteps=570000, episode_reward=0.80 +/- 0.60
Episode length: 318.80 +/- 41.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 319           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 570000        |
| train/                  |               |
|    approx_kl            | 0.00014724507 |
|    clip_fraction        | 0.00413       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.042        |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0365        |
|    n_updates            | 1390          |
|    policy_gradient_loss | -0.000171     |
|    value_loss           | 0.0589        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 140      |
|    time_elapsed    | 943      |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=575000, episode_reward=0.70 +/- 0.78
Episode length: 321.80 +/- 44.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 322          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 575000       |
| train/                  |              |
|    approx_kl            | 8.793641e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0376      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0195       |
|    n_updates            | 1400         |
|    policy_gradient_loss | -1.08e-05    |
|    value_loss           | 0.0625       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 141      |
|    time_elapsed    | 950      |
|    total_timesteps | 577536   |
---------------------------------
Eval num_timesteps=580000, episode_reward=0.40 +/- 0.49
Episode length: 290.00 +/- 28.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 290           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 580000        |
| train/                  |               |
|    approx_kl            | 0.00013300765 |
|    clip_fraction        | 0.00261       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0293       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.013         |
|    n_updates            | 1410          |
|    policy_gradient_loss | -4.94e-05     |
|    value_loss           | 0.0601        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 142      |
|    time_elapsed    | 957      |
|    total_timesteps | 581632   |
---------------------------------
Eval num_timesteps=585000, episode_reward=0.10 +/- 0.30
Episode length: 289.00 +/- 67.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 289           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 585000        |
| train/                  |               |
|    approx_kl            | 0.00015056603 |
|    clip_fraction        | 0.00322       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0221       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0147        |
|    n_updates            | 1420          |
|    policy_gradient_loss | -0.000332     |
|    value_loss           | 0.06          |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 143      |
|    time_elapsed    | 964      |
|    total_timesteps | 585728   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 310           |
|    ep_rew_mean          | 0.45          |
| time/                   |               |
|    fps                  | 608           |
|    iterations           | 144           |
|    time_elapsed         | 969           |
|    total_timesteps      | 589824        |
| train/                  |               |
|    approx_kl            | 3.7593418e-05 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0256       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0194        |
|    n_updates            | 1430          |
|    policy_gradient_loss | -1.66e-05     |
|    value_loss           | 0.0571        |
-------------------------------------------
Eval num_timesteps=590000, episode_reward=0.80 +/- 0.60
Episode length: 323.00 +/- 70.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 590000        |
| train/                  |               |
|    approx_kl            | 0.00019743977 |
|    clip_fraction        | 0.00247       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0213       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0313        |
|    n_updates            | 1440          |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 0.0585        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 145      |
|    time_elapsed    | 976      |
|    total_timesteps | 593920   |
---------------------------------
Eval num_timesteps=595000, episode_reward=0.20 +/- 0.40
Episode length: 308.00 +/- 32.24
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 308            |
|    mean_reward          | 0.2            |
| time/                   |                |
|    total_timesteps      | 595000         |
| train/                  |                |
|    approx_kl            | 0.000105647414 |
|    clip_fraction        | 0.00242        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0182        |
|    explained_variance   | 0              |
|    learning_rate        | 0.01           |
|    loss                 | 0.0159         |
|    n_updates            | 1450           |
|    policy_gradient_loss | -7.47e-05      |
|    value_loss           | 0.0562         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 146      |
|    time_elapsed    | 984      |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=600000, episode_reward=0.40 +/- 0.66
Episode length: 281.20 +/- 46.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 7.68594e-05 |
|    clip_fraction        | 0.00137     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0182     |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.045       |
|    n_updates            | 1460        |
|    policy_gradient_loss | -4.2e-05    |
|    value_loss           | 0.0537      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 147      |
|    time_elapsed    | 990      |
|    total_timesteps | 602112   |
---------------------------------
Eval num_timesteps=605000, episode_reward=0.50 +/- 0.67
Episode length: 297.60 +/- 48.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 298           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 605000        |
| train/                  |               |
|    approx_kl            | 0.00015751403 |
|    clip_fraction        | 0.00183       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0223       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.035         |
|    n_updates            | 1470          |
|    policy_gradient_loss | -0.000146     |
|    value_loss           | 0.0616        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 148      |
|    time_elapsed    | 998      |
|    total_timesteps | 606208   |
---------------------------------
Eval num_timesteps=610000, episode_reward=0.20 +/- 0.40
Episode length: 306.80 +/- 38.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 307           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 610000        |
| train/                  |               |
|    approx_kl            | 6.4856344e-05 |
|    clip_fraction        | 0.00198       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0168       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0124        |
|    n_updates            | 1480          |
|    policy_gradient_loss | -0.000204     |
|    value_loss           | 0.0543        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.32     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 149      |
|    time_elapsed    | 1005     |
|    total_timesteps | 610304   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 296           |
|    ep_rew_mean          | 0.35          |
| time/                   |               |
|    fps                  | 608           |
|    iterations           | 150           |
|    time_elapsed         | 1010          |
|    total_timesteps      | 614400        |
| train/                  |               |
|    approx_kl            | 0.00011552099 |
|    clip_fraction        | 0.00149       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0141       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0336        |
|    n_updates            | 1490          |
|    policy_gradient_loss | -7.91e-05     |
|    value_loss           | 0.0568        |
-------------------------------------------
Eval num_timesteps=615000, episode_reward=0.50 +/- 0.50
Episode length: 333.00 +/- 48.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 333           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 615000        |
| train/                  |               |
|    approx_kl            | 3.2404816e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0128       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0458        |
|    n_updates            | 1500          |
|    policy_gradient_loss | -6.15e-05     |
|    value_loss           | 0.0654        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 151      |
|    time_elapsed    | 1017     |
|    total_timesteps | 618496   |
---------------------------------
Eval num_timesteps=620000, episode_reward=0.50 +/- 0.67
Episode length: 304.20 +/- 56.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 620000        |
| train/                  |               |
|    approx_kl            | 3.4847617e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0119       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0297        |
|    n_updates            | 1510          |
|    policy_gradient_loss | -4.37e-05     |
|    value_loss           | 0.0591        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 152      |
|    time_elapsed    | 1024     |
|    total_timesteps | 622592   |
---------------------------------
Eval num_timesteps=625000, episode_reward=0.70 +/- 0.90
Episode length: 310.80 +/- 65.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 311          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 625000       |
| train/                  |              |
|    approx_kl            | 3.107534e-05 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0115      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0323       |
|    n_updates            | 1520         |
|    policy_gradient_loss | -4.15e-05    |
|    value_loss           | 0.0579       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 153      |
|    time_elapsed    | 1031     |
|    total_timesteps | 626688   |
---------------------------------
Eval num_timesteps=630000, episode_reward=0.30 +/- 0.46
Episode length: 292.00 +/- 53.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 292          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 630000       |
| train/                  |              |
|    approx_kl            | 8.056461e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0123      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0115       |
|    n_updates            | 1530         |
|    policy_gradient_loss | 2.31e-06     |
|    value_loss           | 0.0601       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 154      |
|    time_elapsed    | 1038     |
|    total_timesteps | 630784   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 297           |
|    ep_rew_mean          | 0.37          |
| time/                   |               |
|    fps                  | 608           |
|    iterations           | 155           |
|    time_elapsed         | 1044          |
|    total_timesteps      | 634880        |
| train/                  |               |
|    approx_kl            | 6.1033672e-05 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0401        |
|    n_updates            | 1540          |
|    policy_gradient_loss | -6.53e-05     |
|    value_loss           | 0.058         |
-------------------------------------------
Eval num_timesteps=635000, episode_reward=0.50 +/- 0.67
Episode length: 293.20 +/- 44.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 293           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 635000        |
| train/                  |               |
|    approx_kl            | 9.3571085e-05 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.04          |
|    n_updates            | 1550          |
|    policy_gradient_loss | -1.36e-05     |
|    value_loss           | 0.0584        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 156      |
|    time_elapsed    | 1051     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=640000, episode_reward=0.60 +/- 0.66
Episode length: 311.00 +/- 78.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 311           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 640000        |
| train/                  |               |
|    approx_kl            | 1.3171622e-05 |
|    clip_fraction        | 0.000464      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0366        |
|    n_updates            | 1560          |
|    policy_gradient_loss | -0.000117     |
|    value_loss           | 0.066         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 157      |
|    time_elapsed    | 1058     |
|    total_timesteps | 643072   |
---------------------------------
Eval num_timesteps=645000, episode_reward=0.20 +/- 0.40
Episode length: 295.40 +/- 61.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 295           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 645000        |
| train/                  |               |
|    approx_kl            | 1.2500779e-05 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0206        |
|    n_updates            | 1570          |
|    policy_gradient_loss | -4.51e-06     |
|    value_loss           | 0.0563        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | 0.33     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 158      |
|    time_elapsed    | 1065     |
|    total_timesteps | 647168   |
---------------------------------
Eval num_timesteps=650000, episode_reward=0.20 +/- 0.40
Episode length: 267.20 +/- 44.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 267           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 650000        |
| train/                  |               |
|    approx_kl            | 3.6310754e-05 |
|    clip_fraction        | 0.000903      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.011        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0191        |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.000149     |
|    value_loss           | 0.0589        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 159      |
|    time_elapsed    | 1072     |
|    total_timesteps | 651264   |
---------------------------------
Eval num_timesteps=655000, episode_reward=0.50 +/- 0.67
Episode length: 314.20 +/- 48.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 314          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 655000       |
| train/                  |              |
|    approx_kl            | 1.377397e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0509       |
|    n_updates            | 1590         |
|    policy_gradient_loss | -5.1e-06     |
|    value_loss           | 0.0665       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 160      |
|    time_elapsed    | 1079     |
|    total_timesteps | 655360   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 298           |
|    ep_rew_mean          | 0.5           |
| time/                   |               |
|    fps                  | 608           |
|    iterations           | 161           |
|    time_elapsed         | 1084          |
|    total_timesteps      | 659456        |
| train/                  |               |
|    approx_kl            | 1.8391293e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00985      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0181        |
|    n_updates            | 1600          |
|    policy_gradient_loss | 2e-07         |
|    value_loss           | 0.062         |
-------------------------------------------
Eval num_timesteps=660000, episode_reward=0.40 +/- 0.66
Episode length: 298.80 +/- 40.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 299          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 660000       |
| train/                  |              |
|    approx_kl            | 6.569589e-05 |
|    clip_fraction        | 0.00105      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00988     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0235       |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.000105    |
|    value_loss           | 0.0712       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 162      |
|    time_elapsed    | 1091     |
|    total_timesteps | 663552   |
---------------------------------
Eval num_timesteps=665000, episode_reward=0.60 +/- 0.49
Episode length: 322.60 +/- 42.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 665000        |
| train/                  |               |
|    approx_kl            | 1.1863594e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0089       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0318        |
|    n_updates            | 1620          |
|    policy_gradient_loss | 1.54e-06      |
|    value_loss           | 0.0615        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 163      |
|    time_elapsed    | 1098     |
|    total_timesteps | 667648   |
---------------------------------
Eval num_timesteps=670000, episode_reward=0.30 +/- 0.46
Episode length: 318.60 +/- 63.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 319           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 670000        |
| train/                  |               |
|    approx_kl            | 7.2260445e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00942      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0404        |
|    n_updates            | 1630          |
|    policy_gradient_loss | 1.01e-06      |
|    value_loss           | 0.0612        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 164      |
|    time_elapsed    | 1105     |
|    total_timesteps | 671744   |
---------------------------------
Eval num_timesteps=675000, episode_reward=0.50 +/- 0.67
Episode length: 305.60 +/- 56.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 675000        |
| train/                  |               |
|    approx_kl            | 2.9954332e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00937      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0211        |
|    n_updates            | 1640          |
|    policy_gradient_loss | -1.51e-05     |
|    value_loss           | 0.0593        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.61     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 165      |
|    time_elapsed    | 1112     |
|    total_timesteps | 675840   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 304           |
|    ep_rew_mean          | 0.57          |
| time/                   |               |
|    fps                  | 608           |
|    iterations           | 166           |
|    time_elapsed         | 1118          |
|    total_timesteps      | 679936        |
| train/                  |               |
|    approx_kl            | 2.3280358e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0069       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.017         |
|    n_updates            | 1650          |
|    policy_gradient_loss | -5.53e-05     |
|    value_loss           | 0.0657        |
-------------------------------------------
Eval num_timesteps=680000, episode_reward=0.80 +/- 0.60
Episode length: 309.00 +/- 51.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 309           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 680000        |
| train/                  |               |
|    approx_kl            | 4.4173707e-05 |
|    clip_fraction        | 0.00061       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00531      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.035         |
|    n_updates            | 1660          |
|    policy_gradient_loss | -6.45e-05     |
|    value_loss           | 0.0648        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 167      |
|    time_elapsed    | 1125     |
|    total_timesteps | 684032   |
---------------------------------
Eval num_timesteps=685000, episode_reward=0.20 +/- 0.40
Episode length: 322.40 +/- 50.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 322       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 685000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00569  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0171    |
|    n_updates            | 1670      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 0.0658    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 168      |
|    time_elapsed    | 1132     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=690000, episode_reward=0.60 +/- 0.80
Episode length: 279.80 +/- 56.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 280           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 690000        |
| train/                  |               |
|    approx_kl            | 4.3228836e-05 |
|    clip_fraction        | 0.000415      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00408      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0336        |
|    n_updates            | 1680          |
|    policy_gradient_loss | -0.000152     |
|    value_loss           | 0.0568        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 169      |
|    time_elapsed    | 1139     |
|    total_timesteps | 692224   |
---------------------------------
Eval num_timesteps=695000, episode_reward=0.40 +/- 0.66
Episode length: 292.40 +/- 57.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 292           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 695000        |
| train/                  |               |
|    approx_kl            | 2.1127024e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00514      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0261        |
|    n_updates            | 1690          |
|    policy_gradient_loss | -6.41e-06     |
|    value_loss           | 0.0593        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 170      |
|    time_elapsed    | 1146     |
|    total_timesteps | 696320   |
---------------------------------
Eval num_timesteps=700000, episode_reward=0.50 +/- 0.50
Episode length: 305.40 +/- 47.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 700000        |
| train/                  |               |
|    approx_kl            | 3.9273975e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00629      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0253        |
|    n_updates            | 1700          |
|    policy_gradient_loss | -1.2e-06      |
|    value_loss           | 0.0652        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 171      |
|    time_elapsed    | 1153     |
|    total_timesteps | 700416   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 300           |
|    ep_rew_mean          | 0.42          |
| time/                   |               |
|    fps                  | 607           |
|    iterations           | 172           |
|    time_elapsed         | 1158          |
|    total_timesteps      | 704512        |
| train/                  |               |
|    approx_kl            | 1.2102319e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0082       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0191        |
|    n_updates            | 1710          |
|    policy_gradient_loss | -1.54e-06     |
|    value_loss           | 0.0583        |
-------------------------------------------
Eval num_timesteps=705000, episode_reward=0.40 +/- 0.49
Episode length: 289.60 +/- 48.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 290           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 705000        |
| train/                  |               |
|    approx_kl            | 4.0061772e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0092       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0336        |
|    n_updates            | 1720          |
|    policy_gradient_loss | -1.19e-05     |
|    value_loss           | 0.0586        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 173      |
|    time_elapsed    | 1165     |
|    total_timesteps | 708608   |
---------------------------------
Eval num_timesteps=710000, episode_reward=0.50 +/- 0.50
Episode length: 326.20 +/- 57.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 326          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 710000       |
| train/                  |              |
|    approx_kl            | 0.0002765786 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0133      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0366       |
|    n_updates            | 1730         |
|    policy_gradient_loss | -9.69e-05    |
|    value_loss           | 0.062        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 174      |
|    time_elapsed    | 1172     |
|    total_timesteps | 712704   |
---------------------------------
Eval num_timesteps=715000, episode_reward=0.20 +/- 0.40
Episode length: 291.00 +/- 28.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 715000        |
| train/                  |               |
|    approx_kl            | 3.2959113e-05 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0291        |
|    n_updates            | 1740          |
|    policy_gradient_loss | 1.73e-05      |
|    value_loss           | 0.063         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 175      |
|    time_elapsed    | 1179     |
|    total_timesteps | 716800   |
---------------------------------
Eval num_timesteps=720000, episode_reward=0.60 +/- 0.49
Episode length: 317.00 +/- 58.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 317           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 720000        |
| train/                  |               |
|    approx_kl            | 1.8156381e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0145       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0209        |
|    n_updates            | 1750          |
|    policy_gradient_loss | -2.06e-06     |
|    value_loss           | 0.0576        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 176      |
|    time_elapsed    | 1187     |
|    total_timesteps | 720896   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 295           |
|    ep_rew_mean          | 0.42          |
| time/                   |               |
|    fps                  | 608           |
|    iterations           | 177           |
|    time_elapsed         | 1192          |
|    total_timesteps      | 724992        |
| train/                  |               |
|    approx_kl            | 3.1471427e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0125       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0236        |
|    n_updates            | 1760          |
|    policy_gradient_loss | 2.13e-06      |
|    value_loss           | 0.0548        |
-------------------------------------------
Eval num_timesteps=725000, episode_reward=0.80 +/- 0.60
Episode length: 308.80 +/- 48.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 309           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 725000        |
| train/                  |               |
|    approx_kl            | 5.6956298e-05 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0316        |
|    n_updates            | 1770          |
|    policy_gradient_loss | -0.000161     |
|    value_loss           | 0.0656        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 178      |
|    time_elapsed    | 1199     |
|    total_timesteps | 729088   |
---------------------------------
Eval num_timesteps=730000, episode_reward=0.40 +/- 0.49
Episode length: 318.20 +/- 24.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 730000        |
| train/                  |               |
|    approx_kl            | 6.3670945e-05 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0174       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0395        |
|    n_updates            | 1780          |
|    policy_gradient_loss | -0.000159     |
|    value_loss           | 0.0534        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 179      |
|    time_elapsed    | 1206     |
|    total_timesteps | 733184   |
---------------------------------
Eval num_timesteps=735000, episode_reward=0.50 +/- 0.67
Episode length: 305.40 +/- 46.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 735000        |
| train/                  |               |
|    approx_kl            | 1.9529776e-05 |
|    clip_fraction        | 0.000269      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0173       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0308        |
|    n_updates            | 1790          |
|    policy_gradient_loss | -5.09e-07     |
|    value_loss           | 0.0601        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 180      |
|    time_elapsed    | 1213     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=740000, episode_reward=0.40 +/- 0.66
Episode length: 303.80 +/- 61.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 740000        |
| train/                  |               |
|    approx_kl            | 3.5518722e-05 |
|    clip_fraction        | 0.00132       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0217       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.03          |
|    n_updates            | 1800          |
|    policy_gradient_loss | -3.27e-05     |
|    value_loss           | 0.0635        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 181      |
|    time_elapsed    | 1220     |
|    total_timesteps | 741376   |
---------------------------------
Eval num_timesteps=745000, episode_reward=0.40 +/- 0.49
Episode length: 300.60 +/- 52.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 301           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 745000        |
| train/                  |               |
|    approx_kl            | 0.00014154211 |
|    clip_fraction        | 0.00256       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0156       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0125        |
|    n_updates            | 1810          |
|    policy_gradient_loss | -0.000226     |
|    value_loss           | 0.0551        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 182      |
|    time_elapsed    | 1228     |
|    total_timesteps | 745472   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 298          |
|    ep_rew_mean          | 0.43         |
| time/                   |              |
|    fps                  | 607          |
|    iterations           | 183          |
|    time_elapsed         | 1232         |
|    total_timesteps      | 749568       |
| train/                  |              |
|    approx_kl            | 4.042129e-05 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0192      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0232       |
|    n_updates            | 1820         |
|    policy_gradient_loss | -8.21e-05    |
|    value_loss           | 0.0627       |
------------------------------------------
Eval num_timesteps=750000, episode_reward=0.50 +/- 0.50
Episode length: 323.80 +/- 56.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 324           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 750000        |
| train/                  |               |
|    approx_kl            | 0.00012252752 |
|    clip_fraction        | 0.00168       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0135       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0362        |
|    n_updates            | 1830          |
|    policy_gradient_loss | -0.000165     |
|    value_loss           | 0.0586        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 184      |
|    time_elapsed    | 1240     |
|    total_timesteps | 753664   |
---------------------------------
Eval num_timesteps=755000, episode_reward=0.30 +/- 0.46
Episode length: 285.80 +/- 47.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 286           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 755000        |
| train/                  |               |
|    approx_kl            | 3.3418968e-05 |
|    clip_fraction        | 0.00107       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0109       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0186        |
|    n_updates            | 1840          |
|    policy_gradient_loss | 1.92e-05      |
|    value_loss           | 0.0653        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 185      |
|    time_elapsed    | 1247     |
|    total_timesteps | 757760   |
---------------------------------
Eval num_timesteps=760000, episode_reward=0.30 +/- 0.46
Episode length: 309.40 +/- 53.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 309           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 760000        |
| train/                  |               |
|    approx_kl            | 6.3707324e-05 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00854      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.042         |
|    n_updates            | 1850          |
|    policy_gradient_loss | -0.000312     |
|    value_loss           | 0.0596        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 186      |
|    time_elapsed    | 1254     |
|    total_timesteps | 761856   |
---------------------------------
Eval num_timesteps=765000, episode_reward=0.70 +/- 0.78
Episode length: 306.80 +/- 62.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 307           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 765000        |
| train/                  |               |
|    approx_kl            | 1.7234517e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00956      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0309        |
|    n_updates            | 1860          |
|    policy_gradient_loss | -5.53e-06     |
|    value_loss           | 0.0537        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 187      |
|    time_elapsed    | 1261     |
|    total_timesteps | 765952   |
---------------------------------
Eval num_timesteps=770000, episode_reward=0.50 +/- 0.67
Episode length: 306.80 +/- 32.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 307           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 770000        |
| train/                  |               |
|    approx_kl            | 4.6910733e-05 |
|    clip_fraction        | 0.000806      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00832      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0257        |
|    n_updates            | 1870          |
|    policy_gradient_loss | -5.75e-05     |
|    value_loss           | 0.0558        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 188      |
|    time_elapsed    | 1268     |
|    total_timesteps | 770048   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 306           |
|    ep_rew_mean          | 0.5           |
| time/                   |               |
|    fps                  | 607           |
|    iterations           | 189           |
|    time_elapsed         | 1273          |
|    total_timesteps      | 774144        |
| train/                  |               |
|    approx_kl            | 5.2088377e-05 |
|    clip_fraction        | 0.00061       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00666      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0249        |
|    n_updates            | 1880          |
|    policy_gradient_loss | -9.49e-05     |
|    value_loss           | 0.0659        |
-------------------------------------------
Eval num_timesteps=775000, episode_reward=0.50 +/- 0.67
Episode length: 293.40 +/- 47.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 293           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 775000        |
| train/                  |               |
|    approx_kl            | 1.8422259e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00705      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0255        |
|    n_updates            | 1890          |
|    policy_gradient_loss | -7.26e-07     |
|    value_loss           | 0.0572        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 190      |
|    time_elapsed    | 1280     |
|    total_timesteps | 778240   |
---------------------------------
Eval num_timesteps=780000, episode_reward=0.10 +/- 0.30
Episode length: 289.20 +/- 41.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 289          |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 7.357552e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00506     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.01         |
|    loss                 | 0.018        |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.000296    |
|    value_loss           | 0.0642       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 191      |
|    time_elapsed    | 1287     |
|    total_timesteps | 782336   |
---------------------------------
Eval num_timesteps=785000, episode_reward=0.80 +/- 0.87
Episode length: 305.60 +/- 64.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 306          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 785000       |
| train/                  |              |
|    approx_kl            | 3.243948e-05 |
|    clip_fraction        | 0.000317     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00627     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.028        |
|    n_updates            | 1910         |
|    policy_gradient_loss | -1.04e-05    |
|    value_loss           | 0.0585       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 192      |
|    time_elapsed    | 1294     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=790000, episode_reward=0.70 +/- 0.90
Episode length: 297.40 +/- 76.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 297           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 790000        |
| train/                  |               |
|    approx_kl            | 2.9817747e-05 |
|    clip_fraction        | 0.000708      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00894      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0498        |
|    n_updates            | 1920          |
|    policy_gradient_loss | -8.96e-05     |
|    value_loss           | 0.066         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 193      |
|    time_elapsed    | 1302     |
|    total_timesteps | 790528   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 302           |
|    ep_rew_mean          | 0.51          |
| time/                   |               |
|    fps                  | 607           |
|    iterations           | 194           |
|    time_elapsed         | 1307          |
|    total_timesteps      | 794624        |
| train/                  |               |
|    approx_kl            | 6.0170016e-05 |
|    clip_fraction        | 0.001         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00976      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0376        |
|    n_updates            | 1930          |
|    policy_gradient_loss | -0.000201     |
|    value_loss           | 0.0546        |
-------------------------------------------
Eval num_timesteps=795000, episode_reward=0.50 +/- 0.81
Episode length: 308.80 +/- 74.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 309          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 795000       |
| train/                  |              |
|    approx_kl            | 5.994308e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00658     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0384       |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.000198    |
|    value_loss           | 0.0703       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 195      |
|    time_elapsed    | 1314     |
|    total_timesteps | 798720   |
---------------------------------
Eval num_timesteps=800000, episode_reward=0.40 +/- 0.49
Episode length: 313.60 +/- 66.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 314           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 800000        |
| train/                  |               |
|    approx_kl            | 1.5027006e-05 |
|    clip_fraction        | 0.000366      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00564      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0345        |
|    n_updates            | 1950          |
|    policy_gradient_loss | -3.27e-05     |
|    value_loss           | 0.0589        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 196      |
|    time_elapsed    | 1321     |
|    total_timesteps | 802816   |
---------------------------------
Eval num_timesteps=805000, episode_reward=0.50 +/- 0.67
Episode length: 287.00 +/- 55.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 805000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00564  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.03      |
|    n_updates            | 1960      |
|    policy_gradient_loss | -3.91e-10 |
|    value_loss           | 0.0606    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 197      |
|    time_elapsed    | 1328     |
|    total_timesteps | 806912   |
---------------------------------
Eval num_timesteps=810000, episode_reward=0.70 +/- 0.64
Episode length: 311.20 +/- 54.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 311           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 810000        |
| train/                  |               |
|    approx_kl            | 1.0523363e-05 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00494      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0266        |
|    n_updates            | 1970          |
|    policy_gradient_loss | -5.01e-06     |
|    value_loss           | 0.0666        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 198      |
|    time_elapsed    | 1335     |
|    total_timesteps | 811008   |
---------------------------------
Eval num_timesteps=815000, episode_reward=0.30 +/- 0.46
Episode length: 322.60 +/- 50.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 815000        |
| train/                  |               |
|    approx_kl            | 3.4226672e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00344      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0282        |
|    n_updates            | 1980          |
|    policy_gradient_loss | -4.41e-05     |
|    value_loss           | 0.0659        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 199      |
|    time_elapsed    | 1343     |
|    total_timesteps | 815104   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 294      |
|    ep_rew_mean          | 0.39     |
| time/                   |          |
|    fps                  | 607      |
|    iterations           | 200      |
|    time_elapsed         | 1348     |
|    total_timesteps      | 819200   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00347 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0233   |
|    n_updates            | 1990     |
|    policy_gradient_loss | 5.74e-10 |
|    value_loss           | 0.054    |
--------------------------------------
Eval num_timesteps=820000, episode_reward=0.60 +/- 0.49
Episode length: 309.60 +/- 24.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 820000       |
| train/                  |              |
|    approx_kl            | 2.678542e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00436     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0274       |
|    n_updates            | 2000         |
|    policy_gradient_loss | -5.78e-05    |
|    value_loss           | 0.0636       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 201      |
|    time_elapsed    | 1355     |
|    total_timesteps | 823296   |
---------------------------------
Eval num_timesteps=825000, episode_reward=0.40 +/- 0.49
Episode length: 304.20 +/- 51.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 304          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 825000       |
| train/                  |              |
|    approx_kl            | 8.762072e-06 |
|    clip_fraction        | 0.000415     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00354     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0226       |
|    n_updates            | 2010         |
|    policy_gradient_loss | -3.26e-05    |
|    value_loss           | 0.0682       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 202      |
|    time_elapsed    | 1362     |
|    total_timesteps | 827392   |
---------------------------------
Eval num_timesteps=830000, episode_reward=0.10 +/- 0.30
Episode length: 284.20 +/- 41.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 284           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 830000        |
| train/                  |               |
|    approx_kl            | 5.1372248e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00244      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.023         |
|    n_updates            | 2020          |
|    policy_gradient_loss | -9.03e-05     |
|    value_loss           | 0.0582        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 203      |
|    time_elapsed    | 1369     |
|    total_timesteps | 831488   |
---------------------------------
Eval num_timesteps=835000, episode_reward=0.40 +/- 0.66
Episode length: 307.40 +/- 37.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 835000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00255  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0373    |
|    n_updates            | 2030      |
|    policy_gradient_loss | 5.97e-10  |
|    value_loss           | 0.0528    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 204      |
|    time_elapsed    | 1376     |
|    total_timesteps | 835584   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 304      |
|    ep_rew_mean          | 0.39     |
| time/                   |          |
|    fps                  | 607      |
|    iterations           | 205      |
|    time_elapsed         | 1382     |
|    total_timesteps      | 839680   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00255 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0168   |
|    n_updates            | 2040     |
|    policy_gradient_loss | 3.5e-10  |
|    value_loss           | 0.0579   |
--------------------------------------
Eval num_timesteps=840000, episode_reward=0.70 +/- 0.90
Episode length: 291.00 +/- 51.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 5.173337e-06 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00285     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0317       |
|    n_updates            | 2050         |
|    policy_gradient_loss | 2.06e-06     |
|    value_loss           | 0.0546       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 206      |
|    time_elapsed    | 1389     |
|    total_timesteps | 843776   |
---------------------------------
Eval num_timesteps=845000, episode_reward=0.50 +/- 0.92
Episode length: 304.20 +/- 42.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 304          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 845000       |
| train/                  |              |
|    approx_kl            | 3.251237e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00186     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0249       |
|    n_updates            | 2060         |
|    policy_gradient_loss | -8.85e-05    |
|    value_loss           | 0.0608       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 207      |
|    time_elapsed    | 1396     |
|    total_timesteps | 847872   |
---------------------------------
Eval num_timesteps=850000, episode_reward=0.80 +/- 0.75
Episode length: 304.00 +/- 43.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 850000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00182  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0329    |
|    n_updates            | 2070      |
|    policy_gradient_loss | -3.97e-10 |
|    value_loss           | 0.0692    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 208      |
|    time_elapsed    | 1403     |
|    total_timesteps | 851968   |
---------------------------------
Eval num_timesteps=855000, episode_reward=0.20 +/- 0.40
Episode length: 297.60 +/- 53.42
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 298      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 855000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00182 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0133   |
|    n_updates            | 2080     |
|    policy_gradient_loss | 6.24e-10 |
|    value_loss           | 0.0571   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 209      |
|    time_elapsed    | 1410     |
|    total_timesteps | 856064   |
---------------------------------
Eval num_timesteps=860000, episode_reward=0.50 +/- 0.67
Episode length: 300.20 +/- 62.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 860000        |
| train/                  |               |
|    approx_kl            | 1.7418279e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00164      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0335        |
|    n_updates            | 2090          |
|    policy_gradient_loss | -7.19e-07     |
|    value_loss           | 0.0575        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 210      |
|    time_elapsed    | 1417     |
|    total_timesteps | 860160   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 300      |
|    ep_rew_mean          | 0.41     |
| time/                   |          |
|    fps                  | 607      |
|    iterations           | 211      |
|    time_elapsed         | 1422     |
|    total_timesteps      | 864256   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00156 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0231   |
|    n_updates            | 2100     |
|    policy_gradient_loss | 1.05e-09 |
|    value_loss           | 0.057    |
--------------------------------------
Eval num_timesteps=865000, episode_reward=0.70 +/- 0.64
Episode length: 325.20 +/- 51.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 865000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00156  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0301    |
|    n_updates            | 2110      |
|    policy_gradient_loss | -3.08e-10 |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 212      |
|    time_elapsed    | 1430     |
|    total_timesteps | 868352   |
---------------------------------
Eval num_timesteps=870000, episode_reward=0.40 +/- 0.49
Episode length: 312.60 +/- 62.79
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 313      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 870000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00156 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0321   |
|    n_updates            | 2120     |
|    policy_gradient_loss | 6.82e-10 |
|    value_loss           | 0.065    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 213      |
|    time_elapsed    | 1437     |
|    total_timesteps | 872448   |
---------------------------------
Eval num_timesteps=875000, episode_reward=0.40 +/- 0.49
Episode length: 301.00 +/- 51.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 301           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 875000        |
| train/                  |               |
|    approx_kl            | 2.5074289e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00126      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0196        |
|    n_updates            | 2130          |
|    policy_gradient_loss | -3.23e-06     |
|    value_loss           | 0.0601        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 214      |
|    time_elapsed    | 1444     |
|    total_timesteps | 876544   |
---------------------------------
Eval num_timesteps=880000, episode_reward=0.00 +/- 0.00
Episode length: 295.00 +/- 46.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 295           |
|    mean_reward          | 0             |
| time/                   |               |
|    total_timesteps      | 880000        |
| train/                  |               |
|    approx_kl            | 1.4835779e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000933     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0151        |
|    n_updates            | 2140          |
|    policy_gradient_loss | -3.44e-06     |
|    value_loss           | 0.0629        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 215      |
|    time_elapsed    | 1451     |
|    total_timesteps | 880640   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 296       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 216       |
|    time_elapsed         | 1456      |
|    total_timesteps      | 884736    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000926 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0314    |
|    n_updates            | 2150      |
|    policy_gradient_loss | -1.36e-09 |
|    value_loss           | 0.0558    |
---------------------------------------
Eval num_timesteps=885000, episode_reward=0.40 +/- 0.66
Episode length: 306.00 +/- 69.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 885000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000926 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.037     |
|    n_updates            | 2160      |
|    policy_gradient_loss | -1.94e-09 |
|    value_loss           | 0.0652    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 217      |
|    time_elapsed    | 1463     |
|    total_timesteps | 888832   |
---------------------------------
Eval num_timesteps=890000, episode_reward=0.60 +/- 0.49
Episode length: 296.40 +/- 57.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 890000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000926 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0322    |
|    n_updates            | 2170      |
|    policy_gradient_loss | 5.16e-10  |
|    value_loss           | 0.0564    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 218      |
|    time_elapsed    | 1470     |
|    total_timesteps | 892928   |
---------------------------------
Eval num_timesteps=895000, episode_reward=0.50 +/- 0.67
Episode length: 304.40 +/- 33.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 895000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000926 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0523    |
|    n_updates            | 2180      |
|    policy_gradient_loss | 1.28e-10  |
|    value_loss           | 0.062     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 219      |
|    time_elapsed    | 1477     |
|    total_timesteps | 897024   |
---------------------------------
Eval num_timesteps=900000, episode_reward=0.30 +/- 0.46
Episode length: 285.80 +/- 46.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 900000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000926 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0221    |
|    n_updates            | 2190      |
|    policy_gradient_loss | -6.68e-10 |
|    value_loss           | 0.0614    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 220      |
|    time_elapsed    | 1484     |
|    total_timesteps | 901120   |
---------------------------------
Eval num_timesteps=905000, episode_reward=0.60 +/- 0.66
Episode length: 339.80 +/- 53.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 340       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 905000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000926 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0204    |
|    n_updates            | 2200      |
|    policy_gradient_loss | -6.84e-11 |
|    value_loss           | 0.0528    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 221      |
|    time_elapsed    | 1492     |
|    total_timesteps | 905216   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.42      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 222       |
|    time_elapsed         | 1496      |
|    total_timesteps      | 909312    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000926 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0232    |
|    n_updates            | 2210      |
|    policy_gradient_loss | -2.94e-10 |
|    value_loss           | 0.0566    |
---------------------------------------
Eval num_timesteps=910000, episode_reward=0.50 +/- 0.67
Episode length: 299.00 +/- 77.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 299           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 910000        |
| train/                  |               |
|    approx_kl            | 1.0652002e-08 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000913     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0259        |
|    n_updates            | 2220          |
|    policy_gradient_loss | 1.82e-06      |
|    value_loss           | 0.0601        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 223      |
|    time_elapsed    | 1504     |
|    total_timesteps | 913408   |
---------------------------------
Eval num_timesteps=915000, episode_reward=0.40 +/- 0.49
Episode length: 302.00 +/- 48.12
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 302      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 915000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00106 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0255   |
|    n_updates            | 2230     |
|    policy_gradient_loss | 8.59e-10 |
|    value_loss           | 0.0607   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 224      |
|    time_elapsed    | 1511     |
|    total_timesteps | 917504   |
---------------------------------
Eval num_timesteps=920000, episode_reward=0.40 +/- 0.66
Episode length: 313.60 +/- 54.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 920000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00106  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0469    |
|    n_updates            | 2240      |
|    policy_gradient_loss | -2.07e-10 |
|    value_loss           | 0.0584    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 225      |
|    time_elapsed    | 1518     |
|    total_timesteps | 921600   |
---------------------------------
Eval num_timesteps=925000, episode_reward=0.90 +/- 1.04
Episode length: 332.20 +/- 86.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 332       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 925000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00106  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0556    |
|    n_updates            | 2250      |
|    policy_gradient_loss | -6.36e-10 |
|    value_loss           | 0.0619    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 226      |
|    time_elapsed    | 1525     |
|    total_timesteps | 925696   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 294      |
|    ep_rew_mean          | 0.37     |
| time/                   |          |
|    fps                  | 607      |
|    iterations           | 227      |
|    time_elapsed         | 1530     |
|    total_timesteps      | 929792   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00106 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0456   |
|    n_updates            | 2260     |
|    policy_gradient_loss | 1.8e-10  |
|    value_loss           | 0.0528   |
--------------------------------------
Eval num_timesteps=930000, episode_reward=0.60 +/- 0.66
Episode length: 293.00 +/- 51.86
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 293      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 930000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00106 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0257   |
|    n_updates            | 2270     |
|    policy_gradient_loss | 1e-09    |
|    value_loss           | 0.0634   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 228      |
|    time_elapsed    | 1537     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=935000, episode_reward=0.20 +/- 0.40
Episode length: 292.20 +/- 57.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 292      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 935000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00106 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.019    |
|    n_updates            | 2280     |
|    policy_gradient_loss | 7.13e-11 |
|    value_loss           | 0.0539   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 229      |
|    time_elapsed    | 1544     |
|    total_timesteps | 937984   |
---------------------------------
Eval num_timesteps=940000, episode_reward=0.50 +/- 0.67
Episode length: 295.40 +/- 62.28
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 295      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 940000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00106 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0336   |
|    n_updates            | 2290     |
|    policy_gradient_loss | 6.29e-10 |
|    value_loss           | 0.0539   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 230      |
|    time_elapsed    | 1552     |
|    total_timesteps | 942080   |
---------------------------------
Eval num_timesteps=945000, episode_reward=0.30 +/- 0.46
Episode length: 310.60 +/- 53.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 311           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 945000        |
| train/                  |               |
|    approx_kl            | 1.0015836e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000973     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.025         |
|    n_updates            | 2300          |
|    policy_gradient_loss | 9.2e-07       |
|    value_loss           | 0.0528        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 231      |
|    time_elapsed    | 1559     |
|    total_timesteps | 946176   |
---------------------------------
Eval num_timesteps=950000, episode_reward=0.50 +/- 0.67
Episode length: 308.00 +/- 45.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 950000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000919 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0257    |
|    n_updates            | 2310      |
|    policy_gradient_loss | 7e-10     |
|    value_loss           | 0.0594    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 232      |
|    time_elapsed    | 1566     |
|    total_timesteps | 950272   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 299       |
|    ep_rew_mean          | 0.36      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 233       |
|    time_elapsed         | 1571      |
|    total_timesteps      | 954368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000919 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0245    |
|    n_updates            | 2320      |
|    policy_gradient_loss | 1.83e-10  |
|    value_loss           | 0.052     |
---------------------------------------
Eval num_timesteps=955000, episode_reward=0.50 +/- 0.67
Episode length: 309.80 +/- 46.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 955000       |
| train/                  |              |
|    approx_kl            | 9.509822e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00068     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0564       |
|    n_updates            | 2330         |
|    policy_gradient_loss | 2.11e-06     |
|    value_loss           | 0.0631       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 234      |
|    time_elapsed    | 1578     |
|    total_timesteps | 958464   |
---------------------------------
Eval num_timesteps=960000, episode_reward=0.40 +/- 0.49
Episode length: 288.00 +/- 29.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 960000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000744 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0467    |
|    n_updates            | 2340      |
|    policy_gradient_loss | -4.51e-11 |
|    value_loss           | 0.0633    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 235      |
|    time_elapsed    | 1585     |
|    total_timesteps | 962560   |
---------------------------------
Eval num_timesteps=965000, episode_reward=0.30 +/- 0.46
Episode length: 286.60 +/- 39.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 965000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000744 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0182    |
|    n_updates            | 2350      |
|    policy_gradient_loss | -8.66e-11 |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 236      |
|    time_elapsed    | 1592     |
|    total_timesteps | 966656   |
---------------------------------
Eval num_timesteps=970000, episode_reward=0.90 +/- 0.70
Episode length: 306.20 +/- 47.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 970000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000744 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0495    |
|    n_updates            | 2360      |
|    policy_gradient_loss | 5.22e-10  |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 237      |
|    time_elapsed    | 1599     |
|    total_timesteps | 970752   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 298       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 238       |
|    time_elapsed         | 1604      |
|    total_timesteps      | 974848    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000744 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0211    |
|    n_updates            | 2370      |
|    policy_gradient_loss | 6.55e-11  |
|    value_loss           | 0.0593    |
---------------------------------------
Eval num_timesteps=975000, episode_reward=0.20 +/- 0.40
Episode length: 303.40 +/- 50.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 975000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000744 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0179    |
|    n_updates            | 2380      |
|    policy_gradient_loss | -3.34e-10 |
|    value_loss           | 0.0555    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 239      |
|    time_elapsed    | 1611     |
|    total_timesteps | 978944   |
---------------------------------
Eval num_timesteps=980000, episode_reward=0.40 +/- 0.49
Episode length: 285.00 +/- 43.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 980000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000744 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0388    |
|    n_updates            | 2390      |
|    policy_gradient_loss | 8.56e-10  |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 240      |
|    time_elapsed    | 1618     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=985000, episode_reward=0.70 +/- 0.64
Episode length: 302.40 +/- 47.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 985000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000744 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0343    |
|    n_updates            | 2400      |
|    policy_gradient_loss | 3.38e-10  |
|    value_loss           | 0.0607    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 241      |
|    time_elapsed    | 1626     |
|    total_timesteps | 987136   |
---------------------------------
Eval num_timesteps=990000, episode_reward=0.50 +/- 0.50
Episode length: 304.40 +/- 42.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 990000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000744 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0353    |
|    n_updates            | 2410      |
|    policy_gradient_loss | 3.78e-11  |
|    value_loss           | 0.0553    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 242      |
|    time_elapsed    | 1633     |
|    total_timesteps | 991232   |
---------------------------------
Eval num_timesteps=995000, episode_reward=0.50 +/- 0.50
Episode length: 326.40 +/- 46.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 326           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 995000        |
| train/                  |               |
|    approx_kl            | 1.0936579e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000584     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0381        |
|    n_updates            | 2420          |
|    policy_gradient_loss | 4.14e-07      |
|    value_loss           | 0.0628        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 243      |
|    time_elapsed    | 1640     |
|    total_timesteps | 995328   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 295       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 244       |
|    time_elapsed         | 1645      |
|    total_timesteps      | 999424    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000676 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0269    |
|    n_updates            | 2430      |
|    policy_gradient_loss | 2.21e-10  |
|    value_loss           | 0.0592    |
---------------------------------------
Eval num_timesteps=1000000, episode_reward=0.40 +/- 0.49
Episode length: 311.60 +/- 40.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1000000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000676 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0192    |
|    n_updates            | 2440      |
|    policy_gradient_loss | -7.06e-10 |
|    value_loss           | 0.0647    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 245      |
|    time_elapsed    | 1652     |
|    total_timesteps | 1003520  |
---------------------------------
Eval num_timesteps=1005000, episode_reward=0.40 +/- 0.66
Episode length: 282.20 +/- 52.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 282       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1005000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000676 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0264    |
|    n_updates            | 2450      |
|    policy_gradient_loss | 6.03e-10  |
|    value_loss           | 0.0548    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 246      |
|    time_elapsed    | 1659     |
|    total_timesteps | 1007616  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=0.50 +/- 0.50
Episode length: 313.00 +/- 60.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1010000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000676 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.026     |
|    n_updates            | 2460      |
|    policy_gradient_loss | 5.46e-10  |
|    value_loss           | 0.0542    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 247      |
|    time_elapsed    | 1666     |
|    total_timesteps | 1011712  |
---------------------------------
Eval num_timesteps=1015000, episode_reward=0.30 +/- 0.46
Episode length: 291.00 +/- 52.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 291       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1015000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000676 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.031     |
|    n_updates            | 2470      |
|    policy_gradient_loss | -6.2e-10  |
|    value_loss           | 0.056     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 248      |
|    time_elapsed    | 1673     |
|    total_timesteps | 1015808  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 298         |
|    ep_rew_mean          | 0.38        |
| time/                   |             |
|    fps                  | 607         |
|    iterations           | 249         |
|    time_elapsed         | 1678        |
|    total_timesteps      | 1019904     |
| train/                  |             |
|    approx_kl            | 1.31126e-05 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000648   |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.01        |
|    loss                 | 0.041       |
|    n_updates            | 2480        |
|    policy_gradient_loss | 2.13e-06    |
|    value_loss           | 0.0566      |
-----------------------------------------
Eval num_timesteps=1020000, episode_reward=0.40 +/- 0.49
Episode length: 294.80 +/- 60.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 295          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 1020000      |
| train/                  |              |
|    approx_kl            | 6.490387e-06 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000612    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0219       |
|    n_updates            | 2490         |
|    policy_gradient_loss | 1.03e-06     |
|    value_loss           | 0.0556       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 250      |
|    time_elapsed    | 1685     |
|    total_timesteps | 1024000  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=0.40 +/- 0.66
Episode length: 314.80 +/- 64.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 315           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 1025000       |
| train/                  |               |
|    approx_kl            | 2.4555688e-05 |
|    clip_fraction        | 0.000366      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000509     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0183        |
|    n_updates            | 2500          |
|    policy_gradient_loss | -3.53e-05     |
|    value_loss           | 0.0533        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 251      |
|    time_elapsed    | 1692     |
|    total_timesteps | 1028096  |
---------------------------------
Eval num_timesteps=1030000, episode_reward=0.40 +/- 0.49
Episode length: 288.00 +/- 31.14
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 288      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 1030000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00061 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.042    |
|    n_updates            | 2510     |
|    policy_gradient_loss | 1.78e-10 |
|    value_loss           | 0.0574   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 252      |
|    time_elapsed    | 1700     |
|    total_timesteps | 1032192  |
---------------------------------
Eval num_timesteps=1035000, episode_reward=0.50 +/- 0.67
Episode length: 295.00 +/- 41.34
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 295      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 1035000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00061 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0311   |
|    n_updates            | 2520     |
|    policy_gradient_loss | -5.7e-10 |
|    value_loss           | 0.0585   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 253      |
|    time_elapsed    | 1707     |
|    total_timesteps | 1036288  |
---------------------------------
Eval num_timesteps=1040000, episode_reward=0.60 +/- 0.80
Episode length: 296.00 +/- 45.43
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 296      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 1040000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00061 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0232   |
|    n_updates            | 2530     |
|    policy_gradient_loss | 1.64e-10 |
|    value_loss           | 0.0621   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 254      |
|    time_elapsed    | 1714     |
|    total_timesteps | 1040384  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 307       |
|    ep_rew_mean          | 0.4       |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 255       |
|    time_elapsed         | 1719      |
|    total_timesteps      | 1044480   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00061  |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.00451   |
|    n_updates            | 2540      |
|    policy_gradient_loss | -1.78e-10 |
|    value_loss           | 0.0596    |
---------------------------------------
Eval num_timesteps=1045000, episode_reward=0.40 +/- 0.49
Episode length: 282.60 +/- 44.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1045000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00061  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0336    |
|    n_updates            | 2550      |
|    policy_gradient_loss | -3.97e-10 |
|    value_loss           | 0.0531    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 256      |
|    time_elapsed    | 1726     |
|    total_timesteps | 1048576  |
---------------------------------
Eval num_timesteps=1050000, episode_reward=0.40 +/- 0.66
Episode length: 298.60 +/- 41.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1050000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00061  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0444    |
|    n_updates            | 2560      |
|    policy_gradient_loss | -4.66e-10 |
|    value_loss           | 0.0575    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 257      |
|    time_elapsed    | 1733     |
|    total_timesteps | 1052672  |
---------------------------------
Eval num_timesteps=1055000, episode_reward=0.40 +/- 0.49
Episode length: 300.20 +/- 59.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1055000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00061  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0496    |
|    n_updates            | 2570      |
|    policy_gradient_loss | -4.29e-10 |
|    value_loss           | 0.057     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 258      |
|    time_elapsed    | 1740     |
|    total_timesteps | 1056768  |
---------------------------------
Eval num_timesteps=1060000, episode_reward=0.40 +/- 0.49
Episode length: 299.00 +/- 53.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 299           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 1060000       |
| train/                  |               |
|    approx_kl            | 1.0887597e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000755     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0425        |
|    n_updates            | 2580          |
|    policy_gradient_loss | 3.22e-06      |
|    value_loss           | 0.0654        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 259      |
|    time_elapsed    | 1747     |
|    total_timesteps | 1060864  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 307      |
|    ep_rew_mean          | 0.45     |
| time/                   |          |
|    fps                  | 607      |
|    iterations           | 260      |
|    time_elapsed         | 1752     |
|    total_timesteps      | 1064960  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0006  |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0247   |
|    n_updates            | 2590     |
|    policy_gradient_loss | 6.26e-11 |
|    value_loss           | 0.0606   |
--------------------------------------
Eval num_timesteps=1065000, episode_reward=0.90 +/- 0.70
Episode length: 300.20 +/- 62.65
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 300      |
|    mean_reward          | 0.9      |
| time/                   |          |
|    total_timesteps      | 1065000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0006  |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0207   |
|    n_updates            | 2600     |
|    policy_gradient_loss | 3.66e-10 |
|    value_loss           | 0.0589   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 261      |
|    time_elapsed    | 1759     |
|    total_timesteps | 1069056  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=0.40 +/- 0.49
Episode length: 280.40 +/- 35.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 280      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 1070000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0006  |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0375   |
|    n_updates            | 2610     |
|    policy_gradient_loss | 1.12e-09 |
|    value_loss           | 0.0552   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 262      |
|    time_elapsed    | 1766     |
|    total_timesteps | 1073152  |
---------------------------------
Eval num_timesteps=1075000, episode_reward=0.60 +/- 0.49
Episode length: 323.20 +/- 53.30
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 323      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 1075000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0006  |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0215   |
|    n_updates            | 2620     |
|    policy_gradient_loss | 7.28e-10 |
|    value_loss           | 0.0583   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 263      |
|    time_elapsed    | 1773     |
|    total_timesteps | 1077248  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=0.70 +/- 0.78
Episode length: 291.40 +/- 53.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 1080000      |
| train/                  |              |
|    approx_kl            | 8.490373e-06 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000778    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0302       |
|    n_updates            | 2630         |
|    policy_gradient_loss | -4.54e-06    |
|    value_loss           | 0.0511       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 264      |
|    time_elapsed    | 1780     |
|    total_timesteps | 1081344  |
---------------------------------
Eval num_timesteps=1085000, episode_reward=0.50 +/- 0.50
Episode length: 312.80 +/- 56.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1085000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000592 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0254    |
|    n_updates            | 2640      |
|    policy_gradient_loss | -1.04e-09 |
|    value_loss           | 0.055     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 265      |
|    time_elapsed    | 1787     |
|    total_timesteps | 1085440  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 294           |
|    ep_rew_mean          | 0.33          |
| time/                   |               |
|    fps                  | 607           |
|    iterations           | 266           |
|    time_elapsed         | 1792          |
|    total_timesteps      | 1089536       |
| train/                  |               |
|    approx_kl            | 2.3263667e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00065      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0238        |
|    n_updates            | 2650          |
|    policy_gradient_loss | -1.49e-06     |
|    value_loss           | 0.0605        |
-------------------------------------------
Eval num_timesteps=1090000, episode_reward=0.60 +/- 0.66
Episode length: 297.00 +/- 64.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1090000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000749 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0374    |
|    n_updates            | 2660      |
|    policy_gradient_loss | -1.24e-10 |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 267      |
|    time_elapsed    | 1799     |
|    total_timesteps | 1093632  |
---------------------------------
Eval num_timesteps=1095000, episode_reward=0.40 +/- 0.49
Episode length: 291.60 +/- 43.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1095000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000749 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.027     |
|    n_updates            | 2670      |
|    policy_gradient_loss | -8.25e-10 |
|    value_loss           | 0.0576    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 268      |
|    time_elapsed    | 1807     |
|    total_timesteps | 1097728  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=0.60 +/- 0.66
Episode length: 293.20 +/- 59.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1100000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000749 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0118    |
|    n_updates            | 2680      |
|    policy_gradient_loss | -1.38e-10 |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 269      |
|    time_elapsed    | 1813     |
|    total_timesteps | 1101824  |
---------------------------------
Eval num_timesteps=1105000, episode_reward=0.50 +/- 0.67
Episode length: 295.40 +/- 57.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1105000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000749 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0268    |
|    n_updates            | 2690      |
|    policy_gradient_loss | -9.59e-10 |
|    value_loss           | 0.06      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 270      |
|    time_elapsed    | 1821     |
|    total_timesteps | 1105920  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=0.60 +/- 0.49
Episode length: 289.80 +/- 66.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1110000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000749 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0262    |
|    n_updates            | 2700      |
|    policy_gradient_loss | 3.69e-10  |
|    value_loss           | 0.0618    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 271      |
|    time_elapsed    | 1828     |
|    total_timesteps | 1110016  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 295       |
|    ep_rew_mean          | 0.45      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 272       |
|    time_elapsed         | 1833      |
|    total_timesteps      | 1114112   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000749 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.042     |
|    n_updates            | 2710      |
|    policy_gradient_loss | -1.06e-10 |
|    value_loss           | 0.0609    |
---------------------------------------
Eval num_timesteps=1115000, episode_reward=0.60 +/- 0.66
Episode length: 316.00 +/- 39.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 316           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 1115000       |
| train/                  |               |
|    approx_kl            | 3.9741222e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000483     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0207        |
|    n_updates            | 2720          |
|    policy_gradient_loss | -0.000117     |
|    value_loss           | 0.0586        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 273      |
|    time_elapsed    | 1840     |
|    total_timesteps | 1118208  |
---------------------------------
Eval num_timesteps=1120000, episode_reward=0.30 +/- 0.46
Episode length: 335.80 +/- 35.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 336       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1120000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000475 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.037     |
|    n_updates            | 2730      |
|    policy_gradient_loss | -7.64e-12 |
|    value_loss           | 0.0582    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 274      |
|    time_elapsed    | 1847     |
|    total_timesteps | 1122304  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=0.60 +/- 0.66
Episode length: 299.00 +/- 26.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1125000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000475 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0484    |
|    n_updates            | 2740      |
|    policy_gradient_loss | 4.14e-10  |
|    value_loss           | 0.0584    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 275      |
|    time_elapsed    | 1854     |
|    total_timesteps | 1126400  |
---------------------------------
Eval num_timesteps=1130000, episode_reward=0.20 +/- 0.40
Episode length: 310.00 +/- 51.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 310         |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 1130000     |
| train/                  |             |
|    approx_kl            | 6.89741e-05 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000736   |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0129      |
|    n_updates            | 2750        |
|    policy_gradient_loss | -4.08e-05   |
|    value_loss           | 0.0672      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 276      |
|    time_elapsed    | 1862     |
|    total_timesteps | 1130496  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 307       |
|    ep_rew_mean          | 0.52      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 277       |
|    time_elapsed         | 1866      |
|    total_timesteps      | 1134592   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00077  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0443    |
|    n_updates            | 2760      |
|    policy_gradient_loss | -1.21e-09 |
|    value_loss           | 0.0628    |
---------------------------------------
Eval num_timesteps=1135000, episode_reward=0.80 +/- 0.60
Episode length: 319.20 +/- 53.83
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 319      |
|    mean_reward          | 0.8      |
| time/                   |          |
|    total_timesteps      | 1135000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00077 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.023    |
|    n_updates            | 2770     |
|    policy_gradient_loss | 4e-10    |
|    value_loss           | 0.0594   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 278      |
|    time_elapsed    | 1874     |
|    total_timesteps | 1138688  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=0.70 +/- 0.64
Episode length: 325.20 +/- 74.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 1140000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00077  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0181    |
|    n_updates            | 2780      |
|    policy_gradient_loss | -4.44e-11 |
|    value_loss           | 0.0613    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 279      |
|    time_elapsed    | 1881     |
|    total_timesteps | 1142784  |
---------------------------------
Eval num_timesteps=1145000, episode_reward=0.40 +/- 0.49
Episode length: 302.80 +/- 33.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1145000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00077  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0268    |
|    n_updates            | 2790      |
|    policy_gradient_loss | -5.11e-10 |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 280      |
|    time_elapsed    | 1888     |
|    total_timesteps | 1146880  |
---------------------------------
Eval num_timesteps=1150000, episode_reward=0.10 +/- 0.30
Episode length: 294.20 +/- 42.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 1150000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00077  |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0292    |
|    n_updates            | 2800      |
|    policy_gradient_loss | -3.29e-11 |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 281      |
|    time_elapsed    | 1895     |
|    total_timesteps | 1150976  |
---------------------------------
Eval num_timesteps=1155000, episode_reward=0.50 +/- 0.50
Episode length: 288.80 +/- 45.89
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 289      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 1155000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00077 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0156   |
|    n_updates            | 2810     |
|    policy_gradient_loss | 9.53e-11 |
|    value_loss           | 0.0635   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 282      |
|    time_elapsed    | 1902     |
|    total_timesteps | 1155072  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.52      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 283       |
|    time_elapsed         | 1907      |
|    total_timesteps      | 1159168   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00077  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0456    |
|    n_updates            | 2820      |
|    policy_gradient_loss | -8.52e-10 |
|    value_loss           | 0.0617    |
---------------------------------------
Eval num_timesteps=1160000, episode_reward=0.70 +/- 0.78
Episode length: 326.60 +/- 76.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 327       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 1160000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00077  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0263    |
|    n_updates            | 2830      |
|    policy_gradient_loss | -6.99e-10 |
|    value_loss           | 0.0657    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 284      |
|    time_elapsed    | 1915     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1165000, episode_reward=0.30 +/- 0.64
Episode length: 304.60 +/- 62.25
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 305      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 1165000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00077 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0298   |
|    n_updates            | 2840     |
|    policy_gradient_loss | 6.62e-11 |
|    value_loss           | 0.0646   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 285      |
|    time_elapsed    | 1922     |
|    total_timesteps | 1167360  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=0.70 +/- 0.78
Episode length: 321.20 +/- 67.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 321          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 1170000      |
| train/                  |              |
|    approx_kl            | 1.770616e-05 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000857    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.024        |
|    n_updates            | 2850         |
|    policy_gradient_loss | -3.24e-06    |
|    value_loss           | 0.0698       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 286      |
|    time_elapsed    | 1929     |
|    total_timesteps | 1171456  |
---------------------------------
Eval num_timesteps=1175000, episode_reward=0.40 +/- 0.49
Episode length: 302.80 +/- 64.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1175000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000716 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0356    |
|    n_updates            | 2860      |
|    policy_gradient_loss | 1.96e-10  |
|    value_loss           | 0.0587    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 287      |
|    time_elapsed    | 1936     |
|    total_timesteps | 1175552  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.51      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 288       |
|    time_elapsed         | 1941      |
|    total_timesteps      | 1179648   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000716 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0441    |
|    n_updates            | 2870      |
|    policy_gradient_loss | 1.35e-09  |
|    value_loss           | 0.0548    |
---------------------------------------
Eval num_timesteps=1180000, episode_reward=0.20 +/- 0.40
Episode length: 298.20 +/- 62.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 1180000      |
| train/                  |              |
|    approx_kl            | 1.042994e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000881    |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0388       |
|    n_updates            | 2880         |
|    policy_gradient_loss | 2.81e-07     |
|    value_loss           | 0.0613       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 289      |
|    time_elapsed    | 1948     |
|    total_timesteps | 1183744  |
---------------------------------
Eval num_timesteps=1185000, episode_reward=0.50 +/- 0.50
Episode length: 309.80 +/- 42.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1185000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000713 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0329    |
|    n_updates            | 2890      |
|    policy_gradient_loss | -2e-10    |
|    value_loss           | 0.0589    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 290      |
|    time_elapsed    | 1955     |
|    total_timesteps | 1187840  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=0.30 +/- 0.46
Episode length: 318.60 +/- 41.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 319       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1190000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000713 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0199    |
|    n_updates            | 2900      |
|    policy_gradient_loss | -1.61e-10 |
|    value_loss           | 0.0558    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 291      |
|    time_elapsed    | 1963     |
|    total_timesteps | 1191936  |
---------------------------------
Eval num_timesteps=1195000, episode_reward=0.00 +/- 0.00
Episode length: 290.00 +/- 32.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 1195000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000713 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0191    |
|    n_updates            | 2910      |
|    policy_gradient_loss | 6e-10     |
|    value_loss           | 0.0551    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 292      |
|    time_elapsed    | 1969     |
|    total_timesteps | 1196032  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=0.30 +/- 0.46
Episode length: 287.60 +/- 37.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1200000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000713 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0301    |
|    n_updates            | 2920      |
|    policy_gradient_loss | -6.04e-11 |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 293      |
|    time_elapsed    | 1977     |
|    total_timesteps | 1200128  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.38      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 294       |
|    time_elapsed         | 1982      |
|    total_timesteps      | 1204224   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000713 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0354    |
|    n_updates            | 2930      |
|    policy_gradient_loss | 9.27e-10  |
|    value_loss           | 0.0581    |
---------------------------------------
Eval num_timesteps=1205000, episode_reward=0.60 +/- 0.49
Episode length: 327.40 +/- 40.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 327       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1205000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000713 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0384    |
|    n_updates            | 2940      |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 0.0622    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 295      |
|    time_elapsed    | 1989     |
|    total_timesteps | 1208320  |
---------------------------------
Eval num_timesteps=1210000, episode_reward=0.20 +/- 0.40
Episode length: 293.00 +/- 37.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 1210000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000713 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0383    |
|    n_updates            | 2950      |
|    policy_gradient_loss | 8.27e-10  |
|    value_loss           | 0.0602    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 296      |
|    time_elapsed    | 1996     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1215000, episode_reward=0.50 +/- 0.67
Episode length: 275.60 +/- 38.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 276       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1215000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000713 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.043     |
|    n_updates            | 2960      |
|    policy_gradient_loss | 5.57e-11  |
|    value_loss           | 0.0631    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 297      |
|    time_elapsed    | 2003     |
|    total_timesteps | 1216512  |
---------------------------------
Eval num_timesteps=1220000, episode_reward=0.70 +/- 0.46
Episode length: 354.00 +/- 61.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 354          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 1220000      |
| train/                  |              |
|    approx_kl            | 8.978532e-09 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000826    |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0406       |
|    n_updates            | 2970         |
|    policy_gradient_loss | 1.58e-06     |
|    value_loss           | 0.0593       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 298      |
|    time_elapsed    | 2010     |
|    total_timesteps | 1220608  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 302          |
|    ep_rew_mean          | 0.45         |
| time/                   |              |
|    fps                  | 607          |
|    iterations           | 299          |
|    time_elapsed         | 2016         |
|    total_timesteps      | 1224704      |
| train/                  |              |
|    approx_kl            | 3.961977e-06 |
|    clip_fraction        | 0.000269     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000727    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0402       |
|    n_updates            | 2980         |
|    policy_gradient_loss | -1.78e-06    |
|    value_loss           | 0.065        |
------------------------------------------
Eval num_timesteps=1225000, episode_reward=0.50 +/- 0.50
Episode length: 298.00 +/- 36.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1225000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000944 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0359    |
|    n_updates            | 2990      |
|    policy_gradient_loss | -3.75e-10 |
|    value_loss           | 0.0593    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 300      |
|    time_elapsed    | 2023     |
|    total_timesteps | 1228800  |
---------------------------------
Eval num_timesteps=1230000, episode_reward=0.70 +/- 0.46
Episode length: 304.00 +/- 58.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 1230000       |
| train/                  |               |
|    approx_kl            | 1.2382923e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000913     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0271        |
|    n_updates            | 3000          |
|    policy_gradient_loss | -2.87e-06     |
|    value_loss           | 0.0627        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 301      |
|    time_elapsed    | 2030     |
|    total_timesteps | 1232896  |
---------------------------------
Eval num_timesteps=1235000, episode_reward=0.60 +/- 0.66
Episode length: 280.00 +/- 57.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 280       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1235000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000783 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0227    |
|    n_updates            | 3010      |
|    policy_gradient_loss | 4.95e-10  |
|    value_loss           | 0.0574    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 302      |
|    time_elapsed    | 2037     |
|    total_timesteps | 1236992  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=0.10 +/- 0.30
Episode length: 294.00 +/- 26.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 1240000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000783 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0466    |
|    n_updates            | 3020      |
|    policy_gradient_loss | -8.88e-11 |
|    value_loss           | 0.0634    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 303      |
|    time_elapsed    | 2044     |
|    total_timesteps | 1241088  |
---------------------------------
Eval num_timesteps=1245000, episode_reward=0.60 +/- 0.66
Episode length: 303.60 +/- 41.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1245000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000783 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0279    |
|    n_updates            | 3030      |
|    policy_gradient_loss | -3.96e-10 |
|    value_loss           | 0.0638    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 304      |
|    time_elapsed    | 2051     |
|    total_timesteps | 1245184  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.49      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 305       |
|    time_elapsed         | 2056      |
|    total_timesteps      | 1249280   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000783 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0447    |
|    n_updates            | 3040      |
|    policy_gradient_loss | -3.53e-10 |
|    value_loss           | 0.0679    |
---------------------------------------
Eval num_timesteps=1250000, episode_reward=0.70 +/- 0.64
Episode length: 306.60 +/- 49.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 307          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 1250000      |
| train/                  |              |
|    approx_kl            | 9.460404e-06 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00109     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0206       |
|    n_updates            | 3050         |
|    policy_gradient_loss | -4.53e-06    |
|    value_loss           | 0.0587       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 306      |
|    time_elapsed    | 2063     |
|    total_timesteps | 1253376  |
---------------------------------
Eval num_timesteps=1255000, episode_reward=0.60 +/- 0.92
Episode length: 306.60 +/- 66.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1255000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000966 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0165    |
|    n_updates            | 3060      |
|    policy_gradient_loss | 1.44e-09  |
|    value_loss           | 0.0621    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 307      |
|    time_elapsed    | 2070     |
|    total_timesteps | 1257472  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=0.50 +/- 0.67
Episode length: 297.20 +/- 53.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1260000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000966 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0321    |
|    n_updates            | 3070      |
|    policy_gradient_loss | 3.82e-10  |
|    value_loss           | 0.0599    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 308      |
|    time_elapsed    | 2078     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1265000, episode_reward=0.40 +/- 0.49
Episode length: 326.00 +/- 62.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 326       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1265000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000966 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0378    |
|    n_updates            | 3080      |
|    policy_gradient_loss | -3.74e-10 |
|    value_loss           | 0.0597    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 309      |
|    time_elapsed    | 2085     |
|    total_timesteps | 1265664  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.54      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 310       |
|    time_elapsed         | 2090      |
|    total_timesteps      | 1269760   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000966 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0311    |
|    n_updates            | 3090      |
|    policy_gradient_loss | -2.95e-10 |
|    value_loss           | 0.0635    |
---------------------------------------
Eval num_timesteps=1270000, episode_reward=0.40 +/- 0.66
Episode length: 307.00 +/- 64.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1270000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000966 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0286    |
|    n_updates            | 3100      |
|    policy_gradient_loss | -1.19e-09 |
|    value_loss           | 0.0674    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 311      |
|    time_elapsed    | 2097     |
|    total_timesteps | 1273856  |
---------------------------------
Eval num_timesteps=1275000, episode_reward=0.30 +/- 0.64
Episode length: 288.80 +/- 45.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1275000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000966 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0419    |
|    n_updates            | 3110      |
|    policy_gradient_loss | 2.41e-10  |
|    value_loss           | 0.0583    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 312      |
|    time_elapsed    | 2104     |
|    total_timesteps | 1277952  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=0.30 +/- 0.46
Episode length: 300.20 +/- 50.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1280000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000966 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0552    |
|    n_updates            | 3120      |
|    policy_gradient_loss | -3.07e-10 |
|    value_loss           | 0.0612    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 313      |
|    time_elapsed    | 2111     |
|    total_timesteps | 1282048  |
---------------------------------
Eval num_timesteps=1285000, episode_reward=0.30 +/- 0.46
Episode length: 289.80 +/- 46.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 290           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 1285000       |
| train/                  |               |
|    approx_kl            | 1.9533909e-05 |
|    clip_fraction        | 0.000464      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00138      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0296        |
|    n_updates            | 3130          |
|    policy_gradient_loss | -0.000113     |
|    value_loss           | 0.0698        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 314      |
|    time_elapsed    | 2118     |
|    total_timesteps | 1286144  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=0.60 +/- 0.49
Episode length: 327.00 +/- 67.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 327          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 1290000      |
| train/                  |              |
|    approx_kl            | 1.177279e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00114     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.0411       |
|    n_updates            | 3140         |
|    policy_gradient_loss | 1.02e-07     |
|    value_loss           | 0.0623       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 315      |
|    time_elapsed    | 2126     |
|    total_timesteps | 1290240  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 297       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 316       |
|    time_elapsed         | 2130      |
|    total_timesteps      | 1294336   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00111  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0442    |
|    n_updates            | 3150      |
|    policy_gradient_loss | -9.42e-10 |
|    value_loss           | 0.0616    |
---------------------------------------
Eval num_timesteps=1295000, episode_reward=0.40 +/- 0.66
Episode length: 305.60 +/- 42.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 1295000       |
| train/                  |               |
|    approx_kl            | 5.1618437e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00174      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.00409       |
|    n_updates            | 3160          |
|    policy_gradient_loss | -9.82e-05     |
|    value_loss           | 0.0571        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 317      |
|    time_elapsed    | 2138     |
|    total_timesteps | 1298432  |
---------------------------------
Eval num_timesteps=1300000, episode_reward=0.30 +/- 0.46
Episode length: 309.00 +/- 50.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1300000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00178  |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0239    |
|    n_updates            | 3170      |
|    policy_gradient_loss | -9.01e-10 |
|    value_loss           | 0.0569    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 318      |
|    time_elapsed    | 2145     |
|    total_timesteps | 1302528  |
---------------------------------
Eval num_timesteps=1305000, episode_reward=0.40 +/- 0.66
Episode length: 290.60 +/- 44.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 1305000      |
| train/                  |              |
|    approx_kl            | 9.322088e-06 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00157     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0171       |
|    n_updates            | 3180         |
|    policy_gradient_loss | -1.02e-06    |
|    value_loss           | 0.0593       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 607      |
|    iterations      | 319      |
|    time_elapsed    | 2152     |
|    total_timesteps | 1306624  |
---------------------------------
Eval num_timesteps=1310000, episode_reward=0.80 +/- 0.87
Episode length: 346.80 +/- 58.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 347          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 1310000      |
| train/                  |              |
|    approx_kl            | 8.320043e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0016      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0239       |
|    n_updates            | 3190         |
|    policy_gradient_loss | -2.71e-06    |
|    value_loss           | 0.062        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 320      |
|    time_elapsed    | 2159     |
|    total_timesteps | 1310720  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 301           |
|    ep_rew_mean          | 0.36          |
| time/                   |               |
|    fps                  | 607           |
|    iterations           | 321           |
|    time_elapsed         | 2165          |
|    total_timesteps      | 1314816       |
| train/                  |               |
|    approx_kl            | 1.1168333e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00129      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0269        |
|    n_updates            | 3200          |
|    policy_gradient_loss | -1.03e-05     |
|    value_loss           | 0.0579        |
-------------------------------------------
Eval num_timesteps=1315000, episode_reward=0.20 +/- 0.40
Episode length: 278.00 +/- 50.66
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 278      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 1315000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00125 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0283   |
|    n_updates            | 3210     |
|    policy_gradient_loss | 2.5e-10  |
|    value_loss           | 0.0593   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 322      |
|    time_elapsed    | 2171     |
|    total_timesteps | 1318912  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=0.30 +/- 0.46
Episode length: 300.40 +/- 40.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1320000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00125  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0208    |
|    n_updates            | 3220      |
|    policy_gradient_loss | -6.26e-11 |
|    value_loss           | 0.0547    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 323      |
|    time_elapsed    | 2179     |
|    total_timesteps | 1323008  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=0.60 +/- 0.49
Episode length: 321.60 +/- 53.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 322           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 1325000       |
| train/                  |               |
|    approx_kl            | 4.8641523e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0014       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.00952       |
|    n_updates            | 3230          |
|    policy_gradient_loss | 3.91e-06      |
|    value_loss           | 0.0571        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 324      |
|    time_elapsed    | 2186     |
|    total_timesteps | 1327104  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=0.20 +/- 0.40
Episode length: 298.80 +/- 40.82
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 299      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 1330000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00134 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0536   |
|    n_updates            | 3240     |
|    policy_gradient_loss | 3.5e-10  |
|    value_loss           | 0.0594   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.32     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 325      |
|    time_elapsed    | 2193     |
|    total_timesteps | 1331200  |
---------------------------------
Eval num_timesteps=1335000, episode_reward=0.50 +/- 0.67
Episode length: 309.60 +/- 66.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 1335000      |
| train/                  |              |
|    approx_kl            | 2.165616e-07 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00162     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0243       |
|    n_updates            | 3250         |
|    policy_gradient_loss | 1.52e-06     |
|    value_loss           | 0.0593       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 326      |
|    time_elapsed    | 2200     |
|    total_timesteps | 1335296  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 297      |
|    ep_rew_mean          | 0.35     |
| time/                   |          |
|    fps                  | 607      |
|    iterations           | 327      |
|    time_elapsed         | 2205     |
|    total_timesteps      | 1339392  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00137 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0427   |
|    n_updates            | 3260     |
|    policy_gradient_loss | 7.33e-10 |
|    value_loss           | 0.0646   |
--------------------------------------
Eval num_timesteps=1340000, episode_reward=0.10 +/- 0.30
Episode length: 312.60 +/- 25.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 313      |
|    mean_reward          | 0.1      |
| time/                   |          |
|    total_timesteps      | 1340000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00137 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0307   |
|    n_updates            | 3270     |
|    policy_gradient_loss | 7.85e-10 |
|    value_loss           | 0.058    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 328      |
|    time_elapsed    | 2212     |
|    total_timesteps | 1343488  |
---------------------------------
Eval num_timesteps=1345000, episode_reward=0.30 +/- 0.46
Episode length: 306.00 +/- 62.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 1345000       |
| train/                  |               |
|    approx_kl            | 5.3723925e-06 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00158      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0268        |
|    n_updates            | 3280          |
|    policy_gradient_loss | 7.91e-07      |
|    value_loss           | 0.0601        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 329      |
|    time_elapsed    | 2219     |
|    total_timesteps | 1347584  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=0.30 +/- 0.46
Episode length: 290.00 +/- 53.16
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 290      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 1350000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00159 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0176   |
|    n_updates            | 3290     |
|    policy_gradient_loss | 3.03e-10 |
|    value_loss           | 0.0597   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 330      |
|    time_elapsed    | 2226     |
|    total_timesteps | 1351680  |
---------------------------------
Eval num_timesteps=1355000, episode_reward=0.40 +/- 0.66
Episode length: 295.40 +/- 60.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 295      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 1355000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00159 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0423   |
|    n_updates            | 3300     |
|    policy_gradient_loss | 1.7e-10  |
|    value_loss           | 0.0627   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 331      |
|    time_elapsed    | 2234     |
|    total_timesteps | 1355776  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 311           |
|    ep_rew_mean          | 0.52          |
| time/                   |               |
|    fps                  | 607           |
|    iterations           | 332           |
|    time_elapsed         | 2239          |
|    total_timesteps      | 1359872       |
| train/                  |               |
|    approx_kl            | 0.00010504956 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00257      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0441        |
|    n_updates            | 3310          |
|    policy_gradient_loss | -0.000145     |
|    value_loss           | 0.062         |
-------------------------------------------
Eval num_timesteps=1360000, episode_reward=0.40 +/- 0.49
Episode length: 320.00 +/- 48.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 320           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 1360000       |
| train/                  |               |
|    approx_kl            | 4.4649787e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00215      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.041         |
|    n_updates            | 3320          |
|    policy_gradient_loss | -1.51e-06     |
|    value_loss           | 0.0648        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 333      |
|    time_elapsed    | 2246     |
|    total_timesteps | 1363968  |
---------------------------------
Eval num_timesteps=1365000, episode_reward=0.30 +/- 0.64
Episode length: 275.80 +/- 39.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 1365000      |
| train/                  |              |
|    approx_kl            | 9.478565e-06 |
|    clip_fraction        | 0.000366     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00239     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0306       |
|    n_updates            | 3330         |
|    policy_gradient_loss | -4.58e-06    |
|    value_loss           | 0.0607       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 334      |
|    time_elapsed    | 2253     |
|    total_timesteps | 1368064  |
---------------------------------
Eval num_timesteps=1370000, episode_reward=0.60 +/- 0.66
Episode length: 323.00 +/- 59.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 323      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 1370000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00242 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0646   |
|    n_updates            | 3340     |
|    policy_gradient_loss | 9.85e-10 |
|    value_loss           | 0.0597   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 335      |
|    time_elapsed    | 2260     |
|    total_timesteps | 1372160  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=0.40 +/- 0.49
Episode length: 275.60 +/- 54.89
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 276      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 1375000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00242 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0369   |
|    n_updates            | 3350     |
|    policy_gradient_loss | 7.28e-12 |
|    value_loss           | 0.0635   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 336      |
|    time_elapsed    | 2267     |
|    total_timesteps | 1376256  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=0.80 +/- 0.87
Episode length: 308.20 +/- 42.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 308          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 1380000      |
| train/                  |              |
|    approx_kl            | 3.438574e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00261     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0273       |
|    n_updates            | 3360         |
|    policy_gradient_loss | 1.36e-07     |
|    value_loss           | 0.0527       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 337      |
|    time_elapsed    | 2274     |
|    total_timesteps | 1380352  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 312           |
|    ep_rew_mean          | 0.54          |
| time/                   |               |
|    fps                  | 607           |
|    iterations           | 338           |
|    time_elapsed         | 2279          |
|    total_timesteps      | 1384448       |
| train/                  |               |
|    approx_kl            | 3.3122138e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0018       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0515        |
|    n_updates            | 3370          |
|    policy_gradient_loss | -4.36e-05     |
|    value_loss           | 0.0723        |
-------------------------------------------
Eval num_timesteps=1385000, episode_reward=0.70 +/- 0.46
Episode length: 339.60 +/- 54.68
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 340      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 1385000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00175 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0266   |
|    n_updates            | 3380     |
|    policy_gradient_loss | 1.3e-10  |
|    value_loss           | 0.0634   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 339      |
|    time_elapsed    | 2287     |
|    total_timesteps | 1388544  |
---------------------------------
Eval num_timesteps=1390000, episode_reward=0.50 +/- 0.67
Episode length: 289.60 +/- 43.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1390000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00175  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0186    |
|    n_updates            | 3390      |
|    policy_gradient_loss | -3.16e-10 |
|    value_loss           | 0.0562    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 340      |
|    time_elapsed    | 2294     |
|    total_timesteps | 1392640  |
---------------------------------
Eval num_timesteps=1395000, episode_reward=0.40 +/- 0.49
Episode length: 299.20 +/- 59.82
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 299      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 1395000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00175 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0215   |
|    n_updates            | 3400     |
|    policy_gradient_loss | 1.08e-09 |
|    value_loss           | 0.055    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 341      |
|    time_elapsed    | 2301     |
|    total_timesteps | 1396736  |
---------------------------------
Eval num_timesteps=1400000, episode_reward=0.70 +/- 0.64
Episode length: 326.40 +/- 52.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 326          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 1400000      |
| train/                  |              |
|    approx_kl            | 5.244481e-06 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00174     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.00788      |
|    n_updates            | 3410         |
|    policy_gradient_loss | 3.34e-08     |
|    value_loss           | 0.0671       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 342      |
|    time_elapsed    | 2308     |
|    total_timesteps | 1400832  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.54      |
| time/                   |           |
|    fps                  | 607       |
|    iterations           | 343       |
|    time_elapsed         | 2313      |
|    total_timesteps      | 1404928   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00168  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0253    |
|    n_updates            | 3420      |
|    policy_gradient_loss | -3.47e-10 |
|    value_loss           | 0.0586    |
---------------------------------------
Eval num_timesteps=1405000, episode_reward=0.40 +/- 0.49
Episode length: 321.40 +/- 53.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 321           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 1405000       |
| train/                  |               |
|    approx_kl            | 1.0103351e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00167      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0418        |
|    n_updates            | 3430          |
|    policy_gradient_loss | 1.48e-06      |
|    value_loss           | 0.0617        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 607      |
|    iterations      | 344      |
|    time_elapsed    | 2321     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=0.70 +/- 0.90
Episode length: 316.40 +/- 79.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 1410000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00134  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0523    |
|    n_updates            | 3440      |
|    policy_gradient_loss | -7.74e-10 |
|    value_loss           | 0.0667    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 345      |
|    time_elapsed    | 2328     |
|    total_timesteps | 1413120  |
---------------------------------
Eval num_timesteps=1415000, episode_reward=0.50 +/- 0.92
Episode length: 322.60 +/- 37.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 323          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 1415000      |
| train/                  |              |
|    approx_kl            | 4.052004e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0014      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0195       |
|    n_updates            | 3450         |
|    policy_gradient_loss | -1.15e-05    |
|    value_loss           | 0.0669       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 346      |
|    time_elapsed    | 2335     |
|    total_timesteps | 1417216  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=0.60 +/- 0.80
Episode length: 327.00 +/- 49.79
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 327      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 1420000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00132 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0356   |
|    n_updates            | 3460     |
|    policy_gradient_loss | 1e-09    |
|    value_loss           | 0.0593   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 347      |
|    time_elapsed    | 2342     |
|    total_timesteps | 1421312  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=0.50 +/- 0.67
Episode length: 311.20 +/- 75.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 311          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 1425000      |
| train/                  |              |
|    approx_kl            | 9.723633e-06 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00142     |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0386       |
|    n_updates            | 3470         |
|    policy_gradient_loss | -5.72e-06    |
|    value_loss           | 0.0582       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 348      |
|    time_elapsed    | 2350     |
|    total_timesteps | 1425408  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 302      |
|    ep_rew_mean          | 0.5      |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 349      |
|    time_elapsed         | 2355     |
|    total_timesteps      | 1429504  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00142 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0345   |
|    n_updates            | 3480     |
|    policy_gradient_loss | 1.72e-10 |
|    value_loss           | 0.0591   |
--------------------------------------
Eval num_timesteps=1430000, episode_reward=0.30 +/- 0.64
Episode length: 286.20 +/- 54.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1430000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00142  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0289    |
|    n_updates            | 3490      |
|    policy_gradient_loss | -1.05e-10 |
|    value_loss           | 0.0677    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 350      |
|    time_elapsed    | 2362     |
|    total_timesteps | 1433600  |
---------------------------------
Eval num_timesteps=1435000, episode_reward=0.50 +/- 0.50
Episode length: 308.40 +/- 52.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 308           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 1435000       |
| train/                  |               |
|    approx_kl            | 2.9449904e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0011       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0105        |
|    n_updates            | 3500          |
|    policy_gradient_loss | -6.03e-05     |
|    value_loss           | 0.0571        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 351      |
|    time_elapsed    | 2369     |
|    total_timesteps | 1437696  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=0.50 +/- 0.67
Episode length: 311.60 +/- 39.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 1440000      |
| train/                  |              |
|    approx_kl            | 2.089655e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00104     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0237       |
|    n_updates            | 3510         |
|    policy_gradient_loss | 9.27e-08     |
|    value_loss           | 0.0642       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 352      |
|    time_elapsed    | 2376     |
|    total_timesteps | 1441792  |
---------------------------------
Eval num_timesteps=1445000, episode_reward=0.80 +/- 0.98
Episode length: 307.80 +/- 52.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 308           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 1445000       |
| train/                  |               |
|    approx_kl            | 1.0326985e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00113      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0347        |
|    n_updates            | 3520          |
|    policy_gradient_loss | -1.86e-06     |
|    value_loss           | 0.0571        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 353      |
|    time_elapsed    | 2383     |
|    total_timesteps | 1445888  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 295      |
|    ep_rew_mean          | 0.41     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 354      |
|    time_elapsed         | 2388     |
|    total_timesteps      | 1449984  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00124 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0537   |
|    n_updates            | 3530     |
|    policy_gradient_loss | 7.85e-10 |
|    value_loss           | 0.0651   |
--------------------------------------
Eval num_timesteps=1450000, episode_reward=0.20 +/- 0.40
Episode length: 285.20 +/- 43.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 1450000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00124  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0449    |
|    n_updates            | 3540      |
|    policy_gradient_loss | -3.65e-10 |
|    value_loss           | 0.0576    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 355      |
|    time_elapsed    | 2396     |
|    total_timesteps | 1454080  |
---------------------------------
Eval num_timesteps=1455000, episode_reward=0.60 +/- 0.80
Episode length: 308.40 +/- 43.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 308         |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 1455000     |
| train/                  |             |
|    approx_kl            | 9.48325e-06 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00117    |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0361      |
|    n_updates            | 3550        |
|    policy_gradient_loss | -1.22e-06   |
|    value_loss           | 0.0665      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 356      |
|    time_elapsed    | 2403     |
|    total_timesteps | 1458176  |
---------------------------------
Eval num_timesteps=1460000, episode_reward=0.70 +/- 0.90
Episode length: 308.80 +/- 56.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 1460000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000973 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0524    |
|    n_updates            | 3560      |
|    policy_gradient_loss | -2.9e-10  |
|    value_loss           | 0.0558    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 357      |
|    time_elapsed    | 2410     |
|    total_timesteps | 1462272  |
---------------------------------
Eval num_timesteps=1465000, episode_reward=0.30 +/- 0.46
Episode length: 300.20 +/- 44.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 1465000       |
| train/                  |               |
|    approx_kl            | 1.6618098e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000946     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.036         |
|    n_updates            | 3570          |
|    policy_gradient_loss | 2.64e-06      |
|    value_loss           | 0.0585        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 358      |
|    time_elapsed    | 2417     |
|    total_timesteps | 1466368  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=0.30 +/- 0.64
Episode length: 294.00 +/- 42.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 294           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 1470000       |
| train/                  |               |
|    approx_kl            | 1.1439828e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00072      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0211        |
|    n_updates            | 3580          |
|    policy_gradient_loss | -4.82e-06     |
|    value_loss           | 0.0661        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 359      |
|    time_elapsed    | 2424     |
|    total_timesteps | 1470464  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 360       |
|    time_elapsed         | 2429      |
|    total_timesteps      | 1474560   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0007   |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.038     |
|    n_updates            | 3590      |
|    policy_gradient_loss | 6.55e-11  |
|    value_loss           | 0.0601    |
---------------------------------------
Eval num_timesteps=1475000, episode_reward=0.30 +/- 0.46
Episode length: 282.40 +/- 28.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 282       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1475000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0007   |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0203    |
|    n_updates            | 3600      |
|    policy_gradient_loss | -2.55e-10 |
|    value_loss           | 0.062     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 361      |
|    time_elapsed    | 2436     |
|    total_timesteps | 1478656  |
---------------------------------
Eval num_timesteps=1480000, episode_reward=0.40 +/- 0.49
Episode length: 323.60 +/- 47.34
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 324      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 1480000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0007  |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0219   |
|    n_updates            | 3610     |
|    policy_gradient_loss | 1.77e-10 |
|    value_loss           | 0.0673   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 362      |
|    time_elapsed    | 2444     |
|    total_timesteps | 1482752  |
---------------------------------
Eval num_timesteps=1485000, episode_reward=0.40 +/- 0.66
Episode length: 282.60 +/- 38.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 1485000      |
| train/                  |              |
|    approx_kl            | 2.832654e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000522    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0344       |
|    n_updates            | 3620         |
|    policy_gradient_loss | -9.52e-06    |
|    value_loss           | 0.0592       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 363      |
|    time_elapsed    | 2451     |
|    total_timesteps | 1486848  |
---------------------------------
Eval num_timesteps=1490000, episode_reward=0.60 +/- 0.49
Episode length: 301.80 +/- 58.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1490000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0212    |
|    n_updates            | 3630      |
|    policy_gradient_loss | -9.28e-10 |
|    value_loss           | 0.0638    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 364      |
|    time_elapsed    | 2458     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1495000, episode_reward=0.40 +/- 0.49
Episode length: 301.80 +/- 34.26
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 302      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 1495000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.032    |
|    n_updates            | 3640     |
|    policy_gradient_loss | 4.22e-10 |
|    value_loss           | 0.0627   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 365      |
|    time_elapsed    | 2465     |
|    total_timesteps | 1495040  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 305      |
|    ep_rew_mean          | 0.48     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 366      |
|    time_elapsed         | 2470     |
|    total_timesteps      | 1499136  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.04     |
|    n_updates            | 3650     |
|    policy_gradient_loss | 7.15e-10 |
|    value_loss           | 0.0705   |
--------------------------------------
Eval num_timesteps=1500000, episode_reward=0.30 +/- 0.46
Episode length: 305.80 +/- 35.79
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 306      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 1500000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0427   |
|    n_updates            | 3660     |
|    policy_gradient_loss | -5.4e-10 |
|    value_loss           | 0.0536   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 367      |
|    time_elapsed    | 2477     |
|    total_timesteps | 1503232  |
---------------------------------
Eval num_timesteps=1505000, episode_reward=0.70 +/- 0.64
Episode length: 299.80 +/- 40.89
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 300      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 1505000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0204   |
|    n_updates            | 3670     |
|    policy_gradient_loss | 1.04e-10 |
|    value_loss           | 0.0563   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 368      |
|    time_elapsed    | 2484     |
|    total_timesteps | 1507328  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=0.20 +/- 0.40
Episode length: 270.00 +/- 25.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 270       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 1510000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0352    |
|    n_updates            | 3680      |
|    policy_gradient_loss | -1.29e-10 |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 369      |
|    time_elapsed    | 2491     |
|    total_timesteps | 1511424  |
---------------------------------
Eval num_timesteps=1515000, episode_reward=0.60 +/- 0.92
Episode length: 310.20 +/- 56.50
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 310      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 1515000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0471   |
|    n_updates            | 3690     |
|    policy_gradient_loss | 7.01e-10 |
|    value_loss           | 0.0582   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 370      |
|    time_elapsed    | 2499     |
|    total_timesteps | 1515520  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 306       |
|    ep_rew_mean          | 0.45      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 371       |
|    time_elapsed         | 2504      |
|    total_timesteps      | 1519616   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0323    |
|    n_updates            | 3700      |
|    policy_gradient_loss | -8.07e-10 |
|    value_loss           | 0.0613    |
---------------------------------------
Eval num_timesteps=1520000, episode_reward=0.50 +/- 0.67
Episode length: 304.00 +/- 47.92
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 304      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 1520000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.02     |
|    n_updates            | 3710     |
|    policy_gradient_loss | 8.95e-11 |
|    value_loss           | 0.0597   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 372      |
|    time_elapsed    | 2511     |
|    total_timesteps | 1523712  |
---------------------------------
Eval num_timesteps=1525000, episode_reward=0.20 +/- 0.40
Episode length: 287.20 +/- 51.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 287      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 1525000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.019    |
|    n_updates            | 3720     |
|    policy_gradient_loss | -1.4e-10 |
|    value_loss           | 0.0585   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 373      |
|    time_elapsed    | 2518     |
|    total_timesteps | 1527808  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=0.70 +/- 0.64
Episode length: 317.40 +/- 69.24
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 317      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 1530000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0186   |
|    n_updates            | 3730     |
|    policy_gradient_loss | 4.95e-11 |
|    value_loss           | 0.0562   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 374      |
|    time_elapsed    | 2525     |
|    total_timesteps | 1531904  |
---------------------------------
Eval num_timesteps=1535000, episode_reward=0.40 +/- 0.66
Episode length: 306.60 +/- 64.13
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 307      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 1535000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0261   |
|    n_updates            | 3740     |
|    policy_gradient_loss | 3.74e-10 |
|    value_loss           | 0.0593   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 375      |
|    time_elapsed    | 2532     |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1540000, episode_reward=0.50 +/- 1.02
Episode length: 297.00 +/- 42.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 297      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 1540000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0235   |
|    n_updates            | 3750     |
|    policy_gradient_loss | 3.87e-10 |
|    value_loss           | 0.0609   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 376      |
|    time_elapsed    | 2539     |
|    total_timesteps | 1540096  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 306       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 377       |
|    time_elapsed         | 2544      |
|    total_timesteps      | 1544192   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0426    |
|    n_updates            | 3760      |
|    policy_gradient_loss | -1.07e-10 |
|    value_loss           | 0.0556    |
---------------------------------------
Eval num_timesteps=1545000, episode_reward=0.10 +/- 0.30
Episode length: 282.80 +/- 54.19
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 283      |
|    mean_reward          | 0.1      |
| time/                   |          |
|    total_timesteps      | 1545000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0554   |
|    n_updates            | 3770     |
|    policy_gradient_loss | 1.57e-10 |
|    value_loss           | 0.0669   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 378      |
|    time_elapsed    | 2551     |
|    total_timesteps | 1548288  |
---------------------------------
Eval num_timesteps=1550000, episode_reward=0.80 +/- 0.87
Episode length: 330.20 +/- 64.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 330       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 1550000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0389    |
|    n_updates            | 3780      |
|    policy_gradient_loss | 2.86e-10  |
|    value_loss           | 0.0587    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 379      |
|    time_elapsed    | 2559     |
|    total_timesteps | 1552384  |
---------------------------------
Eval num_timesteps=1555000, episode_reward=0.60 +/- 0.66
Episode length: 300.00 +/- 46.61
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 300      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 1555000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0297   |
|    n_updates            | 3790     |
|    policy_gradient_loss | 7.13e-10 |
|    value_loss           | 0.0626   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 380      |
|    time_elapsed    | 2566     |
|    total_timesteps | 1556480  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=0.20 +/- 0.40
Episode length: 301.20 +/- 50.97
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 301      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 1560000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.043    |
|    n_updates            | 3800     |
|    policy_gradient_loss | -1.1e-09 |
|    value_loss           | 0.0608   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 381      |
|    time_elapsed    | 2573     |
|    total_timesteps | 1560576  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.56      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 382       |
|    time_elapsed         | 2578      |
|    total_timesteps      | 1564672   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0354    |
|    n_updates            | 3810      |
|    policy_gradient_loss | -6.11e-11 |
|    value_loss           | 0.0616    |
---------------------------------------
Eval num_timesteps=1565000, episode_reward=0.10 +/- 0.30
Episode length: 255.20 +/- 41.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 255       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 1565000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0373    |
|    n_updates            | 3820      |
|    policy_gradient_loss | -5.54e-10 |
|    value_loss           | 0.0676    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 383      |
|    time_elapsed    | 2585     |
|    total_timesteps | 1568768  |
---------------------------------
Eval num_timesteps=1570000, episode_reward=0.40 +/- 0.49
Episode length: 293.80 +/- 45.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1570000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0293    |
|    n_updates            | 3830      |
|    policy_gradient_loss | -1.63e-10 |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 384      |
|    time_elapsed    | 2592     |
|    total_timesteps | 1572864  |
---------------------------------
Eval num_timesteps=1575000, episode_reward=0.40 +/- 0.66
Episode length: 287.40 +/- 47.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1575000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00054  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0141    |
|    n_updates            | 3840      |
|    policy_gradient_loss | -3.38e-11 |
|    value_loss           | 0.062     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 385      |
|    time_elapsed    | 2599     |
|    total_timesteps | 1576960  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=0.60 +/- 0.66
Episode length: 308.80 +/- 56.56
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 309      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 1580000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00054 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0325   |
|    n_updates            | 3850     |
|    policy_gradient_loss | 7.82e-11 |
|    value_loss           | 0.0647   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 386      |
|    time_elapsed    | 2606     |
|    total_timesteps | 1581056  |
---------------------------------
Eval num_timesteps=1585000, episode_reward=0.80 +/- 0.60
Episode length: 323.80 +/- 48.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 324           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 1585000       |
| train/                  |               |
|    approx_kl            | 4.9917333e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000792     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0252        |
|    n_updates            | 3860          |
|    policy_gradient_loss | -1.6e-06      |
|    value_loss           | 0.0611        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 387      |
|    time_elapsed    | 2614     |
|    total_timesteps | 1585152  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 388       |
|    time_elapsed         | 2619      |
|    total_timesteps      | 1589248   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000568 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0405    |
|    n_updates            | 3870      |
|    policy_gradient_loss | -7.04e-10 |
|    value_loss           | 0.0555    |
---------------------------------------
Eval num_timesteps=1590000, episode_reward=0.50 +/- 0.67
Episode length: 295.20 +/- 44.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1590000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000568 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0249    |
|    n_updates            | 3880      |
|    policy_gradient_loss | 2.47e-11  |
|    value_loss           | 0.0684    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 389      |
|    time_elapsed    | 2626     |
|    total_timesteps | 1593344  |
---------------------------------
Eval num_timesteps=1595000, episode_reward=0.10 +/- 0.30
Episode length: 286.60 +/- 40.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 1595000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000568 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.023     |
|    n_updates            | 3890      |
|    policy_gradient_loss | -5.22e-10 |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 390      |
|    time_elapsed    | 2633     |
|    total_timesteps | 1597440  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=0.20 +/- 0.60
Episode length: 273.00 +/- 41.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 273       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 1600000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000568 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0206    |
|    n_updates            | 3900      |
|    policy_gradient_loss | -2.36e-10 |
|    value_loss           | 0.055     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 391      |
|    time_elapsed    | 2640     |
|    total_timesteps | 1601536  |
---------------------------------
Eval num_timesteps=1605000, episode_reward=0.60 +/- 0.80
Episode length: 322.80 +/- 48.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 323       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1605000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000568 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0292    |
|    n_updates            | 3910      |
|    policy_gradient_loss | 3.59e-10  |
|    value_loss           | 0.0639    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 392      |
|    time_elapsed    | 2647     |
|    total_timesteps | 1605632  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 300           |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 393           |
|    time_elapsed         | 2652          |
|    total_timesteps      | 1609728       |
| train/                  |               |
|    approx_kl            | 5.6294375e-06 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000436     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0371        |
|    n_updates            | 3920          |
|    policy_gradient_loss | 6.14e-07      |
|    value_loss           | 0.0623        |
-------------------------------------------
Eval num_timesteps=1610000, episode_reward=0.80 +/- 0.87
Episode length: 343.20 +/- 50.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 343       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 1610000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0483    |
|    n_updates            | 3930      |
|    policy_gradient_loss | 5.36e-10  |
|    value_loss           | 0.0656    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 394      |
|    time_elapsed    | 2660     |
|    total_timesteps | 1613824  |
---------------------------------
Eval num_timesteps=1615000, episode_reward=0.90 +/- 0.83
Episode length: 315.20 +/- 51.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 1615000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0518    |
|    n_updates            | 3940      |
|    policy_gradient_loss | 5.01e-10  |
|    value_loss           | 0.0612    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 395      |
|    time_elapsed    | 2667     |
|    total_timesteps | 1617920  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=0.50 +/- 0.50
Episode length: 300.60 +/- 44.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1620000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0255    |
|    n_updates            | 3950      |
|    policy_gradient_loss | -3.66e-10 |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 396      |
|    time_elapsed    | 2674     |
|    total_timesteps | 1622016  |
---------------------------------
Eval num_timesteps=1625000, episode_reward=0.60 +/- 0.49
Episode length: 325.20 +/- 52.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1625000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0405    |
|    n_updates            | 3960      |
|    policy_gradient_loss | 3.99e-10  |
|    value_loss           | 0.0607    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 397      |
|    time_elapsed    | 2681     |
|    total_timesteps | 1626112  |
---------------------------------
Eval num_timesteps=1630000, episode_reward=0.40 +/- 0.49
Episode length: 297.00 +/- 64.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1630000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0266    |
|    n_updates            | 3970      |
|    policy_gradient_loss | -6.1e-10  |
|    value_loss           | 0.0525    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 398      |
|    time_elapsed    | 2688     |
|    total_timesteps | 1630208  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 298       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 399       |
|    time_elapsed         | 2693      |
|    total_timesteps      | 1634304   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0342    |
|    n_updates            | 3980      |
|    policy_gradient_loss | -3.03e-10 |
|    value_loss           | 0.0658    |
---------------------------------------
Eval num_timesteps=1635000, episode_reward=0.50 +/- 0.50
Episode length: 277.60 +/- 38.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1635000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0129    |
|    n_updates            | 3990      |
|    policy_gradient_loss | -5.3e-10  |
|    value_loss           | 0.0619    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 400      |
|    time_elapsed    | 2700     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1640000, episode_reward=0.40 +/- 0.49
Episode length: 319.40 +/- 67.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 319       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1640000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0406    |
|    n_updates            | 4000      |
|    policy_gradient_loss | 1.85e-10  |
|    value_loss           | 0.0571    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 401      |
|    time_elapsed    | 2707     |
|    total_timesteps | 1642496  |
---------------------------------
Eval num_timesteps=1645000, episode_reward=0.60 +/- 0.66
Episode length: 342.40 +/- 49.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 342       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1645000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0285    |
|    n_updates            | 4010      |
|    policy_gradient_loss | -1.07e-10 |
|    value_loss           | 0.0564    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 402      |
|    time_elapsed    | 2715     |
|    total_timesteps | 1646592  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=0.30 +/- 0.64
Episode length: 295.80 +/- 64.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1650000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0521    |
|    n_updates            | 4020      |
|    policy_gradient_loss | 4.63e-10  |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 403      |
|    time_elapsed    | 2722     |
|    total_timesteps | 1650688  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 302       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 404       |
|    time_elapsed         | 2727      |
|    total_timesteps      | 1654784   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0409    |
|    n_updates            | 4030      |
|    policy_gradient_loss | 8.4e-10   |
|    value_loss           | 0.0639    |
---------------------------------------
Eval num_timesteps=1655000, episode_reward=0.20 +/- 0.40
Episode length: 293.40 +/- 44.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 1655000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0257    |
|    n_updates            | 4040      |
|    policy_gradient_loss | -7.04e-10 |
|    value_loss           | 0.0543    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 405      |
|    time_elapsed    | 2734     |
|    total_timesteps | 1658880  |
---------------------------------
Eval num_timesteps=1660000, episode_reward=0.30 +/- 0.46
Episode length: 286.60 +/- 49.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1660000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0325    |
|    n_updates            | 4050      |
|    policy_gradient_loss | -2.91e-12 |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 406      |
|    time_elapsed    | 2741     |
|    total_timesteps | 1662976  |
---------------------------------
Eval num_timesteps=1665000, episode_reward=0.30 +/- 0.46
Episode length: 311.40 +/- 59.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1665000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0148    |
|    n_updates            | 4060      |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 0.0645    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 407      |
|    time_elapsed    | 2748     |
|    total_timesteps | 1667072  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=0.30 +/- 0.46
Episode length: 305.40 +/- 24.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1670000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0505    |
|    n_updates            | 4070      |
|    policy_gradient_loss | -1.02e-11 |
|    value_loss           | 0.066     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 408      |
|    time_elapsed    | 2755     |
|    total_timesteps | 1671168  |
---------------------------------
Eval num_timesteps=1675000, episode_reward=0.80 +/- 0.60
Episode length: 299.80 +/- 43.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 1675000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0231    |
|    n_updates            | 4080      |
|    policy_gradient_loss | -7.06e-10 |
|    value_loss           | 0.06      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 409      |
|    time_elapsed    | 2762     |
|    total_timesteps | 1675264  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.56      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 410       |
|    time_elapsed         | 2767      |
|    total_timesteps      | 1679360   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0253    |
|    n_updates            | 4090      |
|    policy_gradient_loss | -1.08e-10 |
|    value_loss           | 0.0621    |
---------------------------------------
Eval num_timesteps=1680000, episode_reward=0.10 +/- 0.30
Episode length: 298.20 +/- 62.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 1680000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0405    |
|    n_updates            | 4100      |
|    policy_gradient_loss | 7.73e-10  |
|    value_loss           | 0.0649    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 411      |
|    time_elapsed    | 2775     |
|    total_timesteps | 1683456  |
---------------------------------
Eval num_timesteps=1685000, episode_reward=0.30 +/- 0.46
Episode length: 306.60 +/- 50.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1685000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0283    |
|    n_updates            | 4110      |
|    policy_gradient_loss | -1.96e-10 |
|    value_loss           | 0.064     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 412      |
|    time_elapsed    | 2782     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=0.30 +/- 0.46
Episode length: 295.00 +/- 25.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1690000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0327    |
|    n_updates            | 4120      |
|    policy_gradient_loss | 8.41e-10  |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 413      |
|    time_elapsed    | 2789     |
|    total_timesteps | 1691648  |
---------------------------------
Eval num_timesteps=1695000, episode_reward=0.30 +/- 0.46
Episode length: 302.20 +/- 27.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1695000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0411    |
|    n_updates            | 4130      |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 0.0554    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 414      |
|    time_elapsed    | 2796     |
|    total_timesteps | 1695744  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 307       |
|    ep_rew_mean          | 0.51      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 415       |
|    time_elapsed         | 2801      |
|    total_timesteps      | 1699840   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0162    |
|    n_updates            | 4140      |
|    policy_gradient_loss | -2.49e-10 |
|    value_loss           | 0.0605    |
---------------------------------------
Eval num_timesteps=1700000, episode_reward=0.40 +/- 0.49
Episode length: 316.60 +/- 68.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1700000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0277    |
|    n_updates            | 4150      |
|    policy_gradient_loss | -4.57e-10 |
|    value_loss           | 0.0619    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 416      |
|    time_elapsed    | 2808     |
|    total_timesteps | 1703936  |
---------------------------------
Eval num_timesteps=1705000, episode_reward=0.50 +/- 0.67
Episode length: 295.40 +/- 27.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1705000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0362    |
|    n_updates            | 4160      |
|    policy_gradient_loss | 1.76e-10  |
|    value_loss           | 0.0697    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 417      |
|    time_elapsed    | 2815     |
|    total_timesteps | 1708032  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=0.60 +/- 0.92
Episode length: 311.20 +/- 64.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1710000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.056     |
|    n_updates            | 4170      |
|    policy_gradient_loss | 1.09e-09  |
|    value_loss           | 0.0602    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 418      |
|    time_elapsed    | 2823     |
|    total_timesteps | 1712128  |
---------------------------------
Eval num_timesteps=1715000, episode_reward=0.30 +/- 0.64
Episode length: 292.80 +/- 51.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1715000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0328    |
|    n_updates            | 4180      |
|    policy_gradient_loss | -8.69e-10 |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 419      |
|    time_elapsed    | 2830     |
|    total_timesteps | 1716224  |
---------------------------------
Eval num_timesteps=1720000, episode_reward=0.10 +/- 0.30
Episode length: 308.20 +/- 62.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 1720000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0286    |
|    n_updates            | 4190      |
|    policy_gradient_loss | 4.37e-10  |
|    value_loss           | 0.0584    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 420      |
|    time_elapsed    | 2837     |
|    total_timesteps | 1720320  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.49      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 421       |
|    time_elapsed         | 2842      |
|    total_timesteps      | 1724416   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0281    |
|    n_updates            | 4200      |
|    policy_gradient_loss | 4.58e-11  |
|    value_loss           | 0.0641    |
---------------------------------------
Eval num_timesteps=1725000, episode_reward=0.40 +/- 0.66
Episode length: 302.40 +/- 54.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1725000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0383    |
|    n_updates            | 4210      |
|    policy_gradient_loss | 4.45e-10  |
|    value_loss           | 0.0554    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 422      |
|    time_elapsed    | 2849     |
|    total_timesteps | 1728512  |
---------------------------------
Eval num_timesteps=1730000, episode_reward=0.60 +/- 0.66
Episode length: 331.80 +/- 42.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 332       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1730000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0242    |
|    n_updates            | 4220      |
|    policy_gradient_loss | -3.75e-10 |
|    value_loss           | 0.068     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 423      |
|    time_elapsed    | 2857     |
|    total_timesteps | 1732608  |
---------------------------------
Eval num_timesteps=1735000, episode_reward=1.20 +/- 0.98
Episode length: 307.00 +/- 38.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 1.2       |
| time/                   |           |
|    total_timesteps      | 1735000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0395    |
|    n_updates            | 4230      |
|    policy_gradient_loss | 6.63e-10  |
|    value_loss           | 0.0744    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 424      |
|    time_elapsed    | 2864     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=0.40 +/- 0.49
Episode length: 276.20 +/- 47.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 276       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1740000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0318    |
|    n_updates            | 4240      |
|    policy_gradient_loss | 1.1e-10   |
|    value_loss           | 0.0664    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 425      |
|    time_elapsed    | 2871     |
|    total_timesteps | 1740800  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.57      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 426       |
|    time_elapsed         | 2876      |
|    total_timesteps      | 1744896   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0178    |
|    n_updates            | 4250      |
|    policy_gradient_loss | 8.29e-11  |
|    value_loss           | 0.0638    |
---------------------------------------
Eval num_timesteps=1745000, episode_reward=0.60 +/- 0.66
Episode length: 304.20 +/- 57.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1745000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0271    |
|    n_updates            | 4260      |
|    policy_gradient_loss | -3.75e-10 |
|    value_loss           | 0.0686    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 427      |
|    time_elapsed    | 2883     |
|    total_timesteps | 1748992  |
---------------------------------
Eval num_timesteps=1750000, episode_reward=0.70 +/- 0.64
Episode length: 302.00 +/- 44.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 1750000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.014     |
|    n_updates            | 4270      |
|    policy_gradient_loss | 8.15e-11  |
|    value_loss           | 0.0708    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 428      |
|    time_elapsed    | 2890     |
|    total_timesteps | 1753088  |
---------------------------------
Eval num_timesteps=1755000, episode_reward=1.00 +/- 0.45
Episode length: 312.80 +/- 69.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 1         |
| time/                   |           |
|    total_timesteps      | 1755000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0449    |
|    n_updates            | 4280      |
|    policy_gradient_loss | -5.65e-10 |
|    value_loss           | 0.0694    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 429      |
|    time_elapsed    | 2897     |
|    total_timesteps | 1757184  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=0.60 +/- 0.66
Episode length: 317.80 +/- 40.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 318       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1760000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0292    |
|    n_updates            | 4290      |
|    policy_gradient_loss | -9.1e-10  |
|    value_loss           | 0.0642    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 430      |
|    time_elapsed    | 2905     |
|    total_timesteps | 1761280  |
---------------------------------
Eval num_timesteps=1765000, episode_reward=0.10 +/- 0.30
Episode length: 269.20 +/- 42.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 269       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 1765000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0434    |
|    n_updates            | 4300      |
|    policy_gradient_loss | -3.89e-10 |
|    value_loss           | 0.0568    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 431      |
|    time_elapsed    | 2912     |
|    total_timesteps | 1765376  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 296       |
|    ep_rew_mean          | 0.52      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 432       |
|    time_elapsed         | 2916      |
|    total_timesteps      | 1769472   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0365    |
|    n_updates            | 4310      |
|    policy_gradient_loss | -2.58e-10 |
|    value_loss           | 0.0608    |
---------------------------------------
Eval num_timesteps=1770000, episode_reward=0.30 +/- 0.46
Episode length: 304.60 +/- 34.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1770000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0333    |
|    n_updates            | 4320      |
|    policy_gradient_loss | -8.68e-10 |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 433      |
|    time_elapsed    | 2924     |
|    total_timesteps | 1773568  |
---------------------------------
Eval num_timesteps=1775000, episode_reward=0.60 +/- 0.49
Episode length: 292.20 +/- 45.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1775000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0349    |
|    n_updates            | 4330      |
|    policy_gradient_loss | -6.69e-11 |
|    value_loss           | 0.0612    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 434      |
|    time_elapsed    | 2931     |
|    total_timesteps | 1777664  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=0.30 +/- 0.46
Episode length: 276.00 +/- 38.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 276       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1780000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0669    |
|    n_updates            | 4340      |
|    policy_gradient_loss | -1.41e-10 |
|    value_loss           | 0.0721    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 435      |
|    time_elapsed    | 2938     |
|    total_timesteps | 1781760  |
---------------------------------
Eval num_timesteps=1785000, episode_reward=0.30 +/- 0.64
Episode length: 288.80 +/- 46.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1785000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0409    |
|    n_updates            | 4350      |
|    policy_gradient_loss | -3.14e-10 |
|    value_loss           | 0.054     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 436      |
|    time_elapsed    | 2945     |
|    total_timesteps | 1785856  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 437       |
|    time_elapsed         | 2950      |
|    total_timesteps      | 1789952   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0142    |
|    n_updates            | 4360      |
|    policy_gradient_loss | 3.13e-11  |
|    value_loss           | 0.0551    |
---------------------------------------
Eval num_timesteps=1790000, episode_reward=0.50 +/- 0.50
Episode length: 321.20 +/- 62.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 321       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1790000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000378 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0368    |
|    n_updates            | 4370      |
|    policy_gradient_loss | 3.22e-10  |
|    value_loss           | 0.0592    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 438      |
|    time_elapsed    | 2957     |
|    total_timesteps | 1794048  |
---------------------------------
Eval num_timesteps=1795000, episode_reward=0.30 +/- 0.46
Episode length: 323.20 +/- 60.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 1795000       |
| train/                  |               |
|    approx_kl            | 1.6637205e-07 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000336     |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0207        |
|    n_updates            | 4380          |
|    policy_gradient_loss | 2.74e-06      |
|    value_loss           | 0.0633        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 439      |
|    time_elapsed    | 2964     |
|    total_timesteps | 1798144  |
---------------------------------
Eval num_timesteps=1800000, episode_reward=0.60 +/- 0.92
Episode length: 312.00 +/- 73.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 1800000      |
| train/                  |              |
|    approx_kl            | 4.600096e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000547    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0342       |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.000119    |
|    value_loss           | 0.0612       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 440      |
|    time_elapsed    | 2971     |
|    total_timesteps | 1802240  |
---------------------------------
Eval num_timesteps=1805000, episode_reward=0.50 +/- 0.67
Episode length: 316.60 +/- 59.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1805000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000569 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0253    |
|    n_updates            | 4400      |
|    policy_gradient_loss | -1.84e-10 |
|    value_loss           | 0.0631    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 441      |
|    time_elapsed    | 2979     |
|    total_timesteps | 1806336  |
---------------------------------
Eval num_timesteps=1810000, episode_reward=0.60 +/- 0.80
Episode length: 315.60 +/- 77.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1810000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000569 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0296    |
|    n_updates            | 4410      |
|    policy_gradient_loss | -1.08e-10 |
|    value_loss           | 0.0618    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 442      |
|    time_elapsed    | 2986     |
|    total_timesteps | 1810432  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 300          |
|    ep_rew_mean          | 0.39         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 443          |
|    time_elapsed         | 2991         |
|    total_timesteps      | 1814528      |
| train/                  |              |
|    approx_kl            | 4.123969e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000555    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0297       |
|    n_updates            | 4420         |
|    policy_gradient_loss | -4.16e-07    |
|    value_loss           | 0.0597       |
------------------------------------------
Eval num_timesteps=1815000, episode_reward=0.40 +/- 0.49
Episode length: 292.40 +/- 38.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1815000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000467 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0262    |
|    n_updates            | 4430      |
|    policy_gradient_loss | 2.66e-10  |
|    value_loss           | 0.0544    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 444      |
|    time_elapsed    | 2998     |
|    total_timesteps | 1818624  |
---------------------------------
Eval num_timesteps=1820000, episode_reward=0.40 +/- 0.80
Episode length: 303.80 +/- 57.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1820000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000467 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0168    |
|    n_updates            | 4440      |
|    policy_gradient_loss | 6.8e-10   |
|    value_loss           | 0.0558    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 445      |
|    time_elapsed    | 3005     |
|    total_timesteps | 1822720  |
---------------------------------
Eval num_timesteps=1825000, episode_reward=0.40 +/- 0.66
Episode length: 307.80 +/- 62.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1825000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000467 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.02      |
|    n_updates            | 4450      |
|    policy_gradient_loss | -1.7e-10  |
|    value_loss           | 0.0529    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 446      |
|    time_elapsed    | 3012     |
|    total_timesteps | 1826816  |
---------------------------------
Eval num_timesteps=1830000, episode_reward=0.10 +/- 0.30
Episode length: 270.20 +/- 44.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 270       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 1830000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000467 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0443    |
|    n_updates            | 4460      |
|    policy_gradient_loss | -8.99e-10 |
|    value_loss           | 0.0558    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 447      |
|    time_elapsed    | 3019     |
|    total_timesteps | 1830912  |
---------------------------------
Eval num_timesteps=1835000, episode_reward=0.50 +/- 0.50
Episode length: 299.40 +/- 59.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1835000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000467 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0574    |
|    n_updates            | 4470      |
|    policy_gradient_loss | 2.69e-10  |
|    value_loss           | 0.0639    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 448      |
|    time_elapsed    | 3026     |
|    total_timesteps | 1835008  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.31      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 449       |
|    time_elapsed         | 3031      |
|    total_timesteps      | 1839104   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000467 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.02      |
|    n_updates            | 4480      |
|    policy_gradient_loss | 5.25e-10  |
|    value_loss           | 0.0551    |
---------------------------------------
Eval num_timesteps=1840000, episode_reward=0.40 +/- 0.49
Episode length: 305.20 +/- 44.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 1840000      |
| train/                  |              |
|    approx_kl            | 4.845031e-06 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000477    |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0225       |
|    n_updates            | 4490         |
|    policy_gradient_loss | 1.54e-06     |
|    value_loss           | 0.0528       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 450      |
|    time_elapsed    | 3039     |
|    total_timesteps | 1843200  |
---------------------------------
Eval num_timesteps=1845000, episode_reward=0.20 +/- 0.40
Episode length: 280.20 +/- 35.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 280       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 1845000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000423 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0215    |
|    n_updates            | 4500      |
|    policy_gradient_loss | -6.06e-10 |
|    value_loss           | 0.0569    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 451      |
|    time_elapsed    | 3046     |
|    total_timesteps | 1847296  |
---------------------------------
Eval num_timesteps=1850000, episode_reward=0.50 +/- 0.50
Episode length: 320.60 +/- 38.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 321       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1850000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000423 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0205    |
|    n_updates            | 4510      |
|    policy_gradient_loss | -8.73e-10 |
|    value_loss           | 0.0529    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 452      |
|    time_elapsed    | 3053     |
|    total_timesteps | 1851392  |
---------------------------------
Eval num_timesteps=1855000, episode_reward=0.80 +/- 1.17
Episode length: 329.40 +/- 48.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 329           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 1855000       |
| train/                  |               |
|    approx_kl            | 5.4138945e-07 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000349     |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0469        |
|    n_updates            | 4520          |
|    policy_gradient_loss | 3.5e-07       |
|    value_loss           | 0.0593        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 453      |
|    time_elapsed    | 3060     |
|    total_timesteps | 1855488  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.33      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 454       |
|    time_elapsed         | 3065      |
|    total_timesteps      | 1859584   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0336    |
|    n_updates            | 4530      |
|    policy_gradient_loss | -3.28e-10 |
|    value_loss           | 0.0531    |
---------------------------------------
Eval num_timesteps=1860000, episode_reward=0.60 +/- 0.92
Episode length: 313.80 +/- 61.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1860000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.017     |
|    n_updates            | 4540      |
|    policy_gradient_loss | -6.68e-10 |
|    value_loss           | 0.0515    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 455      |
|    time_elapsed    | 3072     |
|    total_timesteps | 1863680  |
---------------------------------
Eval num_timesteps=1865000, episode_reward=0.40 +/- 0.49
Episode length: 305.20 +/- 39.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1865000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0228    |
|    n_updates            | 4550      |
|    policy_gradient_loss | -1.49e-10 |
|    value_loss           | 0.0596    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 456      |
|    time_elapsed    | 3080     |
|    total_timesteps | 1867776  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=0.30 +/- 0.46
Episode length: 302.60 +/- 46.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1870000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0406    |
|    n_updates            | 4560      |
|    policy_gradient_loss | -8.37e-11 |
|    value_loss           | 0.0615    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 457      |
|    time_elapsed    | 3087     |
|    total_timesteps | 1871872  |
---------------------------------
Eval num_timesteps=1875000, episode_reward=0.60 +/- 0.49
Episode length: 306.80 +/- 53.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1875000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0364    |
|    n_updates            | 4570      |
|    policy_gradient_loss | 9.39e-11  |
|    value_loss           | 0.0586    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 458      |
|    time_elapsed    | 3094     |
|    total_timesteps | 1875968  |
---------------------------------
Eval num_timesteps=1880000, episode_reward=0.80 +/- 0.60
Episode length: 329.80 +/- 45.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 330       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 1880000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0412    |
|    n_updates            | 4580      |
|    policy_gradient_loss | -1.24e-11 |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 459      |
|    time_elapsed    | 3101     |
|    total_timesteps | 1880064  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.42      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 460       |
|    time_elapsed         | 3107      |
|    total_timesteps      | 1884160   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0337    |
|    n_updates            | 4590      |
|    policy_gradient_loss | 8.27e-10  |
|    value_loss           | 0.0536    |
---------------------------------------
Eval num_timesteps=1885000, episode_reward=0.20 +/- 0.40
Episode length: 268.20 +/- 27.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 268       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 1885000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.016     |
|    n_updates            | 4600      |
|    policy_gradient_loss | -5.18e-10 |
|    value_loss           | 0.0606    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 461      |
|    time_elapsed    | 3113     |
|    total_timesteps | 1888256  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=0.60 +/- 0.66
Episode length: 320.60 +/- 66.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 321           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 1890000       |
| train/                  |               |
|    approx_kl            | 1.5463011e-06 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000448     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0111        |
|    n_updates            | 4610          |
|    policy_gradient_loss | -6.38e-07     |
|    value_loss           | 0.0608        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 462      |
|    time_elapsed    | 3121     |
|    total_timesteps | 1892352  |
---------------------------------
Eval num_timesteps=1895000, episode_reward=0.30 +/- 0.46
Episode length: 306.60 +/- 30.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1895000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000676 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0277    |
|    n_updates            | 4620      |
|    policy_gradient_loss | 2.11e-10  |
|    value_loss           | 0.0636    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 463      |
|    time_elapsed    | 3128     |
|    total_timesteps | 1896448  |
---------------------------------
Eval num_timesteps=1900000, episode_reward=0.50 +/- 0.50
Episode length: 308.80 +/- 59.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1900000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000677 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0353    |
|    n_updates            | 4630      |
|    policy_gradient_loss | 3.96e-10  |
|    value_loss           | 0.0559    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 464      |
|    time_elapsed    | 3135     |
|    total_timesteps | 1900544  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 292       |
|    ep_rew_mean          | 0.37      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 465       |
|    time_elapsed         | 3140      |
|    total_timesteps      | 1904640   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000677 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0323    |
|    n_updates            | 4640      |
|    policy_gradient_loss | -1.18e-09 |
|    value_loss           | 0.0625    |
---------------------------------------
Eval num_timesteps=1905000, episode_reward=0.50 +/- 0.67
Episode length: 277.60 +/- 28.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1905000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000677 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0182    |
|    n_updates            | 4650      |
|    policy_gradient_loss | 5.64e-10  |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 466      |
|    time_elapsed    | 3147     |
|    total_timesteps | 1908736  |
---------------------------------
Eval num_timesteps=1910000, episode_reward=0.30 +/- 0.46
Episode length: 323.00 +/- 53.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 323       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1910000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000677 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0217    |
|    n_updates            | 4660      |
|    policy_gradient_loss | 6.17e-10  |
|    value_loss           | 0.0553    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 467      |
|    time_elapsed    | 3154     |
|    total_timesteps | 1912832  |
---------------------------------
Eval num_timesteps=1915000, episode_reward=0.50 +/- 0.50
Episode length: 306.80 +/- 32.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1915000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000677 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0459    |
|    n_updates            | 4670      |
|    policy_gradient_loss | 4.91e-10  |
|    value_loss           | 0.0624    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 468      |
|    time_elapsed    | 3161     |
|    total_timesteps | 1916928  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=0.40 +/- 0.66
Episode length: 264.40 +/- 37.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 264       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1920000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000677 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0558    |
|    n_updates            | 4680      |
|    policy_gradient_loss | 2.36e-10  |
|    value_loss           | 0.069     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 469      |
|    time_elapsed    | 3168     |
|    total_timesteps | 1921024  |
---------------------------------
Eval num_timesteps=1925000, episode_reward=0.40 +/- 0.66
Episode length: 299.40 +/- 44.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1925000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000677 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0266    |
|    n_updates            | 4690      |
|    policy_gradient_loss | -3.57e-10 |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 470      |
|    time_elapsed    | 3175     |
|    total_timesteps | 1925120  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 298          |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 471          |
|    time_elapsed         | 3180         |
|    total_timesteps      | 1929216      |
| train/                  |              |
|    approx_kl            | 2.566274e-06 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000618    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0304       |
|    n_updates            | 4700         |
|    policy_gradient_loss | 4.52e-06     |
|    value_loss           | 0.058        |
------------------------------------------
Eval num_timesteps=1930000, episode_reward=0.20 +/- 0.40
Episode length: 272.20 +/- 41.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 272         |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 1930000     |
| train/                  |             |
|    approx_kl            | 3.94613e-06 |
|    clip_fraction        | 0.000122    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00101    |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.04        |
|    n_updates            | 4710        |
|    policy_gradient_loss | 1.19e-06    |
|    value_loss           | 0.0595      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 472      |
|    time_elapsed    | 3187     |
|    total_timesteps | 1933312  |
---------------------------------
Eval num_timesteps=1935000, episode_reward=0.80 +/- 0.75
Episode length: 317.00 +/- 63.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 1935000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000943 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0242    |
|    n_updates            | 4720      |
|    policy_gradient_loss | 6.82e-10  |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 473      |
|    time_elapsed    | 3194     |
|    total_timesteps | 1937408  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=0.40 +/- 0.49
Episode length: 302.60 +/- 62.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 1940000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000943 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.046     |
|    n_updates            | 4730      |
|    policy_gradient_loss | 1.74e-10  |
|    value_loss           | 0.0633    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 474      |
|    time_elapsed    | 3202     |
|    total_timesteps | 1941504  |
---------------------------------
Eval num_timesteps=1945000, episode_reward=0.70 +/- 0.78
Episode length: 327.60 +/- 79.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 328       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 1945000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000943 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0265    |
|    n_updates            | 4740      |
|    policy_gradient_loss | -4.84e-10 |
|    value_loss           | 0.0666    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 475      |
|    time_elapsed    | 3209     |
|    total_timesteps | 1945600  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 306           |
|    ep_rew_mean          | 0.5           |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 476           |
|    time_elapsed         | 3214          |
|    total_timesteps      | 1949696       |
| train/                  |               |
|    approx_kl            | 2.3431494e-07 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000866     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0127        |
|    n_updates            | 4750          |
|    policy_gradient_loss | 2.35e-07      |
|    value_loss           | 0.0695        |
-------------------------------------------
Eval num_timesteps=1950000, episode_reward=0.60 +/- 0.49
Episode length: 313.20 +/- 50.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 313           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 1950000       |
| train/                  |               |
|    approx_kl            | 1.8225692e-06 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00117      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0447        |
|    n_updates            | 4760          |
|    policy_gradient_loss | -1.79e-06     |
|    value_loss           | 0.0584        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 477      |
|    time_elapsed    | 3221     |
|    total_timesteps | 1953792  |
---------------------------------
Eval num_timesteps=1955000, episode_reward=0.70 +/- 0.46
Episode length: 317.80 +/- 44.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 318          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 1955000      |
| train/                  |              |
|    approx_kl            | 6.208924e-06 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00107     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0234       |
|    n_updates            | 4770         |
|    policy_gradient_loss | 8.13e-07     |
|    value_loss           | 0.0651       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 478      |
|    time_elapsed    | 3228     |
|    total_timesteps | 1957888  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=0.50 +/- 0.50
Episode length: 331.40 +/- 67.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 331       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1960000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000921 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0298    |
|    n_updates            | 4780      |
|    policy_gradient_loss | -6.46e-10 |
|    value_loss           | 0.0728    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 479      |
|    time_elapsed    | 3235     |
|    total_timesteps | 1961984  |
---------------------------------
Eval num_timesteps=1965000, episode_reward=0.40 +/- 0.49
Episode length: 298.60 +/- 52.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 299          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 1965000      |
| train/                  |              |
|    approx_kl            | 1.631926e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000692    |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.022        |
|    n_updates            | 4790         |
|    policy_gradient_loss | -3.74e-06    |
|    value_loss           | 0.062        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 480      |
|    time_elapsed    | 3243     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1970000, episode_reward=0.30 +/- 0.46
Episode length: 302.80 +/- 38.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1970000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000731 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0246    |
|    n_updates            | 4800      |
|    policy_gradient_loss | -1.92e-10 |
|    value_loss           | 0.0631    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 481      |
|    time_elapsed    | 3250     |
|    total_timesteps | 1970176  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 307       |
|    ep_rew_mean          | 0.49      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 482       |
|    time_elapsed         | 3255      |
|    total_timesteps      | 1974272   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000731 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0239    |
|    n_updates            | 4810      |
|    policy_gradient_loss | 4.95e-10  |
|    value_loss           | 0.0592    |
---------------------------------------
Eval num_timesteps=1975000, episode_reward=0.30 +/- 0.46
Episode length: 296.60 +/- 59.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 1975000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000731 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0284    |
|    n_updates            | 4820      |
|    policy_gradient_loss | -6.99e-10 |
|    value_loss           | 0.0596    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 483      |
|    time_elapsed    | 3262     |
|    total_timesteps | 1978368  |
---------------------------------
Eval num_timesteps=1980000, episode_reward=0.60 +/- 0.66
Episode length: 309.40 +/- 53.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 1980000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000731 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0474    |
|    n_updates            | 4830      |
|    policy_gradient_loss | 1.38e-10  |
|    value_loss           | 0.063     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 484      |
|    time_elapsed    | 3269     |
|    total_timesteps | 1982464  |
---------------------------------
Eval num_timesteps=1985000, episode_reward=0.50 +/- 0.92
Episode length: 313.80 +/- 54.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 314           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 1985000       |
| train/                  |               |
|    approx_kl            | 4.3979395e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000546     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0254        |
|    n_updates            | 4840          |
|    policy_gradient_loss | -4.54e-06     |
|    value_loss           | 0.0641        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 485      |
|    time_elapsed    | 3276     |
|    total_timesteps | 1986560  |
---------------------------------
Eval num_timesteps=1990000, episode_reward=0.50 +/- 0.67
Episode length: 265.80 +/- 48.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 266       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1990000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000421 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0578    |
|    n_updates            | 4850      |
|    policy_gradient_loss | -6.66e-10 |
|    value_loss           | 0.0737    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 486      |
|    time_elapsed    | 3283     |
|    total_timesteps | 1990656  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 306       |
|    ep_rew_mean          | 0.49      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 487       |
|    time_elapsed         | 3288      |
|    total_timesteps      | 1994752   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000421 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0477    |
|    n_updates            | 4860      |
|    policy_gradient_loss | 6.04e-11  |
|    value_loss           | 0.0642    |
---------------------------------------
Eval num_timesteps=1995000, episode_reward=0.50 +/- 0.67
Episode length: 306.60 +/- 53.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 1995000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000421 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0153    |
|    n_updates            | 4870      |
|    policy_gradient_loss | -4.23e-10 |
|    value_loss           | 0.0602    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 488      |
|    time_elapsed    | 3295     |
|    total_timesteps | 1998848  |
---------------------------------
Eval num_timesteps=2000000, episode_reward=0.50 +/- 0.50
Episode length: 284.00 +/- 47.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 284       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2000000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000421 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0185    |
|    n_updates            | 4880      |
|    policy_gradient_loss | -6.15e-10 |
|    value_loss           | 0.0678    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 489      |
|    time_elapsed    | 3302     |
|    total_timesteps | 2002944  |
---------------------------------
Eval num_timesteps=2005000, episode_reward=0.40 +/- 0.49
Episode length: 290.20 +/- 33.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2005000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000421 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0476    |
|    n_updates            | 4890      |
|    policy_gradient_loss | 1.16e-10  |
|    value_loss           | 0.0685    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 490      |
|    time_elapsed    | 3309     |
|    total_timesteps | 2007040  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=0.20 +/- 0.40
Episode length: 290.40 +/- 35.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2010000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000421 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.015     |
|    n_updates            | 4900      |
|    policy_gradient_loss | 3.09e-10  |
|    value_loss           | 0.0512    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 491      |
|    time_elapsed    | 3316     |
|    total_timesteps | 2011136  |
---------------------------------
Eval num_timesteps=2015000, episode_reward=0.30 +/- 0.46
Episode length: 319.20 +/- 39.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 319           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2015000       |
| train/                  |               |
|    approx_kl            | 1.3686076e-07 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00035      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0435        |
|    n_updates            | 4910          |
|    policy_gradient_loss | 6.67e-07      |
|    value_loss           | 0.0629        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 492      |
|    time_elapsed    | 3324     |
|    total_timesteps | 2015232  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 493       |
|    time_elapsed         | 3329      |
|    total_timesteps      | 2019328   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000353 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0329    |
|    n_updates            | 4920      |
|    policy_gradient_loss | -7.14e-10 |
|    value_loss           | 0.0568    |
---------------------------------------
Eval num_timesteps=2020000, episode_reward=0.80 +/- 0.75
Episode length: 317.00 +/- 89.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 2020000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000353 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0489    |
|    n_updates            | 4930      |
|    policy_gradient_loss | -2.63e-10 |
|    value_loss           | 0.0643    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 494      |
|    time_elapsed    | 3336     |
|    total_timesteps | 2023424  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=0.30 +/- 0.46
Episode length: 294.00 +/- 60.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 2025000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000353 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0227    |
|    n_updates            | 4940      |
|    policy_gradient_loss | 4.04e-10  |
|    value_loss           | 0.0612    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 495      |
|    time_elapsed    | 3343     |
|    total_timesteps | 2027520  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=0.50 +/- 0.67
Episode length: 308.40 +/- 64.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2030000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000353 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0274    |
|    n_updates            | 4950      |
|    policy_gradient_loss | 4.78e-10  |
|    value_loss           | 0.0705    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 496      |
|    time_elapsed    | 3350     |
|    total_timesteps | 2031616  |
---------------------------------
Eval num_timesteps=2035000, episode_reward=0.60 +/- 0.49
Episode length: 313.20 +/- 35.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 313           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2035000       |
| train/                  |               |
|    approx_kl            | 2.0298903e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000444     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0184        |
|    n_updates            | 4960          |
|    policy_gradient_loss | 1.78e-06      |
|    value_loss           | 0.0576        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 497      |
|    time_elapsed    | 3358     |
|    total_timesteps | 2035712  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 297       |
|    ep_rew_mean          | 0.41      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 498       |
|    time_elapsed         | 3363      |
|    total_timesteps      | 2039808   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000427 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0339    |
|    n_updates            | 4970      |
|    policy_gradient_loss | -7.25e-10 |
|    value_loss           | 0.0593    |
---------------------------------------
Eval num_timesteps=2040000, episode_reward=0.70 +/- 0.90
Episode length: 281.00 +/- 47.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 2040000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000427 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0292    |
|    n_updates            | 4980      |
|    policy_gradient_loss | -4.17e-10 |
|    value_loss           | 0.0625    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 499      |
|    time_elapsed    | 3370     |
|    total_timesteps | 2043904  |
---------------------------------
Eval num_timesteps=2045000, episode_reward=0.20 +/- 0.40
Episode length: 291.20 +/- 56.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 291       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2045000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000427 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0168    |
|    n_updates            | 4990      |
|    policy_gradient_loss | 5.6e-11   |
|    value_loss           | 0.0635    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 500      |
|    time_elapsed    | 3377     |
|    total_timesteps | 2048000  |
---------------------------------
Eval num_timesteps=2050000, episode_reward=0.20 +/- 0.40
Episode length: 277.80 +/- 34.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2050000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000427 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0324    |
|    n_updates            | 5000      |
|    policy_gradient_loss | -8.01e-10 |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 501      |
|    time_elapsed    | 3384     |
|    total_timesteps | 2052096  |
---------------------------------
Eval num_timesteps=2055000, episode_reward=0.10 +/- 0.30
Episode length: 313.20 +/- 38.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 2055000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000427 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0334    |
|    n_updates            | 5010      |
|    policy_gradient_loss | -5.65e-10 |
|    value_loss           | 0.0653    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 502      |
|    time_elapsed    | 3391     |
|    total_timesteps | 2056192  |
---------------------------------
Eval num_timesteps=2060000, episode_reward=0.30 +/- 0.46
Episode length: 328.60 +/- 59.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 329          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2060000      |
| train/                  |              |
|    approx_kl            | 8.923496e-06 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000498    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0282       |
|    n_updates            | 5020         |
|    policy_gradient_loss | -2.9e-06     |
|    value_loss           | 0.06         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 503      |
|    time_elapsed    | 3398     |
|    total_timesteps | 2060288  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.5       |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 504       |
|    time_elapsed         | 3403      |
|    total_timesteps      | 2064384   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0262    |
|    n_updates            | 5030      |
|    policy_gradient_loss | 8.84e-11  |
|    value_loss           | 0.0602    |
---------------------------------------
Eval num_timesteps=2065000, episode_reward=0.40 +/- 0.49
Episode length: 303.60 +/- 49.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2065000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0495    |
|    n_updates            | 5040      |
|    policy_gradient_loss | 4.71e-10  |
|    value_loss           | 0.0638    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 505      |
|    time_elapsed    | 3410     |
|    total_timesteps | 2068480  |
---------------------------------
Eval num_timesteps=2070000, episode_reward=0.40 +/- 0.66
Episode length: 299.20 +/- 43.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2070000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0194    |
|    n_updates            | 5050      |
|    policy_gradient_loss | -5.65e-10 |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 506      |
|    time_elapsed    | 3418     |
|    total_timesteps | 2072576  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=0.30 +/- 0.46
Episode length: 285.40 +/- 47.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 2075000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0382    |
|    n_updates            | 5060      |
|    policy_gradient_loss | 8.23e-10  |
|    value_loss           | 0.0567    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 507      |
|    time_elapsed    | 3424     |
|    total_timesteps | 2076672  |
---------------------------------
Eval num_timesteps=2080000, episode_reward=0.30 +/- 0.46
Episode length: 289.40 +/- 37.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 2080000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0319    |
|    n_updates            | 5070      |
|    policy_gradient_loss | 1.2e-10   |
|    value_loss           | 0.0594    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 508      |
|    time_elapsed    | 3432     |
|    total_timesteps | 2080768  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 509       |
|    time_elapsed         | 3437      |
|    total_timesteps      | 2084864   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0509    |
|    n_updates            | 5080      |
|    policy_gradient_loss | -3.65e-10 |
|    value_loss           | 0.068     |
---------------------------------------
Eval num_timesteps=2085000, episode_reward=0.20 +/- 0.40
Episode length: 281.20 +/- 32.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2085000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0415    |
|    n_updates            | 5090      |
|    policy_gradient_loss | -3.22e-10 |
|    value_loss           | 0.0592    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 510      |
|    time_elapsed    | 3444     |
|    total_timesteps | 2088960  |
---------------------------------
Eval num_timesteps=2090000, episode_reward=0.80 +/- 0.60
Episode length: 322.80 +/- 68.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 323       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 2090000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0508    |
|    n_updates            | 5100      |
|    policy_gradient_loss | 7.32e-10  |
|    value_loss           | 0.0661    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 511      |
|    time_elapsed    | 3451     |
|    total_timesteps | 2093056  |
---------------------------------
Eval num_timesteps=2095000, episode_reward=0.50 +/- 0.67
Episode length: 309.80 +/- 41.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2095000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.048     |
|    n_updates            | 5110      |
|    policy_gradient_loss | -1.75e-10 |
|    value_loss           | 0.0603    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 512      |
|    time_elapsed    | 3458     |
|    total_timesteps | 2097152  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=0.80 +/- 1.08
Episode length: 319.20 +/- 80.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 319       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 2100000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0225    |
|    n_updates            | 5120      |
|    policy_gradient_loss | 2.91e-10  |
|    value_loss           | 0.0587    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 513      |
|    time_elapsed    | 3465     |
|    total_timesteps | 2101248  |
---------------------------------
Eval num_timesteps=2105000, episode_reward=0.60 +/- 0.66
Episode length: 281.00 +/- 59.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 2105000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0275    |
|    n_updates            | 5130      |
|    policy_gradient_loss | -7.31e-10 |
|    value_loss           | 0.066     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 514      |
|    time_elapsed    | 3472     |
|    total_timesteps | 2105344  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.57      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 515       |
|    time_elapsed         | 3478      |
|    total_timesteps      | 2109440   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0382    |
|    n_updates            | 5140      |
|    policy_gradient_loss | -7.79e-10 |
|    value_loss           | 0.0628    |
---------------------------------------
Eval num_timesteps=2110000, episode_reward=0.40 +/- 0.49
Episode length: 297.40 +/- 35.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2110000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0352    |
|    n_updates            | 5150      |
|    policy_gradient_loss | 9.43e-10  |
|    value_loss           | 0.0652    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 516      |
|    time_elapsed    | 3484     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2115000, episode_reward=0.50 +/- 0.50
Episode length: 307.40 +/- 52.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2115000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0133    |
|    n_updates            | 5160      |
|    policy_gradient_loss | 6.31e-10  |
|    value_loss           | 0.0622    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 517      |
|    time_elapsed    | 3492     |
|    total_timesteps | 2117632  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=0.40 +/- 0.49
Episode length: 294.20 +/- 39.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2120000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0374    |
|    n_updates            | 5170      |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 0.07      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 518      |
|    time_elapsed    | 3499     |
|    total_timesteps | 2121728  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=0.60 +/- 0.80
Episode length: 329.20 +/- 57.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 329       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 2125000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0408    |
|    n_updates            | 5180      |
|    policy_gradient_loss | -1.02e-09 |
|    value_loss           | 0.0682    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 519      |
|    time_elapsed    | 3506     |
|    total_timesteps | 2125824  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.59      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 520       |
|    time_elapsed         | 3511      |
|    total_timesteps      | 2129920   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.031     |
|    n_updates            | 5190      |
|    policy_gradient_loss | 6.08e-11  |
|    value_loss           | 0.0678    |
---------------------------------------
Eval num_timesteps=2130000, episode_reward=0.70 +/- 0.64
Episode length: 277.40 +/- 47.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 277       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 2130000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000528 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0263    |
|    n_updates            | 5200      |
|    policy_gradient_loss | 1.04e-09  |
|    value_loss           | 0.0646    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 521      |
|    time_elapsed    | 3518     |
|    total_timesteps | 2134016  |
---------------------------------
Eval num_timesteps=2135000, episode_reward=0.50 +/- 0.50
Episode length: 286.60 +/- 54.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 287           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 2135000       |
| train/                  |               |
|    approx_kl            | 2.3622124e-07 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00063      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0229        |
|    n_updates            | 5210          |
|    policy_gradient_loss | 2.62e-06      |
|    value_loss           | 0.0523        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 522      |
|    time_elapsed    | 3525     |
|    total_timesteps | 2138112  |
---------------------------------
Eval num_timesteps=2140000, episode_reward=0.70 +/- 0.78
Episode length: 305.00 +/- 57.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 2140000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000506 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0263    |
|    n_updates            | 5220      |
|    policy_gradient_loss | -2.71e-10 |
|    value_loss           | 0.0653    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 523      |
|    time_elapsed    | 3532     |
|    total_timesteps | 2142208  |
---------------------------------
Eval num_timesteps=2145000, episode_reward=0.30 +/- 0.46
Episode length: 284.00 +/- 42.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 284       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 2145000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000506 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0401    |
|    n_updates            | 5230      |
|    policy_gradient_loss | 1.09e-09  |
|    value_loss           | 0.0629    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 524      |
|    time_elapsed    | 3539     |
|    total_timesteps | 2146304  |
---------------------------------
Eval num_timesteps=2150000, episode_reward=0.40 +/- 0.66
Episode length: 305.60 +/- 68.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2150000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000506 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.039     |
|    n_updates            | 5240      |
|    policy_gradient_loss | -8.95e-11 |
|    value_loss           | 0.06      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 525      |
|    time_elapsed    | 3547     |
|    total_timesteps | 2150400  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 299       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 526       |
|    time_elapsed         | 3551      |
|    total_timesteps      | 2154496   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000506 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0427    |
|    n_updates            | 5250      |
|    policy_gradient_loss | -2.3e-10  |
|    value_loss           | 0.0645    |
---------------------------------------
Eval num_timesteps=2155000, episode_reward=0.40 +/- 0.49
Episode length: 285.40 +/- 46.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2155000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000506 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0433    |
|    n_updates            | 5260      |
|    policy_gradient_loss | 4.37e-12  |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 527      |
|    time_elapsed    | 3559     |
|    total_timesteps | 2158592  |
---------------------------------
Eval num_timesteps=2160000, episode_reward=0.30 +/- 0.46
Episode length: 305.80 +/- 52.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2160000       |
| train/                  |               |
|    approx_kl            | 5.9313606e-08 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000442     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0156        |
|    n_updates            | 5270          |
|    policy_gradient_loss | 9.59e-08      |
|    value_loss           | 0.0586        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 528      |
|    time_elapsed    | 3566     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2165000, episode_reward=0.20 +/- 0.40
Episode length: 293.20 +/- 51.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2165000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00082  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.026     |
|    n_updates            | 5280      |
|    policy_gradient_loss | -3.46e-10 |
|    value_loss           | 0.0664    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 529      |
|    time_elapsed    | 3573     |
|    total_timesteps | 2166784  |
---------------------------------
Eval num_timesteps=2170000, episode_reward=0.40 +/- 0.49
Episode length: 274.60 +/- 40.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 275       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2170000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00082  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0442    |
|    n_updates            | 5290      |
|    policy_gradient_loss | -1.02e-10 |
|    value_loss           | 0.0605    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 530      |
|    time_elapsed    | 3580     |
|    total_timesteps | 2170880  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 295       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 531       |
|    time_elapsed         | 3585      |
|    total_timesteps      | 2174976   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00082  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.025     |
|    n_updates            | 5300      |
|    policy_gradient_loss | -4.66e-11 |
|    value_loss           | 0.0669    |
---------------------------------------
Eval num_timesteps=2175000, episode_reward=0.40 +/- 0.49
Episode length: 287.80 +/- 51.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 288           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2175000       |
| train/                  |               |
|    approx_kl            | 1.6414415e-06 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000729     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0317        |
|    n_updates            | 5310          |
|    policy_gradient_loss | 3.25e-06      |
|    value_loss           | 0.0632        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 532      |
|    time_elapsed    | 3592     |
|    total_timesteps | 2179072  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=0.30 +/- 0.46
Episode length: 292.20 +/- 47.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 292           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2180000       |
| train/                  |               |
|    approx_kl            | 5.9604645e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00096      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.024         |
|    n_updates            | 5320          |
|    policy_gradient_loss | -1.2e-09      |
|    value_loss           | 0.0627        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 533      |
|    time_elapsed    | 3599     |
|    total_timesteps | 2183168  |
---------------------------------
Eval num_timesteps=2185000, episode_reward=0.60 +/- 0.49
Episode length: 314.00 +/- 28.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 2185000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000962 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0168    |
|    n_updates            | 5330      |
|    policy_gradient_loss | 6.28e-10  |
|    value_loss           | 0.0647    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 534      |
|    time_elapsed    | 3606     |
|    total_timesteps | 2187264  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=0.60 +/- 0.92
Episode length: 311.40 +/- 57.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 2190000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000962 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0432    |
|    n_updates            | 5340      |
|    policy_gradient_loss | -1.13e-10 |
|    value_loss           | 0.0641    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 535      |
|    time_elapsed    | 3613     |
|    total_timesteps | 2191360  |
---------------------------------
Eval num_timesteps=2195000, episode_reward=0.50 +/- 0.67
Episode length: 300.60 +/- 40.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2195000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000962 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0316    |
|    n_updates            | 5350      |
|    policy_gradient_loss | 1.03e-10  |
|    value_loss           | 0.0708    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 536      |
|    time_elapsed    | 3620     |
|    total_timesteps | 2195456  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 298       |
|    ep_rew_mean          | 0.49      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 537       |
|    time_elapsed         | 3625      |
|    total_timesteps      | 2199552   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000962 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0262    |
|    n_updates            | 5360      |
|    policy_gradient_loss | -2.26e-10 |
|    value_loss           | 0.0549    |
---------------------------------------
Eval num_timesteps=2200000, episode_reward=0.70 +/- 0.78
Episode length: 332.60 +/- 39.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 333       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 2200000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000962 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0135    |
|    n_updates            | 5370      |
|    policy_gradient_loss | 5.98e-10  |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 538      |
|    time_elapsed    | 3633     |
|    total_timesteps | 2203648  |
---------------------------------
Eval num_timesteps=2205000, episode_reward=0.40 +/- 0.49
Episode length: 281.80 +/- 47.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 282       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2205000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000962 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0232    |
|    n_updates            | 5380      |
|    policy_gradient_loss | -1.18e-10 |
|    value_loss           | 0.0659    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 539      |
|    time_elapsed    | 3640     |
|    total_timesteps | 2207744  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=0.40 +/- 0.66
Episode length: 286.00 +/- 65.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2210000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000962 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0291    |
|    n_updates            | 5390      |
|    policy_gradient_loss | -8.08e-11 |
|    value_loss           | 0.0564    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 540      |
|    time_elapsed    | 3647     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2215000, episode_reward=0.70 +/- 0.64
Episode length: 319.60 +/- 41.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 320           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 2215000       |
| train/                  |               |
|    approx_kl            | 8.8008965e-06 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000871     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0172        |
|    n_updates            | 5400          |
|    policy_gradient_loss | -1.65e-06     |
|    value_loss           | 0.0604        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 541      |
|    time_elapsed    | 3654     |
|    total_timesteps | 2215936  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=0.80 +/- 0.87
Episode length: 338.60 +/- 69.21
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 339      |
|    mean_reward          | 0.8      |
| time/                   |          |
|    total_timesteps      | 2220000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00076 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0233   |
|    n_updates            | 5410     |
|    policy_gradient_loss | 3.75e-10 |
|    value_loss           | 0.0603   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 542      |
|    time_elapsed    | 3661     |
|    total_timesteps | 2220032  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 543       |
|    time_elapsed         | 3667      |
|    total_timesteps      | 2224128   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00076  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0187    |
|    n_updates            | 5420      |
|    policy_gradient_loss | -6.85e-10 |
|    value_loss           | 0.0637    |
---------------------------------------
Eval num_timesteps=2225000, episode_reward=0.70 +/- 0.90
Episode length: 310.60 +/- 52.08
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 311      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 2225000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00076 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0303   |
|    n_updates            | 5430     |
|    policy_gradient_loss | -4.7e-10 |
|    value_loss           | 0.0584   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 544      |
|    time_elapsed    | 3674     |
|    total_timesteps | 2228224  |
---------------------------------
Eval num_timesteps=2230000, episode_reward=1.10 +/- 0.94
Episode length: 308.00 +/- 59.52
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 308      |
|    mean_reward          | 1.1      |
| time/                   |          |
|    total_timesteps      | 2230000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00076 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0375   |
|    n_updates            | 5440     |
|    policy_gradient_loss | -4.5e-10 |
|    value_loss           | 0.0563   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 545      |
|    time_elapsed    | 3681     |
|    total_timesteps | 2232320  |
---------------------------------
Eval num_timesteps=2235000, episode_reward=0.60 +/- 0.49
Episode length: 291.00 +/- 55.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2235000       |
| train/                  |               |
|    approx_kl            | 2.4375273e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00107      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0413        |
|    n_updates            | 5450          |
|    policy_gradient_loss | -6.6e-06      |
|    value_loss           | 0.0625        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 546      |
|    time_elapsed    | 3688     |
|    total_timesteps | 2236416  |
---------------------------------
Eval num_timesteps=2240000, episode_reward=0.50 +/- 0.67
Episode length: 307.40 +/- 69.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2240000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00081  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.038     |
|    n_updates            | 5460      |
|    policy_gradient_loss | -2.02e-10 |
|    value_loss           | 0.0571    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 547      |
|    time_elapsed    | 3695     |
|    total_timesteps | 2240512  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 548       |
|    time_elapsed         | 3700      |
|    total_timesteps      | 2244608   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000809 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0319    |
|    n_updates            | 5470      |
|    policy_gradient_loss | -4.6e-10  |
|    value_loss           | 0.0607    |
---------------------------------------
Eval num_timesteps=2245000, episode_reward=0.10 +/- 0.30
Episode length: 296.80 +/- 48.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 2245000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000809 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0407    |
|    n_updates            | 5480      |
|    policy_gradient_loss | 9.05e-10  |
|    value_loss           | 0.0625    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 549      |
|    time_elapsed    | 3707     |
|    total_timesteps | 2248704  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=0.90 +/- 0.70
Episode length: 319.20 +/- 55.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 319       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 2250000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000809 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0597    |
|    n_updates            | 5490      |
|    policy_gradient_loss | -1.47e-10 |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 550      |
|    time_elapsed    | 3714     |
|    total_timesteps | 2252800  |
---------------------------------
Eval num_timesteps=2255000, episode_reward=0.50 +/- 0.67
Episode length: 300.20 +/- 48.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 2255000       |
| train/                  |               |
|    approx_kl            | 4.7907815e-07 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000686     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0258        |
|    n_updates            | 5500          |
|    policy_gradient_loss | -6.1e-07      |
|    value_loss           | 0.0592        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 551      |
|    time_elapsed    | 3722     |
|    total_timesteps | 2256896  |
---------------------------------
Eval num_timesteps=2260000, episode_reward=0.80 +/- 0.75
Episode length: 315.20 +/- 59.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 2260000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000661 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0182    |
|    n_updates            | 5510      |
|    policy_gradient_loss | -4.29e-11 |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 552      |
|    time_elapsed    | 3729     |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2265000, episode_reward=0.10 +/- 0.30
Episode length: 293.60 +/- 23.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 294           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 2265000       |
| train/                  |               |
|    approx_kl            | 5.7465353e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00107      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.011         |
|    n_updates            | 5520          |
|    policy_gradient_loss | -3.96e-05     |
|    value_loss           | 0.063         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 553      |
|    time_elapsed    | 3736     |
|    total_timesteps | 2265088  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 307      |
|    ep_rew_mean          | 0.54     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 554      |
|    time_elapsed         | 3741     |
|    total_timesteps      | 2269184  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00111 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0272   |
|    n_updates            | 5530     |
|    policy_gradient_loss | 1.06e-10 |
|    value_loss           | 0.0638   |
--------------------------------------
Eval num_timesteps=2270000, episode_reward=0.50 +/- 0.81
Episode length: 299.20 +/- 57.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 299          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 2270000      |
| train/                  |              |
|    approx_kl            | 8.127681e-06 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000988    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0487       |
|    n_updates            | 5540         |
|    policy_gradient_loss | 6.01e-08     |
|    value_loss           | 0.0644       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 555      |
|    time_elapsed    | 3748     |
|    total_timesteps | 2273280  |
---------------------------------
Eval num_timesteps=2275000, episode_reward=0.20 +/- 0.40
Episode length: 272.60 +/- 20.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 273           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 2275000       |
| train/                  |               |
|    approx_kl            | 2.0964362e-06 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000804     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0107        |
|    n_updates            | 5550          |
|    policy_gradient_loss | 1.45e-06      |
|    value_loss           | 0.0622        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 556      |
|    time_elapsed    | 3755     |
|    total_timesteps | 2277376  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=0.70 +/- 0.64
Episode length: 324.00 +/- 62.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 324           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 2280000       |
| train/                  |               |
|    approx_kl            | 3.3294782e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000934     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0325        |
|    n_updates            | 5560          |
|    policy_gradient_loss | 2.54e-06      |
|    value_loss           | 0.0633        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 557      |
|    time_elapsed    | 3762     |
|    total_timesteps | 2281472  |
---------------------------------
Eval num_timesteps=2285000, episode_reward=0.50 +/- 0.67
Episode length: 305.00 +/- 50.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 2285000      |
| train/                  |              |
|    approx_kl            | 1.798202e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000718    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0288       |
|    n_updates            | 5570         |
|    policy_gradient_loss | -1.06e-05    |
|    value_loss           | 0.0582       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 317      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 558      |
|    time_elapsed    | 3770     |
|    total_timesteps | 2285568  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 321       |
|    ep_rew_mean          | 0.6       |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 559       |
|    time_elapsed         | 3775      |
|    total_timesteps      | 2289664   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000686 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0181    |
|    n_updates            | 5580      |
|    policy_gradient_loss | -1.23e-10 |
|    value_loss           | 0.058     |
---------------------------------------
Eval num_timesteps=2290000, episode_reward=0.70 +/- 1.19
Episode length: 302.40 +/- 59.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 302          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 2290000      |
| train/                  |              |
|    approx_kl            | 5.540182e-05 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000794    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0404       |
|    n_updates            | 5590         |
|    policy_gradient_loss | -2.52e-06    |
|    value_loss           | 0.0606       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 321      |
|    ep_rew_mean     | 0.63     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 560      |
|    time_elapsed    | 3782     |
|    total_timesteps | 2293760  |
---------------------------------
Eval num_timesteps=2295000, episode_reward=0.80 +/- 0.75
Episode length: 313.20 +/- 43.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 2295000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00115  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0418    |
|    n_updates            | 5600      |
|    policy_gradient_loss | -6.32e-10 |
|    value_loss           | 0.0646    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 561      |
|    time_elapsed    | 3789     |
|    total_timesteps | 2297856  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=0.50 +/- 0.67
Episode length: 303.40 +/- 53.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 303           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 2300000       |
| train/                  |               |
|    approx_kl            | 2.6820082e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00115      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.024         |
|    n_updates            | 5610          |
|    policy_gradient_loss | 2.17e-06      |
|    value_loss           | 0.0623        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 562      |
|    time_elapsed    | 3796     |
|    total_timesteps | 2301952  |
---------------------------------
Eval num_timesteps=2305000, episode_reward=0.60 +/- 0.66
Episode length: 306.40 +/- 25.25
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 306      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 2305000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00117 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0287   |
|    n_updates            | 5620     |
|    policy_gradient_loss | 6.74e-10 |
|    value_loss           | 0.0615   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 563      |
|    time_elapsed    | 3803     |
|    total_timesteps | 2306048  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=0.40 +/- 0.49
Episode length: 292.80 +/- 41.86
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 293      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 2310000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00117 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0397   |
|    n_updates            | 5630     |
|    policy_gradient_loss | 3.51e-10 |
|    value_loss           | 0.0584   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 564      |
|    time_elapsed    | 3810     |
|    total_timesteps | 2310144  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 305          |
|    ep_rew_mean          | 0.47         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 565          |
|    time_elapsed         | 3815         |
|    total_timesteps      | 2314240      |
| train/                  |              |
|    approx_kl            | 5.561061e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00193     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0379       |
|    n_updates            | 5640         |
|    policy_gradient_loss | -5.59e-05    |
|    value_loss           | 0.0596       |
------------------------------------------
Eval num_timesteps=2315000, episode_reward=0.30 +/- 0.46
Episode length: 270.20 +/- 34.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2315000      |
| train/                  |              |
|    approx_kl            | 5.029753e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00231     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0289       |
|    n_updates            | 5650         |
|    policy_gradient_loss | -2.25e-06    |
|    value_loss           | 0.0572       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 566      |
|    time_elapsed    | 3822     |
|    total_timesteps | 2318336  |
---------------------------------
Eval num_timesteps=2320000, episode_reward=0.60 +/- 0.66
Episode length: 317.60 +/- 60.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2320000       |
| train/                  |               |
|    approx_kl            | 6.2670733e-07 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00259      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0359        |
|    n_updates            | 5660          |
|    policy_gradient_loss | 9.93e-07      |
|    value_loss           | 0.056         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 567      |
|    time_elapsed    | 3829     |
|    total_timesteps | 2322432  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=0.60 +/- 0.49
Episode length: 329.60 +/- 60.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 330       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 2325000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00214  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0326    |
|    n_updates            | 5670      |
|    policy_gradient_loss | -9.86e-10 |
|    value_loss           | 0.0548    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 568      |
|    time_elapsed    | 3837     |
|    total_timesteps | 2326528  |
---------------------------------
Eval num_timesteps=2330000, episode_reward=0.40 +/- 0.66
Episode length: 278.40 +/- 54.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2330000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00214  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0429    |
|    n_updates            | 5680      |
|    policy_gradient_loss | -4.51e-11 |
|    value_loss           | 0.0645    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 569      |
|    time_elapsed    | 3844     |
|    total_timesteps | 2330624  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 310      |
|    ep_rew_mean          | 0.5      |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 570      |
|    time_elapsed         | 3849     |
|    total_timesteps      | 2334720  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00214 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0326   |
|    n_updates            | 5690     |
|    policy_gradient_loss | 4.23e-10 |
|    value_loss           | 0.0636   |
--------------------------------------
Eval num_timesteps=2335000, episode_reward=0.20 +/- 0.40
Episode length: 286.80 +/- 27.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2335000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00214  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0511    |
|    n_updates            | 5700      |
|    policy_gradient_loss | -5.31e-10 |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 571      |
|    time_elapsed    | 3856     |
|    total_timesteps | 2338816  |
---------------------------------
Eval num_timesteps=2340000, episode_reward=0.20 +/- 0.40
Episode length: 270.20 +/- 43.98
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 270      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 2340000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00214 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0399   |
|    n_updates            | 5710     |
|    policy_gradient_loss | 5.31e-11 |
|    value_loss           | 0.0542   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 572      |
|    time_elapsed    | 3863     |
|    total_timesteps | 2342912  |
---------------------------------
Eval num_timesteps=2345000, episode_reward=0.20 +/- 0.40
Episode length: 298.40 +/- 35.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 2345000      |
| train/                  |              |
|    approx_kl            | 5.234615e-06 |
|    clip_fraction        | 0.000269     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00243     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0316       |
|    n_updates            | 5720         |
|    policy_gradient_loss | -1.29e-06    |
|    value_loss           | 0.0582       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 573      |
|    time_elapsed    | 3870     |
|    total_timesteps | 2347008  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=0.50 +/- 0.50
Episode length: 319.40 +/- 60.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 319       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2350000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00156  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0477    |
|    n_updates            | 5730      |
|    policy_gradient_loss | -5.63e-10 |
|    value_loss           | 0.0716    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 574      |
|    time_elapsed    | 3877     |
|    total_timesteps | 2351104  |
---------------------------------
Eval num_timesteps=2355000, episode_reward=0.50 +/- 0.50
Episode length: 308.40 +/- 50.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2355000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00156  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0387    |
|    n_updates            | 5740      |
|    policy_gradient_loss | -4.54e-10 |
|    value_loss           | 0.0588    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 575      |
|    time_elapsed    | 3884     |
|    total_timesteps | 2355200  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 296           |
|    ep_rew_mean          | 0.42          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 576           |
|    time_elapsed         | 3890          |
|    total_timesteps      | 2359296       |
| train/                  |               |
|    approx_kl            | 1.3350451e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00141      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0471        |
|    n_updates            | 5750          |
|    policy_gradient_loss | -2.9e-06      |
|    value_loss           | 0.067         |
-------------------------------------------
Eval num_timesteps=2360000, episode_reward=0.50 +/- 0.81
Episode length: 314.20 +/- 68.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 314           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 2360000       |
| train/                  |               |
|    approx_kl            | 7.0684036e-06 |
|    clip_fraction        | 0.000269      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00125      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0364        |
|    n_updates            | 5760          |
|    policy_gradient_loss | -7.27e-07     |
|    value_loss           | 0.0592        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 577      |
|    time_elapsed    | 3897     |
|    total_timesteps | 2363392  |
---------------------------------
Eval num_timesteps=2365000, episode_reward=0.10 +/- 0.30
Episode length: 272.60 +/- 40.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 273           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 2365000       |
| train/                  |               |
|    approx_kl            | 1.1033553e-06 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00113      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0311        |
|    n_updates            | 5770          |
|    policy_gradient_loss | 2.58e-06      |
|    value_loss           | 0.063         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 578      |
|    time_elapsed    | 3904     |
|    total_timesteps | 2367488  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=0.60 +/- 0.80
Episode length: 306.40 +/- 59.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2370000       |
| train/                  |               |
|    approx_kl            | 2.0921289e-07 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00121      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0279        |
|    n_updates            | 5780          |
|    policy_gradient_loss | 2.47e-06      |
|    value_loss           | 0.0578        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 579      |
|    time_elapsed    | 3911     |
|    total_timesteps | 2371584  |
---------------------------------
Eval num_timesteps=2375000, episode_reward=0.40 +/- 0.49
Episode length: 291.40 +/- 41.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2375000       |
| train/                  |               |
|    approx_kl            | 6.1421742e-06 |
|    clip_fraction        | 2.44e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00149      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0368        |
|    n_updates            | 5790          |
|    policy_gradient_loss | 6.66e-08      |
|    value_loss           | 0.0606        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 580      |
|    time_elapsed    | 3918     |
|    total_timesteps | 2375680  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 291       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 581       |
|    time_elapsed         | 3923      |
|    total_timesteps      | 2379776   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00167  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.032     |
|    n_updates            | 5800      |
|    policy_gradient_loss | -2.96e-10 |
|    value_loss           | 0.0628    |
---------------------------------------
Eval num_timesteps=2380000, episode_reward=0.60 +/- 0.66
Episode length: 281.20 +/- 46.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 281      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 2380000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00167 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0388   |
|    n_updates            | 5810     |
|    policy_gradient_loss | 3.21e-10 |
|    value_loss           | 0.059    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 582      |
|    time_elapsed    | 3930     |
|    total_timesteps | 2383872  |
---------------------------------
Eval num_timesteps=2385000, episode_reward=0.70 +/- 0.64
Episode length: 316.20 +/- 48.74
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 316      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 2385000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00167 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.033    |
|    n_updates            | 5820     |
|    policy_gradient_loss | 7.28e-12 |
|    value_loss           | 0.0708   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 583      |
|    time_elapsed    | 3937     |
|    total_timesteps | 2387968  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=0.30 +/- 0.64
Episode length: 273.40 +/- 39.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 273          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2390000      |
| train/                  |              |
|    approx_kl            | 8.558069e-06 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00178     |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0335       |
|    n_updates            | 5830         |
|    policy_gradient_loss | 2.31e-06     |
|    value_loss           | 0.062        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 584      |
|    time_elapsed    | 3944     |
|    total_timesteps | 2392064  |
---------------------------------
Eval num_timesteps=2395000, episode_reward=0.30 +/- 0.46
Episode length: 280.80 +/- 53.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 2395000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00158  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.042     |
|    n_updates            | 5840      |
|    policy_gradient_loss | -1.89e-10 |
|    value_loss           | 0.0705    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 585      |
|    time_elapsed    | 3951     |
|    total_timesteps | 2396160  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=0.20 +/- 0.40
Episode length: 298.20 +/- 50.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2400000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00158  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0263    |
|    n_updates            | 5850      |
|    policy_gradient_loss | -3.64e-12 |
|    value_loss           | 0.0669    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 586      |
|    time_elapsed    | 3958     |
|    total_timesteps | 2400256  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 300           |
|    ep_rew_mean          | 0.5           |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 587           |
|    time_elapsed         | 3963          |
|    total_timesteps      | 2404352       |
| train/                  |               |
|    approx_kl            | 3.5865058e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00235      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0499        |
|    n_updates            | 5860          |
|    policy_gradient_loss | -3.43e-06     |
|    value_loss           | 0.0571        |
-------------------------------------------
Eval num_timesteps=2405000, episode_reward=0.50 +/- 0.50
Episode length: 314.40 +/- 34.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 314          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 2405000      |
| train/                  |              |
|    approx_kl            | 9.834244e-05 |
|    clip_fraction        | 0.000464     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00397     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0397       |
|    n_updates            | 5870         |
|    policy_gradient_loss | -9.28e-05    |
|    value_loss           | 0.0584       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 588      |
|    time_elapsed    | 3970     |
|    total_timesteps | 2408448  |
---------------------------------
Eval num_timesteps=2410000, episode_reward=0.20 +/- 0.40
Episode length: 316.00 +/- 55.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 316           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 2410000       |
| train/                  |               |
|    approx_kl            | 1.1189608e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00411      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0115        |
|    n_updates            | 5880          |
|    policy_gradient_loss | 9.53e-07      |
|    value_loss           | 0.0557        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 589      |
|    time_elapsed    | 3978     |
|    total_timesteps | 2412544  |
---------------------------------
Eval num_timesteps=2415000, episode_reward=0.40 +/- 0.49
Episode length: 305.60 +/- 43.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2415000       |
| train/                  |               |
|    approx_kl            | 1.0719377e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00395      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0221        |
|    n_updates            | 5890          |
|    policy_gradient_loss | 1.01e-06      |
|    value_loss           | 0.0598        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 590      |
|    time_elapsed    | 3985     |
|    total_timesteps | 2416640  |
---------------------------------
Eval num_timesteps=2420000, episode_reward=0.70 +/- 0.90
Episode length: 318.40 +/- 58.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 2420000       |
| train/                  |               |
|    approx_kl            | 1.6590639e-07 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00497      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0347        |
|    n_updates            | 5900          |
|    policy_gradient_loss | 1.08e-06      |
|    value_loss           | 0.0524        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 591      |
|    time_elapsed    | 3992     |
|    total_timesteps | 2420736  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 299           |
|    ep_rew_mean          | 0.38          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 592           |
|    time_elapsed         | 3997          |
|    total_timesteps      | 2424832       |
| train/                  |               |
|    approx_kl            | 5.9853483e-06 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0044       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0205        |
|    n_updates            | 5910          |
|    policy_gradient_loss | -7.73e-07     |
|    value_loss           | 0.0614        |
-------------------------------------------
Eval num_timesteps=2425000, episode_reward=0.80 +/- 0.75
Episode length: 294.40 +/- 39.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 294           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 2425000       |
| train/                  |               |
|    approx_kl            | 0.00011713237 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.006        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0319        |
|    n_updates            | 5920          |
|    policy_gradient_loss | -7.58e-05     |
|    value_loss           | 0.0625        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 593      |
|    time_elapsed    | 4004     |
|    total_timesteps | 2428928  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=0.20 +/- 0.40
Episode length: 298.20 +/- 27.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 298           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 2430000       |
| train/                  |               |
|    approx_kl            | 2.7493152e-05 |
|    clip_fraction        | 0.00061       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00584      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0283        |
|    n_updates            | 5930          |
|    policy_gradient_loss | -5.58e-05     |
|    value_loss           | 0.0621        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 594      |
|    time_elapsed    | 4012     |
|    total_timesteps | 2433024  |
---------------------------------
Eval num_timesteps=2435000, episode_reward=0.40 +/- 0.49
Episode length: 320.40 +/- 34.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 320           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2435000       |
| train/                  |               |
|    approx_kl            | 3.5241203e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00658      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0109        |
|    n_updates            | 5940          |
|    policy_gradient_loss | -1.05e-05     |
|    value_loss           | 0.0515        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 595      |
|    time_elapsed    | 4019     |
|    total_timesteps | 2437120  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=0.70 +/- 0.78
Episode length: 310.40 +/- 55.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 2440000      |
| train/                  |              |
|    approx_kl            | 6.901896e-05 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00899     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0367       |
|    n_updates            | 5950         |
|    policy_gradient_loss | -6e-05       |
|    value_loss           | 0.0656       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 596      |
|    time_elapsed    | 4026     |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2445000, episode_reward=0.10 +/- 0.30
Episode length: 274.80 +/- 47.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 275           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 2445000       |
| train/                  |               |
|    approx_kl            | 5.1298397e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00756      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0561        |
|    n_updates            | 5960          |
|    policy_gradient_loss | -3.47e-05     |
|    value_loss           | 0.0535        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 597      |
|    time_elapsed    | 4033     |
|    total_timesteps | 2445312  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 305           |
|    ep_rew_mean          | 0.46          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 598           |
|    time_elapsed         | 4038          |
|    total_timesteps      | 2449408       |
| train/                  |               |
|    approx_kl            | 3.7642923e-05 |
|    clip_fraction        | 0.0011        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00581      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0274        |
|    n_updates            | 5970          |
|    policy_gradient_loss | -5.18e-05     |
|    value_loss           | 0.0596        |
-------------------------------------------
Eval num_timesteps=2450000, episode_reward=0.60 +/- 0.66
Episode length: 328.00 +/- 55.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 328       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 2450000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00584  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0232    |
|    n_updates            | 5980      |
|    policy_gradient_loss | 9.93e-11  |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 599      |
|    time_elapsed    | 4046     |
|    total_timesteps | 2453504  |
---------------------------------
Eval num_timesteps=2455000, episode_reward=0.60 +/- 0.66
Episode length: 296.20 +/- 55.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 296          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 2455000      |
| train/                  |              |
|    approx_kl            | 2.940942e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00609     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0247       |
|    n_updates            | 5990         |
|    policy_gradient_loss | 1.03e-06     |
|    value_loss           | 0.057        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 600      |
|    time_elapsed    | 4052     |
|    total_timesteps | 2457600  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=0.40 +/- 0.49
Episode length: 296.00 +/- 68.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 296          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 2460000      |
| train/                  |              |
|    approx_kl            | 0.0001293815 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00797     |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0371       |
|    n_updates            | 6000         |
|    policy_gradient_loss | -6.74e-05    |
|    value_loss           | 0.0668       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 601      |
|    time_elapsed    | 4060     |
|    total_timesteps | 2461696  |
---------------------------------
Eval num_timesteps=2465000, episode_reward=0.40 +/- 0.66
Episode length: 290.20 +/- 69.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 290          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 2465000      |
| train/                  |              |
|    approx_kl            | 9.340601e-05 |
|    clip_fraction        | 0.000659     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00574     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0397       |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.000142    |
|    value_loss           | 0.0569       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 602      |
|    time_elapsed    | 4067     |
|    total_timesteps | 2465792  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 305          |
|    ep_rew_mean          | 0.43         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 603          |
|    time_elapsed         | 4071         |
|    total_timesteps      | 2469888      |
| train/                  |              |
|    approx_kl            | 6.132059e-05 |
|    clip_fraction        | 0.000903     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00398     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0249       |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.000128    |
|    value_loss           | 0.0669       |
------------------------------------------
Eval num_timesteps=2470000, episode_reward=0.00 +/- 0.00
Episode length: 270.60 +/- 37.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 271       |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 2470000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00398  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0106    |
|    n_updates            | 6030      |
|    policy_gradient_loss | -3.96e-10 |
|    value_loss           | 0.0548    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 604      |
|    time_elapsed    | 4078     |
|    total_timesteps | 2473984  |
---------------------------------
Eval num_timesteps=2475000, episode_reward=0.40 +/- 0.49
Episode length: 303.00 +/- 40.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 303           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2475000       |
| train/                  |               |
|    approx_kl            | 1.3504992e-05 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00452      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0262        |
|    n_updates            | 6040          |
|    policy_gradient_loss | -5.27e-06     |
|    value_loss           | 0.0596        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 605      |
|    time_elapsed    | 4086     |
|    total_timesteps | 2478080  |
---------------------------------
Eval num_timesteps=2480000, episode_reward=0.30 +/- 0.46
Episode length: 305.80 +/- 56.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 306          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2480000      |
| train/                  |              |
|    approx_kl            | 7.062481e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00463     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0383       |
|    n_updates            | 6050         |
|    policy_gradient_loss | 1.4e-08      |
|    value_loss           | 0.0625       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 606      |
|    time_elapsed    | 4093     |
|    total_timesteps | 2482176  |
---------------------------------
Eval num_timesteps=2485000, episode_reward=0.60 +/- 0.66
Episode length: 325.80 +/- 38.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 326         |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 2485000     |
| train/                  |             |
|    approx_kl            | 9.31729e-05 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00305    |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.029       |
|    n_updates            | 6060        |
|    policy_gradient_loss | -8.22e-05   |
|    value_loss           | 0.0659      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 607      |
|    time_elapsed    | 4100     |
|    total_timesteps | 2486272  |
---------------------------------
Eval num_timesteps=2490000, episode_reward=0.40 +/- 0.66
Episode length: 290.00 +/- 62.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 290          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 2490000      |
| train/                  |              |
|    approx_kl            | 5.598122e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00278     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.0346       |
|    n_updates            | 6070         |
|    policy_gradient_loss | 8.19e-07     |
|    value_loss           | 0.0615       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 608      |
|    time_elapsed    | 4108     |
|    total_timesteps | 2490368  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 304           |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 609           |
|    time_elapsed         | 4112          |
|    total_timesteps      | 2494464       |
| train/                  |               |
|    approx_kl            | 3.7643767e-06 |
|    clip_fraction        | 2.44e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00291      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0385        |
|    n_updates            | 6080          |
|    policy_gradient_loss | 1.3e-06       |
|    value_loss           | 0.0624        |
-------------------------------------------
Eval num_timesteps=2495000, episode_reward=0.50 +/- 0.67
Episode length: 313.20 +/- 54.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 313           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 2495000       |
| train/                  |               |
|    approx_kl            | 2.0482505e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00306      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0447        |
|    n_updates            | 6090          |
|    policy_gradient_loss | 2.23e-06      |
|    value_loss           | 0.0684        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 610      |
|    time_elapsed    | 4120     |
|    total_timesteps | 2498560  |
---------------------------------
Eval num_timesteps=2500000, episode_reward=0.40 +/- 0.49
Episode length: 284.20 +/- 51.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 2500000      |
| train/                  |              |
|    approx_kl            | 6.632002e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00434     |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0277       |
|    n_updates            | 6100         |
|    policy_gradient_loss | -0.000164    |
|    value_loss           | 0.0603       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 611      |
|    time_elapsed    | 4127     |
|    total_timesteps | 2502656  |
---------------------------------
Eval num_timesteps=2505000, episode_reward=0.40 +/- 0.49
Episode length: 298.40 +/- 53.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 2505000      |
| train/                  |              |
|    approx_kl            | 2.913129e-05 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00492     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0243       |
|    n_updates            | 6110         |
|    policy_gradient_loss | 4.17e-07     |
|    value_loss           | 0.0571       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 612      |
|    time_elapsed    | 4134     |
|    total_timesteps | 2506752  |
---------------------------------
Eval num_timesteps=2510000, episode_reward=0.30 +/- 0.46
Episode length: 283.40 +/- 38.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 283           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2510000       |
| train/                  |               |
|    approx_kl            | 0.00012770564 |
|    clip_fraction        | 0.00061       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00643      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0479        |
|    n_updates            | 6120          |
|    policy_gradient_loss | -4.48e-05     |
|    value_loss           | 0.0659        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 613      |
|    time_elapsed    | 4141     |
|    total_timesteps | 2510848  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 307           |
|    ep_rew_mean          | 0.48          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 614           |
|    time_elapsed         | 4146          |
|    total_timesteps      | 2514944       |
| train/                  |               |
|    approx_kl            | 6.5680942e-06 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00846      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0291        |
|    n_updates            | 6130          |
|    policy_gradient_loss | -2.7e-07      |
|    value_loss           | 0.0623        |
-------------------------------------------
Eval num_timesteps=2515000, episode_reward=0.50 +/- 0.50
Episode length: 298.20 +/- 50.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 298           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 2515000       |
| train/                  |               |
|    approx_kl            | 1.5576428e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0086       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0512        |
|    n_updates            | 6140          |
|    policy_gradient_loss | 4.85e-06      |
|    value_loss           | 0.0562        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 615      |
|    time_elapsed    | 4153     |
|    total_timesteps | 2519040  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=0.80 +/- 0.75
Episode length: 314.60 +/- 30.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 315          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 2520000      |
| train/                  |              |
|    approx_kl            | 4.839628e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00792     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0379       |
|    n_updates            | 6150         |
|    policy_gradient_loss | -8.86e-06    |
|    value_loss           | 0.0564       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 616      |
|    time_elapsed    | 4160     |
|    total_timesteps | 2523136  |
---------------------------------
Eval num_timesteps=2525000, episode_reward=0.80 +/- 0.60
Episode length: 300.60 +/- 61.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 301           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 2525000       |
| train/                  |               |
|    approx_kl            | 2.1003172e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0089       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0416        |
|    n_updates            | 6160          |
|    policy_gradient_loss | -3.16e-06     |
|    value_loss           | 0.0608        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 617      |
|    time_elapsed    | 4167     |
|    total_timesteps | 2527232  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=0.40 +/- 0.66
Episode length: 300.20 +/- 48.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2530000       |
| train/                  |               |
|    approx_kl            | 8.0394544e-05 |
|    clip_fraction        | 0.000903      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00872      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0351        |
|    n_updates            | 6170          |
|    policy_gradient_loss | 0.000115      |
|    value_loss           | 0.061         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 618      |
|    time_elapsed    | 4174     |
|    total_timesteps | 2531328  |
---------------------------------
Eval num_timesteps=2535000, episode_reward=0.70 +/- 0.78
Episode length: 283.80 +/- 51.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 2535000      |
| train/                  |              |
|    approx_kl            | 7.406874e-05 |
|    clip_fraction        | 0.00105      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0106      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0265       |
|    n_updates            | 6180         |
|    policy_gradient_loss | -0.000156    |
|    value_loss           | 0.0605       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 619      |
|    time_elapsed    | 4181     |
|    total_timesteps | 2535424  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 303           |
|    ep_rew_mean          | 0.44          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 620           |
|    time_elapsed         | 4187          |
|    total_timesteps      | 2539520       |
| train/                  |               |
|    approx_kl            | 1.7993108e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0126       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0309        |
|    n_updates            | 6190          |
|    policy_gradient_loss | -0.000105     |
|    value_loss           | 0.0573        |
-------------------------------------------
Eval num_timesteps=2540000, episode_reward=0.40 +/- 0.66
Episode length: 263.60 +/- 37.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 264           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2540000       |
| train/                  |               |
|    approx_kl            | 2.0727166e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0121       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0401        |
|    n_updates            | 6200          |
|    policy_gradient_loss | 1.4e-06       |
|    value_loss           | 0.0621        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 621      |
|    time_elapsed    | 4193     |
|    total_timesteps | 2543616  |
---------------------------------
Eval num_timesteps=2545000, episode_reward=0.60 +/- 0.66
Episode length: 323.60 +/- 61.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 324           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2545000       |
| train/                  |               |
|    approx_kl            | 2.9235263e-05 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.052         |
|    n_updates            | 6210          |
|    policy_gradient_loss | -7.97e-06     |
|    value_loss           | 0.0644        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 622      |
|    time_elapsed    | 4201     |
|    total_timesteps | 2547712  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=0.80 +/- 0.40
Episode length: 323.20 +/- 37.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 2550000       |
| train/                  |               |
|    approx_kl            | 0.00010101829 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0324        |
|    n_updates            | 6220          |
|    policy_gradient_loss | -1.24e-05     |
|    value_loss           | 0.0656        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 623      |
|    time_elapsed    | 4208     |
|    total_timesteps | 2551808  |
---------------------------------
Eval num_timesteps=2555000, episode_reward=0.70 +/- 0.78
Episode length: 305.40 +/- 64.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 2555000       |
| train/                  |               |
|    approx_kl            | 4.5902576e-05 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00966      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0285        |
|    n_updates            | 6230          |
|    policy_gradient_loss | -3.11e-05     |
|    value_loss           | 0.0597        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 624      |
|    time_elapsed    | 4215     |
|    total_timesteps | 2555904  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=0.30 +/- 0.46
Episode length: 296.60 +/- 43.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2560000      |
| train/                  |              |
|    approx_kl            | 2.799381e-05 |
|    clip_fraction        | 0.00061      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00765     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0324       |
|    n_updates            | 6240         |
|    policy_gradient_loss | -5.05e-05    |
|    value_loss           | 0.0584       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 625      |
|    time_elapsed    | 4222     |
|    total_timesteps | 2560000  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 300           |
|    ep_rew_mean          | 0.5           |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 626           |
|    time_elapsed         | 4227          |
|    total_timesteps      | 2564096       |
| train/                  |               |
|    approx_kl            | 2.4596477e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00623      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0237        |
|    n_updates            | 6250          |
|    policy_gradient_loss | -1.25e-05     |
|    value_loss           | 0.0602        |
-------------------------------------------
Eval num_timesteps=2565000, episode_reward=0.90 +/- 1.04
Episode length: 309.20 +/- 54.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 309           |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 2565000       |
| train/                  |               |
|    approx_kl            | 2.0026477e-05 |
|    clip_fraction        | 0.000806      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00706      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0374        |
|    n_updates            | 6260          |
|    policy_gradient_loss | -1.54e-05     |
|    value_loss           | 0.0543        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 627      |
|    time_elapsed    | 4234     |
|    total_timesteps | 2568192  |
---------------------------------
Eval num_timesteps=2570000, episode_reward=0.60 +/- 0.80
Episode length: 312.20 +/- 56.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 2570000      |
| train/                  |              |
|    approx_kl            | 5.541733e-06 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00776     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0274       |
|    n_updates            | 6270         |
|    policy_gradient_loss | -3.23e-06    |
|    value_loss           | 0.0613       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 628      |
|    time_elapsed    | 4241     |
|    total_timesteps | 2572288  |
---------------------------------
Eval num_timesteps=2575000, episode_reward=0.30 +/- 0.46
Episode length: 295.80 +/- 48.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 296           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2575000       |
| train/                  |               |
|    approx_kl            | 2.9085044e-05 |
|    clip_fraction        | 0.000269      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.009        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0371        |
|    n_updates            | 6280          |
|    policy_gradient_loss | 4.75e-05      |
|    value_loss           | 0.0613        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 629      |
|    time_elapsed    | 4249     |
|    total_timesteps | 2576384  |
---------------------------------
Eval num_timesteps=2580000, episode_reward=0.30 +/- 0.46
Episode length: 311.20 +/- 38.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 311          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2580000      |
| train/                  |              |
|    approx_kl            | 5.918299e-05 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00743     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0355       |
|    n_updates            | 6290         |
|    policy_gradient_loss | -2.23e-05    |
|    value_loss           | 0.056        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 630      |
|    time_elapsed    | 4256     |
|    total_timesteps | 2580480  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 311           |
|    ep_rew_mean          | 0.56          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 631           |
|    time_elapsed         | 4261          |
|    total_timesteps      | 2584576       |
| train/                  |               |
|    approx_kl            | 5.2335992e-05 |
|    clip_fraction        | 0.000708      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00933      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0398        |
|    n_updates            | 6300          |
|    policy_gradient_loss | -0.000141     |
|    value_loss           | 0.0616        |
-------------------------------------------
Eval num_timesteps=2585000, episode_reward=0.80 +/- 0.75
Episode length: 320.20 +/- 73.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 320           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 2585000       |
| train/                  |               |
|    approx_kl            | 1.9072992e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0231        |
|    n_updates            | 6310          |
|    policy_gradient_loss | -4.35e-06     |
|    value_loss           | 0.053         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 632      |
|    time_elapsed    | 4268     |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2590000, episode_reward=0.90 +/- 0.70
Episode length: 336.20 +/- 63.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 336           |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 2590000       |
| train/                  |               |
|    approx_kl            | 7.0545793e-06 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0344        |
|    n_updates            | 6320          |
|    policy_gradient_loss | 4.38e-05      |
|    value_loss           | 0.064         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 633      |
|    time_elapsed    | 4276     |
|    total_timesteps | 2592768  |
---------------------------------
Eval num_timesteps=2595000, episode_reward=0.60 +/- 0.66
Episode length: 318.00 +/- 63.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2595000       |
| train/                  |               |
|    approx_kl            | 2.3386528e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0122       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0275        |
|    n_updates            | 6330          |
|    policy_gradient_loss | -8.75e-07     |
|    value_loss           | 0.0595        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 634      |
|    time_elapsed    | 4283     |
|    total_timesteps | 2596864  |
---------------------------------
Eval num_timesteps=2600000, episode_reward=0.30 +/- 0.46
Episode length: 292.40 +/- 38.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 292           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2600000       |
| train/                  |               |
|    approx_kl            | 2.6283538e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0116       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0384        |
|    n_updates            | 6340          |
|    policy_gradient_loss | 1.47e-05      |
|    value_loss           | 0.0561        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 635      |
|    time_elapsed    | 4290     |
|    total_timesteps | 2600960  |
---------------------------------
Eval num_timesteps=2605000, episode_reward=0.70 +/- 0.64
Episode length: 317.40 +/- 51.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 317           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 2605000       |
| train/                  |               |
|    approx_kl            | 4.0663435e-05 |
|    clip_fraction        | 0.001         |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0457        |
|    n_updates            | 6350          |
|    policy_gradient_loss | -2.21e-05     |
|    value_loss           | 0.063         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 636      |
|    time_elapsed    | 4297     |
|    total_timesteps | 2605056  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 307          |
|    ep_rew_mean          | 0.44         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 637          |
|    time_elapsed         | 4302         |
|    total_timesteps      | 2609152      |
| train/                  |              |
|    approx_kl            | 3.654514e-05 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0178      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0375       |
|    n_updates            | 6360         |
|    policy_gradient_loss | -0.000169    |
|    value_loss           | 0.0577       |
------------------------------------------
Eval num_timesteps=2610000, episode_reward=0.70 +/- 0.90
Episode length: 296.40 +/- 59.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 296           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 2610000       |
| train/                  |               |
|    approx_kl            | 0.00012496235 |
|    clip_fraction        | 0.00181       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0425        |
|    n_updates            | 6370          |
|    policy_gradient_loss | -0.000145     |
|    value_loss           | 0.0617        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 638      |
|    time_elapsed    | 4309     |
|    total_timesteps | 2613248  |
---------------------------------
Eval num_timesteps=2615000, episode_reward=0.30 +/- 0.46
Episode length: 286.60 +/- 43.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 287          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2615000      |
| train/                  |              |
|    approx_kl            | 1.384005e-05 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0126      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0302       |
|    n_updates            | 6380         |
|    policy_gradient_loss | 5.59e-05     |
|    value_loss           | 0.0618       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 639      |
|    time_elapsed    | 4317     |
|    total_timesteps | 2617344  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=0.50 +/- 0.67
Episode length: 310.20 +/- 50.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 310         |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 2620000     |
| train/                  |             |
|    approx_kl            | 6.25667e-05 |
|    clip_fraction        | 0.000562    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0123     |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.024       |
|    n_updates            | 6390        |
|    policy_gradient_loss | 5.04e-05    |
|    value_loss           | 0.0598      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 640      |
|    time_elapsed    | 4324     |
|    total_timesteps | 2621440  |
---------------------------------
Eval num_timesteps=2625000, episode_reward=0.60 +/- 0.66
Episode length: 291.00 +/- 72.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2625000       |
| train/                  |               |
|    approx_kl            | 4.0275103e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0296        |
|    n_updates            | 6400          |
|    policy_gradient_loss | -1.89e-05     |
|    value_loss           | 0.0521        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 641      |
|    time_elapsed    | 4331     |
|    total_timesteps | 2625536  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 302           |
|    ep_rew_mean          | 0.44          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 642           |
|    time_elapsed         | 4336          |
|    total_timesteps      | 2629632       |
| train/                  |               |
|    approx_kl            | 0.00039563514 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0189       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0451        |
|    n_updates            | 6410          |
|    policy_gradient_loss | -0.000153     |
|    value_loss           | 0.0586        |
-------------------------------------------
Eval num_timesteps=2630000, episode_reward=0.80 +/- 0.87
Episode length: 339.00 +/- 53.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 339           |
|    mean_reward          | 0.8           |
| time/                   |               |
|    total_timesteps      | 2630000       |
| train/                  |               |
|    approx_kl            | 0.00010572885 |
|    clip_fraction        | 0.00229       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0262       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.029         |
|    n_updates            | 6420          |
|    policy_gradient_loss | -8.87e-05     |
|    value_loss           | 0.068         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 643      |
|    time_elapsed    | 4343     |
|    total_timesteps | 2633728  |
---------------------------------
Eval num_timesteps=2635000, episode_reward=0.60 +/- 0.66
Episode length: 297.20 +/- 60.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 297           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2635000       |
| train/                  |               |
|    approx_kl            | 4.8498026e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0257       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0222        |
|    n_updates            | 6430          |
|    policy_gradient_loss | 0.000146      |
|    value_loss           | 0.0635        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 644      |
|    time_elapsed    | 4350     |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=0.20 +/- 0.40
Episode length: 294.80 +/- 35.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 295           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 2640000       |
| train/                  |               |
|    approx_kl            | 7.6632554e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.025        |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0389        |
|    n_updates            | 6440          |
|    policy_gradient_loss | 5.99e-07      |
|    value_loss           | 0.0628        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 645      |
|    time_elapsed    | 4357     |
|    total_timesteps | 2641920  |
---------------------------------
Eval num_timesteps=2645000, episode_reward=0.40 +/- 0.49
Episode length: 294.00 +/- 34.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 294          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 2645000      |
| train/                  |              |
|    approx_kl            | 4.487275e-05 |
|    clip_fraction        | 0.00173      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0291      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0288       |
|    n_updates            | 6450         |
|    policy_gradient_loss | -0.000153    |
|    value_loss           | 0.0631       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 646      |
|    time_elapsed    | 4365     |
|    total_timesteps | 2646016  |
---------------------------------
Eval num_timesteps=2650000, episode_reward=0.40 +/- 0.49
Episode length: 282.40 +/- 46.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 282           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2650000       |
| train/                  |               |
|    approx_kl            | 7.7974735e-05 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0234       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0136        |
|    n_updates            | 6460          |
|    policy_gradient_loss | -0.000199     |
|    value_loss           | 0.0583        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 647      |
|    time_elapsed    | 4371     |
|    total_timesteps | 2650112  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 303          |
|    ep_rew_mean          | 0.46         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 648          |
|    time_elapsed         | 4377         |
|    total_timesteps      | 2654208      |
| train/                  |              |
|    approx_kl            | 0.0001251685 |
|    clip_fraction        | 0.00193      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0263      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0172       |
|    n_updates            | 6470         |
|    policy_gradient_loss | -0.000151    |
|    value_loss           | 0.0617       |
------------------------------------------
Eval num_timesteps=2655000, episode_reward=1.00 +/- 0.77
Episode length: 318.00 +/- 67.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 318          |
|    mean_reward          | 1            |
| time/                   |              |
|    total_timesteps      | 2655000      |
| train/                  |              |
|    approx_kl            | 7.323954e-05 |
|    clip_fraction        | 0.000757     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0286      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0247       |
|    n_updates            | 6480         |
|    policy_gradient_loss | -0.000136    |
|    value_loss           | 0.0536       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 649      |
|    time_elapsed    | 4384     |
|    total_timesteps | 2658304  |
---------------------------------
Eval num_timesteps=2660000, episode_reward=0.30 +/- 0.64
Episode length: 281.00 +/- 48.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 281           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2660000       |
| train/                  |               |
|    approx_kl            | 2.5809757e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0282       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0275        |
|    n_updates            | 6490          |
|    policy_gradient_loss | 4.77e-06      |
|    value_loss           | 0.0563        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 650      |
|    time_elapsed    | 4391     |
|    total_timesteps | 2662400  |
---------------------------------
Eval num_timesteps=2665000, episode_reward=0.20 +/- 0.40
Episode length: 282.40 +/- 33.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 282           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 2665000       |
| train/                  |               |
|    approx_kl            | 0.00013111156 |
|    clip_fraction        | 0.00171       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0227       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0163        |
|    n_updates            | 6500          |
|    policy_gradient_loss | -3.67e-05     |
|    value_loss           | 0.0575        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 651      |
|    time_elapsed    | 4398     |
|    total_timesteps | 2666496  |
---------------------------------
Eval num_timesteps=2670000, episode_reward=0.10 +/- 0.30
Episode length: 296.20 +/- 48.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 296           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 2670000       |
| train/                  |               |
|    approx_kl            | 1.9328669e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0224       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0249        |
|    n_updates            | 6510          |
|    policy_gradient_loss | 4.42e-06      |
|    value_loss           | 0.0643        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 652      |
|    time_elapsed    | 4405     |
|    total_timesteps | 2670592  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 302           |
|    ep_rew_mean          | 0.43          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 653           |
|    time_elapsed         | 4410          |
|    total_timesteps      | 2674688       |
| train/                  |               |
|    approx_kl            | 0.00024402888 |
|    clip_fraction        | 0.00232       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0169        |
|    n_updates            | 6520          |
|    policy_gradient_loss | -0.000151     |
|    value_loss           | 0.0638        |
-------------------------------------------
Eval num_timesteps=2675000, episode_reward=0.50 +/- 0.67
Episode length: 298.60 +/- 67.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 299           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 2675000       |
| train/                  |               |
|    approx_kl            | 0.00010405561 |
|    clip_fraction        | 0.00339       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0249       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0255        |
|    n_updates            | 6530          |
|    policy_gradient_loss | -0.000197     |
|    value_loss           | 0.0591        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 654      |
|    time_elapsed    | 4417     |
|    total_timesteps | 2678784  |
---------------------------------
Eval num_timesteps=2680000, episode_reward=0.30 +/- 0.46
Episode length: 255.40 +/- 29.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 255           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2680000       |
| train/                  |               |
|    approx_kl            | 0.00016142489 |
|    clip_fraction        | 0.0019        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0217       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0492        |
|    n_updates            | 6540          |
|    policy_gradient_loss | -1.67e-05     |
|    value_loss           | 0.0712        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 655      |
|    time_elapsed    | 4424     |
|    total_timesteps | 2682880  |
---------------------------------
Eval num_timesteps=2685000, episode_reward=0.60 +/- 0.66
Episode length: 300.80 +/- 53.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 301           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2685000       |
| train/                  |               |
|    approx_kl            | 0.00012267214 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0171       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0264        |
|    n_updates            | 6550          |
|    policy_gradient_loss | -5.24e-05     |
|    value_loss           | 0.0613        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 656      |
|    time_elapsed    | 4431     |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=0.40 +/- 0.49
Episode length: 279.80 +/- 55.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 280           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2690000       |
| train/                  |               |
|    approx_kl            | 0.00011052762 |
|    clip_fraction        | 0.00161       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0196       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0229        |
|    n_updates            | 6560          |
|    policy_gradient_loss | -3.78e-05     |
|    value_loss           | 0.0612        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 657      |
|    time_elapsed    | 4438     |
|    total_timesteps | 2691072  |
---------------------------------
Eval num_timesteps=2695000, episode_reward=0.50 +/- 0.67
Episode length: 311.80 +/- 33.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 312           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 2695000       |
| train/                  |               |
|    approx_kl            | 0.00013507635 |
|    clip_fraction        | 0.00232       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0231        |
|    n_updates            | 6570          |
|    policy_gradient_loss | -0.00014      |
|    value_loss           | 0.0561        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 658      |
|    time_elapsed    | 4445     |
|    total_timesteps | 2695168  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 304           |
|    ep_rew_mean          | 0.53          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 659           |
|    time_elapsed         | 4450          |
|    total_timesteps      | 2699264       |
| train/                  |               |
|    approx_kl            | 4.0289597e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0124       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0334        |
|    n_updates            | 6580          |
|    policy_gradient_loss | 1.28e-07      |
|    value_loss           | 0.066         |
-------------------------------------------
Eval num_timesteps=2700000, episode_reward=0.30 +/- 0.46
Episode length: 306.20 +/- 63.97
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 306            |
|    mean_reward          | 0.3            |
| time/                   |                |
|    total_timesteps      | 2700000        |
| train/                  |                |
|    approx_kl            | 1.31766865e-05 |
|    clip_fraction        | 0.00022        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.011         |
|    explained_variance   | 0              |
|    learning_rate        | 0.01           |
|    loss                 | 0.0425         |
|    n_updates            | 6590           |
|    policy_gradient_loss | 1.34e-05       |
|    value_loss           | 0.0642         |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 660      |
|    time_elapsed    | 4457     |
|    total_timesteps | 2703360  |
---------------------------------
Eval num_timesteps=2705000, episode_reward=0.60 +/- 0.66
Episode length: 299.80 +/- 41.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2705000       |
| train/                  |               |
|    approx_kl            | 1.7685088e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0279        |
|    n_updates            | 6600          |
|    policy_gradient_loss | 5.05e-07      |
|    value_loss           | 0.0648        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 661      |
|    time_elapsed    | 4464     |
|    total_timesteps | 2707456  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=0.60 +/- 0.80
Episode length: 323.40 +/- 85.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2710000       |
| train/                  |               |
|    approx_kl            | 2.5766436e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00775      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0187        |
|    n_updates            | 6610          |
|    policy_gradient_loss | -4.83e-05     |
|    value_loss           | 0.0592        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 662      |
|    time_elapsed    | 4472     |
|    total_timesteps | 2711552  |
---------------------------------
Eval num_timesteps=2715000, episode_reward=0.60 +/- 0.80
Episode length: 302.60 +/- 68.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 303          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 2715000      |
| train/                  |              |
|    approx_kl            | 3.587315e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00671     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0212       |
|    n_updates            | 6620         |
|    policy_gradient_loss | -4.07e-05    |
|    value_loss           | 0.0567       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 663      |
|    time_elapsed    | 4479     |
|    total_timesteps | 2715648  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 309          |
|    ep_rew_mean          | 0.53         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 664          |
|    time_elapsed         | 4484         |
|    total_timesteps      | 2719744      |
| train/                  |              |
|    approx_kl            | 4.394038e-05 |
|    clip_fraction        | 0.001        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00544     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0237       |
|    n_updates            | 6630         |
|    policy_gradient_loss | -3.74e-05    |
|    value_loss           | 0.0599       |
------------------------------------------
Eval num_timesteps=2720000, episode_reward=0.90 +/- 0.94
Episode length: 324.80 +/- 56.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 325           |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 2720000       |
| train/                  |               |
|    approx_kl            | 1.3998069e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00597      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0303        |
|    n_updates            | 6640          |
|    policy_gradient_loss | -5.84e-06     |
|    value_loss           | 0.0596        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 665      |
|    time_elapsed    | 4491     |
|    total_timesteps | 2723840  |
---------------------------------
Eval num_timesteps=2725000, episode_reward=0.40 +/- 0.49
Episode length: 302.60 +/- 61.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 303           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2725000       |
| train/                  |               |
|    approx_kl            | 1.0790391e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00559      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0425        |
|    n_updates            | 6650          |
|    policy_gradient_loss | -3.31e-06     |
|    value_loss           | 0.0664        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 666      |
|    time_elapsed    | 4498     |
|    total_timesteps | 2727936  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=0.70 +/- 0.64
Episode length: 324.00 +/- 50.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 324         |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 2730000     |
| train/                  |             |
|    approx_kl            | 0.000113612 |
|    clip_fraction        | 0.00125     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00405    |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0328      |
|    n_updates            | 6660        |
|    policy_gradient_loss | -9.93e-05   |
|    value_loss           | 0.0638      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 667      |
|    time_elapsed    | 4506     |
|    total_timesteps | 2732032  |
---------------------------------
Eval num_timesteps=2735000, episode_reward=0.70 +/- 0.64
Episode length: 317.80 +/- 58.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 318         |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 2735000     |
| train/                  |             |
|    approx_kl            | 8.82757e-06 |
|    clip_fraction        | 7.32e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00363    |
|    explained_variance   | 5.96e-08    |
|    learning_rate        | 0.01        |
|    loss                 | 0.0314      |
|    n_updates            | 6670        |
|    policy_gradient_loss | -1.73e-06   |
|    value_loss           | 0.059       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 668      |
|    time_elapsed    | 4513     |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2740000, episode_reward=0.30 +/- 0.64
Episode length: 271.60 +/- 33.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2740000      |
| train/                  |              |
|    approx_kl            | 6.937276e-05 |
|    clip_fraction        | 0.000659     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00272     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0383       |
|    n_updates            | 6680         |
|    policy_gradient_loss | -7.09e-05    |
|    value_loss           | 0.0592       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 669      |
|    time_elapsed    | 4519     |
|    total_timesteps | 2740224  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 299       |
|    ep_rew_mean          | 0.39      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 670       |
|    time_elapsed         | 4525      |
|    total_timesteps      | 2744320   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00245  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0278    |
|    n_updates            | 6690      |
|    policy_gradient_loss | -1.95e-10 |
|    value_loss           | 0.0606    |
---------------------------------------
Eval num_timesteps=2745000, episode_reward=0.50 +/- 0.67
Episode length: 297.20 +/- 38.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 2745000      |
| train/                  |              |
|    approx_kl            | 4.650821e-06 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00283     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.00694      |
|    n_updates            | 6700         |
|    policy_gradient_loss | -1.64e-06    |
|    value_loss           | 0.0609       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 671      |
|    time_elapsed    | 4532     |
|    total_timesteps | 2748416  |
---------------------------------
Eval num_timesteps=2750000, episode_reward=0.60 +/- 0.66
Episode length: 296.20 +/- 46.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 296           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 2750000       |
| train/                  |               |
|    approx_kl            | 1.9354193e-06 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00315      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0384        |
|    n_updates            | 6710          |
|    policy_gradient_loss | 1.04e-06      |
|    value_loss           | 0.0608        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 672      |
|    time_elapsed    | 4539     |
|    total_timesteps | 2752512  |
---------------------------------
Eval num_timesteps=2755000, episode_reward=0.30 +/- 0.46
Episode length: 276.00 +/- 53.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 276           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2755000       |
| train/                  |               |
|    approx_kl            | 3.4949742e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00454      |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0315        |
|    n_updates            | 6720          |
|    policy_gradient_loss | -7.07e-05     |
|    value_loss           | 0.0584        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 673      |
|    time_elapsed    | 4546     |
|    total_timesteps | 2756608  |
---------------------------------
Eval num_timesteps=2760000, episode_reward=0.60 +/- 0.80
Episode length: 317.40 +/- 47.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 317          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 2760000      |
| train/                  |              |
|    approx_kl            | 3.767875e-05 |
|    clip_fraction        | 0.000464     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00509     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.0231       |
|    n_updates            | 6730         |
|    policy_gradient_loss | 7.96e-05     |
|    value_loss           | 0.0542       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 674      |
|    time_elapsed    | 4553     |
|    total_timesteps | 2760704  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 297          |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 675          |
|    time_elapsed         | 4558         |
|    total_timesteps      | 2764800      |
| train/                  |              |
|    approx_kl            | 1.376665e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.007       |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0265       |
|    n_updates            | 6740         |
|    policy_gradient_loss | -1.98e-05    |
|    value_loss           | 0.0642       |
------------------------------------------
Eval num_timesteps=2765000, episode_reward=0.20 +/- 0.40
Episode length: 292.20 +/- 53.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 292           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 2765000       |
| train/                  |               |
|    approx_kl            | 8.4833824e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00529      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.00932       |
|    n_updates            | 6750          |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 0.0575        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 676      |
|    time_elapsed    | 4565     |
|    total_timesteps | 2768896  |
---------------------------------
Eval num_timesteps=2770000, episode_reward=0.60 +/- 0.49
Episode length: 312.80 +/- 41.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 313          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 2770000      |
| train/                  |              |
|    approx_kl            | 3.149362e-05 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00392     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0284       |
|    n_updates            | 6760         |
|    policy_gradient_loss | -5.62e-05    |
|    value_loss           | 0.0658       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 677      |
|    time_elapsed    | 4573     |
|    total_timesteps | 2772992  |
---------------------------------
Eval num_timesteps=2775000, episode_reward=0.30 +/- 0.46
Episode length: 300.00 +/- 42.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 300         |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 8.56242e-06 |
|    clip_fraction        | 0.00022     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00298    |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0141      |
|    n_updates            | 6770        |
|    policy_gradient_loss | -0.000131   |
|    value_loss           | 0.0526      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 678      |
|    time_elapsed    | 4579     |
|    total_timesteps | 2777088  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=0.40 +/- 0.92
Episode length: 305.00 +/- 62.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 2780000      |
| train/                  |              |
|    approx_kl            | 3.901802e-05 |
|    clip_fraction        | 0.000513     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00359     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0501       |
|    n_updates            | 6780         |
|    policy_gradient_loss | -5.88e-05    |
|    value_loss           | 0.0701       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 679      |
|    time_elapsed    | 4587     |
|    total_timesteps | 2781184  |
---------------------------------
Eval num_timesteps=2785000, episode_reward=0.50 +/- 0.67
Episode length: 312.00 +/- 68.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 2785000      |
| train/                  |              |
|    approx_kl            | 6.017508e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00363     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0299       |
|    n_updates            | 6790         |
|    policy_gradient_loss | -2.43e-07    |
|    value_loss           | 0.0535       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 680      |
|    time_elapsed    | 4594     |
|    total_timesteps | 2785280  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 291          |
|    ep_rew_mean          | 0.4          |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 681          |
|    time_elapsed         | 4599         |
|    total_timesteps      | 2789376      |
| train/                  |              |
|    approx_kl            | 4.857585e-05 |
|    clip_fraction        | 0.000562     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00281     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.0234       |
|    n_updates            | 6800         |
|    policy_gradient_loss | -8.11e-05    |
|    value_loss           | 0.0606       |
------------------------------------------
Eval num_timesteps=2790000, episode_reward=0.30 +/- 0.46
Episode length: 315.80 +/- 54.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 316           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2790000       |
| train/                  |               |
|    approx_kl            | 1.1053329e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00223      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0265        |
|    n_updates            | 6810          |
|    policy_gradient_loss | -4.17e-05     |
|    value_loss           | 0.0536        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 682      |
|    time_elapsed    | 4606     |
|    total_timesteps | 2793472  |
---------------------------------
Eval num_timesteps=2795000, episode_reward=0.30 +/- 0.46
Episode length: 305.00 +/- 42.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2795000      |
| train/                  |              |
|    approx_kl            | 1.211332e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00169     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0257       |
|    n_updates            | 6820         |
|    policy_gradient_loss | -4.78e-05    |
|    value_loss           | 0.0637       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 683      |
|    time_elapsed    | 4613     |
|    total_timesteps | 2797568  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=0.70 +/- 0.78
Episode length: 315.80 +/- 54.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 316          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 2800000      |
| train/                  |              |
|    approx_kl            | 7.163326e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00168     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0217       |
|    n_updates            | 6830         |
|    policy_gradient_loss | 5.97e-07     |
|    value_loss           | 0.0588       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 684      |
|    time_elapsed    | 4620     |
|    total_timesteps | 2801664  |
---------------------------------
Eval num_timesteps=2805000, episode_reward=0.50 +/- 0.67
Episode length: 335.00 +/- 42.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 335       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2805000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00167  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0303    |
|    n_updates            | 6840      |
|    policy_gradient_loss | -1.07e-09 |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 685      |
|    time_elapsed    | 4628     |
|    total_timesteps | 2805760  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 302          |
|    ep_rew_mean          | 0.43         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 686          |
|    time_elapsed         | 4633         |
|    total_timesteps      | 2809856      |
| train/                  |              |
|    approx_kl            | 3.786928e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00247     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0505       |
|    n_updates            | 6850         |
|    policy_gradient_loss | -5.36e-05    |
|    value_loss           | 0.0631       |
------------------------------------------
Eval num_timesteps=2810000, episode_reward=0.30 +/- 0.64
Episode length: 296.20 +/- 67.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 296           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2810000       |
| train/                  |               |
|    approx_kl            | 1.6392616e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00335      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0279        |
|    n_updates            | 6860          |
|    policy_gradient_loss | -1.59e-05     |
|    value_loss           | 0.0632        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 687      |
|    time_elapsed    | 4640     |
|    total_timesteps | 2813952  |
---------------------------------
Eval num_timesteps=2815000, episode_reward=0.60 +/- 0.66
Episode length: 285.00 +/- 35.41
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 285      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 2815000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00345 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0306   |
|    n_updates            | 6870     |
|    policy_gradient_loss | 7.09e-10 |
|    value_loss           | 0.0525   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 688      |
|    time_elapsed    | 4647     |
|    total_timesteps | 2818048  |
---------------------------------
Eval num_timesteps=2820000, episode_reward=0.60 +/- 0.66
Episode length: 311.40 +/- 49.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 311          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 2820000      |
| train/                  |              |
|    approx_kl            | 7.314622e-06 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00304     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0268       |
|    n_updates            | 6880         |
|    policy_gradient_loss | -5.87e-07    |
|    value_loss           | 0.0602       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 689      |
|    time_elapsed    | 4654     |
|    total_timesteps | 2822144  |
---------------------------------
Eval num_timesteps=2825000, episode_reward=0.70 +/- 0.78
Episode length: 331.80 +/- 40.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 332        |
|    mean_reward          | 0.7        |
| time/                   |            |
|    total_timesteps      | 2825000    |
| train/                  |            |
|    approx_kl            | 6.3638e-06 |
|    clip_fraction        | 0.000342   |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00328   |
|    explained_variance   | 0          |
|    learning_rate        | 0.01       |
|    loss                 | 0.0324     |
|    n_updates            | 6890       |
|    policy_gradient_loss | 9.34e-07   |
|    value_loss           | 0.0595     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 690      |
|    time_elapsed    | 4662     |
|    total_timesteps | 2826240  |
---------------------------------
Eval num_timesteps=2830000, episode_reward=0.00 +/- 0.00
Episode length: 271.60 +/- 38.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 272          |
|    mean_reward          | 0            |
| time/                   |              |
|    total_timesteps      | 2830000      |
| train/                  |              |
|    approx_kl            | 7.882205e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00402     |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.01         |
|    loss                 | 0.0261       |
|    n_updates            | 6900         |
|    policy_gradient_loss | -1.05e-06    |
|    value_loss           | 0.0577       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 691      |
|    time_elapsed    | 4668     |
|    total_timesteps | 2830336  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 306           |
|    ep_rew_mean          | 0.41          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 692           |
|    time_elapsed         | 4674          |
|    total_timesteps      | 2834432       |
| train/                  |               |
|    approx_kl            | 3.7052712e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00306      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0367        |
|    n_updates            | 6910          |
|    policy_gradient_loss | -1.03e-05     |
|    value_loss           | 0.061         |
-------------------------------------------
Eval num_timesteps=2835000, episode_reward=0.30 +/- 0.46
Episode length: 284.20 +/- 45.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2835000      |
| train/                  |              |
|    approx_kl            | 4.847825e-07 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00279     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0307       |
|    n_updates            | 6920         |
|    policy_gradient_loss | 7.78e-07     |
|    value_loss           | 0.0563       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 693      |
|    time_elapsed    | 4681     |
|    total_timesteps | 2838528  |
---------------------------------
Eval num_timesteps=2840000, episode_reward=0.50 +/- 0.50
Episode length: 320.60 +/- 59.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 321       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2840000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00275  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0261    |
|    n_updates            | 6930      |
|    policy_gradient_loss | -5.87e-10 |
|    value_loss           | 0.0597    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 694      |
|    time_elapsed    | 4688     |
|    total_timesteps | 2842624  |
---------------------------------
Eval num_timesteps=2845000, episode_reward=0.10 +/- 0.30
Episode length: 290.80 +/- 41.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 291           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 2845000       |
| train/                  |               |
|    approx_kl            | 4.3557855e-05 |
|    clip_fraction        | 0.000464      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00175      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0251        |
|    n_updates            | 6940          |
|    policy_gradient_loss | -5.7e-05      |
|    value_loss           | 0.0602        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 695      |
|    time_elapsed    | 4695     |
|    total_timesteps | 2846720  |
---------------------------------
Eval num_timesteps=2850000, episode_reward=0.50 +/- 0.67
Episode length: 319.00 +/- 53.11
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 319      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 2850000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00187 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0262   |
|    n_updates            | 6950     |
|    policy_gradient_loss | 1.66e-10 |
|    value_loss           | 0.056    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 696      |
|    time_elapsed    | 4702     |
|    total_timesteps | 2850816  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.42      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 697       |
|    time_elapsed         | 4707      |
|    total_timesteps      | 2854912   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00187  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0248    |
|    n_updates            | 6960      |
|    policy_gradient_loss | -1.14e-10 |
|    value_loss           | 0.0606    |
---------------------------------------
Eval num_timesteps=2855000, episode_reward=0.70 +/- 0.64
Episode length: 314.40 +/- 51.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 314          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 2855000      |
| train/                  |              |
|    approx_kl            | 5.756723e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00223     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.04         |
|    n_updates            | 6970         |
|    policy_gradient_loss | -4.9e-06     |
|    value_loss           | 0.0564       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 698      |
|    time_elapsed    | 4714     |
|    total_timesteps | 2859008  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=0.40 +/- 0.66
Episode length: 304.00 +/- 44.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2860000       |
| train/                  |               |
|    approx_kl            | 7.1481627e-06 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00268      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.032         |
|    n_updates            | 6980          |
|    policy_gradient_loss | -3.15e-06     |
|    value_loss           | 0.0628        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 699      |
|    time_elapsed    | 4722     |
|    total_timesteps | 2863104  |
---------------------------------
Eval num_timesteps=2865000, episode_reward=0.60 +/- 0.66
Episode length: 315.00 +/- 55.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 315      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 2865000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00272 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0225   |
|    n_updates            | 6990     |
|    policy_gradient_loss | 2.74e-10 |
|    value_loss           | 0.0586   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 700      |
|    time_elapsed    | 4729     |
|    total_timesteps | 2867200  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=0.30 +/- 0.46
Episode length: 287.60 +/- 31.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 2870000      |
| train/                  |              |
|    approx_kl            | 3.900197e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00167     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0302       |
|    n_updates            | 7000         |
|    policy_gradient_loss | -8e-05       |
|    value_loss           | 0.0624       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 701      |
|    time_elapsed    | 4736     |
|    total_timesteps | 2871296  |
---------------------------------
Eval num_timesteps=2875000, episode_reward=0.70 +/- 0.64
Episode length: 303.60 +/- 34.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 2875000       |
| train/                  |               |
|    approx_kl            | 2.2976106e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00114      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.014         |
|    n_updates            | 7010          |
|    policy_gradient_loss | -4.3e-05      |
|    value_loss           | 0.0617        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 702      |
|    time_elapsed    | 4743     |
|    total_timesteps | 2875392  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 301      |
|    ep_rew_mean          | 0.4      |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 703      |
|    time_elapsed         | 4748     |
|    total_timesteps      | 2879488  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00109 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0308   |
|    n_updates            | 7020     |
|    policy_gradient_loss | 7.53e-10 |
|    value_loss           | 0.0547   |
--------------------------------------
Eval num_timesteps=2880000, episode_reward=0.50 +/- 0.50
Episode length: 319.80 +/- 56.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 320       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2880000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00109  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0226    |
|    n_updates            | 7030      |
|    policy_gradient_loss | -3.41e-10 |
|    value_loss           | 0.0633    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 704      |
|    time_elapsed    | 4755     |
|    total_timesteps | 2883584  |
---------------------------------
Eval num_timesteps=2885000, episode_reward=0.30 +/- 0.64
Episode length: 302.20 +/- 59.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 2885000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00109  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00715   |
|    n_updates            | 7040      |
|    policy_gradient_loss | -9.07e-10 |
|    value_loss           | 0.0593    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 705      |
|    time_elapsed    | 4762     |
|    total_timesteps | 2887680  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=0.30 +/- 0.46
Episode length: 302.80 +/- 35.35
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 303      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 2890000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00109 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0318   |
|    n_updates            | 7050     |
|    policy_gradient_loss | 7.57e-11 |
|    value_loss           | 0.0636   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 706      |
|    time_elapsed    | 4769     |
|    total_timesteps | 2891776  |
---------------------------------
Eval num_timesteps=2895000, episode_reward=0.20 +/- 0.40
Episode length: 280.80 +/- 58.22
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 281      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 2895000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00109 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.029    |
|    n_updates            | 7060     |
|    policy_gradient_loss | 0        |
|    value_loss           | 0.0677   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 707      |
|    time_elapsed    | 4776     |
|    total_timesteps | 2895872  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 296           |
|    ep_rew_mean          | 0.42          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 708           |
|    time_elapsed         | 4781          |
|    total_timesteps      | 2899968       |
| train/                  |               |
|    approx_kl            | 2.1112632e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00119      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0351        |
|    n_updates            | 7070          |
|    policy_gradient_loss | 2.01e-06      |
|    value_loss           | 0.058         |
-------------------------------------------
Eval num_timesteps=2900000, episode_reward=0.40 +/- 0.49
Episode length: 314.00 +/- 43.55
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 314      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 2900000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00129 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0188   |
|    n_updates            | 7080     |
|    policy_gradient_loss | 9.35e-11 |
|    value_loss           | 0.0603   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 709      |
|    time_elapsed    | 4788     |
|    total_timesteps | 2904064  |
---------------------------------
Eval num_timesteps=2905000, episode_reward=0.60 +/- 0.66
Episode length: 319.20 +/- 55.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 319          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 2905000      |
| train/                  |              |
|    approx_kl            | 5.070935e-05 |
|    clip_fraction        | 0.000366     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000951    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0206       |
|    n_updates            | 7090         |
|    policy_gradient_loss | -2.97e-06    |
|    value_loss           | 0.0595       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 710      |
|    time_elapsed    | 4796     |
|    total_timesteps | 2908160  |
---------------------------------
Eval num_timesteps=2910000, episode_reward=0.20 +/- 0.40
Episode length: 295.20 +/- 66.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2910000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.016     |
|    n_updates            | 7100      |
|    policy_gradient_loss | -8.83e-10 |
|    value_loss           | 0.0527    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 711      |
|    time_elapsed    | 4803     |
|    total_timesteps | 2912256  |
---------------------------------
Eval num_timesteps=2915000, episode_reward=0.70 +/- 0.78
Episode length: 334.00 +/- 41.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 334       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 2915000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0263    |
|    n_updates            | 7110      |
|    policy_gradient_loss | -4.18e-10 |
|    value_loss           | 0.0653    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 712      |
|    time_elapsed    | 4810     |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2920000, episode_reward=0.10 +/- 0.30
Episode length: 264.00 +/- 29.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 264       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 2920000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0193    |
|    n_updates            | 7120      |
|    policy_gradient_loss | -5.41e-10 |
|    value_loss           | 0.0582    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 713      |
|    time_elapsed    | 4817     |
|    total_timesteps | 2920448  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 295       |
|    ep_rew_mean          | 0.36      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 714       |
|    time_elapsed         | 4822      |
|    total_timesteps      | 2924544   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0408    |
|    n_updates            | 7130      |
|    policy_gradient_loss | -4.14e-10 |
|    value_loss           | 0.0588    |
---------------------------------------
Eval num_timesteps=2925000, episode_reward=0.60 +/- 0.66
Episode length: 335.20 +/- 67.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 335       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 2925000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0106    |
|    n_updates            | 7140      |
|    policy_gradient_loss | -6.8e-10  |
|    value_loss           | 0.0625    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 715      |
|    time_elapsed    | 4830     |
|    total_timesteps | 2928640  |
---------------------------------
Eval num_timesteps=2930000, episode_reward=0.40 +/- 0.66
Episode length: 276.80 +/- 43.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 277       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2930000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0304    |
|    n_updates            | 7150      |
|    policy_gradient_loss | 4.37e-10  |
|    value_loss           | 0.0667    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 716      |
|    time_elapsed    | 4836     |
|    total_timesteps | 2932736  |
---------------------------------
Eval num_timesteps=2935000, episode_reward=0.60 +/- 0.66
Episode length: 298.00 +/- 60.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 2935000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.026     |
|    n_updates            | 7160      |
|    policy_gradient_loss | 4.55e-10  |
|    value_loss           | 0.058     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 717      |
|    time_elapsed    | 4844     |
|    total_timesteps | 2936832  |
---------------------------------
Eval num_timesteps=2940000, episode_reward=0.30 +/- 0.46
Episode length: 298.20 +/- 49.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 2940000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0235    |
|    n_updates            | 7170      |
|    policy_gradient_loss | -5.38e-11 |
|    value_loss           | 0.0608    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 718      |
|    time_elapsed    | 4851     |
|    total_timesteps | 2940928  |
---------------------------------
Eval num_timesteps=2945000, episode_reward=0.40 +/- 0.49
Episode length: 284.60 +/- 52.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2945000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0396    |
|    n_updates            | 7180      |
|    policy_gradient_loss | -1.24e-09 |
|    value_loss           | 0.0592    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 719      |
|    time_elapsed    | 4857     |
|    total_timesteps | 2945024  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 720       |
|    time_elapsed         | 4863      |
|    total_timesteps      | 2949120   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0256    |
|    n_updates            | 7190      |
|    policy_gradient_loss | 2.11e-11  |
|    value_loss           | 0.055     |
---------------------------------------
Eval num_timesteps=2950000, episode_reward=0.20 +/- 0.40
Episode length: 278.20 +/- 43.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 2950000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000867 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0296    |
|    n_updates            | 7200      |
|    policy_gradient_loss | 5.01e-10  |
|    value_loss           | 0.055     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 721      |
|    time_elapsed    | 4870     |
|    total_timesteps | 2953216  |
---------------------------------
Eval num_timesteps=2955000, episode_reward=0.40 +/- 0.49
Episode length: 305.80 +/- 47.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 306           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 2955000       |
| train/                  |               |
|    approx_kl            | 5.8176956e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00091      |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0313        |
|    n_updates            | 7210          |
|    policy_gradient_loss | 2.62e-06      |
|    value_loss           | 0.0581        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 722      |
|    time_elapsed    | 4877     |
|    total_timesteps | 2957312  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=0.40 +/- 0.66
Episode length: 290.20 +/- 41.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 2960000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000992 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0283    |
|    n_updates            | 7220      |
|    policy_gradient_loss | -8.3e-10  |
|    value_loss           | 0.0647    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 723      |
|    time_elapsed    | 4884     |
|    total_timesteps | 2961408  |
---------------------------------
Eval num_timesteps=2965000, episode_reward=0.30 +/- 0.64
Episode length: 281.20 +/- 30.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 281           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 2965000       |
| train/                  |               |
|    approx_kl            | 1.9297688e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00094      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.061         |
|    n_updates            | 7230          |
|    policy_gradient_loss | 2.09e-06      |
|    value_loss           | 0.0579        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 724      |
|    time_elapsed    | 4891     |
|    total_timesteps | 2965504  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.4       |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 725       |
|    time_elapsed         | 4896      |
|    total_timesteps      | 2969600   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.001    |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0288    |
|    n_updates            | 7240      |
|    policy_gradient_loss | -9.76e-10 |
|    value_loss           | 0.062     |
---------------------------------------
Eval num_timesteps=2970000, episode_reward=0.70 +/- 0.78
Episode length: 317.40 +/- 35.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 317      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 2970000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.001   |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.00778  |
|    n_updates            | 7250     |
|    policy_gradient_loss | 1.47e-10 |
|    value_loss           | 0.0578   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 726      |
|    time_elapsed    | 4903     |
|    total_timesteps | 2973696  |
---------------------------------
Eval num_timesteps=2975000, episode_reward=0.40 +/- 0.49
Episode length: 292.20 +/- 43.10
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 292      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 2975000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.001   |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0296   |
|    n_updates            | 7260     |
|    policy_gradient_loss | -5.1e-10 |
|    value_loss           | 0.0575   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 727      |
|    time_elapsed    | 4910     |
|    total_timesteps | 2977792  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=0.30 +/- 0.46
Episode length: 300.20 +/- 64.43
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 300      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 2980000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.001   |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0154   |
|    n_updates            | 7270     |
|    policy_gradient_loss | -1e-10   |
|    value_loss           | 0.0611   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 728      |
|    time_elapsed    | 4917     |
|    total_timesteps | 2981888  |
---------------------------------
Eval num_timesteps=2985000, episode_reward=0.50 +/- 0.50
Episode length: 312.00 +/- 62.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 2985000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.001    |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0289    |
|    n_updates            | 7280      |
|    policy_gradient_loss | -1.16e-11 |
|    value_loss           | 0.0701    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 729      |
|    time_elapsed    | 4925     |
|    total_timesteps | 2985984  |
---------------------------------
Eval num_timesteps=2990000, episode_reward=0.20 +/- 0.40
Episode length: 273.40 +/- 53.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 273           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 2990000       |
| train/                  |               |
|    approx_kl            | 1.8109786e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000754     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0314        |
|    n_updates            | 7290          |
|    policy_gradient_loss | 6.94e-07      |
|    value_loss           | 0.0661        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 730      |
|    time_elapsed    | 4932     |
|    total_timesteps | 2990080  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 304            |
|    ep_rew_mean          | 0.58           |
| time/                   |                |
|    fps                  | 606            |
|    iterations           | 731            |
|    time_elapsed         | 4937           |
|    total_timesteps      | 2994176        |
| train/                  |                |
|    approx_kl            | -5.9604645e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000787      |
|    explained_variance   | 0              |
|    learning_rate        | 0.01           |
|    loss                 | 0.0267         |
|    n_updates            | 7300           |
|    policy_gradient_loss | 9.09e-10       |
|    value_loss           | 0.0688         |
--------------------------------------------
Eval num_timesteps=2995000, episode_reward=0.80 +/- 0.75
Episode length: 309.20 +/- 33.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 2995000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000788 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0369    |
|    n_updates            | 7310      |
|    policy_gradient_loss | 1.8e-10   |
|    value_loss           | 0.0676    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.61     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 732      |
|    time_elapsed    | 4944     |
|    total_timesteps | 2998272  |
---------------------------------
Eval num_timesteps=3000000, episode_reward=0.60 +/- 0.49
Episode length: 335.60 +/- 44.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 336       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3000000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000788 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00823   |
|    n_updates            | 7320      |
|    policy_gradient_loss | 1.53e-11  |
|    value_loss           | 0.0583    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 733      |
|    time_elapsed    | 4951     |
|    total_timesteps | 3002368  |
---------------------------------
Eval num_timesteps=3005000, episode_reward=0.20 +/- 0.40
Episode length: 268.80 +/- 48.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 269         |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 3005000     |
| train/                  |             |
|    approx_kl            | 2.42221e-05 |
|    clip_fraction        | 0.000171    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000839   |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0155      |
|    n_updates            | 7330        |
|    policy_gradient_loss | -1.9e-06    |
|    value_loss           | 0.0598      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 734      |
|    time_elapsed    | 4958     |
|    total_timesteps | 3006464  |
---------------------------------
Eval num_timesteps=3010000, episode_reward=0.00 +/- 0.00
Episode length: 261.40 +/- 35.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 261       |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 3010000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0011   |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0377    |
|    n_updates            | 7340      |
|    policy_gradient_loss | -1.53e-11 |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 735      |
|    time_elapsed    | 4965     |
|    total_timesteps | 3010560  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 309      |
|    ep_rew_mean          | 0.53     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 736      |
|    time_elapsed         | 4970     |
|    total_timesteps      | 3014656  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0011  |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0115   |
|    n_updates            | 7350     |
|    policy_gradient_loss | 1.14e-10 |
|    value_loss           | 0.0529   |
--------------------------------------
Eval num_timesteps=3015000, episode_reward=0.40 +/- 0.66
Episode length: 294.40 +/- 27.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3015000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0011   |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0225    |
|    n_updates            | 7360      |
|    policy_gradient_loss | -4.27e-10 |
|    value_loss           | 0.0552    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 737      |
|    time_elapsed    | 4977     |
|    total_timesteps | 3018752  |
---------------------------------
Eval num_timesteps=3020000, episode_reward=0.40 +/- 0.92
Episode length: 285.20 +/- 53.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3020000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0011   |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.044     |
|    n_updates            | 7370      |
|    policy_gradient_loss | -6.24e-10 |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 738      |
|    time_elapsed    | 4984     |
|    total_timesteps | 3022848  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=0.50 +/- 0.50
Episode length: 287.00 +/- 43.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3025000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0011   |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0458    |
|    n_updates            | 7380      |
|    policy_gradient_loss | 5.99e-10  |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 739      |
|    time_elapsed    | 4991     |
|    total_timesteps | 3026944  |
---------------------------------
Eval num_timesteps=3030000, episode_reward=0.40 +/- 0.49
Episode length: 284.60 +/- 54.62
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 285      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 3030000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0011  |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0444   |
|    n_updates            | 7390     |
|    policy_gradient_loss | 3.19e-10 |
|    value_loss           | 0.0587   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 740      |
|    time_elapsed    | 4998     |
|    total_timesteps | 3031040  |
---------------------------------
Eval num_timesteps=3035000, episode_reward=0.30 +/- 0.64
Episode length: 308.80 +/- 35.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3035000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0011   |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0349    |
|    n_updates            | 7400      |
|    policy_gradient_loss | 0         |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 741      |
|    time_elapsed    | 5005     |
|    total_timesteps | 3035136  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 307           |
|    ep_rew_mean          | 0.47          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 742           |
|    time_elapsed         | 5010          |
|    total_timesteps      | 3039232       |
| train/                  |               |
|    approx_kl            | 2.0463194e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000915     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0372        |
|    n_updates            | 7410          |
|    policy_gradient_loss | 2.97e-07      |
|    value_loss           | 0.0656        |
-------------------------------------------
Eval num_timesteps=3040000, episode_reward=0.40 +/- 0.66
Episode length: 305.00 +/- 30.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3040000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000958 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.017     |
|    n_updates            | 7420      |
|    policy_gradient_loss | 8.15e-11  |
|    value_loss           | 0.0554    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 743      |
|    time_elapsed    | 5018     |
|    total_timesteps | 3043328  |
---------------------------------
Eval num_timesteps=3045000, episode_reward=0.40 +/- 0.49
Episode length: 310.20 +/- 48.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3045000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000958 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0292    |
|    n_updates            | 7430      |
|    policy_gradient_loss | 4.19e-10  |
|    value_loss           | 0.0571    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 744      |
|    time_elapsed    | 5025     |
|    total_timesteps | 3047424  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=0.30 +/- 0.46
Episode length: 290.80 +/- 45.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 291       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3050000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000958 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0171    |
|    n_updates            | 7440      |
|    policy_gradient_loss | 6.14e-10  |
|    value_loss           | 0.0645    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 745      |
|    time_elapsed    | 5032     |
|    total_timesteps | 3051520  |
---------------------------------
Eval num_timesteps=3055000, episode_reward=0.30 +/- 0.46
Episode length: 295.60 +/- 64.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3055000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000958 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0252    |
|    n_updates            | 7450      |
|    policy_gradient_loss | -3.89e-10 |
|    value_loss           | 0.0503    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 746      |
|    time_elapsed    | 5039     |
|    total_timesteps | 3055616  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 747       |
|    time_elapsed         | 5044      |
|    total_timesteps      | 3059712   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000958 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0268    |
|    n_updates            | 7460      |
|    policy_gradient_loss | -5.18e-10 |
|    value_loss           | 0.0576    |
---------------------------------------
Eval num_timesteps=3060000, episode_reward=0.60 +/- 0.66
Episode length: 307.60 +/- 64.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 308        |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 3060000    |
| train/                  |            |
|    approx_kl            | 4.3062e-05 |
|    clip_fraction        | 0.000195   |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.000766  |
|    explained_variance   | 0          |
|    learning_rate        | 0.01       |
|    loss                 | 0.0201     |
|    n_updates            | 7470       |
|    policy_gradient_loss | -2.83e-06  |
|    value_loss           | 0.0608     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 748      |
|    time_elapsed    | 5051     |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3065000, episode_reward=0.30 +/- 0.46
Episode length: 284.80 +/- 49.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 285           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 3065000       |
| train/                  |               |
|    approx_kl            | 7.7210425e-06 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000523     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0151        |
|    n_updates            | 7480          |
|    policy_gradient_loss | 3.06e-06      |
|    value_loss           | 0.0557        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 749      |
|    time_elapsed    | 5058     |
|    total_timesteps | 3067904  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=0.50 +/- 0.50
Episode length: 276.20 +/- 31.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 276           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3070000       |
| train/                  |               |
|    approx_kl            | 1.9221043e-06 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000523     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0204        |
|    n_updates            | 7490          |
|    policy_gradient_loss | 1.19e-07      |
|    value_loss           | 0.0604        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 750      |
|    time_elapsed    | 5065     |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=0.70 +/- 0.46
Episode length: 344.60 +/- 38.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 345       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3075000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000529 |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0227    |
|    n_updates            | 7500      |
|    policy_gradient_loss | 2e-09     |
|    value_loss           | 0.0667    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 751      |
|    time_elapsed    | 5072     |
|    total_timesteps | 3076096  |
---------------------------------
Eval num_timesteps=3080000, episode_reward=0.90 +/- 0.70
Episode length: 330.40 +/- 57.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 330       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 3080000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000529 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0414    |
|    n_updates            | 7510      |
|    policy_gradient_loss | -5.77e-10 |
|    value_loss           | 0.0623    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 752      |
|    time_elapsed    | 5080     |
|    total_timesteps | 3080192  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.51      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 753       |
|    time_elapsed         | 5085      |
|    total_timesteps      | 3084288   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000529 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0213    |
|    n_updates            | 7520      |
|    policy_gradient_loss | 6.64e-10  |
|    value_loss           | 0.058     |
---------------------------------------
Eval num_timesteps=3085000, episode_reward=0.20 +/- 0.40
Episode length: 303.80 +/- 62.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3085000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000529 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0586    |
|    n_updates            | 7530      |
|    policy_gradient_loss | -6.12e-10 |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 754      |
|    time_elapsed    | 5092     |
|    total_timesteps | 3088384  |
---------------------------------
Eval num_timesteps=3090000, episode_reward=0.60 +/- 0.80
Episode length: 339.80 +/- 56.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 340       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3090000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000529 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0562    |
|    n_updates            | 7540      |
|    policy_gradient_loss | -3.73e-10 |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 755      |
|    time_elapsed    | 5099     |
|    total_timesteps | 3092480  |
---------------------------------
Eval num_timesteps=3095000, episode_reward=0.30 +/- 0.64
Episode length: 294.80 +/- 38.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3095000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000529 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0385    |
|    n_updates            | 7550      |
|    policy_gradient_loss | -2.61e-10 |
|    value_loss           | 0.0589    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 756      |
|    time_elapsed    | 5106     |
|    total_timesteps | 3096576  |
---------------------------------
Eval num_timesteps=3100000, episode_reward=0.10 +/- 0.30
Episode length: 289.00 +/- 42.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 289           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 3100000       |
| train/                  |               |
|    approx_kl            | 5.7783414e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000846     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0466        |
|    n_updates            | 7560          |
|    policy_gradient_loss | -2.78e-05     |
|    value_loss           | 0.0586        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 757      |
|    time_elapsed    | 5113     |
|    total_timesteps | 3100672  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 306       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 758       |
|    time_elapsed         | 5118      |
|    total_timesteps      | 3104768   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000863 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0523    |
|    n_updates            | 7570      |
|    policy_gradient_loss | 5.16e-10  |
|    value_loss           | 0.0597    |
---------------------------------------
Eval num_timesteps=3105000, episode_reward=0.60 +/- 0.80
Episode length: 325.00 +/- 65.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3105000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000863 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0406    |
|    n_updates            | 7580      |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 0.0606    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 759      |
|    time_elapsed    | 5126     |
|    total_timesteps | 3108864  |
---------------------------------
Eval num_timesteps=3110000, episode_reward=0.70 +/- 0.46
Episode length: 303.80 +/- 71.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3110000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000863 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0271    |
|    n_updates            | 7590      |
|    policy_gradient_loss | 1.73e-10  |
|    value_loss           | 0.0659    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 760      |
|    time_elapsed    | 5133     |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3115000, episode_reward=0.90 +/- 0.94
Episode length: 318.60 +/- 60.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 319          |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 3115000      |
| train/                  |              |
|    approx_kl            | 4.191496e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00117     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0113       |
|    n_updates            | 7600         |
|    policy_gradient_loss | -3.28e-05    |
|    value_loss           | 0.0572       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 761      |
|    time_elapsed    | 5140     |
|    total_timesteps | 3117056  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=0.20 +/- 0.40
Episode length: 300.00 +/- 49.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 3120000      |
| train/                  |              |
|    approx_kl            | 7.052411e-06 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00131     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0328       |
|    n_updates            | 7610         |
|    policy_gradient_loss | -1.68e-06    |
|    value_loss           | 0.0582       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 762      |
|    time_elapsed    | 5147     |
|    total_timesteps | 3121152  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=0.60 +/- 0.66
Episode length: 330.00 +/- 43.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 330       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3125000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0014   |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0288    |
|    n_updates            | 7620      |
|    policy_gradient_loss | -7.86e-11 |
|    value_loss           | 0.0667    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 763      |
|    time_elapsed    | 5155     |
|    total_timesteps | 3125248  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 299      |
|    ep_rew_mean          | 0.46     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 764      |
|    time_elapsed         | 5159     |
|    total_timesteps      | 3129344  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0014  |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0481   |
|    n_updates            | 7630     |
|    policy_gradient_loss | 1.06e-09 |
|    value_loss           | 0.0583   |
--------------------------------------
Eval num_timesteps=3130000, episode_reward=0.20 +/- 0.40
Episode length: 300.60 +/- 44.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3130000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0014   |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0236    |
|    n_updates            | 7640      |
|    policy_gradient_loss | -2.75e-10 |
|    value_loss           | 0.0699    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 765      |
|    time_elapsed    | 5167     |
|    total_timesteps | 3133440  |
---------------------------------
Eval num_timesteps=3135000, episode_reward=0.60 +/- 0.66
Episode length: 285.80 +/- 47.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 286      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 3135000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0014  |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0436   |
|    n_updates            | 7650     |
|    policy_gradient_loss | -1.3e-10 |
|    value_loss           | 0.0582   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 766      |
|    time_elapsed    | 5173     |
|    total_timesteps | 3137536  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=0.50 +/- 0.67
Episode length: 314.80 +/- 47.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3140000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0014   |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0533    |
|    n_updates            | 7660      |
|    policy_gradient_loss | 6.56e-10  |
|    value_loss           | 0.0673    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 767      |
|    time_elapsed    | 5181     |
|    total_timesteps | 3141632  |
---------------------------------
Eval num_timesteps=3145000, episode_reward=0.60 +/- 0.66
Episode length: 330.20 +/- 57.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 330          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 3145000      |
| train/                  |              |
|    approx_kl            | 2.241529e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00173     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0306       |
|    n_updates            | 7670         |
|    policy_gradient_loss | -6.39e-05    |
|    value_loss           | 0.069        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 768      |
|    time_elapsed    | 5188     |
|    total_timesteps | 3145728  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 299      |
|    ep_rew_mean          | 0.54     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 769      |
|    time_elapsed         | 5193     |
|    total_timesteps      | 3149824  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00166 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0419   |
|    n_updates            | 7680     |
|    policy_gradient_loss | 6.01e-10 |
|    value_loss           | 0.0592   |
--------------------------------------
Eval num_timesteps=3150000, episode_reward=0.10 +/- 0.30
Episode length: 275.80 +/- 34.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 276      |
|    mean_reward          | 0.1      |
| time/                   |          |
|    total_timesteps      | 3150000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00166 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.011    |
|    n_updates            | 7690     |
|    policy_gradient_loss | 5e-10    |
|    value_loss           | 0.0649   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 770      |
|    time_elapsed    | 5200     |
|    total_timesteps | 3153920  |
---------------------------------
Eval num_timesteps=3155000, episode_reward=0.70 +/- 0.64
Episode length: 347.00 +/- 56.03
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 347      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 3155000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00166 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0139   |
|    n_updates            | 7700     |
|    policy_gradient_loss | 1.74e-10 |
|    value_loss           | 0.0615   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 771      |
|    time_elapsed    | 5207     |
|    total_timesteps | 3158016  |
---------------------------------
Eval num_timesteps=3160000, episode_reward=0.30 +/- 0.46
Episode length: 323.60 +/- 44.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 324       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3160000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00166  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0472    |
|    n_updates            | 7710      |
|    policy_gradient_loss | -6.77e-11 |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 772      |
|    time_elapsed    | 5215     |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3165000, episode_reward=1.20 +/- 0.98
Episode length: 320.20 +/- 35.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 320       |
|    mean_reward          | 1.2       |
| time/                   |           |
|    total_timesteps      | 3165000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00166  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0472    |
|    n_updates            | 7720      |
|    policy_gradient_loss | -2.62e-10 |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 773      |
|    time_elapsed    | 5222     |
|    total_timesteps | 3166208  |
---------------------------------
Eval num_timesteps=3170000, episode_reward=0.40 +/- 0.92
Episode length: 282.60 +/- 49.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 3170000      |
| train/                  |              |
|    approx_kl            | 8.535778e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0011      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0363       |
|    n_updates            | 7730         |
|    policy_gradient_loss | -7.47e-05    |
|    value_loss           | 0.065        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 774      |
|    time_elapsed    | 5229     |
|    total_timesteps | 3170304  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 298       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 775       |
|    time_elapsed         | 5234      |
|    total_timesteps      | 3174400   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000995 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0517    |
|    n_updates            | 7740      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 0.0638    |
---------------------------------------
Eval num_timesteps=3175000, episode_reward=0.40 +/- 0.92
Episode length: 300.40 +/- 51.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3175000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000995 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0557    |
|    n_updates            | 7750      |
|    policy_gradient_loss | -5.09e-10 |
|    value_loss           | 0.0592    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 776      |
|    time_elapsed    | 5241     |
|    total_timesteps | 3178496  |
---------------------------------
Eval num_timesteps=3180000, episode_reward=0.50 +/- 0.50
Episode length: 306.80 +/- 53.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 307           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3180000       |
| train/                  |               |
|    approx_kl            | 2.7255344e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00135      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0211        |
|    n_updates            | 7760          |
|    policy_gradient_loss | -9.45e-05     |
|    value_loss           | 0.0648        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 777      |
|    time_elapsed    | 5248     |
|    total_timesteps | 3182592  |
---------------------------------
Eval num_timesteps=3185000, episode_reward=0.60 +/- 0.49
Episode length: 305.20 +/- 56.73
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 305      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 3185000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00123 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0133   |
|    n_updates            | 7770     |
|    policy_gradient_loss | 2.74e-10 |
|    value_loss           | 0.0659   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 778      |
|    time_elapsed    | 5256     |
|    total_timesteps | 3186688  |
---------------------------------
Eval num_timesteps=3190000, episode_reward=0.50 +/- 0.50
Episode length: 302.80 +/- 43.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 303           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3190000       |
| train/                  |               |
|    approx_kl            | 6.6627545e-06 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00107      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0348        |
|    n_updates            | 7780          |
|    policy_gradient_loss | -5.51e-06     |
|    value_loss           | 0.0606        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 779      |
|    time_elapsed    | 5263     |
|    total_timesteps | 3190784  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 307           |
|    ep_rew_mean          | 0.46          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 780           |
|    time_elapsed         | 5268          |
|    total_timesteps      | 3194880       |
| train/                  |               |
|    approx_kl            | 1.3059718e-05 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00081      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0261        |
|    n_updates            | 7790          |
|    policy_gradient_loss | -1.54e-05     |
|    value_loss           | 0.0516        |
-------------------------------------------
Eval num_timesteps=3195000, episode_reward=0.70 +/- 0.64
Episode length: 321.00 +/- 56.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 321       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3195000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0162    |
|    n_updates            | 7800      |
|    policy_gradient_loss | 3.94e-10  |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 781      |
|    time_elapsed    | 5275     |
|    total_timesteps | 3198976  |
---------------------------------
Eval num_timesteps=3200000, episode_reward=0.30 +/- 0.46
Episode length: 305.00 +/- 46.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3200000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0213    |
|    n_updates            | 7810      |
|    policy_gradient_loss | 8.57e-10  |
|    value_loss           | 0.058     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 782      |
|    time_elapsed    | 5282     |
|    total_timesteps | 3203072  |
---------------------------------
Eval num_timesteps=3205000, episode_reward=0.30 +/- 0.46
Episode length: 286.20 +/- 44.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3205000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.017     |
|    n_updates            | 7820      |
|    policy_gradient_loss | -3.54e-10 |
|    value_loss           | 0.0671    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 0.61     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 783      |
|    time_elapsed    | 5289     |
|    total_timesteps | 3207168  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=0.30 +/- 0.46
Episode length: 319.60 +/- 44.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 320       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3210000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0189    |
|    n_updates            | 7830      |
|    policy_gradient_loss | 1.09e-10  |
|    value_loss           | 0.0704    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 314      |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 784      |
|    time_elapsed    | 5296     |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3215000, episode_reward=0.40 +/- 0.49
Episode length: 303.60 +/- 34.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3215000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0119    |
|    n_updates            | 7840      |
|    policy_gradient_loss | 3.46e-10  |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 785      |
|    time_elapsed    | 5304     |
|    total_timesteps | 3215360  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 310       |
|    ep_rew_mean          | 0.57      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 786       |
|    time_elapsed         | 5308      |
|    total_timesteps      | 3219456   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.049     |
|    n_updates            | 7850      |
|    policy_gradient_loss | 1.03e-10  |
|    value_loss           | 0.0665    |
---------------------------------------
Eval num_timesteps=3220000, episode_reward=1.20 +/- 0.98
Episode length: 300.80 +/- 51.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 1.2       |
| time/                   |           |
|    total_timesteps      | 3220000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0312    |
|    n_updates            | 7860      |
|    policy_gradient_loss | 1.44e-10  |
|    value_loss           | 0.0593    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 787      |
|    time_elapsed    | 5316     |
|    total_timesteps | 3223552  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=0.40 +/- 0.66
Episode length: 309.80 +/- 46.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3225000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0253    |
|    n_updates            | 7870      |
|    policy_gradient_loss | 8.59e-11  |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.64     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 788      |
|    time_elapsed    | 5323     |
|    total_timesteps | 3227648  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=0.30 +/- 0.46
Episode length: 289.20 +/- 30.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3230000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0233    |
|    n_updates            | 7880      |
|    policy_gradient_loss | 1.59e-09  |
|    value_loss           | 0.067     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 789      |
|    time_elapsed    | 5330     |
|    total_timesteps | 3231744  |
---------------------------------
Eval num_timesteps=3235000, episode_reward=0.50 +/- 0.67
Episode length: 291.40 +/- 53.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 291       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3235000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000771 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0343    |
|    n_updates            | 7890      |
|    policy_gradient_loss | 9.42e-11  |
|    value_loss           | 0.0655    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 790      |
|    time_elapsed    | 5337     |
|    total_timesteps | 3235840  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 300         |
|    ep_rew_mean          | 0.49        |
| time/                   |             |
|    fps                  | 606         |
|    iterations           | 791         |
|    time_elapsed         | 5342        |
|    total_timesteps      | 3239936     |
| train/                  |             |
|    approx_kl            | 4.31473e-06 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0009     |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0211      |
|    n_updates            | 7900        |
|    policy_gradient_loss | -8.47e-07   |
|    value_loss           | 0.0578      |
-----------------------------------------
Eval num_timesteps=3240000, episode_reward=0.70 +/- 0.46
Episode length: 330.60 +/- 41.60
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 331      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 3240000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00114 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0423   |
|    n_updates            | 7910     |
|    policy_gradient_loss | 7.28e-11 |
|    value_loss           | 0.0598   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 792      |
|    time_elapsed    | 5349     |
|    total_timesteps | 3244032  |
---------------------------------
Eval num_timesteps=3245000, episode_reward=0.30 +/- 0.46
Episode length: 287.00 +/- 39.06
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 287      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 3245000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00114 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0371   |
|    n_updates            | 7920     |
|    policy_gradient_loss | 4.39e-10 |
|    value_loss           | 0.0561   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 793      |
|    time_elapsed    | 5356     |
|    total_timesteps | 3248128  |
---------------------------------
Eval num_timesteps=3250000, episode_reward=0.30 +/- 0.46
Episode length: 309.60 +/- 48.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 310           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 3250000       |
| train/                  |               |
|    approx_kl            | 1.8594437e-07 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00119      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0152        |
|    n_updates            | 7930          |
|    policy_gradient_loss | 4.66e-07      |
|    value_loss           | 0.0542        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 794      |
|    time_elapsed    | 5363     |
|    total_timesteps | 3252224  |
---------------------------------
Eval num_timesteps=3255000, episode_reward=0.40 +/- 0.66
Episode length: 308.40 +/- 49.43
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 308      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 3255000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00126 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0216   |
|    n_updates            | 7940     |
|    policy_gradient_loss | 4.63e-10 |
|    value_loss           | 0.0652   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 795      |
|    time_elapsed    | 5370     |
|    total_timesteps | 3256320  |
---------------------------------
Eval num_timesteps=3260000, episode_reward=0.40 +/- 0.49
Episode length: 288.60 +/- 55.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3260000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00126  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0375    |
|    n_updates            | 7950      |
|    policy_gradient_loss | -2.65e-10 |
|    value_loss           | 0.0714    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 796      |
|    time_elapsed    | 5377     |
|    total_timesteps | 3260416  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 301           |
|    ep_rew_mean          | 0.51          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 797           |
|    time_elapsed         | 5383          |
|    total_timesteps      | 3264512       |
| train/                  |               |
|    approx_kl            | 6.4028427e-10 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0013       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.031         |
|    n_updates            | 7960          |
|    policy_gradient_loss | 3.56e-06      |
|    value_loss           | 0.0703        |
-------------------------------------------
Eval num_timesteps=3265000, episode_reward=0.70 +/- 0.64
Episode length: 310.60 +/- 53.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 311           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 3265000       |
| train/                  |               |
|    approx_kl            | 1.4317207e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00095      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0137        |
|    n_updates            | 7970          |
|    policy_gradient_loss | -5.74e-06     |
|    value_loss           | 0.0591        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 798      |
|    time_elapsed    | 5390     |
|    total_timesteps | 3268608  |
---------------------------------
Eval num_timesteps=3270000, episode_reward=0.30 +/- 0.46
Episode length: 322.40 +/- 41.30
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 322      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 3270000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00096 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0322   |
|    n_updates            | 7980     |
|    policy_gradient_loss | 1.23e-09 |
|    value_loss           | 0.0541   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 799      |
|    time_elapsed    | 5397     |
|    total_timesteps | 3272704  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=0.20 +/- 0.40
Episode length: 284.20 +/- 28.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 284      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 3275000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00096 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0259   |
|    n_updates            | 7990     |
|    policy_gradient_loss | 5.39e-10 |
|    value_loss           | 0.0573   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 800      |
|    time_elapsed    | 5404     |
|    total_timesteps | 3276800  |
---------------------------------
Eval num_timesteps=3280000, episode_reward=0.50 +/- 0.50
Episode length: 281.00 +/- 42.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3280000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00096  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0306    |
|    n_updates            | 8000      |
|    policy_gradient_loss | -5.24e-11 |
|    value_loss           | 0.0657    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 801      |
|    time_elapsed    | 5411     |
|    total_timesteps | 3280896  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 300      |
|    ep_rew_mean          | 0.52     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 802      |
|    time_elapsed         | 5416     |
|    total_timesteps      | 3284992  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00096 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0238   |
|    n_updates            | 8010     |
|    policy_gradient_loss | 2.9e-10  |
|    value_loss           | 0.0662   |
--------------------------------------
Eval num_timesteps=3285000, episode_reward=0.90 +/- 0.54
Episode length: 322.60 +/- 62.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 323           |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 3285000       |
| train/                  |               |
|    approx_kl            | 6.0547492e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00154      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.033         |
|    n_updates            | 8020          |
|    policy_gradient_loss | -0.000122     |
|    value_loss           | 0.0605        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 803      |
|    time_elapsed    | 5424     |
|    total_timesteps | 3289088  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=0.40 +/- 0.49
Episode length: 298.00 +/- 63.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 298      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 3290000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00156 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0237   |
|    n_updates            | 8030     |
|    policy_gradient_loss | 7.68e-10 |
|    value_loss           | 0.0607   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 804      |
|    time_elapsed    | 5431     |
|    total_timesteps | 3293184  |
---------------------------------
Eval num_timesteps=3295000, episode_reward=0.50 +/- 0.67
Episode length: 293.60 +/- 35.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 294           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3295000       |
| train/                  |               |
|    approx_kl            | 2.8125345e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0019       |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0333        |
|    n_updates            | 8040          |
|    policy_gradient_loss | -3.92e-05     |
|    value_loss           | 0.0659        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 805      |
|    time_elapsed    | 5438     |
|    total_timesteps | 3297280  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=0.30 +/- 0.46
Episode length: 278.60 +/- 41.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 279           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 3300000       |
| train/                  |               |
|    approx_kl            | 3.9199804e-06 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00171      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0536        |
|    n_updates            | 8050          |
|    policy_gradient_loss | -1.11e-06     |
|    value_loss           | 0.0642        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 806      |
|    time_elapsed    | 5445     |
|    total_timesteps | 3301376  |
---------------------------------
Eval num_timesteps=3305000, episode_reward=0.20 +/- 0.40
Episode length: 277.40 +/- 37.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 277       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3305000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00147  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0273    |
|    n_updates            | 8060      |
|    policy_gradient_loss | -3.31e-10 |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 807      |
|    time_elapsed    | 5452     |
|    total_timesteps | 3305472  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 306           |
|    ep_rew_mean          | 0.55          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 808           |
|    time_elapsed         | 5457          |
|    total_timesteps      | 3309568       |
| train/                  |               |
|    approx_kl            | 1.0319025e-05 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00138      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0136        |
|    n_updates            | 8070          |
|    policy_gradient_loss | -1e-06        |
|    value_loss           | 0.0621        |
-------------------------------------------
Eval num_timesteps=3310000, episode_reward=0.90 +/- 1.22
Episode length: 317.80 +/- 71.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 3310000       |
| train/                  |               |
|    approx_kl            | 5.9802318e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00202      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0201        |
|    n_updates            | 8080          |
|    policy_gradient_loss | -0.00012      |
|    value_loss           | 0.0624        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 809      |
|    time_elapsed    | 5464     |
|    total_timesteps | 3313664  |
---------------------------------
Eval num_timesteps=3315000, episode_reward=0.90 +/- 0.54
Episode length: 346.20 +/- 62.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 346           |
|    mean_reward          | 0.9           |
| time/                   |               |
|    total_timesteps      | 3315000       |
| train/                  |               |
|    approx_kl            | 2.0667794e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00226      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.031         |
|    n_updates            | 8090          |
|    policy_gradient_loss | -1.58e-07     |
|    value_loss           | 0.0583        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 810      |
|    time_elapsed    | 5471     |
|    total_timesteps | 3317760  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=0.20 +/- 0.40
Episode length: 322.60 +/- 65.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 323       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3320000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0024   |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0207    |
|    n_updates            | 8100      |
|    policy_gradient_loss | -9.31e-11 |
|    value_loss           | 0.0649    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 811      |
|    time_elapsed    | 5479     |
|    total_timesteps | 3321856  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=0.40 +/- 0.66
Episode length: 281.00 +/- 28.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 281           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 3325000       |
| train/                  |               |
|    approx_kl            | 3.1655654e-06 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00217      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0205        |
|    n_updates            | 8110          |
|    policy_gradient_loss | -2.76e-06     |
|    value_loss           | 0.0608        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 812      |
|    time_elapsed    | 5486     |
|    total_timesteps | 3325952  |
---------------------------------
Eval num_timesteps=3330000, episode_reward=0.40 +/- 0.66
Episode length: 276.20 +/- 34.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 276       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3330000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00228  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.057     |
|    n_updates            | 8120      |
|    policy_gradient_loss | -8.87e-10 |
|    value_loss           | 0.0653    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 813      |
|    time_elapsed    | 5493     |
|    total_timesteps | 3330048  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 309      |
|    ep_rew_mean          | 0.51     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 814      |
|    time_elapsed         | 5498     |
|    total_timesteps      | 3334144  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00228 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0283   |
|    n_updates            | 8130     |
|    policy_gradient_loss | 3.78e-11 |
|    value_loss           | 0.0722   |
--------------------------------------
Eval num_timesteps=3335000, episode_reward=0.50 +/- 0.92
Episode length: 300.40 +/- 38.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 300           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3335000       |
| train/                  |               |
|    approx_kl            | 0.00014014274 |
|    clip_fraction        | 0.000415      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00331      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.032         |
|    n_updates            | 8140          |
|    policy_gradient_loss | -5.18e-05     |
|    value_loss           | 0.0602        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 815      |
|    time_elapsed    | 5505     |
|    total_timesteps | 3338240  |
---------------------------------
Eval num_timesteps=3340000, episode_reward=0.30 +/- 0.64
Episode length: 268.80 +/- 21.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 269          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 3340000      |
| train/                  |              |
|    approx_kl            | 7.040886e-06 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00395     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0208       |
|    n_updates            | 8150         |
|    policy_gradient_loss | 1.59e-06     |
|    value_loss           | 0.0565       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 816      |
|    time_elapsed    | 5512     |
|    total_timesteps | 3342336  |
---------------------------------
Eval num_timesteps=3345000, episode_reward=0.40 +/- 0.49
Episode length: 323.80 +/- 61.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 324          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 3345000      |
| train/                  |              |
|    approx_kl            | 6.430887e-05 |
|    clip_fraction        | 0.00129      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00553     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0187       |
|    n_updates            | 8160         |
|    policy_gradient_loss | -0.000129    |
|    value_loss           | 0.0622       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 817      |
|    time_elapsed    | 5519     |
|    total_timesteps | 3346432  |
---------------------------------
Eval num_timesteps=3350000, episode_reward=0.20 +/- 0.40
Episode length: 294.80 +/- 43.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 295           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 3350000       |
| train/                  |               |
|    approx_kl            | 2.5815592e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00404      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0631        |
|    n_updates            | 8170          |
|    policy_gradient_loss | -0.000112     |
|    value_loss           | 0.0728        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 818      |
|    time_elapsed    | 5526     |
|    total_timesteps | 3350528  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 309           |
|    ep_rew_mean          | 0.46          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 819           |
|    time_elapsed         | 5531          |
|    total_timesteps      | 3354624       |
| train/                  |               |
|    approx_kl            | 5.6110875e-06 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0044       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0274        |
|    n_updates            | 8180          |
|    policy_gradient_loss | -4.34e-06     |
|    value_loss           | 0.0537        |
-------------------------------------------
Eval num_timesteps=3355000, episode_reward=0.30 +/- 0.46
Episode length: 312.40 +/- 47.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 312           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 3355000       |
| train/                  |               |
|    approx_kl            | 4.1821768e-05 |
|    clip_fraction        | 0.00061       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00425      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.028         |
|    n_updates            | 8190          |
|    policy_gradient_loss | -3.25e-05     |
|    value_loss           | 0.0555        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 820      |
|    time_elapsed    | 5538     |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3360000, episode_reward=0.20 +/- 0.40
Episode length: 270.00 +/- 43.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 270          |
|    mean_reward          | 0.2          |
| time/                   |              |
|    total_timesteps      | 3360000      |
| train/                  |              |
|    approx_kl            | 1.228007e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00402     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0281       |
|    n_updates            | 8200         |
|    policy_gradient_loss | -6.32e-07    |
|    value_loss           | 0.0619       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 821      |
|    time_elapsed    | 5545     |
|    total_timesteps | 3362816  |
---------------------------------
Eval num_timesteps=3365000, episode_reward=0.50 +/- 0.50
Episode length: 314.60 +/- 53.67
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 315      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 3365000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00359 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0348   |
|    n_updates            | 8210     |
|    policy_gradient_loss | 8.77e-10 |
|    value_loss           | 0.0588   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 822      |
|    time_elapsed    | 5552     |
|    total_timesteps | 3366912  |
---------------------------------
Eval num_timesteps=3370000, episode_reward=0.50 +/- 0.67
Episode length: 304.00 +/- 42.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 304           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3370000       |
| train/                  |               |
|    approx_kl            | 1.6842183e-05 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00345      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0211        |
|    n_updates            | 8220          |
|    policy_gradient_loss | -1.04e-06     |
|    value_loss           | 0.0608        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 823      |
|    time_elapsed    | 5560     |
|    total_timesteps | 3371008  |
---------------------------------
Eval num_timesteps=3375000, episode_reward=0.80 +/- 0.75
Episode length: 321.20 +/- 52.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 321          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 3375000      |
| train/                  |              |
|    approx_kl            | 8.748844e-06 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00384     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0486       |
|    n_updates            | 8230         |
|    policy_gradient_loss | -3.34e-06    |
|    value_loss           | 0.0656       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 824      |
|    time_elapsed    | 5567     |
|    total_timesteps | 3375104  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 308         |
|    ep_rew_mean          | 0.44        |
| time/                   |             |
|    fps                  | 606         |
|    iterations           | 825         |
|    time_elapsed         | 5572        |
|    total_timesteps      | 3379200     |
| train/                  |             |
|    approx_kl            | 4.21796e-06 |
|    clip_fraction        | 4.88e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00379    |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0459      |
|    n_updates            | 8240        |
|    policy_gradient_loss | 8e-07       |
|    value_loss           | 0.0607      |
-----------------------------------------
Eval num_timesteps=3380000, episode_reward=0.50 +/- 0.50
Episode length: 308.80 +/- 39.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 309         |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 3380000     |
| train/                  |             |
|    approx_kl            | 6.40098e-06 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00476    |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0383      |
|    n_updates            | 8250        |
|    policy_gradient_loss | -2.99e-06   |
|    value_loss           | 0.0619      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 826      |
|    time_elapsed    | 5579     |
|    total_timesteps | 3383296  |
---------------------------------
Eval num_timesteps=3385000, episode_reward=0.60 +/- 0.80
Episode length: 330.80 +/- 67.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 331           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 3385000       |
| train/                  |               |
|    approx_kl            | 3.3115793e-07 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00502      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0341        |
|    n_updates            | 8260          |
|    policy_gradient_loss | 3.3e-06       |
|    value_loss           | 0.0641        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 827      |
|    time_elapsed    | 5586     |
|    total_timesteps | 3387392  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=0.50 +/- 0.67
Episode length: 306.40 +/- 52.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 306          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 3390000      |
| train/                  |              |
|    approx_kl            | 2.481768e-05 |
|    clip_fraction        | 0.000317     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00411     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0149       |
|    n_updates            | 8270         |
|    policy_gradient_loss | -2.82e-06    |
|    value_loss           | 0.0614       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 828      |
|    time_elapsed    | 5594     |
|    total_timesteps | 3391488  |
---------------------------------
Eval num_timesteps=3395000, episode_reward=0.30 +/- 0.64
Episode length: 284.20 +/- 38.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 284           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 3395000       |
| train/                  |               |
|    approx_kl            | 1.7220998e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00466      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0469        |
|    n_updates            | 8280          |
|    policy_gradient_loss | -2.51e-06     |
|    value_loss           | 0.0705        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 829      |
|    time_elapsed    | 5601     |
|    total_timesteps | 3395584  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 297           |
|    ep_rew_mean          | 0.53          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 830           |
|    time_elapsed         | 5605          |
|    total_timesteps      | 3399680       |
| train/                  |               |
|    approx_kl            | 3.1233358e-06 |
|    clip_fraction        | 0.000269      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.004        |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.016         |
|    n_updates            | 8290          |
|    policy_gradient_loss | -3.22e-07     |
|    value_loss           | 0.0699        |
-------------------------------------------
Eval num_timesteps=3400000, episode_reward=0.70 +/- 0.78
Episode length: 310.20 +/- 54.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 310           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 3400000       |
| train/                  |               |
|    approx_kl            | 5.9604645e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00487      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0314        |
|    n_updates            | 8300          |
|    policy_gradient_loss | 2.69e-10      |
|    value_loss           | 0.0598        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 831      |
|    time_elapsed    | 5613     |
|    total_timesteps | 3403776  |
---------------------------------
Eval num_timesteps=3405000, episode_reward=0.40 +/- 0.66
Episode length: 291.20 +/- 37.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 3405000      |
| train/                  |              |
|    approx_kl            | 6.336899e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00753     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0414       |
|    n_updates            | 8310         |
|    policy_gradient_loss | -5.32e-05    |
|    value_loss           | 0.0586       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 832      |
|    time_elapsed    | 5620     |
|    total_timesteps | 3407872  |
---------------------------------
Eval num_timesteps=3410000, episode_reward=0.30 +/- 0.46
Episode length: 309.60 +/- 57.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 3410000      |
| train/                  |              |
|    approx_kl            | 9.840151e-07 |
|    clip_fraction        | 2.44e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00655     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0608       |
|    n_updates            | 8320         |
|    policy_gradient_loss | 1.7e-06      |
|    value_loss           | 0.0628       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 833      |
|    time_elapsed    | 5627     |
|    total_timesteps | 3411968  |
---------------------------------
Eval num_timesteps=3415000, episode_reward=0.20 +/- 0.40
Episode length: 286.40 +/- 32.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 286           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 3415000       |
| train/                  |               |
|    approx_kl            | 2.5238332e-05 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00589      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0543        |
|    n_updates            | 8330          |
|    policy_gradient_loss | -1.06e-05     |
|    value_loss           | 0.0553        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 834      |
|    time_elapsed    | 5634     |
|    total_timesteps | 3416064  |
---------------------------------
Eval num_timesteps=3420000, episode_reward=0.60 +/- 0.66
Episode length: 307.40 +/- 52.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 307           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 3420000       |
| train/                  |               |
|    approx_kl            | 3.4297118e-06 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00532      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.014         |
|    n_updates            | 8340          |
|    policy_gradient_loss | -2.5e-06      |
|    value_loss           | 0.068         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 835      |
|    time_elapsed    | 5641     |
|    total_timesteps | 3420160  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 306          |
|    ep_rew_mean          | 0.47         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 836          |
|    time_elapsed         | 5646         |
|    total_timesteps      | 3424256      |
| train/                  |              |
|    approx_kl            | 8.899267e-06 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0059      |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.01         |
|    loss                 | 0.0218       |
|    n_updates            | 8350         |
|    policy_gradient_loss | -7.89e-07    |
|    value_loss           | 0.0582       |
------------------------------------------
Eval num_timesteps=3425000, episode_reward=0.20 +/- 0.40
Episode length: 283.40 +/- 50.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 283           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 3425000       |
| train/                  |               |
|    approx_kl            | 8.3434075e-05 |
|    clip_fraction        | 0.000708      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0037       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.034         |
|    n_updates            | 8360          |
|    policy_gradient_loss | -0.000116     |
|    value_loss           | 0.0604        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 837      |
|    time_elapsed    | 5653     |
|    total_timesteps | 3428352  |
---------------------------------
Eval num_timesteps=3430000, episode_reward=0.30 +/- 0.46
Episode length: 314.60 +/- 56.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 315           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 3430000       |
| train/                  |               |
|    approx_kl            | 4.5306224e-06 |
|    clip_fraction        | 0.000244      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00442      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0619        |
|    n_updates            | 8370          |
|    policy_gradient_loss | -4.08e-06     |
|    value_loss           | 0.0655        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 838      |
|    time_elapsed    | 5660     |
|    total_timesteps | 3432448  |
---------------------------------
Eval num_timesteps=3435000, episode_reward=0.60 +/- 0.92
Episode length: 309.60 +/- 46.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 310          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 3435000      |
| train/                  |              |
|    approx_kl            | 7.868647e-05 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00384     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0273       |
|    n_updates            | 8380         |
|    policy_gradient_loss | -8.95e-05    |
|    value_loss           | 0.0591       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 839      |
|    time_elapsed    | 5667     |
|    total_timesteps | 3436544  |
---------------------------------
Eval num_timesteps=3440000, episode_reward=0.70 +/- 0.90
Episode length: 333.80 +/- 65.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 334          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 3440000      |
| train/                  |              |
|    approx_kl            | 2.789007e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00248     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0392       |
|    n_updates            | 8390         |
|    policy_gradient_loss | -7.5e-05     |
|    value_loss           | 0.0673       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 840      |
|    time_elapsed    | 5675     |
|    total_timesteps | 3440640  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 299          |
|    ep_rew_mean          | 0.45         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 841          |
|    time_elapsed         | 5680         |
|    total_timesteps      | 3444736      |
| train/                  |              |
|    approx_kl            | 4.182948e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00266     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0392       |
|    n_updates            | 8400         |
|    policy_gradient_loss | 1.33e-08     |
|    value_loss           | 0.0577       |
------------------------------------------
Eval num_timesteps=3445000, episode_reward=0.50 +/- 0.67
Episode length: 275.60 +/- 48.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 276          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 3445000      |
| train/                  |              |
|    approx_kl            | 7.663926e-06 |
|    clip_fraction        | 0.000269     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00295     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0241       |
|    n_updates            | 8410         |
|    policy_gradient_loss | 2.23e-06     |
|    value_loss           | 0.0671       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 842      |
|    time_elapsed    | 5687     |
|    total_timesteps | 3448832  |
---------------------------------
Eval num_timesteps=3450000, episode_reward=0.00 +/- 0.00
Episode length: 289.80 +/- 54.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 290           |
|    mean_reward          | 0             |
| time/                   |               |
|    total_timesteps      | 3450000       |
| train/                  |               |
|    approx_kl            | 1.5266007e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00235      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0407        |
|    n_updates            | 8420          |
|    policy_gradient_loss | -2.52e-05     |
|    value_loss           | 0.0647        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 843      |
|    time_elapsed    | 5694     |
|    total_timesteps | 3452928  |
---------------------------------
Eval num_timesteps=3455000, episode_reward=0.40 +/- 0.49
Episode length: 303.60 +/- 53.84
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 304      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 3455000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00225 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0294   |
|    n_updates            | 8430     |
|    policy_gradient_loss | -4.9e-10 |
|    value_loss           | 0.0566   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 844      |
|    time_elapsed    | 5701     |
|    total_timesteps | 3457024  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=0.10 +/- 0.30
Episode length: 291.40 +/- 51.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 3460000      |
| train/                  |              |
|    approx_kl            | 9.820476e-06 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0026      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.00877      |
|    n_updates            | 8440         |
|    policy_gradient_loss | -3.55e-06    |
|    value_loss           | 0.0623       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 845      |
|    time_elapsed    | 5708     |
|    total_timesteps | 3461120  |
---------------------------------
Eval num_timesteps=3465000, episode_reward=0.40 +/- 0.66
Episode length: 318.20 +/- 55.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 318           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 3465000       |
| train/                  |               |
|    approx_kl            | 4.0718805e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00205      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0352        |
|    n_updates            | 8450          |
|    policy_gradient_loss | -2.17e-05     |
|    value_loss           | 0.0712        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 846      |
|    time_elapsed    | 5715     |
|    total_timesteps | 3465216  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 295          |
|    ep_rew_mean          | 0.46         |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 847          |
|    time_elapsed         | 5720         |
|    total_timesteps      | 3469312      |
| train/                  |              |
|    approx_kl            | 6.241753e-06 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00177     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0201       |
|    n_updates            | 8460         |
|    policy_gradient_loss | -2.06e-06    |
|    value_loss           | 0.0634       |
------------------------------------------
Eval num_timesteps=3470000, episode_reward=0.40 +/- 0.49
Episode length: 302.20 +/- 58.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 302          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 3470000      |
| train/                  |              |
|    approx_kl            | 1.255791e-05 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00177     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0334       |
|    n_updates            | 8470         |
|    policy_gradient_loss | 4.04e-09     |
|    value_loss           | 0.0625       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 848      |
|    time_elapsed    | 5727     |
|    total_timesteps | 3473408  |
---------------------------------
Eval num_timesteps=3475000, episode_reward=0.10 +/- 0.30
Episode length: 284.80 +/- 52.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 285           |
|    mean_reward          | 0.1           |
| time/                   |               |
|    total_timesteps      | 3475000       |
| train/                  |               |
|    approx_kl            | 0.00013052757 |
|    clip_fraction        | 0.000659      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00125      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0538        |
|    n_updates            | 8480          |
|    policy_gradient_loss | -0.000134     |
|    value_loss           | 0.0653        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 849      |
|    time_elapsed    | 5734     |
|    total_timesteps | 3477504  |
---------------------------------
Eval num_timesteps=3480000, episode_reward=0.40 +/- 0.49
Episode length: 296.60 +/- 39.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3480000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00118  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0309    |
|    n_updates            | 8490      |
|    policy_gradient_loss | -3.91e-10 |
|    value_loss           | 0.0653    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 850      |
|    time_elapsed    | 5742     |
|    total_timesteps | 3481600  |
---------------------------------
Eval num_timesteps=3485000, episode_reward=0.70 +/- 0.64
Episode length: 330.20 +/- 60.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 330       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3485000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00118  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0398    |
|    n_updates            | 8500      |
|    policy_gradient_loss | -1.39e-10 |
|    value_loss           | 0.0576    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 851      |
|    time_elapsed    | 5749     |
|    total_timesteps | 3485696  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 294      |
|    ep_rew_mean          | 0.42     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 852      |
|    time_elapsed         | 5754     |
|    total_timesteps      | 3489792  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00118 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0323   |
|    n_updates            | 8510     |
|    policy_gradient_loss | 8.15e-11 |
|    value_loss           | 0.0628   |
--------------------------------------
Eval num_timesteps=3490000, episode_reward=0.30 +/- 0.46
Episode length: 282.20 +/- 46.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 282       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3490000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00118  |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0298    |
|    n_updates            | 8520      |
|    policy_gradient_loss | -2.07e-10 |
|    value_loss           | 0.0593    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 853      |
|    time_elapsed    | 5761     |
|    total_timesteps | 3493888  |
---------------------------------
Eval num_timesteps=3495000, episode_reward=0.40 +/- 0.49
Episode length: 301.60 +/- 39.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 302          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 3495000      |
| train/                  |              |
|    approx_kl            | 9.125375e-06 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00144     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0338       |
|    n_updates            | 8530         |
|    policy_gradient_loss | -2.62e-05    |
|    value_loss           | 0.0616       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 854      |
|    time_elapsed    | 5768     |
|    total_timesteps | 3497984  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=0.60 +/- 0.49
Episode length: 330.80 +/- 42.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 331           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 3500000       |
| train/                  |               |
|    approx_kl            | 5.3973767e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00239      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0472        |
|    n_updates            | 8540          |
|    policy_gradient_loss | -8.05e-05     |
|    value_loss           | 0.0693        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 855      |
|    time_elapsed    | 5775     |
|    total_timesteps | 3502080  |
---------------------------------
Eval num_timesteps=3505000, episode_reward=0.70 +/- 1.00
Episode length: 333.20 +/- 49.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 333          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 3505000      |
| train/                  |              |
|    approx_kl            | 6.039336e-06 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0021      |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0375       |
|    n_updates            | 8550         |
|    policy_gradient_loss | -1.74e-06    |
|    value_loss           | 0.0638       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 856      |
|    time_elapsed    | 5783     |
|    total_timesteps | 3506176  |
---------------------------------
Eval num_timesteps=3510000, episode_reward=0.60 +/- 0.49
Episode length: 290.40 +/- 51.56
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 290      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 3510000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00201 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0263   |
|    n_updates            | 8560     |
|    policy_gradient_loss | 5.96e-10 |
|    value_loss           | 0.0588   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 857      |
|    time_elapsed    | 5790     |
|    total_timesteps | 3510272  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 312      |
|    ep_rew_mean          | 0.57     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 858      |
|    time_elapsed         | 5795     |
|    total_timesteps      | 3514368  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00201 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0267   |
|    n_updates            | 8570     |
|    policy_gradient_loss | 1.15e-09 |
|    value_loss           | 0.0572   |
--------------------------------------
Eval num_timesteps=3515000, episode_reward=0.80 +/- 0.75
Episode length: 283.40 +/- 38.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 283          |
|    mean_reward          | 0.8          |
| time/                   |              |
|    total_timesteps      | 3515000      |
| train/                  |              |
|    approx_kl            | 4.631006e-05 |
|    clip_fraction        | 0.000464     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00152     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0238       |
|    n_updates            | 8580         |
|    policy_gradient_loss | -7.73e-05    |
|    value_loss           | 0.0687       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 315      |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 859      |
|    time_elapsed    | 5802     |
|    total_timesteps | 3518464  |
---------------------------------
Eval num_timesteps=3520000, episode_reward=0.30 +/- 0.46
Episode length: 273.60 +/- 42.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 274       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3520000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00151  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00702   |
|    n_updates            | 8590      |
|    policy_gradient_loss | -4.23e-10 |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 860      |
|    time_elapsed    | 5809     |
|    total_timesteps | 3522560  |
---------------------------------
Eval num_timesteps=3525000, episode_reward=0.50 +/- 0.67
Episode length: 296.60 +/- 45.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 297           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3525000       |
| train/                  |               |
|    approx_kl            | 1.9329134e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0011       |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0318        |
|    n_updates            | 8600          |
|    policy_gradient_loss | -1.47e-05     |
|    value_loss           | 0.0587        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 861      |
|    time_elapsed    | 5816     |
|    total_timesteps | 3526656  |
---------------------------------
Eval num_timesteps=3530000, episode_reward=0.40 +/- 0.49
Episode length: 292.40 +/- 53.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 292      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 3530000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00106 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0556   |
|    n_updates            | 8610     |
|    policy_gradient_loss | 1.42e-09 |
|    value_loss           | 0.0623   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 862      |
|    time_elapsed    | 5823     |
|    total_timesteps | 3530752  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 310      |
|    ep_rew_mean          | 0.57     |
| time/                   |          |
|    fps                  | 606      |
|    iterations           | 863      |
|    time_elapsed         | 5828     |
|    total_timesteps      | 3534848  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00106 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0285   |
|    n_updates            | 8620     |
|    policy_gradient_loss | 2.28e-10 |
|    value_loss           | 0.0645   |
--------------------------------------
Eval num_timesteps=3535000, episode_reward=0.50 +/- 0.50
Episode length: 293.20 +/- 47.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 293           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3535000       |
| train/                  |               |
|    approx_kl            | 4.4714194e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000887     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0179        |
|    n_updates            | 8630          |
|    policy_gradient_loss | 1.01e-06      |
|    value_loss           | 0.0643        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 864      |
|    time_elapsed    | 5835     |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3540000, episode_reward=0.30 +/- 0.46
Episode length: 300.00 +/- 63.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3540000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000722 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0192    |
|    n_updates            | 8640      |
|    policy_gradient_loss | -4.86e-10 |
|    value_loss           | 0.0596    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 865      |
|    time_elapsed    | 5842     |
|    total_timesteps | 3543040  |
---------------------------------
Eval num_timesteps=3545000, episode_reward=0.50 +/- 0.67
Episode length: 332.40 +/- 74.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 332           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 3545000       |
| train/                  |               |
|    approx_kl            | 1.8851861e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000659     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0191        |
|    n_updates            | 8650          |
|    policy_gradient_loss | 1.9e-06       |
|    value_loss           | 0.0616        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 866      |
|    time_elapsed    | 5849     |
|    total_timesteps | 3547136  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=0.60 +/- 0.49
Episode length: 310.60 +/- 63.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3550000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000534 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.01      |
|    n_updates            | 8660      |
|    policy_gradient_loss | 4e-11     |
|    value_loss           | 0.0575    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 867      |
|    time_elapsed    | 5857     |
|    total_timesteps | 3551232  |
---------------------------------
Eval num_timesteps=3555000, episode_reward=0.50 +/- 0.67
Episode length: 339.60 +/- 57.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 340       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3555000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000534 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0237    |
|    n_updates            | 8670      |
|    policy_gradient_loss | 8.88e-11  |
|    value_loss           | 0.0641    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 868      |
|    time_elapsed    | 5864     |
|    total_timesteps | 3555328  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 869       |
|    time_elapsed         | 5869      |
|    total_timesteps      | 3559424   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000534 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0424    |
|    n_updates            | 8680      |
|    policy_gradient_loss | 9.09e-11  |
|    value_loss           | 0.0685    |
---------------------------------------
Eval num_timesteps=3560000, episode_reward=0.50 +/- 0.67
Episode length: 303.60 +/- 45.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3560000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000534 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0304    |
|    n_updates            | 8690      |
|    policy_gradient_loss | -8e-12    |
|    value_loss           | 0.0606    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 870      |
|    time_elapsed    | 5876     |
|    total_timesteps | 3563520  |
---------------------------------
Eval num_timesteps=3565000, episode_reward=0.40 +/- 0.49
Episode length: 291.60 +/- 19.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3565000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000534 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0445    |
|    n_updates            | 8700      |
|    policy_gradient_loss | 2.14e-10  |
|    value_loss           | 0.0652    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 871      |
|    time_elapsed    | 5883     |
|    total_timesteps | 3567616  |
---------------------------------
Eval num_timesteps=3570000, episode_reward=0.50 +/- 0.50
Episode length: 280.40 +/- 49.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 3570000      |
| train/                  |              |
|    approx_kl            | 4.399146e-06 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000631    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0508       |
|    n_updates            | 8710         |
|    policy_gradient_loss | 2.02e-06     |
|    value_loss           | 0.0603       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 872      |
|    time_elapsed    | 5890     |
|    total_timesteps | 3571712  |
---------------------------------
Eval num_timesteps=3575000, episode_reward=0.60 +/- 0.80
Episode length: 302.40 +/- 33.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3575000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000383 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0307    |
|    n_updates            | 8720      |
|    policy_gradient_loss | -1.03e-10 |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 873      |
|    time_elapsed    | 5897     |
|    total_timesteps | 3575808  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 299       |
|    ep_rew_mean          | 0.56      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 874       |
|    time_elapsed         | 5902      |
|    total_timesteps      | 3579904   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000383 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0316    |
|    n_updates            | 8730      |
|    policy_gradient_loss | 2.83e-10  |
|    value_loss           | 0.061     |
---------------------------------------
Eval num_timesteps=3580000, episode_reward=0.40 +/- 0.92
Episode length: 303.20 +/- 49.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 303          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 3580000      |
| train/                  |              |
|    approx_kl            | 9.027572e-06 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000334    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0207       |
|    n_updates            | 8740         |
|    policy_gradient_loss | 1.95e-06     |
|    value_loss           | 0.0672       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 875      |
|    time_elapsed    | 5910     |
|    total_timesteps | 3584000  |
---------------------------------
Eval num_timesteps=3585000, episode_reward=0.50 +/- 0.81
Episode length: 317.20 +/- 66.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3585000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000408 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0296    |
|    n_updates            | 8750      |
|    policy_gradient_loss | 1.89e-11  |
|    value_loss           | 0.0639    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 876      |
|    time_elapsed    | 5917     |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3590000, episode_reward=0.80 +/- 0.75
Episode length: 321.40 +/- 55.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 321       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 3590000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000408 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0283    |
|    n_updates            | 8760      |
|    policy_gradient_loss | 8.19e-10  |
|    value_loss           | 0.0639    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 877      |
|    time_elapsed    | 5924     |
|    total_timesteps | 3592192  |
---------------------------------
Eval num_timesteps=3595000, episode_reward=0.60 +/- 0.80
Episode length: 282.80 +/- 31.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3595000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000408 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0263    |
|    n_updates            | 8770      |
|    policy_gradient_loss | -6.7e-10  |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 878      |
|    time_elapsed    | 5931     |
|    total_timesteps | 3596288  |
---------------------------------
Eval num_timesteps=3600000, episode_reward=0.40 +/- 0.66
Episode length: 308.60 +/- 57.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3600000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000408 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0242    |
|    n_updates            | 8780      |
|    policy_gradient_loss | -5.49e-10 |
|    value_loss           | 0.0556    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 879      |
|    time_elapsed    | 5938     |
|    total_timesteps | 3600384  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 288       |
|    ep_rew_mean          | 0.42      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 880       |
|    time_elapsed         | 5943      |
|    total_timesteps      | 3604480   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000408 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0597    |
|    n_updates            | 8790      |
|    policy_gradient_loss | 2.42e-10  |
|    value_loss           | 0.0662    |
---------------------------------------
Eval num_timesteps=3605000, episode_reward=0.40 +/- 0.66
Episode length: 316.20 +/- 28.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3605000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000408 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0594    |
|    n_updates            | 8800      |
|    policy_gradient_loss | 7.06e-10  |
|    value_loss           | 0.0606    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 881      |
|    time_elapsed    | 5951     |
|    total_timesteps | 3608576  |
---------------------------------
Eval num_timesteps=3610000, episode_reward=0.30 +/- 0.46
Episode length: 287.20 +/- 42.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3610000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000408 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0355    |
|    n_updates            | 8810      |
|    policy_gradient_loss | 2.44e-10  |
|    value_loss           | 0.0592    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 882      |
|    time_elapsed    | 5958     |
|    total_timesteps | 3612672  |
---------------------------------
Eval num_timesteps=3615000, episode_reward=0.40 +/- 0.80
Episode length: 309.40 +/- 30.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3615000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000408 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0674    |
|    n_updates            | 8820      |
|    policy_gradient_loss | -9.68e-11 |
|    value_loss           | 0.0685    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 883      |
|    time_elapsed    | 5965     |
|    total_timesteps | 3616768  |
---------------------------------
Eval num_timesteps=3620000, episode_reward=0.30 +/- 0.46
Episode length: 313.20 +/- 49.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 313         |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 3620000     |
| train/                  |             |
|    approx_kl            | 9.37788e-06 |
|    clip_fraction        | 0.000171    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00043    |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0384      |
|    n_updates            | 8830        |
|    policy_gradient_loss | 7.51e-07    |
|    value_loss           | 0.0582      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 884      |
|    time_elapsed    | 5972     |
|    total_timesteps | 3620864  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.45      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 885       |
|    time_elapsed         | 5977      |
|    total_timesteps      | 3624960   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0156    |
|    n_updates            | 8840      |
|    policy_gradient_loss | 5.42e-10  |
|    value_loss           | 0.0601    |
---------------------------------------
Eval num_timesteps=3625000, episode_reward=0.60 +/- 0.66
Episode length: 307.20 +/- 27.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3625000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0503    |
|    n_updates            | 8850      |
|    policy_gradient_loss | -1.32e-09 |
|    value_loss           | 0.0637    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 886      |
|    time_elapsed    | 5984     |
|    total_timesteps | 3629056  |
---------------------------------
Eval num_timesteps=3630000, episode_reward=0.60 +/- 0.80
Episode length: 312.80 +/- 49.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3630000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0146    |
|    n_updates            | 8860      |
|    policy_gradient_loss | -1.33e-10 |
|    value_loss           | 0.0603    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 887      |
|    time_elapsed    | 5992     |
|    total_timesteps | 3633152  |
---------------------------------
Eval num_timesteps=3635000, episode_reward=0.40 +/- 0.80
Episode length: 316.60 +/- 60.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3635000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00764   |
|    n_updates            | 8870      |
|    policy_gradient_loss | -1.33e-10 |
|    value_loss           | 0.0638    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 888      |
|    time_elapsed    | 5999     |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3640000, episode_reward=0.50 +/- 0.50
Episode length: 306.60 +/- 47.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3640000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0457    |
|    n_updates            | 8880      |
|    policy_gradient_loss | -5.1e-10  |
|    value_loss           | 0.0678    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 889      |
|    time_elapsed    | 6006     |
|    total_timesteps | 3641344  |
---------------------------------
Eval num_timesteps=3645000, episode_reward=0.60 +/- 0.49
Episode length: 300.00 +/- 38.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3645000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0511    |
|    n_updates            | 8890      |
|    policy_gradient_loss | -4.47e-11 |
|    value_loss           | 0.0672    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 890      |
|    time_elapsed    | 6013     |
|    total_timesteps | 3645440  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.51      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 891       |
|    time_elapsed         | 6018      |
|    total_timesteps      | 3649536   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.025     |
|    n_updates            | 8900      |
|    policy_gradient_loss | 5.89e-11  |
|    value_loss           | 0.0583    |
---------------------------------------
Eval num_timesteps=3650000, episode_reward=0.50 +/- 0.50
Episode length: 310.40 +/- 49.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3650000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0517    |
|    n_updates            | 8910      |
|    policy_gradient_loss | -2.83e-10 |
|    value_loss           | 0.0673    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 892      |
|    time_elapsed    | 6026     |
|    total_timesteps | 3653632  |
---------------------------------
Eval num_timesteps=3655000, episode_reward=0.20 +/- 0.40
Episode length: 295.40 +/- 30.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3655000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0387    |
|    n_updates            | 8920      |
|    policy_gradient_loss | -4.02e-10 |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 893      |
|    time_elapsed    | 6032     |
|    total_timesteps | 3657728  |
---------------------------------
Eval num_timesteps=3660000, episode_reward=0.30 +/- 0.46
Episode length: 311.80 +/- 48.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3660000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0262    |
|    n_updates            | 8930      |
|    policy_gradient_loss | -2.38e-10 |
|    value_loss           | 0.0666    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 894      |
|    time_elapsed    | 6040     |
|    total_timesteps | 3661824  |
---------------------------------
Eval num_timesteps=3665000, episode_reward=0.60 +/- 0.92
Episode length: 312.40 +/- 40.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3665000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0256    |
|    n_updates            | 8940      |
|    policy_gradient_loss | -4e-11    |
|    value_loss           | 0.0667    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 895      |
|    time_elapsed    | 6047     |
|    total_timesteps | 3665920  |
---------------------------------
Eval num_timesteps=3670000, episode_reward=0.30 +/- 0.46
Episode length: 308.80 +/- 59.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3670000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0366    |
|    n_updates            | 8950      |
|    policy_gradient_loss | -3.55e-10 |
|    value_loss           | 0.0605    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 896      |
|    time_elapsed    | 6054     |
|    total_timesteps | 3670016  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.54      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 897       |
|    time_elapsed         | 6059      |
|    total_timesteps      | 3674112   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0288    |
|    n_updates            | 8960      |
|    policy_gradient_loss | 4.84e-10  |
|    value_loss           | 0.0701    |
---------------------------------------
Eval num_timesteps=3675000, episode_reward=0.70 +/- 0.90
Episode length: 320.40 +/- 77.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 320       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3675000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0333    |
|    n_updates            | 8970      |
|    policy_gradient_loss | -8.33e-10 |
|    value_loss           | 0.0533    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 898      |
|    time_elapsed    | 6066     |
|    total_timesteps | 3678208  |
---------------------------------
Eval num_timesteps=3680000, episode_reward=0.40 +/- 0.49
Episode length: 292.80 +/- 51.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3680000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0314    |
|    n_updates            | 8980      |
|    policy_gradient_loss | 5.82e-10  |
|    value_loss           | 0.0677    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 899      |
|    time_elapsed    | 6073     |
|    total_timesteps | 3682304  |
---------------------------------
Eval num_timesteps=3685000, episode_reward=0.80 +/- 0.75
Episode length: 297.40 +/- 45.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 3685000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0233    |
|    n_updates            | 8990      |
|    policy_gradient_loss | 5.47e-10  |
|    value_loss           | 0.0586    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 900      |
|    time_elapsed    | 6081     |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3690000, episode_reward=0.10 +/- 0.30
Episode length: 303.20 +/- 53.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 3690000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000355 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0448    |
|    n_updates            | 9000      |
|    policy_gradient_loss | 4.48e-10  |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 901      |
|    time_elapsed    | 6088     |
|    total_timesteps | 3690496  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 298           |
|    ep_rew_mean          | 0.46          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 902           |
|    time_elapsed         | 6093          |
|    total_timesteps      | 3694592       |
| train/                  |               |
|    approx_kl            | 1.9841667e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000439     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0251        |
|    n_updates            | 9010          |
|    policy_gradient_loss | 7.64e-07      |
|    value_loss           | 0.0602        |
-------------------------------------------
Eval num_timesteps=3695000, episode_reward=0.40 +/- 0.66
Episode length: 331.20 +/- 74.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 331       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3695000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000487 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0411    |
|    n_updates            | 9020      |
|    policy_gradient_loss | 1.46e-10  |
|    value_loss           | 0.064     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 903      |
|    time_elapsed    | 6100     |
|    total_timesteps | 3698688  |
---------------------------------
Eval num_timesteps=3700000, episode_reward=0.60 +/- 0.66
Episode length: 298.20 +/- 76.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3700000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000487 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0348    |
|    n_updates            | 9030      |
|    policy_gradient_loss | 5.97e-11  |
|    value_loss           | 0.0614    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 904      |
|    time_elapsed    | 6107     |
|    total_timesteps | 3702784  |
---------------------------------
Eval num_timesteps=3705000, episode_reward=0.50 +/- 0.67
Episode length: 329.60 +/- 42.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 330          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 3705000      |
| train/                  |              |
|    approx_kl            | 7.996277e-06 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00051     |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0387       |
|    n_updates            | 9040         |
|    policy_gradient_loss | 2.56e-06     |
|    value_loss           | 0.0632       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 905      |
|    time_elapsed    | 6114     |
|    total_timesteps | 3706880  |
---------------------------------
Eval num_timesteps=3710000, episode_reward=0.20 +/- 0.40
Episode length: 302.80 +/- 28.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3710000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0259    |
|    n_updates            | 9050      |
|    policy_gradient_loss | -3.09e-10 |
|    value_loss           | 0.0712    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 906      |
|    time_elapsed    | 6121     |
|    total_timesteps | 3710976  |
---------------------------------
Eval num_timesteps=3715000, episode_reward=0.40 +/- 0.49
Episode length: 315.00 +/- 45.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3715000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.024     |
|    n_updates            | 9060      |
|    policy_gradient_loss | -1.42e-10 |
|    value_loss           | 0.0588    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 907      |
|    time_elapsed    | 6129     |
|    total_timesteps | 3715072  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 306       |
|    ep_rew_mean          | 0.52      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 908       |
|    time_elapsed         | 6134      |
|    total_timesteps      | 3719168   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0515    |
|    n_updates            | 9070      |
|    policy_gradient_loss | 3.67e-11  |
|    value_loss           | 0.0628    |
---------------------------------------
Eval num_timesteps=3720000, episode_reward=0.30 +/- 0.64
Episode length: 291.20 +/- 44.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 291       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3720000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0389    |
|    n_updates            | 9080      |
|    policy_gradient_loss | -4.09e-10 |
|    value_loss           | 0.0641    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 909      |
|    time_elapsed    | 6141     |
|    total_timesteps | 3723264  |
---------------------------------
Eval num_timesteps=3725000, episode_reward=0.30 +/- 0.46
Episode length: 293.00 +/- 60.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3725000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0364    |
|    n_updates            | 9090      |
|    policy_gradient_loss | 7.27e-10  |
|    value_loss           | 0.0624    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 910      |
|    time_elapsed    | 6148     |
|    total_timesteps | 3727360  |
---------------------------------
Eval num_timesteps=3730000, episode_reward=0.30 +/- 0.64
Episode length: 297.80 +/- 38.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3730000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0483    |
|    n_updates            | 9100      |
|    policy_gradient_loss | 6.15e-10  |
|    value_loss           | 0.067     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 911      |
|    time_elapsed    | 6155     |
|    total_timesteps | 3731456  |
---------------------------------
Eval num_timesteps=3735000, episode_reward=0.40 +/- 0.49
Episode length: 313.20 +/- 54.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3735000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.023     |
|    n_updates            | 9110      |
|    policy_gradient_loss | 3.17e-10  |
|    value_loss           | 0.0614    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 912      |
|    time_elapsed    | 6162     |
|    total_timesteps | 3735552  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 913       |
|    time_elapsed         | 6167      |
|    total_timesteps      | 3739648   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0461    |
|    n_updates            | 9120      |
|    policy_gradient_loss | -2.36e-10 |
|    value_loss           | 0.066     |
---------------------------------------
Eval num_timesteps=3740000, episode_reward=0.20 +/- 0.40
Episode length: 321.00 +/- 52.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 321       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3740000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0271    |
|    n_updates            | 9130      |
|    policy_gradient_loss | -8.93e-10 |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 914      |
|    time_elapsed    | 6175     |
|    total_timesteps | 3743744  |
---------------------------------
Eval num_timesteps=3745000, episode_reward=0.70 +/- 0.90
Episode length: 305.80 +/- 65.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3745000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0343    |
|    n_updates            | 9140      |
|    policy_gradient_loss | 5.62e-11  |
|    value_loss           | 0.0469    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 915      |
|    time_elapsed    | 6182     |
|    total_timesteps | 3747840  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=0.50 +/- 0.50
Episode length: 325.60 +/- 24.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 326       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3750000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0228    |
|    n_updates            | 9150      |
|    policy_gradient_loss | 4.75e-10  |
|    value_loss           | 0.0663    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 916      |
|    time_elapsed    | 6189     |
|    total_timesteps | 3751936  |
---------------------------------
Eval num_timesteps=3755000, episode_reward=0.30 +/- 0.64
Episode length: 312.40 +/- 48.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3755000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.037     |
|    n_updates            | 9160      |
|    policy_gradient_loss | -6.55e-10 |
|    value_loss           | 0.0587    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 917      |
|    time_elapsed    | 6196     |
|    total_timesteps | 3756032  |
---------------------------------
Eval num_timesteps=3760000, episode_reward=0.60 +/- 0.66
Episode length: 332.60 +/- 29.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 333       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3760000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0196    |
|    n_updates            | 9170      |
|    policy_gradient_loss | -4.33e-10 |
|    value_loss           | 0.0554    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 918      |
|    time_elapsed    | 6204     |
|    total_timesteps | 3760128  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.41      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 919       |
|    time_elapsed         | 6209      |
|    total_timesteps      | 3764224   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0338    |
|    n_updates            | 9180      |
|    policy_gradient_loss | -8.07e-10 |
|    value_loss           | 0.0575    |
---------------------------------------
Eval num_timesteps=3765000, episode_reward=0.50 +/- 0.67
Episode length: 307.20 +/- 61.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3765000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.021     |
|    n_updates            | 9190      |
|    policy_gradient_loss | -1.7e-10  |
|    value_loss           | 0.0642    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 920      |
|    time_elapsed    | 6216     |
|    total_timesteps | 3768320  |
---------------------------------
Eval num_timesteps=3770000, episode_reward=0.60 +/- 0.66
Episode length: 313.60 +/- 54.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3770000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0596    |
|    n_updates            | 9200      |
|    policy_gradient_loss | 1.15e-09  |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 921      |
|    time_elapsed    | 6223     |
|    total_timesteps | 3772416  |
---------------------------------
Eval num_timesteps=3775000, episode_reward=0.90 +/- 0.83
Episode length: 315.40 +/- 75.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 3775000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0344    |
|    n_updates            | 9210      |
|    policy_gradient_loss | 4.15e-10  |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 922      |
|    time_elapsed    | 6230     |
|    total_timesteps | 3776512  |
---------------------------------
Eval num_timesteps=3780000, episode_reward=0.10 +/- 0.30
Episode length: 294.80 +/- 34.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 3780000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.02      |
|    n_updates            | 9220      |
|    policy_gradient_loss | 1.05e-10  |
|    value_loss           | 0.0557    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 923      |
|    time_elapsed    | 6237     |
|    total_timesteps | 3780608  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 293       |
|    ep_rew_mean          | 0.38      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 924       |
|    time_elapsed         | 6242      |
|    total_timesteps      | 3784704   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000553 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0354    |
|    n_updates            | 9230      |
|    policy_gradient_loss | 6.97e-10  |
|    value_loss           | 0.0618    |
---------------------------------------
Eval num_timesteps=3785000, episode_reward=0.40 +/- 0.49
Episode length: 289.80 +/- 36.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 290         |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 3785000     |
| train/                  |             |
|    approx_kl            | 3.94378e-05 |
|    clip_fraction        | 0.00022     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000333   |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.01        |
|    loss                 | 0.0328      |
|    n_updates            | 9240        |
|    policy_gradient_loss | -7.23e-05   |
|    value_loss           | 0.0607      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 925      |
|    time_elapsed    | 6250     |
|    total_timesteps | 3788800  |
---------------------------------
Eval num_timesteps=3790000, episode_reward=0.50 +/- 0.50
Episode length: 301.60 +/- 51.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3790000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000327 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0258    |
|    n_updates            | 9250      |
|    policy_gradient_loss | 2.52e-10  |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 926      |
|    time_elapsed    | 6257     |
|    total_timesteps | 3792896  |
---------------------------------
Eval num_timesteps=3795000, episode_reward=0.40 +/- 0.66
Episode length: 312.60 +/- 58.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 313           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 3795000       |
| train/                  |               |
|    approx_kl            | 4.9862865e-06 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000385     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0388        |
|    n_updates            | 9260          |
|    policy_gradient_loss | -3.28e-06     |
|    value_loss           | 0.0616        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 927      |
|    time_elapsed    | 6264     |
|    total_timesteps | 3796992  |
---------------------------------
Eval num_timesteps=3800000, episode_reward=0.00 +/- 0.00
Episode length: 274.00 +/- 22.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 274       |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 3800000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000384 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0346    |
|    n_updates            | 9270      |
|    policy_gradient_loss | -5.31e-11 |
|    value_loss           | 0.0568    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 928      |
|    time_elapsed    | 6271     |
|    total_timesteps | 3801088  |
---------------------------------
Eval num_timesteps=3805000, episode_reward=0.50 +/- 0.67
Episode length: 296.00 +/- 61.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3805000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000384 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0288    |
|    n_updates            | 9280      |
|    policy_gradient_loss | 1.27e-10  |
|    value_loss           | 0.0641    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 929      |
|    time_elapsed    | 6278     |
|    total_timesteps | 3805184  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 293           |
|    ep_rew_mean          | 0.43          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 930           |
|    time_elapsed         | 6283          |
|    total_timesteps      | 3809280       |
| train/                  |               |
|    approx_kl            | 2.2489548e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000266     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0226        |
|    n_updates            | 9290          |
|    policy_gradient_loss | -1.54e-05     |
|    value_loss           | 0.0621        |
-------------------------------------------
Eval num_timesteps=3810000, episode_reward=0.50 +/- 0.67
Episode length: 322.40 +/- 37.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 322       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3810000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000262 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.021     |
|    n_updates            | 9300      |
|    policy_gradient_loss | -1.68e-10 |
|    value_loss           | 0.0634    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 931      |
|    time_elapsed    | 6290     |
|    total_timesteps | 3813376  |
---------------------------------
Eval num_timesteps=3815000, episode_reward=0.50 +/- 0.67
Episode length: 302.00 +/- 44.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 302          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 3815000      |
| train/                  |              |
|    approx_kl            | 3.182268e-05 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000264    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0428       |
|    n_updates            | 9310         |
|    policy_gradient_loss | -5.58e-05    |
|    value_loss           | 0.0609       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 932      |
|    time_elapsed    | 6297     |
|    total_timesteps | 3817472  |
---------------------------------
Eval num_timesteps=3820000, episode_reward=0.60 +/- 1.02
Episode length: 316.60 +/- 40.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3820000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0312    |
|    n_updates            | 9320      |
|    policy_gradient_loss | 5.89e-10  |
|    value_loss           | 0.0665    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 933      |
|    time_elapsed    | 6305     |
|    total_timesteps | 3821568  |
---------------------------------
Eval num_timesteps=3825000, episode_reward=0.40 +/- 0.49
Episode length: 295.60 +/- 46.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3825000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0372    |
|    n_updates            | 9330      |
|    policy_gradient_loss | -7.26e-10 |
|    value_loss           | 0.0583    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 934      |
|    time_elapsed    | 6312     |
|    total_timesteps | 3825664  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 296       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 935       |
|    time_elapsed         | 6317      |
|    total_timesteps      | 3829760   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0292    |
|    n_updates            | 9340      |
|    policy_gradient_loss | 9.12e-10  |
|    value_loss           | 0.0514    |
---------------------------------------
Eval num_timesteps=3830000, episode_reward=0.40 +/- 0.66
Episode length: 291.80 +/- 54.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3830000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0298    |
|    n_updates            | 9350      |
|    policy_gradient_loss | -3.77e-10 |
|    value_loss           | 0.0609    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 936      |
|    time_elapsed    | 6324     |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3835000, episode_reward=0.60 +/- 0.66
Episode length: 318.60 +/- 62.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 319       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3835000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.037     |
|    n_updates            | 9360      |
|    policy_gradient_loss | -4.23e-10 |
|    value_loss           | 0.0562    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 937      |
|    time_elapsed    | 6331     |
|    total_timesteps | 3837952  |
---------------------------------
Eval num_timesteps=3840000, episode_reward=0.50 +/- 0.67
Episode length: 300.20 +/- 34.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3840000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.048     |
|    n_updates            | 9370      |
|    policy_gradient_loss | 9.51e-10  |
|    value_loss           | 0.0577    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 938      |
|    time_elapsed    | 6338     |
|    total_timesteps | 3842048  |
---------------------------------
Eval num_timesteps=3845000, episode_reward=0.70 +/- 0.64
Episode length: 311.00 +/- 63.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3845000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0175    |
|    n_updates            | 9380      |
|    policy_gradient_loss | 2.34e-10  |
|    value_loss           | 0.06      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 939      |
|    time_elapsed    | 6346     |
|    total_timesteps | 3846144  |
---------------------------------
Eval num_timesteps=3850000, episode_reward=1.00 +/- 0.77
Episode length: 303.00 +/- 65.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 1         |
| time/                   |           |
|    total_timesteps      | 3850000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0121    |
|    n_updates            | 9390      |
|    policy_gradient_loss | 1.67e-10  |
|    value_loss           | 0.0669    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 940      |
|    time_elapsed    | 6353     |
|    total_timesteps | 3850240  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 301           |
|    ep_rew_mean          | 0.47          |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 941           |
|    time_elapsed         | 6358          |
|    total_timesteps      | 3854336       |
| train/                  |               |
|    approx_kl            | 2.6254856e-06 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000277     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0261        |
|    n_updates            | 9400          |
|    policy_gradient_loss | 7.93e-08      |
|    value_loss           | 0.0609        |
-------------------------------------------
Eval num_timesteps=3855000, episode_reward=0.60 +/- 0.80
Episode length: 314.40 +/- 68.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3855000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000198 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0198    |
|    n_updates            | 9410      |
|    policy_gradient_loss | 1.35e-10  |
|    value_loss           | 0.0713    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 942      |
|    time_elapsed    | 6365     |
|    total_timesteps | 3858432  |
---------------------------------
Eval num_timesteps=3860000, episode_reward=0.60 +/- 0.66
Episode length: 311.40 +/- 58.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3860000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000198 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0147    |
|    n_updates            | 9420      |
|    policy_gradient_loss | -2.69e-10 |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 943      |
|    time_elapsed    | 6372     |
|    total_timesteps | 3862528  |
---------------------------------
Eval num_timesteps=3865000, episode_reward=0.40 +/- 0.49
Episode length: 326.40 +/- 50.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 326       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3865000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000198 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0371    |
|    n_updates            | 9430      |
|    policy_gradient_loss | -2.5e-10  |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 944      |
|    time_elapsed    | 6380     |
|    total_timesteps | 3866624  |
---------------------------------
Eval num_timesteps=3870000, episode_reward=0.50 +/- 0.67
Episode length: 312.60 +/- 56.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3870000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000198 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0416    |
|    n_updates            | 9440      |
|    policy_gradient_loss | 5e-10     |
|    value_loss           | 0.0619    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 945      |
|    time_elapsed    | 6387     |
|    total_timesteps | 3870720  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 310       |
|    ep_rew_mean          | 0.55      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 946       |
|    time_elapsed         | 6392      |
|    total_timesteps      | 3874816   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000198 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0209    |
|    n_updates            | 9450      |
|    policy_gradient_loss | -7.79e-10 |
|    value_loss           | 0.0634    |
---------------------------------------
Eval num_timesteps=3875000, episode_reward=0.50 +/- 0.67
Episode length: 316.40 +/- 50.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3875000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000198 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0289    |
|    n_updates            | 9460      |
|    policy_gradient_loss | 1.27e-10  |
|    value_loss           | 0.0588    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 947      |
|    time_elapsed    | 6399     |
|    total_timesteps | 3878912  |
---------------------------------
Eval num_timesteps=3880000, episode_reward=0.80 +/- 0.75
Episode length: 328.00 +/- 51.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 328       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 3880000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000198 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0274    |
|    n_updates            | 9470      |
|    policy_gradient_loss | 8.16e-10  |
|    value_loss           | 0.0553    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 948      |
|    time_elapsed    | 6406     |
|    total_timesteps | 3883008  |
---------------------------------
Eval num_timesteps=3885000, episode_reward=0.30 +/- 0.46
Episode length: 292.20 +/- 62.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3885000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000198 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0292    |
|    n_updates            | 9480      |
|    policy_gradient_loss | 1.87e-10  |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 949      |
|    time_elapsed    | 6414     |
|    total_timesteps | 3887104  |
---------------------------------
Eval num_timesteps=3890000, episode_reward=0.40 +/- 0.66
Episode length: 311.60 +/- 37.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 312          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 3890000      |
| train/                  |              |
|    approx_kl            | 1.791789e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000264    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.032        |
|    n_updates            | 9490         |
|    policy_gradient_loss | -1.72e-06    |
|    value_loss           | 0.0556       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 950      |
|    time_elapsed    | 6421     |
|    total_timesteps | 3891200  |
---------------------------------
Eval num_timesteps=3895000, episode_reward=0.40 +/- 0.49
Episode length: 289.60 +/- 45.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3895000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0416    |
|    n_updates            | 9500      |
|    policy_gradient_loss | 6.98e-10  |
|    value_loss           | 0.0586    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 951      |
|    time_elapsed    | 6428     |
|    total_timesteps | 3895296  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 308       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 952       |
|    time_elapsed         | 6433      |
|    total_timesteps      | 3899392   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0288    |
|    n_updates            | 9510      |
|    policy_gradient_loss | 1.23e-10  |
|    value_loss           | 0.0647    |
---------------------------------------
Eval num_timesteps=3900000, episode_reward=1.10 +/- 0.83
Episode length: 326.60 +/- 55.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 327       |
|    mean_reward          | 1.1       |
| time/                   |           |
|    total_timesteps      | 3900000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0326    |
|    n_updates            | 9520      |
|    policy_gradient_loss | 1.3e-10   |
|    value_loss           | 0.0667    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 953      |
|    time_elapsed    | 6440     |
|    total_timesteps | 3903488  |
---------------------------------
Eval num_timesteps=3905000, episode_reward=0.40 +/- 0.80
Episode length: 313.20 +/- 47.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3905000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0335    |
|    n_updates            | 9530      |
|    policy_gradient_loss | 1.51e-10  |
|    value_loss           | 0.063     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 954      |
|    time_elapsed    | 6447     |
|    total_timesteps | 3907584  |
---------------------------------
Eval num_timesteps=3910000, episode_reward=0.60 +/- 0.49
Episode length: 345.80 +/- 47.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 346       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 3910000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0188    |
|    n_updates            | 9540      |
|    policy_gradient_loss | -2.27e-10 |
|    value_loss           | 0.0546    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 955      |
|    time_elapsed    | 6455     |
|    total_timesteps | 3911680  |
---------------------------------
Eval num_timesteps=3915000, episode_reward=0.20 +/- 0.40
Episode length: 325.00 +/- 58.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3915000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.027     |
|    n_updates            | 9550      |
|    policy_gradient_loss | 1.14e-10  |
|    value_loss           | 0.0642    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 956      |
|    time_elapsed    | 6462     |
|    total_timesteps | 3915776  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.49      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 957       |
|    time_elapsed         | 6467      |
|    total_timesteps      | 3919872   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0473    |
|    n_updates            | 9560      |
|    policy_gradient_loss | -1.27e-09 |
|    value_loss           | 0.0676    |
---------------------------------------
Eval num_timesteps=3920000, episode_reward=0.20 +/- 0.40
Episode length: 259.20 +/- 25.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 259       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 3920000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0314    |
|    n_updates            | 9570      |
|    policy_gradient_loss | -5.35e-10 |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 958      |
|    time_elapsed    | 6474     |
|    total_timesteps | 3923968  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=0.10 +/- 0.30
Episode length: 301.40 +/- 38.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 3925000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0176    |
|    n_updates            | 9580      |
|    policy_gradient_loss | 1.78e-10  |
|    value_loss           | 0.0553    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 959      |
|    time_elapsed    | 6481     |
|    total_timesteps | 3928064  |
---------------------------------
Eval num_timesteps=3930000, episode_reward=0.30 +/- 0.46
Episode length: 298.00 +/- 65.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3930000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000238 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.031     |
|    n_updates            | 9590      |
|    policy_gradient_loss | -2.62e-10 |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 960      |
|    time_elapsed    | 6488     |
|    total_timesteps | 3932160  |
---------------------------------
Eval num_timesteps=3935000, episode_reward=0.40 +/- 0.49
Episode length: 266.60 +/- 47.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 267           |
|    mean_reward          | 0.4           |
| time/                   |               |
|    total_timesteps      | 3935000       |
| train/                  |               |
|    approx_kl            | 2.6228357e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000161     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0663        |
|    n_updates            | 9600          |
|    policy_gradient_loss | -5.67e-06     |
|    value_loss           | 0.0621        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 605      |
|    iterations      | 961      |
|    time_elapsed    | 6495     |
|    total_timesteps | 3936256  |
---------------------------------
Eval num_timesteps=3940000, episode_reward=0.40 +/- 0.66
Episode length: 302.80 +/- 46.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3940000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0418    |
|    n_updates            | 9610      |
|    policy_gradient_loss | -1.27e-10 |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 962      |
|    time_elapsed    | 6502     |
|    total_timesteps | 3940352  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.5       |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 963       |
|    time_elapsed         | 6507      |
|    total_timesteps      | 3944448   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0297    |
|    n_updates            | 9620      |
|    policy_gradient_loss | 4.07e-10  |
|    value_loss           | 0.063     |
---------------------------------------
Eval num_timesteps=3945000, episode_reward=0.70 +/- 0.78
Episode length: 305.00 +/- 52.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3945000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0436    |
|    n_updates            | 9630      |
|    policy_gradient_loss | -2.07e-10 |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 964      |
|    time_elapsed    | 6514     |
|    total_timesteps | 3948544  |
---------------------------------
Eval num_timesteps=3950000, episode_reward=0.40 +/- 0.49
Episode length: 299.80 +/- 46.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3950000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0299    |
|    n_updates            | 9640      |
|    policy_gradient_loss | 2.54e-10  |
|    value_loss           | 0.0558    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 965      |
|    time_elapsed    | 6521     |
|    total_timesteps | 3952640  |
---------------------------------
Eval num_timesteps=3955000, episode_reward=0.50 +/- 0.50
Episode length: 274.40 +/- 32.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 274       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3955000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0283    |
|    n_updates            | 9650      |
|    policy_gradient_loss | 4.18e-10  |
|    value_loss           | 0.064     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 966      |
|    time_elapsed    | 6528     |
|    total_timesteps | 3956736  |
---------------------------------
Eval num_timesteps=3960000, episode_reward=0.50 +/- 0.67
Episode length: 317.80 +/- 69.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 318       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3960000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.022     |
|    n_updates            | 9660      |
|    policy_gradient_loss | 6.77e-10  |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 967      |
|    time_elapsed    | 6536     |
|    total_timesteps | 3960832  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.5       |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 968       |
|    time_elapsed         | 6540      |
|    total_timesteps      | 3964928   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0421    |
|    n_updates            | 9670      |
|    policy_gradient_loss | -1.28e-10 |
|    value_loss           | 0.0566    |
---------------------------------------
Eval num_timesteps=3965000, episode_reward=0.30 +/- 0.64
Episode length: 313.80 +/- 56.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 3965000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0293    |
|    n_updates            | 9680      |
|    policy_gradient_loss | -4.18e-10 |
|    value_loss           | 0.0624    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 969      |
|    time_elapsed    | 6548     |
|    total_timesteps | 3969024  |
---------------------------------
Eval num_timesteps=3970000, episode_reward=0.80 +/- 0.75
Episode length: 324.60 +/- 53.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 3970000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0452    |
|    n_updates            | 9690      |
|    policy_gradient_loss | 1.44e-10  |
|    value_loss           | 0.06      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 970      |
|    time_elapsed    | 6555     |
|    total_timesteps | 3973120  |
---------------------------------
Eval num_timesteps=3975000, episode_reward=0.70 +/- 0.64
Episode length: 299.80 +/- 53.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 3975000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.032     |
|    n_updates            | 9700      |
|    policy_gradient_loss | -8.7e-10  |
|    value_loss           | 0.0559    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 971      |
|    time_elapsed    | 6562     |
|    total_timesteps | 3977216  |
---------------------------------
Eval num_timesteps=3980000, episode_reward=0.80 +/- 0.60
Episode length: 312.00 +/- 53.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 3980000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0488    |
|    n_updates            | 9710      |
|    policy_gradient_loss | 9.56e-10  |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 972      |
|    time_elapsed    | 6569     |
|    total_timesteps | 3981312  |
---------------------------------
Eval num_timesteps=3985000, episode_reward=0.10 +/- 0.30
Episode length: 278.00 +/- 37.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 3985000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0245    |
|    n_updates            | 9720      |
|    policy_gradient_loss | 2.97e-10  |
|    value_loss           | 0.0559    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 973      |
|    time_elapsed    | 6576     |
|    total_timesteps | 3985408  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.41      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 974       |
|    time_elapsed         | 6581      |
|    total_timesteps      | 3989504   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0244    |
|    n_updates            | 9730      |
|    policy_gradient_loss | -9e-10    |
|    value_loss           | 0.0542    |
---------------------------------------
Eval num_timesteps=3990000, episode_reward=0.40 +/- 0.49
Episode length: 325.80 +/- 47.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 326       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 3990000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0225    |
|    n_updates            | 9740      |
|    policy_gradient_loss | 1.96e-10  |
|    value_loss           | 0.0636    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 975      |
|    time_elapsed    | 6589     |
|    total_timesteps | 3993600  |
---------------------------------
Eval num_timesteps=3995000, episode_reward=0.50 +/- 0.92
Episode length: 306.20 +/- 37.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 3995000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0382    |
|    n_updates            | 9750      |
|    policy_gradient_loss | 4.03e-10  |
|    value_loss           | 0.0553    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 976      |
|    time_elapsed    | 6596     |
|    total_timesteps | 3997696  |
---------------------------------
Eval num_timesteps=4000000, episode_reward=0.60 +/- 0.66
Episode length: 320.40 +/- 63.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 320       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4000000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.016     |
|    n_updates            | 9760      |
|    policy_gradient_loss | -1.83e-10 |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 977      |
|    time_elapsed    | 6603     |
|    total_timesteps | 4001792  |
---------------------------------
Eval num_timesteps=4005000, episode_reward=0.60 +/- 0.49
Episode length: 302.40 +/- 40.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4005000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0349    |
|    n_updates            | 9770      |
|    policy_gradient_loss | 6.61e-10  |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 978      |
|    time_elapsed    | 6610     |
|    total_timesteps | 4005888  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 309       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 979       |
|    time_elapsed         | 6615      |
|    total_timesteps      | 4009984   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0352    |
|    n_updates            | 9780      |
|    policy_gradient_loss | -1.31e-11 |
|    value_loss           | 0.0623    |
---------------------------------------
Eval num_timesteps=4010000, episode_reward=0.90 +/- 1.04
Episode length: 302.40 +/- 45.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 4010000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0364    |
|    n_updates            | 9790      |
|    policy_gradient_loss | -1.09e-10 |
|    value_loss           | 0.0615    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 980      |
|    time_elapsed    | 6623     |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4015000, episode_reward=0.50 +/- 0.67
Episode length: 294.00 +/- 60.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4015000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0201    |
|    n_updates            | 9800      |
|    policy_gradient_loss | 7.61e-10  |
|    value_loss           | 0.0686    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 981      |
|    time_elapsed    | 6630     |
|    total_timesteps | 4018176  |
---------------------------------
Eval num_timesteps=4020000, episode_reward=0.40 +/- 0.49
Episode length: 287.00 +/- 54.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4020000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0221    |
|    n_updates            | 9810      |
|    policy_gradient_loss | 4.26e-10  |
|    value_loss           | 0.0645    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 982      |
|    time_elapsed    | 6637     |
|    total_timesteps | 4022272  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=0.20 +/- 0.60
Episode length: 299.00 +/- 65.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4025000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.023     |
|    n_updates            | 9820      |
|    policy_gradient_loss | -7.59e-10 |
|    value_loss           | 0.062     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 983      |
|    time_elapsed    | 6644     |
|    total_timesteps | 4026368  |
---------------------------------
Eval num_timesteps=4030000, episode_reward=0.70 +/- 0.64
Episode length: 313.40 +/- 30.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4030000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.027     |
|    n_updates            | 9830      |
|    policy_gradient_loss | 6.28e-10  |
|    value_loss           | 0.0644    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 984      |
|    time_elapsed    | 6651     |
|    total_timesteps | 4030464  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.51      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 985       |
|    time_elapsed         | 6656      |
|    total_timesteps      | 4034560   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0549    |
|    n_updates            | 9840      |
|    policy_gradient_loss | 6.54e-10  |
|    value_loss           | 0.059     |
---------------------------------------
Eval num_timesteps=4035000, episode_reward=0.70 +/- 0.78
Episode length: 308.40 +/- 45.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4035000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000157 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0444    |
|    n_updates            | 9850      |
|    policy_gradient_loss | 1.52e-10  |
|    value_loss           | 0.0659    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 986      |
|    time_elapsed    | 6664     |
|    total_timesteps | 4038656  |
---------------------------------
Eval num_timesteps=4040000, episode_reward=0.40 +/- 0.66
Episode length: 314.80 +/- 62.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 315          |
|    mean_reward          | 0.4          |
| time/                   |              |
|    total_timesteps      | 4040000      |
| train/                  |              |
|    approx_kl            | 7.567054e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000193    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0166       |
|    n_updates            | 9860         |
|    policy_gradient_loss | 3.81e-07     |
|    value_loss           | 0.0615       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 987      |
|    time_elapsed    | 6671     |
|    total_timesteps | 4042752  |
---------------------------------
Eval num_timesteps=4045000, episode_reward=0.30 +/- 0.46
Episode length: 308.40 +/- 44.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4045000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0553    |
|    n_updates            | 9870      |
|    policy_gradient_loss | -3.55e-10 |
|    value_loss           | 0.0656    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 988      |
|    time_elapsed    | 6678     |
|    total_timesteps | 4046848  |
---------------------------------
Eval num_timesteps=4050000, episode_reward=0.10 +/- 0.30
Episode length: 265.40 +/- 41.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 265       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 4050000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0254    |
|    n_updates            | 9880      |
|    policy_gradient_loss | 5.61e-10  |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 989      |
|    time_elapsed    | 6685     |
|    total_timesteps | 4050944  |
---------------------------------
Eval num_timesteps=4055000, episode_reward=0.20 +/- 0.40
Episode length: 266.60 +/- 38.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 267       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4055000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0359    |
|    n_updates            | 9890      |
|    policy_gradient_loss | 4.39e-10  |
|    value_loss           | 0.0581    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 605      |
|    iterations      | 990      |
|    time_elapsed    | 6692     |
|    total_timesteps | 4055040  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 302       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 991       |
|    time_elapsed         | 6697      |
|    total_timesteps      | 4059136   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0226    |
|    n_updates            | 9900      |
|    policy_gradient_loss | -2.35e-10 |
|    value_loss           | 0.0555    |
---------------------------------------
Eval num_timesteps=4060000, episode_reward=0.50 +/- 0.67
Episode length: 295.40 +/- 42.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4060000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0322    |
|    n_updates            | 9910      |
|    policy_gradient_loss | -3.19e-10 |
|    value_loss           | 0.0641    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 992      |
|    time_elapsed    | 6704     |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4065000, episode_reward=0.50 +/- 0.67
Episode length: 305.00 +/- 40.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4065000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.037     |
|    n_updates            | 9920      |
|    policy_gradient_loss | -6.91e-11 |
|    value_loss           | 0.064     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 993      |
|    time_elapsed    | 6711     |
|    total_timesteps | 4067328  |
---------------------------------
Eval num_timesteps=4070000, episode_reward=0.60 +/- 0.49
Episode length: 310.80 +/- 38.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4070000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0291    |
|    n_updates            | 9930      |
|    policy_gradient_loss | 1.67e-09  |
|    value_loss           | 0.0588    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 994      |
|    time_elapsed    | 6719     |
|    total_timesteps | 4071424  |
---------------------------------
Eval num_timesteps=4075000, episode_reward=0.70 +/- 0.64
Episode length: 308.40 +/- 73.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4075000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0361    |
|    n_updates            | 9940      |
|    policy_gradient_loss | 3.78e-11  |
|    value_loss           | 0.0658    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 995      |
|    time_elapsed    | 6726     |
|    total_timesteps | 4075520  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 302       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 996       |
|    time_elapsed         | 6731      |
|    total_timesteps      | 4079616   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0255    |
|    n_updates            | 9950      |
|    policy_gradient_loss | 7.62e-10  |
|    value_loss           | 0.0602    |
---------------------------------------
Eval num_timesteps=4080000, episode_reward=0.40 +/- 0.66
Episode length: 309.20 +/- 40.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4080000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0246    |
|    n_updates            | 9960      |
|    policy_gradient_loss | -4.71e-10 |
|    value_loss           | 0.0608    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 997      |
|    time_elapsed    | 6738     |
|    total_timesteps | 4083712  |
---------------------------------
Eval num_timesteps=4085000, episode_reward=0.40 +/- 0.49
Episode length: 311.00 +/- 58.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4085000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00806   |
|    n_updates            | 9970      |
|    policy_gradient_loss | -1.92e-10 |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 998      |
|    time_elapsed    | 6745     |
|    total_timesteps | 4087808  |
---------------------------------
Eval num_timesteps=4090000, episode_reward=0.60 +/- 0.66
Episode length: 294.40 +/- 55.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4090000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0313    |
|    n_updates            | 9980      |
|    policy_gradient_loss | -1.77e-10 |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 605      |
|    iterations      | 999      |
|    time_elapsed    | 6753     |
|    total_timesteps | 4091904  |
---------------------------------
Eval num_timesteps=4095000, episode_reward=0.60 +/- 0.66
Episode length: 294.20 +/- 55.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4095000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0397    |
|    n_updates            | 9990      |
|    policy_gradient_loss | 9.83e-10  |
|    value_loss           | 0.0634    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1000     |
|    time_elapsed    | 6759     |
|    total_timesteps | 4096000  |
---------------------------------
Eval num_timesteps=4100000, episode_reward=0.20 +/- 0.40
Episode length: 278.60 +/- 38.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 279       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4100000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0279    |
|    n_updates            | 10000     |
|    policy_gradient_loss | -2.5e-10  |
|    value_loss           | 0.0543    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1001     |
|    time_elapsed    | 6767     |
|    total_timesteps | 4100096  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 311       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 1002      |
|    time_elapsed         | 6772      |
|    total_timesteps      | 4104192   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0346    |
|    n_updates            | 10010     |
|    policy_gradient_loss | 3.13e-11  |
|    value_loss           | 0.0572    |
---------------------------------------
Eval num_timesteps=4105000, episode_reward=0.40 +/- 0.66
Episode length: 275.20 +/- 31.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 275       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4105000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0378    |
|    n_updates            | 10020     |
|    policy_gradient_loss | -6.1e-10  |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 1003     |
|    time_elapsed    | 6778     |
|    total_timesteps | 4108288  |
---------------------------------
Eval num_timesteps=4110000, episode_reward=0.30 +/- 0.46
Episode length: 286.40 +/- 38.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4110000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0239    |
|    n_updates            | 10030     |
|    policy_gradient_loss | -2.67e-10 |
|    value_loss           | 0.0582    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 1004     |
|    time_elapsed    | 6786     |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4115000, episode_reward=0.20 +/- 0.40
Episode length: 302.40 +/- 35.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4115000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0431    |
|    n_updates            | 10040     |
|    policy_gradient_loss | -1.07e-10 |
|    value_loss           | 0.0715    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1005     |
|    time_elapsed    | 6793     |
|    total_timesteps | 4116480  |
---------------------------------
Eval num_timesteps=4120000, episode_reward=0.50 +/- 0.50
Episode length: 297.00 +/- 65.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4120000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0106    |
|    n_updates            | 10050     |
|    policy_gradient_loss | -3.44e-10 |
|    value_loss           | 0.0628    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1006     |
|    time_elapsed    | 6800     |
|    total_timesteps | 4120576  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.52      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 1007      |
|    time_elapsed         | 6805      |
|    total_timesteps      | 4124672   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0423    |
|    n_updates            | 10060     |
|    policy_gradient_loss | -1.06e-10 |
|    value_loss           | 0.0661    |
---------------------------------------
Eval num_timesteps=4125000, episode_reward=0.30 +/- 0.46
Episode length: 309.00 +/- 61.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4125000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0232    |
|    n_updates            | 10070     |
|    policy_gradient_loss | 7.19e-10  |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 1008     |
|    time_elapsed    | 6812     |
|    total_timesteps | 4128768  |
---------------------------------
Eval num_timesteps=4130000, episode_reward=0.20 +/- 0.40
Episode length: 293.20 +/- 47.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4130000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0384    |
|    n_updates            | 10080     |
|    policy_gradient_loss | 5.17e-10  |
|    value_loss           | 0.0608    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 606      |
|    iterations      | 1009     |
|    time_elapsed    | 6819     |
|    total_timesteps | 4132864  |
---------------------------------
Eval num_timesteps=4135000, episode_reward=0.30 +/- 0.46
Episode length: 298.40 +/- 52.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4135000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0099    |
|    n_updates            | 10090     |
|    policy_gradient_loss | 1.41e-10  |
|    value_loss           | 0.0642    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1010     |
|    time_elapsed    | 6826     |
|    total_timesteps | 4136960  |
---------------------------------
Eval num_timesteps=4140000, episode_reward=0.20 +/- 0.40
Episode length: 284.00 +/- 29.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 284       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4140000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0308    |
|    n_updates            | 10100     |
|    policy_gradient_loss | 3.8e-10   |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1011     |
|    time_elapsed    | 6833     |
|    total_timesteps | 4141056  |
---------------------------------
Eval num_timesteps=4145000, episode_reward=0.90 +/- 0.70
Episode length: 314.80 +/- 48.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 4145000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0223    |
|    n_updates            | 10110     |
|    policy_gradient_loss | 1.95e-10  |
|    value_loss           | 0.0592    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1012     |
|    time_elapsed    | 6840     |
|    total_timesteps | 4145152  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 308       |
|    ep_rew_mean          | 0.51      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 1013      |
|    time_elapsed         | 6846      |
|    total_timesteps      | 4149248   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0326    |
|    n_updates            | 10120     |
|    policy_gradient_loss | -6.77e-11 |
|    value_loss           | 0.0631    |
---------------------------------------
Eval num_timesteps=4150000, episode_reward=0.50 +/- 0.67
Episode length: 288.60 +/- 46.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4150000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0218    |
|    n_updates            | 10130     |
|    policy_gradient_loss | 7.37e-10  |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 1014     |
|    time_elapsed    | 6853     |
|    total_timesteps | 4153344  |
---------------------------------
Eval num_timesteps=4155000, episode_reward=0.50 +/- 0.81
Episode length: 326.60 +/- 47.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 327       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4155000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0259    |
|    n_updates            | 10140     |
|    policy_gradient_loss | 4.46e-10  |
|    value_loss           | 0.0606    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1015     |
|    time_elapsed    | 6860     |
|    total_timesteps | 4157440  |
---------------------------------
Eval num_timesteps=4160000, episode_reward=0.30 +/- 0.46
Episode length: 295.60 +/- 48.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4160000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0196    |
|    n_updates            | 10150     |
|    policy_gradient_loss | -2.58e-11 |
|    value_loss           | 0.0664    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1016     |
|    time_elapsed    | 6867     |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4165000, episode_reward=0.30 +/- 0.46
Episode length: 285.20 +/- 40.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4165000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0459    |
|    n_updates            | 10160     |
|    policy_gradient_loss | 8.71e-10  |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1017     |
|    time_elapsed    | 6874     |
|    total_timesteps | 4165632  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 311       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 1018      |
|    time_elapsed         | 6879      |
|    total_timesteps      | 4169728   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0257    |
|    n_updates            | 10170     |
|    policy_gradient_loss | -2.62e-11 |
|    value_loss           | 0.0616    |
---------------------------------------
Eval num_timesteps=4170000, episode_reward=0.50 +/- 0.81
Episode length: 306.60 +/- 72.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4170000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0244    |
|    n_updates            | 10180     |
|    policy_gradient_loss | 2.83e-10  |
|    value_loss           | 0.0573    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1019     |
|    time_elapsed    | 6887     |
|    total_timesteps | 4173824  |
---------------------------------
Eval num_timesteps=4175000, episode_reward=0.40 +/- 0.66
Episode length: 323.80 +/- 67.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 324       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4175000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0347    |
|    n_updates            | 10190     |
|    policy_gradient_loss | -4.89e-10 |
|    value_loss           | 0.0622    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1020     |
|    time_elapsed    | 6894     |
|    total_timesteps | 4177920  |
---------------------------------
Eval num_timesteps=4180000, episode_reward=0.40 +/- 0.66
Episode length: 298.60 +/- 47.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4180000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0156    |
|    n_updates            | 10200     |
|    policy_gradient_loss | 8.32e-10  |
|    value_loss           | 0.0538    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1021     |
|    time_elapsed    | 6901     |
|    total_timesteps | 4182016  |
---------------------------------
Eval num_timesteps=4185000, episode_reward=0.70 +/- 0.78
Episode length: 308.20 +/- 49.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4185000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0476    |
|    n_updates            | 10210     |
|    policy_gradient_loss | -3.35e-11 |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1022     |
|    time_elapsed    | 6908     |
|    total_timesteps | 4186112  |
---------------------------------
Eval num_timesteps=4190000, episode_reward=0.40 +/- 0.49
Episode length: 291.80 +/- 31.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4190000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0225    |
|    n_updates            | 10220     |
|    policy_gradient_loss | -3.62e-10 |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1023     |
|    time_elapsed    | 6916     |
|    total_timesteps | 4190208  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 605       |
|    iterations           | 1024      |
|    time_elapsed         | 6921      |
|    total_timesteps      | 4194304   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0165    |
|    n_updates            | 10230     |
|    policy_gradient_loss | -7.79e-10 |
|    value_loss           | 0.0661    |
---------------------------------------
Eval num_timesteps=4195000, episode_reward=1.00 +/- 0.89
Episode length: 324.60 +/- 49.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 1         |
| time/                   |           |
|    total_timesteps      | 4195000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00458   |
|    n_updates            | 10240     |
|    policy_gradient_loss | 4.58e-10  |
|    value_loss           | 0.0563    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1025     |
|    time_elapsed    | 6928     |
|    total_timesteps | 4198400  |
---------------------------------
Eval num_timesteps=4200000, episode_reward=0.20 +/- 0.60
Episode length: 279.40 +/- 51.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 279       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4200000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000191 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0496    |
|    n_updates            | 10250     |
|    policy_gradient_loss | -2.47e-10 |
|    value_loss           | 0.063     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1026     |
|    time_elapsed    | 6935     |
|    total_timesteps | 4202496  |
---------------------------------
Eval num_timesteps=4205000, episode_reward=0.40 +/- 0.49
Episode length: 316.80 +/- 45.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 317         |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 4205000     |
| train/                  |             |
|    approx_kl            | 2.62463e-05 |
|    clip_fraction        | 9.77e-05    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000201   |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0369      |
|    n_updates            | 10260       |
|    policy_gradient_loss | 2.72e-06    |
|    value_loss           | 0.0598      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1027     |
|    time_elapsed    | 6942     |
|    total_timesteps | 4206592  |
---------------------------------
Eval num_timesteps=4210000, episode_reward=0.60 +/- 0.80
Episode length: 305.80 +/- 63.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4210000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000275 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0238    |
|    n_updates            | 10270     |
|    policy_gradient_loss | -4.7e-10  |
|    value_loss           | 0.0614    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1028     |
|    time_elapsed    | 6949     |
|    total_timesteps | 4210688  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 297       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 1029      |
|    time_elapsed         | 6954      |
|    total_timesteps      | 4214784   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000275 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0348    |
|    n_updates            | 10280     |
|    policy_gradient_loss | 6.52e-10  |
|    value_loss           | 0.0672    |
---------------------------------------
Eval num_timesteps=4215000, episode_reward=0.70 +/- 0.64
Episode length: 298.40 +/- 57.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 298          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 4215000      |
| train/                  |              |
|    approx_kl            | 1.921432e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000194    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0385       |
|    n_updates            | 10290        |
|    policy_gradient_loss | -1.89e-05    |
|    value_loss           | 0.0604       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1030     |
|    time_elapsed    | 6962     |
|    total_timesteps | 4218880  |
---------------------------------
Eval num_timesteps=4220000, episode_reward=0.70 +/- 0.64
Episode length: 325.20 +/- 63.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4220000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000192 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0255    |
|    n_updates            | 10300     |
|    policy_gradient_loss | 1.8e-10   |
|    value_loss           | 0.056     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1031     |
|    time_elapsed    | 6969     |
|    total_timesteps | 4222976  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=0.30 +/- 0.46
Episode length: 288.20 +/- 36.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4225000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000192 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0296    |
|    n_updates            | 10310     |
|    policy_gradient_loss | 7.82e-11  |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1032     |
|    time_elapsed    | 6976     |
|    total_timesteps | 4227072  |
---------------------------------
Eval num_timesteps=4230000, episode_reward=0.70 +/- 0.64
Episode length: 309.20 +/- 54.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4230000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000192 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0318    |
|    n_updates            | 10320     |
|    policy_gradient_loss | 2.21e-10  |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1033     |
|    time_elapsed    | 6983     |
|    total_timesteps | 4231168  |
---------------------------------
Eval num_timesteps=4235000, episode_reward=0.70 +/- 0.64
Episode length: 297.60 +/- 47.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4235000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000192 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0448    |
|    n_updates            | 10330     |
|    policy_gradient_loss | -1.3e-10  |
|    value_loss           | 0.0643    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1034     |
|    time_elapsed    | 6990     |
|    total_timesteps | 4235264  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 1035      |
|    time_elapsed         | 6995      |
|    total_timesteps      | 4239360   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000192 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0177    |
|    n_updates            | 10340     |
|    policy_gradient_loss | 2.14e-10  |
|    value_loss           | 0.0621    |
---------------------------------------
Eval num_timesteps=4240000, episode_reward=0.50 +/- 0.50
Episode length: 302.40 +/- 46.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4240000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000192 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0167    |
|    n_updates            | 10350     |
|    policy_gradient_loss | -7.51e-10 |
|    value_loss           | 0.0635    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1036     |
|    time_elapsed    | 7002     |
|    total_timesteps | 4243456  |
---------------------------------
Eval num_timesteps=4245000, episode_reward=0.20 +/- 0.40
Episode length: 293.60 +/- 22.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4245000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000192 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.026     |
|    n_updates            | 10360     |
|    policy_gradient_loss | -3.7e-10  |
|    value_loss           | 0.0596    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1037     |
|    time_elapsed    | 7009     |
|    total_timesteps | 4247552  |
---------------------------------
Eval num_timesteps=4250000, episode_reward=0.40 +/- 0.66
Episode length: 287.40 +/- 50.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4250000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000192 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.034     |
|    n_updates            | 10370     |
|    policy_gradient_loss | -6.5e-10  |
|    value_loss           | 0.0628    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1038     |
|    time_elapsed    | 7016     |
|    total_timesteps | 4251648  |
---------------------------------
Eval num_timesteps=4255000, episode_reward=0.70 +/- 0.78
Episode length: 300.00 +/- 49.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 300          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 4255000      |
| train/                  |              |
|    approx_kl            | 6.051593e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000312    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0354       |
|    n_updates            | 10380        |
|    policy_gradient_loss | -9.22e-05    |
|    value_loss           | 0.0693       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1039     |
|    time_elapsed    | 7024     |
|    total_timesteps | 4255744  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.52      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 1040      |
|    time_elapsed         | 7029      |
|    total_timesteps      | 4259840   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0137    |
|    n_updates            | 10390     |
|    policy_gradient_loss | 4.03e-10  |
|    value_loss           | 0.055     |
---------------------------------------
Eval num_timesteps=4260000, episode_reward=0.50 +/- 0.50
Episode length: 299.40 +/- 55.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4260000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0321    |
|    n_updates            | 10400     |
|    policy_gradient_loss | 7.31e-10  |
|    value_loss           | 0.0559    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1041     |
|    time_elapsed    | 7036     |
|    total_timesteps | 4263936  |
---------------------------------
Eval num_timesteps=4265000, episode_reward=0.50 +/- 0.67
Episode length: 297.40 +/- 60.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4265000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.015     |
|    n_updates            | 10410     |
|    policy_gradient_loss | 8.37e-10  |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1042     |
|    time_elapsed    | 7043     |
|    total_timesteps | 4268032  |
---------------------------------
Eval num_timesteps=4270000, episode_reward=0.40 +/- 0.66
Episode length: 284.60 +/- 46.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4270000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0516    |
|    n_updates            | 10420     |
|    policy_gradient_loss | 4.47e-10  |
|    value_loss           | 0.0638    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1043     |
|    time_elapsed    | 7050     |
|    total_timesteps | 4272128  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=0.40 +/- 0.66
Episode length: 284.80 +/- 42.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4275000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0233    |
|    n_updates            | 10430     |
|    policy_gradient_loss | -5.02e-11 |
|    value_loss           | 0.0609    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1044     |
|    time_elapsed    | 7057     |
|    total_timesteps | 4276224  |
---------------------------------
Eval num_timesteps=4280000, episode_reward=0.60 +/- 0.49
Episode length: 328.80 +/- 50.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 329       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4280000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0204    |
|    n_updates            | 10440     |
|    policy_gradient_loss | 8.88e-11  |
|    value_loss           | 0.0535    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1045     |
|    time_elapsed    | 7064     |
|    total_timesteps | 4280320  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 605       |
|    iterations           | 1046      |
|    time_elapsed         | 7070      |
|    total_timesteps      | 4284416   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.031     |
|    n_updates            | 10450     |
|    policy_gradient_loss | 9.44e-10  |
|    value_loss           | 0.0582    |
---------------------------------------
Eval num_timesteps=4285000, episode_reward=0.90 +/- 0.70
Episode length: 326.80 +/- 65.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 327       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 4285000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0188    |
|    n_updates            | 10460     |
|    policy_gradient_loss | -3.74e-10 |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1047     |
|    time_elapsed    | 7077     |
|    total_timesteps | 4288512  |
---------------------------------
Eval num_timesteps=4290000, episode_reward=0.20 +/- 0.40
Episode length: 323.20 +/- 50.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 323       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4290000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.009     |
|    n_updates            | 10470     |
|    policy_gradient_loss | 3.27e-11  |
|    value_loss           | 0.0534    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1048     |
|    time_elapsed    | 7084     |
|    total_timesteps | 4292608  |
---------------------------------
Eval num_timesteps=4295000, episode_reward=0.70 +/- 0.64
Episode length: 300.20 +/- 47.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4295000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0354    |
|    n_updates            | 10480     |
|    policy_gradient_loss | -1.06e-09 |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1049     |
|    time_elapsed    | 7091     |
|    total_timesteps | 4296704  |
---------------------------------
Eval num_timesteps=4300000, episode_reward=0.70 +/- 0.64
Episode length: 306.20 +/- 43.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4300000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0445    |
|    n_updates            | 10490     |
|    policy_gradient_loss | 1.46e-10  |
|    value_loss           | 0.0577    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1050     |
|    time_elapsed    | 7098     |
|    total_timesteps | 4300800  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 302       |
|    ep_rew_mean          | 0.41      |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 1051      |
|    time_elapsed         | 7103      |
|    total_timesteps      | 4304896   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0172    |
|    n_updates            | 10500     |
|    policy_gradient_loss | 2.56e-10  |
|    value_loss           | 0.0557    |
---------------------------------------
Eval num_timesteps=4305000, episode_reward=0.30 +/- 0.46
Episode length: 290.40 +/- 47.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4305000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0357    |
|    n_updates            | 10510     |
|    policy_gradient_loss | 8.08e-10  |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1052     |
|    time_elapsed    | 7110     |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4310000, episode_reward=0.20 +/- 0.40
Episode length: 268.00 +/- 44.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 268       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4310000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0197    |
|    n_updates            | 10520     |
|    policy_gradient_loss | -4.15e-11 |
|    value_loss           | 0.0614    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1053     |
|    time_elapsed    | 7117     |
|    total_timesteps | 4313088  |
---------------------------------
Eval num_timesteps=4315000, episode_reward=0.50 +/- 0.50
Episode length: 304.40 +/- 57.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4315000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0402    |
|    n_updates            | 10530     |
|    policy_gradient_loss | 8.16e-10  |
|    value_loss           | 0.0607    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1054     |
|    time_elapsed    | 7124     |
|    total_timesteps | 4317184  |
---------------------------------
Eval num_timesteps=4320000, episode_reward=0.50 +/- 0.50
Episode length: 287.00 +/- 52.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4320000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000326 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0222    |
|    n_updates            | 10540     |
|    policy_gradient_loss | -5.42e-10 |
|    value_loss           | 0.0557    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1055     |
|    time_elapsed    | 7133     |
|    total_timesteps | 4321280  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=0.50 +/- 0.67
Episode length: 297.40 +/- 56.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 297          |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 4325000      |
| train/                  |              |
|    approx_kl            | 3.868519e-05 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000207    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.032        |
|    n_updates            | 10550        |
|    policy_gradient_loss | -3.89e-05    |
|    value_loss           | 0.0589       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1056     |
|    time_elapsed    | 7141     |
|    total_timesteps | 4325376  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.39      |
| time/                   |           |
|    fps                  | 605       |
|    iterations           | 1057      |
|    time_elapsed         | 7147      |
|    total_timesteps      | 4329472   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000193 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0169    |
|    n_updates            | 10560     |
|    policy_gradient_loss | 8.88e-11  |
|    value_loss           | 0.0612    |
---------------------------------------
Eval num_timesteps=4330000, episode_reward=0.30 +/- 0.46
Episode length: 259.60 +/- 23.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 260           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 4330000       |
| train/                  |               |
|    approx_kl            | 2.4457215e-05 |
|    clip_fraction        | 0.000317      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000171     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0425        |
|    n_updates            | 10570         |
|    policy_gradient_loss | 3.58e-06      |
|    value_loss           | 0.0627        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1058     |
|    time_elapsed    | 7155     |
|    total_timesteps | 4333568  |
---------------------------------
Eval num_timesteps=4335000, episode_reward=0.40 +/- 0.66
Episode length: 298.40 +/- 58.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4335000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00016  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0376    |
|    n_updates            | 10580     |
|    policy_gradient_loss | -1.76e-10 |
|    value_loss           | 0.0578    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1059     |
|    time_elapsed    | 7164     |
|    total_timesteps | 4337664  |
---------------------------------
Eval num_timesteps=4340000, episode_reward=0.30 +/- 0.46
Episode length: 279.60 +/- 50.25
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 280      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 4340000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.053    |
|    n_updates            | 10590    |
|    policy_gradient_loss | 1.94e-10 |
|    value_loss           | 0.0696   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1060     |
|    time_elapsed    | 7172     |
|    total_timesteps | 4341760  |
---------------------------------
Eval num_timesteps=4345000, episode_reward=0.30 +/- 0.46
Episode length: 313.40 +/- 54.79
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 313      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 4345000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0405   |
|    n_updates            | 10600    |
|    policy_gradient_loss | 5.21e-10 |
|    value_loss           | 0.0616   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1061     |
|    time_elapsed    | 7181     |
|    total_timesteps | 4345856  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 299       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 605       |
|    iterations           | 1062      |
|    time_elapsed         | 7187      |
|    total_timesteps      | 4349952   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00016  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0188    |
|    n_updates            | 10610     |
|    policy_gradient_loss | -8.37e-10 |
|    value_loss           | 0.0618    |
---------------------------------------
Eval num_timesteps=4350000, episode_reward=0.60 +/- 0.66
Episode length: 284.00 +/- 34.06
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 284      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 4350000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0311   |
|    n_updates            | 10620    |
|    policy_gradient_loss | 5.38e-10 |
|    value_loss           | 0.0591   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 605      |
|    iterations      | 1063     |
|    time_elapsed    | 7195     |
|    total_timesteps | 4354048  |
---------------------------------
Eval num_timesteps=4355000, episode_reward=0.40 +/- 0.49
Episode length: 303.60 +/- 56.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 304      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 4355000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0122   |
|    n_updates            | 10630    |
|    policy_gradient_loss | 1.24e-09 |
|    value_loss           | 0.0569   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 604      |
|    iterations      | 1064     |
|    time_elapsed    | 7204     |
|    total_timesteps | 4358144  |
---------------------------------
Eval num_timesteps=4360000, episode_reward=0.50 +/- 0.50
Episode length: 309.20 +/- 48.23
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 309      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 4360000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0269   |
|    n_updates            | 10640    |
|    policy_gradient_loss | 3.3e-10  |
|    value_loss           | 0.06     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 604      |
|    iterations      | 1065     |
|    time_elapsed    | 7212     |
|    total_timesteps | 4362240  |
---------------------------------
Eval num_timesteps=4365000, episode_reward=0.50 +/- 0.67
Episode length: 328.80 +/- 63.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 329      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 4365000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.026    |
|    n_updates            | 10650    |
|    policy_gradient_loss | -1.8e-10 |
|    value_loss           | 0.0654   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 604      |
|    iterations      | 1066     |
|    time_elapsed    | 7221     |
|    total_timesteps | 4366336  |
---------------------------------
Eval num_timesteps=4370000, episode_reward=0.50 +/- 0.50
Episode length: 302.40 +/- 42.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 302      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 4370000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0267   |
|    n_updates            | 10660    |
|    policy_gradient_loss | 1.6e-10  |
|    value_loss           | 0.0587   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 604      |
|    iterations      | 1067     |
|    time_elapsed    | 7230     |
|    total_timesteps | 4370432  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 309      |
|    ep_rew_mean          | 0.58     |
| time/                   |          |
|    fps                  | 604      |
|    iterations           | 1068     |
|    time_elapsed         | 7236     |
|    total_timesteps      | 4374528  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0559   |
|    n_updates            | 10670    |
|    policy_gradient_loss | -1.8e-10 |
|    value_loss           | 0.0711   |
--------------------------------------
Eval num_timesteps=4375000, episode_reward=0.20 +/- 0.40
Episode length: 266.40 +/- 33.56
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 266      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 4375000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0292   |
|    n_updates            | 10680    |
|    policy_gradient_loss | -1e-09   |
|    value_loss           | 0.0659   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 604      |
|    iterations      | 1069     |
|    time_elapsed    | 7244     |
|    total_timesteps | 4378624  |
---------------------------------
Eval num_timesteps=4380000, episode_reward=0.20 +/- 0.40
Episode length: 292.80 +/- 27.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4380000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00016  |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0458    |
|    n_updates            | 10690     |
|    policy_gradient_loss | -1.75e-10 |
|    value_loss           | 0.0609    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 604      |
|    iterations      | 1070     |
|    time_elapsed    | 7253     |
|    total_timesteps | 4382720  |
---------------------------------
Eval num_timesteps=4385000, episode_reward=0.30 +/- 0.46
Episode length: 279.60 +/- 38.51
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 280      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 4385000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00016 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0216   |
|    n_updates            | 10700    |
|    policy_gradient_loss | 1.53e-10 |
|    value_loss           | 0.0615   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 604      |
|    iterations      | 1071     |
|    time_elapsed    | 7261     |
|    total_timesteps | 4386816  |
---------------------------------
Eval num_timesteps=4390000, episode_reward=0.60 +/- 0.66
Episode length: 284.40 +/- 50.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 284          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 4390000      |
| train/                  |              |
|    approx_kl            | 4.137575e-06 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000141    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0397       |
|    n_updates            | 10710        |
|    policy_gradient_loss | 2.04e-06     |
|    value_loss           | 0.0584       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 1072     |
|    time_elapsed    | 7270     |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4395000, episode_reward=0.40 +/- 0.49
Episode length: 303.80 +/- 52.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4395000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0323    |
|    n_updates            | 10720     |
|    policy_gradient_loss | -2.07e-10 |
|    value_loss           | 0.058     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 1073     |
|    time_elapsed    | 7278     |
|    total_timesteps | 4395008  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 302       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 603       |
|    iterations           | 1074      |
|    time_elapsed         | 7284      |
|    total_timesteps      | 4399104   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0183    |
|    n_updates            | 10730     |
|    policy_gradient_loss | -6.89e-10 |
|    value_loss           | 0.0569    |
---------------------------------------
Eval num_timesteps=4400000, episode_reward=0.50 +/- 0.81
Episode length: 309.20 +/- 56.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4400000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0204    |
|    n_updates            | 10740     |
|    policy_gradient_loss | -5.09e-10 |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 1075     |
|    time_elapsed    | 7293     |
|    total_timesteps | 4403200  |
---------------------------------
Eval num_timesteps=4405000, episode_reward=0.50 +/- 0.50
Episode length: 314.00 +/- 44.27
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 314      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 4405000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0259   |
|    n_updates            | 10750    |
|    policy_gradient_loss | 1.33e-10 |
|    value_loss           | 0.0555   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 1076     |
|    time_elapsed    | 7301     |
|    total_timesteps | 4407296  |
---------------------------------
Eval num_timesteps=4410000, episode_reward=0.70 +/- 0.46
Episode length: 305.40 +/- 43.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4410000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0262    |
|    n_updates            | 10760     |
|    policy_gradient_loss | -5.13e-10 |
|    value_loss           | 0.0635    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 1077     |
|    time_elapsed    | 7310     |
|    total_timesteps | 4411392  |
---------------------------------
Eval num_timesteps=4415000, episode_reward=0.30 +/- 0.46
Episode length: 308.40 +/- 47.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4415000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0139    |
|    n_updates            | 10770     |
|    policy_gradient_loss | -5.64e-11 |
|    value_loss           | 0.0573    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 1078     |
|    time_elapsed    | 7318     |
|    total_timesteps | 4415488  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 297      |
|    ep_rew_mean          | 0.37     |
| time/                   |          |
|    fps                  | 603      |
|    iterations           | 1079     |
|    time_elapsed         | 7325     |
|    total_timesteps      | 4419584  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.021    |
|    n_updates            | 10780    |
|    policy_gradient_loss | 2.31e-10 |
|    value_loss           | 0.0538   |
--------------------------------------
Eval num_timesteps=4420000, episode_reward=0.20 +/- 0.40
Episode length: 297.40 +/- 43.19
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 297      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 4420000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0443   |
|    n_updates            | 10790    |
|    policy_gradient_loss | 7.06e-11 |
|    value_loss           | 0.0592   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 1080     |
|    time_elapsed    | 7333     |
|    total_timesteps | 4423680  |
---------------------------------
Eval num_timesteps=4425000, episode_reward=0.40 +/- 0.49
Episode length: 297.40 +/- 47.35
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 297      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 4425000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0238   |
|    n_updates            | 10800    |
|    policy_gradient_loss | 3.05e-10 |
|    value_loss           | 0.0627   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 603      |
|    iterations      | 1081     |
|    time_elapsed    | 7342     |
|    total_timesteps | 4427776  |
---------------------------------
Eval num_timesteps=4430000, episode_reward=0.50 +/- 0.50
Episode length: 334.60 +/- 70.86
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 335      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 4430000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0468   |
|    n_updates            | 10810    |
|    policy_gradient_loss | 5.04e-10 |
|    value_loss           | 0.0606   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 602      |
|    iterations      | 1082     |
|    time_elapsed    | 7350     |
|    total_timesteps | 4431872  |
---------------------------------
Eval num_timesteps=4435000, episode_reward=0.00 +/- 0.00
Episode length: 291.60 +/- 46.89
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 292      |
|    mean_reward          | 0        |
| time/                   |          |
|    total_timesteps      | 4435000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0303   |
|    n_updates            | 10820    |
|    policy_gradient_loss | 1.15e-09 |
|    value_loss           | 0.0696   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 602      |
|    iterations      | 1083     |
|    time_elapsed    | 7359     |
|    total_timesteps | 4435968  |
---------------------------------
Eval num_timesteps=4440000, episode_reward=0.40 +/- 0.66
Episode length: 310.80 +/- 58.78
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 311      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 4440000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0295   |
|    n_updates            | 10830    |
|    policy_gradient_loss | 8.51e-11 |
|    value_loss           | 0.0617   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 602      |
|    iterations      | 1084     |
|    time_elapsed    | 7368     |
|    total_timesteps | 4440064  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.55      |
| time/                   |           |
|    fps                  | 602       |
|    iterations           | 1085      |
|    time_elapsed         | 7374      |
|    total_timesteps      | 4444160   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0157    |
|    n_updates            | 10840     |
|    policy_gradient_loss | -1.99e-10 |
|    value_loss           | 0.0547    |
---------------------------------------
Eval num_timesteps=4445000, episode_reward=0.60 +/- 0.66
Episode length: 308.40 +/- 40.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4445000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0511    |
|    n_updates            | 10850     |
|    policy_gradient_loss | -1.15e-10 |
|    value_loss           | 0.0642    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 602      |
|    iterations      | 1086     |
|    time_elapsed    | 7382     |
|    total_timesteps | 4448256  |
---------------------------------
Eval num_timesteps=4450000, episode_reward=0.50 +/- 0.50
Episode length: 308.60 +/- 60.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4450000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0409    |
|    n_updates            | 10860     |
|    policy_gradient_loss | -7.89e-11 |
|    value_loss           | 0.0673    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 602      |
|    iterations      | 1087     |
|    time_elapsed    | 7391     |
|    total_timesteps | 4452352  |
---------------------------------
Eval num_timesteps=4455000, episode_reward=0.80 +/- 0.60
Episode length: 321.20 +/- 46.08
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 321      |
|    mean_reward          | 0.8      |
| time/                   |          |
|    total_timesteps      | 4455000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.028    |
|    n_updates            | 10870    |
|    policy_gradient_loss | 2.31e-10 |
|    value_loss           | 0.0617   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 602      |
|    iterations      | 1088     |
|    time_elapsed    | 7400     |
|    total_timesteps | 4456448  |
---------------------------------
Eval num_timesteps=4460000, episode_reward=0.10 +/- 0.30
Episode length: 279.60 +/- 53.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 280      |
|    mean_reward          | 0.1      |
| time/                   |          |
|    total_timesteps      | 4460000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0226   |
|    n_updates            | 10880    |
|    policy_gradient_loss | 1.43e-10 |
|    value_loss           | 0.0606   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 602      |
|    iterations      | 1089     |
|    time_elapsed    | 7408     |
|    total_timesteps | 4460544  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 297       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 602       |
|    iterations           | 1090      |
|    time_elapsed         | 7414      |
|    total_timesteps      | 4464640   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0249    |
|    n_updates            | 10890     |
|    policy_gradient_loss | -2.77e-10 |
|    value_loss           | 0.0632    |
---------------------------------------
Eval num_timesteps=4465000, episode_reward=0.30 +/- 0.46
Episode length: 289.60 +/- 46.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4465000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0163    |
|    n_updates            | 10900     |
|    policy_gradient_loss | -1.53e-10 |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1091     |
|    time_elapsed    | 7423     |
|    total_timesteps | 4468736  |
---------------------------------
Eval num_timesteps=4470000, episode_reward=0.50 +/- 0.67
Episode length: 309.60 +/- 48.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4470000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0177    |
|    n_updates            | 10910     |
|    policy_gradient_loss | -3.85e-10 |
|    value_loss           | 0.0706    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1092     |
|    time_elapsed    | 7431     |
|    total_timesteps | 4472832  |
---------------------------------
Eval num_timesteps=4475000, episode_reward=0.70 +/- 0.64
Episode length: 295.00 +/- 43.94
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 295      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 4475000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0277   |
|    n_updates            | 10920    |
|    policy_gradient_loss | 2.98e-10 |
|    value_loss           | 0.0645   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1093     |
|    time_elapsed    | 7440     |
|    total_timesteps | 4476928  |
---------------------------------
Eval num_timesteps=4480000, episode_reward=0.40 +/- 0.49
Episode length: 297.40 +/- 53.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 297      |
|    mean_reward          | 0.4      |
| time/                   |          |
|    total_timesteps      | 4480000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.033    |
|    n_updates            | 10930    |
|    policy_gradient_loss | 4.06e-10 |
|    value_loss           | 0.0577   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1094     |
|    time_elapsed    | 7448     |
|    total_timesteps | 4481024  |
---------------------------------
Eval num_timesteps=4485000, episode_reward=0.70 +/- 0.64
Episode length: 320.00 +/- 58.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 320      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 4485000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0279   |
|    n_updates            | 10940    |
|    policy_gradient_loss | 2.47e-10 |
|    value_loss           | 0.065    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1095     |
|    time_elapsed    | 7457     |
|    total_timesteps | 4485120  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 302      |
|    ep_rew_mean          | 0.49     |
| time/                   |          |
|    fps                  | 601      |
|    iterations           | 1096     |
|    time_elapsed         | 7463     |
|    total_timesteps      | 4489216  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00015 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0509   |
|    n_updates            | 10950    |
|    policy_gradient_loss | -5.1e-10 |
|    value_loss           | 0.0628   |
--------------------------------------
Eval num_timesteps=4490000, episode_reward=0.70 +/- 0.78
Episode length: 287.80 +/- 41.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4490000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0414    |
|    n_updates            | 10960     |
|    policy_gradient_loss | -2.44e-10 |
|    value_loss           | 0.0661    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1097     |
|    time_elapsed    | 7472     |
|    total_timesteps | 4493312  |
---------------------------------
Eval num_timesteps=4495000, episode_reward=0.40 +/- 0.49
Episode length: 314.20 +/- 53.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4495000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00015  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0349    |
|    n_updates            | 10970     |
|    policy_gradient_loss | -2.07e-11 |
|    value_loss           | 0.0564    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1098     |
|    time_elapsed    | 7480     |
|    total_timesteps | 4497408  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=0.20 +/- 0.40
Episode length: 281.60 +/- 66.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 282           |
|    mean_reward          | 0.2           |
| time/                   |               |
|    total_timesteps      | 4500000       |
| train/                  |               |
|    approx_kl            | 3.5614023e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00014      |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.038         |
|    n_updates            | 10980         |
|    policy_gradient_loss | 2.3e-06       |
|    value_loss           | 0.0635        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 1099     |
|    time_elapsed    | 7489     |
|    total_timesteps | 4501504  |
---------------------------------
Eval num_timesteps=4505000, episode_reward=0.30 +/- 0.64
Episode length: 292.20 +/- 45.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 4505000     |
| train/                  |             |
|    approx_kl            | 4.59679e-06 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000205   |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0289      |
|    n_updates            | 10990       |
|    policy_gradient_loss | 1.62e-06    |
|    value_loss           | 0.055       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 600      |
|    iterations      | 1100     |
|    time_elapsed    | 7497     |
|    total_timesteps | 4505600  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 311       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 600       |
|    iterations           | 1101      |
|    time_elapsed         | 7504      |
|    total_timesteps      | 4509696   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0497    |
|    n_updates            | 11000     |
|    policy_gradient_loss | 1.08e-10  |
|    value_loss           | 0.0637    |
---------------------------------------
Eval num_timesteps=4510000, episode_reward=1.10 +/- 0.83
Episode length: 333.80 +/- 66.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 334       |
|    mean_reward          | 1.1       |
| time/                   |           |
|    total_timesteps      | 4510000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0358    |
|    n_updates            | 11010     |
|    policy_gradient_loss | 6.59e-10  |
|    value_loss           | 0.0584    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 600      |
|    iterations      | 1102     |
|    time_elapsed    | 7513     |
|    total_timesteps | 4513792  |
---------------------------------
Eval num_timesteps=4515000, episode_reward=0.40 +/- 0.49
Episode length: 304.80 +/- 44.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4515000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0338    |
|    n_updates            | 11020     |
|    policy_gradient_loss | 9.03e-10  |
|    value_loss           | 0.0713    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 600      |
|    iterations      | 1103     |
|    time_elapsed    | 7521     |
|    total_timesteps | 4517888  |
---------------------------------
Eval num_timesteps=4520000, episode_reward=0.90 +/- 0.83
Episode length: 308.40 +/- 61.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 4520000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0349    |
|    n_updates            | 11030     |
|    policy_gradient_loss | 1.42e-09  |
|    value_loss           | 0.0632    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 600      |
|    iterations      | 1104     |
|    time_elapsed    | 7530     |
|    total_timesteps | 4521984  |
---------------------------------
Eval num_timesteps=4525000, episode_reward=0.30 +/- 0.64
Episode length: 279.40 +/- 73.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 279       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4525000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.021     |
|    n_updates            | 11040     |
|    policy_gradient_loss | -5.13e-11 |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 600      |
|    iterations      | 1105     |
|    time_elapsed    | 7538     |
|    total_timesteps | 4526080  |
---------------------------------
Eval num_timesteps=4530000, episode_reward=0.40 +/- 0.49
Episode length: 271.60 +/- 50.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 272       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4530000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0375    |
|    n_updates            | 11050     |
|    policy_gradient_loss | -6.98e-10 |
|    value_loss           | 0.059     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 600      |
|    iterations      | 1106     |
|    time_elapsed    | 7547     |
|    total_timesteps | 4530176  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 311       |
|    ep_rew_mean          | 0.55      |
| time/                   |           |
|    fps                  | 600       |
|    iterations           | 1107      |
|    time_elapsed         | 7552      |
|    total_timesteps      | 4534272   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0238    |
|    n_updates            | 11060     |
|    policy_gradient_loss | -2.38e-10 |
|    value_loss           | 0.0641    |
---------------------------------------
Eval num_timesteps=4535000, episode_reward=0.40 +/- 0.49
Episode length: 295.00 +/- 28.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4535000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0432    |
|    n_updates            | 11070     |
|    policy_gradient_loss | 6.3e-10   |
|    value_loss           | 0.066     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.56     |
| time/              |          |
|    fps             | 600      |
|    iterations      | 1108     |
|    time_elapsed    | 7561     |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4540000, episode_reward=0.30 +/- 0.64
Episode length: 278.00 +/- 32.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4540000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0217    |
|    n_updates            | 11080     |
|    policy_gradient_loss | -7.12e-10 |
|    value_loss           | 0.066     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 600      |
|    iterations      | 1109     |
|    time_elapsed    | 7570     |
|    total_timesteps | 4542464  |
---------------------------------
Eval num_timesteps=4545000, episode_reward=0.50 +/- 0.50
Episode length: 302.40 +/- 61.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4545000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.048     |
|    n_updates            | 11090     |
|    policy_gradient_loss | 2.31e-10  |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1110     |
|    time_elapsed    | 7578     |
|    total_timesteps | 4546560  |
---------------------------------
Eval num_timesteps=4550000, episode_reward=0.40 +/- 0.80
Episode length: 309.80 +/- 40.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4550000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0262    |
|    n_updates            | 11100     |
|    policy_gradient_loss | -3.57e-10 |
|    value_loss           | 0.0666    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1111     |
|    time_elapsed    | 7587     |
|    total_timesteps | 4550656  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 310       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 599       |
|    iterations           | 1112      |
|    time_elapsed         | 7593      |
|    total_timesteps      | 4554752   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0396    |
|    n_updates            | 11110     |
|    policy_gradient_loss | -5.69e-10 |
|    value_loss           | 0.062     |
---------------------------------------
Eval num_timesteps=4555000, episode_reward=0.30 +/- 0.46
Episode length: 276.60 +/- 41.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 277       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4555000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0278    |
|    n_updates            | 11120     |
|    policy_gradient_loss | -4.01e-10 |
|    value_loss           | 0.0667    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1113     |
|    time_elapsed    | 7601     |
|    total_timesteps | 4558848  |
---------------------------------
Eval num_timesteps=4560000, episode_reward=0.60 +/- 0.66
Episode length: 289.20 +/- 68.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4560000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0311    |
|    n_updates            | 11130     |
|    policy_gradient_loss | -9.79e-10 |
|    value_loss           | 0.0603    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1114     |
|    time_elapsed    | 7609     |
|    total_timesteps | 4562944  |
---------------------------------
Eval num_timesteps=4565000, episode_reward=0.60 +/- 0.66
Episode length: 289.80 +/- 48.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4565000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0273    |
|    n_updates            | 11140     |
|    policy_gradient_loss | 7.28e-11  |
|    value_loss           | 0.059     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1115     |
|    time_elapsed    | 7618     |
|    total_timesteps | 4567040  |
---------------------------------
Eval num_timesteps=4570000, episode_reward=0.30 +/- 0.64
Episode length: 276.00 +/- 49.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 276       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4570000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0318    |
|    n_updates            | 11150     |
|    policy_gradient_loss | 3.86e-11  |
|    value_loss           | 0.0623    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1116     |
|    time_elapsed    | 7626     |
|    total_timesteps | 4571136  |
---------------------------------
Eval num_timesteps=4575000, episode_reward=0.40 +/- 0.66
Episode length: 267.40 +/- 57.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 267       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4575000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0129    |
|    n_updates            | 11160     |
|    policy_gradient_loss | 1.82e-12  |
|    value_loss           | 0.0562    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1117     |
|    time_elapsed    | 7634     |
|    total_timesteps | 4575232  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 296       |
|    ep_rew_mean          | 0.41      |
| time/                   |           |
|    fps                  | 599       |
|    iterations           | 1118      |
|    time_elapsed         | 7641      |
|    total_timesteps      | 4579328   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0191    |
|    n_updates            | 11170     |
|    policy_gradient_loss | 4.45e-10  |
|    value_loss           | 0.0575    |
---------------------------------------
Eval num_timesteps=4580000, episode_reward=0.60 +/- 0.66
Episode length: 285.20 +/- 52.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4580000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0167    |
|    n_updates            | 11180     |
|    policy_gradient_loss | 2.74e-10  |
|    value_loss           | 0.0623    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1119     |
|    time_elapsed    | 7649     |
|    total_timesteps | 4583424  |
---------------------------------
Eval num_timesteps=4585000, episode_reward=0.40 +/- 0.66
Episode length: 284.00 +/- 77.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 284       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4585000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0325    |
|    n_updates            | 11190     |
|    policy_gradient_loss | -4.51e-11 |
|    value_loss           | 0.0632    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 599      |
|    iterations      | 1120     |
|    time_elapsed    | 7657     |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4590000, episode_reward=0.30 +/- 0.46
Episode length: 287.00 +/- 39.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4590000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0473    |
|    n_updates            | 11200     |
|    policy_gradient_loss | 3.32e-10  |
|    value_loss           | 0.0592    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1121     |
|    time_elapsed    | 7666     |
|    total_timesteps | 4591616  |
---------------------------------
Eval num_timesteps=4595000, episode_reward=0.30 +/- 0.64
Episode length: 305.60 +/- 49.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4595000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0283    |
|    n_updates            | 11210     |
|    policy_gradient_loss | 6.84e-10  |
|    value_loss           | 0.0577    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1122     |
|    time_elapsed    | 7675     |
|    total_timesteps | 4595712  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 598       |
|    iterations           | 1123      |
|    time_elapsed         | 7681      |
|    total_timesteps      | 4599808   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0503    |
|    n_updates            | 11220     |
|    policy_gradient_loss | -3.52e-10 |
|    value_loss           | 0.0644    |
---------------------------------------
Eval num_timesteps=4600000, episode_reward=0.30 +/- 0.64
Episode length: 292.20 +/- 33.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4600000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0278    |
|    n_updates            | 11230     |
|    policy_gradient_loss | 8.4e-10   |
|    value_loss           | 0.0642    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1124     |
|    time_elapsed    | 7689     |
|    total_timesteps | 4603904  |
---------------------------------
Eval num_timesteps=4605000, episode_reward=0.30 +/- 0.46
Episode length: 306.00 +/- 23.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4605000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0251    |
|    n_updates            | 11240     |
|    policy_gradient_loss | -3.8e-10  |
|    value_loss           | 0.0567    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1125     |
|    time_elapsed    | 7698     |
|    total_timesteps | 4608000  |
---------------------------------
Eval num_timesteps=4610000, episode_reward=0.20 +/- 0.40
Episode length: 267.40 +/- 35.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 267       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4610000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0311    |
|    n_updates            | 11250     |
|    policy_gradient_loss | -1.21e-10 |
|    value_loss           | 0.0594    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1126     |
|    time_elapsed    | 7706     |
|    total_timesteps | 4612096  |
---------------------------------
Eval num_timesteps=4615000, episode_reward=0.50 +/- 0.67
Episode length: 305.80 +/- 35.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4615000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.038     |
|    n_updates            | 11260     |
|    policy_gradient_loss | 1.82e-10  |
|    value_loss           | 0.0543    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1127     |
|    time_elapsed    | 7715     |
|    total_timesteps | 4616192  |
---------------------------------
Eval num_timesteps=4620000, episode_reward=0.80 +/- 0.75
Episode length: 315.20 +/- 47.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 4620000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0628    |
|    n_updates            | 11270     |
|    policy_gradient_loss | 7.37e-10  |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1128     |
|    time_elapsed    | 7723     |
|    total_timesteps | 4620288  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 598       |
|    iterations           | 1129      |
|    time_elapsed         | 7729      |
|    total_timesteps      | 4624384   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.037     |
|    n_updates            | 11280     |
|    policy_gradient_loss | -9.87e-10 |
|    value_loss           | 0.0548    |
---------------------------------------
Eval num_timesteps=4625000, episode_reward=0.30 +/- 0.46
Episode length: 311.20 +/- 62.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4625000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0341    |
|    n_updates            | 11290     |
|    policy_gradient_loss | -7.11e-10 |
|    value_loss           | 0.0569    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 598      |
|    iterations      | 1130     |
|    time_elapsed    | 7738     |
|    total_timesteps | 4628480  |
---------------------------------
Eval num_timesteps=4630000, episode_reward=0.40 +/- 0.49
Episode length: 295.20 +/- 34.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4630000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0247    |
|    n_updates            | 11300     |
|    policy_gradient_loss | -3.13e-10 |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1131     |
|    time_elapsed    | 7746     |
|    total_timesteps | 4632576  |
---------------------------------
Eval num_timesteps=4635000, episode_reward=0.20 +/- 0.40
Episode length: 299.80 +/- 47.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4635000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0342    |
|    n_updates            | 11310     |
|    policy_gradient_loss | -2.99e-10 |
|    value_loss           | 0.0623    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1132     |
|    time_elapsed    | 7755     |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4640000, episode_reward=0.40 +/- 0.49
Episode length: 299.80 +/- 43.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4640000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0342    |
|    n_updates            | 11320     |
|    policy_gradient_loss | -2.22e-10 |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1133     |
|    time_elapsed    | 7763     |
|    total_timesteps | 4640768  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 296       |
|    ep_rew_mean          | 0.41      |
| time/                   |           |
|    fps                  | 597       |
|    iterations           | 1134      |
|    time_elapsed         | 7770      |
|    total_timesteps      | 4644864   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0444    |
|    n_updates            | 11330     |
|    policy_gradient_loss | 7.14e-10  |
|    value_loss           | 0.0559    |
---------------------------------------
Eval num_timesteps=4645000, episode_reward=0.40 +/- 0.49
Episode length: 298.20 +/- 75.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4645000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0315    |
|    n_updates            | 11340     |
|    policy_gradient_loss | -4.63e-10 |
|    value_loss           | 0.0583    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1135     |
|    time_elapsed    | 7778     |
|    total_timesteps | 4648960  |
---------------------------------
Eval num_timesteps=4650000, episode_reward=0.20 +/- 0.40
Episode length: 281.40 +/- 44.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4650000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0282    |
|    n_updates            | 11350     |
|    policy_gradient_loss | -1.3e-09  |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1136     |
|    time_elapsed    | 7787     |
|    total_timesteps | 4653056  |
---------------------------------
Eval num_timesteps=4655000, episode_reward=0.20 +/- 0.40
Episode length: 267.80 +/- 52.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 268       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4655000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.019     |
|    n_updates            | 11360     |
|    policy_gradient_loss | -5.02e-10 |
|    value_loss           | 0.0618    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1137     |
|    time_elapsed    | 7795     |
|    total_timesteps | 4657152  |
---------------------------------
Eval num_timesteps=4660000, episode_reward=0.40 +/- 0.49
Episode length: 322.00 +/- 51.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 322       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4660000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0358    |
|    n_updates            | 11370     |
|    policy_gradient_loss | -3.37e-10 |
|    value_loss           | 0.0531    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1138     |
|    time_elapsed    | 7803     |
|    total_timesteps | 4661248  |
---------------------------------
Eval num_timesteps=4665000, episode_reward=0.00 +/- 0.00
Episode length: 273.20 +/- 23.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 273       |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 4665000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0329    |
|    n_updates            | 11380     |
|    policy_gradient_loss | -1.07e-10 |
|    value_loss           | 0.0589    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1139     |
|    time_elapsed    | 7812     |
|    total_timesteps | 4665344  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 293       |
|    ep_rew_mean          | 0.35      |
| time/                   |           |
|    fps                  | 597       |
|    iterations           | 1140      |
|    time_elapsed         | 7818      |
|    total_timesteps      | 4669440   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.024     |
|    n_updates            | 11390     |
|    policy_gradient_loss | 5.81e-10  |
|    value_loss           | 0.0558    |
---------------------------------------
Eval num_timesteps=4670000, episode_reward=0.40 +/- 0.49
Episode length: 315.40 +/- 38.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4670000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0331    |
|    n_updates            | 11400     |
|    policy_gradient_loss | -1.84e-10 |
|    value_loss           | 0.0544    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 597      |
|    iterations      | 1141     |
|    time_elapsed    | 7827     |
|    total_timesteps | 4673536  |
---------------------------------
Eval num_timesteps=4675000, episode_reward=0.30 +/- 0.64
Episode length: 277.20 +/- 44.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 277       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4675000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0129    |
|    n_updates            | 11410     |
|    policy_gradient_loss | 3.73e-10  |
|    value_loss           | 0.0569    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1142     |
|    time_elapsed    | 7835     |
|    total_timesteps | 4677632  |
---------------------------------
Eval num_timesteps=4680000, episode_reward=0.20 +/- 0.40
Episode length: 306.60 +/- 43.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4680000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0157    |
|    n_updates            | 11420     |
|    policy_gradient_loss | -6.79e-10 |
|    value_loss           | 0.0612    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1143     |
|    time_elapsed    | 7844     |
|    total_timesteps | 4681728  |
---------------------------------
Eval num_timesteps=4685000, episode_reward=0.90 +/- 0.83
Episode length: 343.80 +/- 50.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 344       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 4685000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000164 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.014     |
|    n_updates            | 11430     |
|    policy_gradient_loss | 2.52e-10  |
|    value_loss           | 0.0568    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.38     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1144     |
|    time_elapsed    | 7852     |
|    total_timesteps | 4685824  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 301         |
|    ep_rew_mean          | 0.44        |
| time/                   |             |
|    fps                  | 596         |
|    iterations           | 1145        |
|    time_elapsed         | 7858        |
|    total_timesteps      | 4689920     |
| train/                  |             |
|    approx_kl            | 5.20944e-07 |
|    clip_fraction        | 0.000146    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000187   |
|    explained_variance   | 0           |
|    learning_rate        | 0.01        |
|    loss                 | 0.0242      |
|    n_updates            | 11440       |
|    policy_gradient_loss | 1.81e-06    |
|    value_loss           | 0.0527      |
-----------------------------------------
Eval num_timesteps=4690000, episode_reward=0.50 +/- 0.67
Episode length: 301.00 +/- 32.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4690000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0172    |
|    n_updates            | 11450     |
|    policy_gradient_loss | 4.54e-10  |
|    value_loss           | 0.0618    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1146     |
|    time_elapsed    | 7867     |
|    total_timesteps | 4694016  |
---------------------------------
Eval num_timesteps=4695000, episode_reward=0.50 +/- 0.50
Episode length: 307.60 +/- 29.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4695000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0246    |
|    n_updates            | 11460     |
|    policy_gradient_loss | -1.19e-09 |
|    value_loss           | 0.0576    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1147     |
|    time_elapsed    | 7876     |
|    total_timesteps | 4698112  |
---------------------------------
Eval num_timesteps=4700000, episode_reward=0.50 +/- 0.81
Episode length: 283.20 +/- 41.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4700000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0441    |
|    n_updates            | 11470     |
|    policy_gradient_loss | 1.3e-10   |
|    value_loss           | 0.0656    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1148     |
|    time_elapsed    | 7884     |
|    total_timesteps | 4702208  |
---------------------------------
Eval num_timesteps=4705000, episode_reward=0.20 +/- 0.40
Episode length: 287.40 +/- 40.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4705000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0369    |
|    n_updates            | 11480     |
|    policy_gradient_loss | -1.09e-09 |
|    value_loss           | 0.0584    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1149     |
|    time_elapsed    | 7892     |
|    total_timesteps | 4706304  |
---------------------------------
Eval num_timesteps=4710000, episode_reward=0.20 +/- 0.40
Episode length: 282.80 +/- 43.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4710000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0241    |
|    n_updates            | 11490     |
|    policy_gradient_loss | -5.2e-10  |
|    value_loss           | 0.0619    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1150     |
|    time_elapsed    | 7900     |
|    total_timesteps | 4710400  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 299       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 596       |
|    iterations           | 1151      |
|    time_elapsed         | 7907      |
|    total_timesteps      | 4714496   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0205    |
|    n_updates            | 11500     |
|    policy_gradient_loss | 2.76e-10  |
|    value_loss           | 0.0606    |
---------------------------------------
Eval num_timesteps=4715000, episode_reward=0.30 +/- 0.64
Episode length: 288.20 +/- 44.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4715000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0302    |
|    n_updates            | 11510     |
|    policy_gradient_loss | 6.55e-11  |
|    value_loss           | 0.0597    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 596      |
|    iterations      | 1152     |
|    time_elapsed    | 7915     |
|    total_timesteps | 4718592  |
---------------------------------
Eval num_timesteps=4720000, episode_reward=0.20 +/- 0.40
Episode length: 306.20 +/- 42.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4720000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0236    |
|    n_updates            | 11520     |
|    policy_gradient_loss | -1.25e-09 |
|    value_loss           | 0.0636    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1153     |
|    time_elapsed    | 7924     |
|    total_timesteps | 4722688  |
---------------------------------
Eval num_timesteps=4725000, episode_reward=0.30 +/- 0.46
Episode length: 327.00 +/- 26.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 327       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4725000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0431    |
|    n_updates            | 11530     |
|    policy_gradient_loss | -2.74e-10 |
|    value_loss           | 0.0647    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1154     |
|    time_elapsed    | 7932     |
|    total_timesteps | 4726784  |
---------------------------------
Eval num_timesteps=4730000, episode_reward=0.10 +/- 0.30
Episode length: 282.60 +/- 47.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 4730000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0342    |
|    n_updates            | 11540     |
|    policy_gradient_loss | -3.6e-11  |
|    value_loss           | 0.0601    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1155     |
|    time_elapsed    | 7941     |
|    total_timesteps | 4730880  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 308       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 595       |
|    iterations           | 1156      |
|    time_elapsed         | 7947      |
|    total_timesteps      | 4734976   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0303    |
|    n_updates            | 11550     |
|    policy_gradient_loss | 4.66e-10  |
|    value_loss           | 0.0593    |
---------------------------------------
Eval num_timesteps=4735000, episode_reward=0.80 +/- 0.87
Episode length: 312.80 +/- 58.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 4735000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0272    |
|    n_updates            | 11560     |
|    policy_gradient_loss | 1.56e-10  |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1157     |
|    time_elapsed    | 7956     |
|    total_timesteps | 4739072  |
---------------------------------
Eval num_timesteps=4740000, episode_reward=0.60 +/- 0.92
Episode length: 311.80 +/- 67.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4740000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0269    |
|    n_updates            | 11570     |
|    policy_gradient_loss | -1.67e-10 |
|    value_loss           | 0.0658    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1158     |
|    time_elapsed    | 7964     |
|    total_timesteps | 4743168  |
---------------------------------
Eval num_timesteps=4745000, episode_reward=0.40 +/- 0.49
Episode length: 292.00 +/- 39.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4745000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0295    |
|    n_updates            | 11580     |
|    policy_gradient_loss | 5.71e-10  |
|    value_loss           | 0.0575    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1159     |
|    time_elapsed    | 7973     |
|    total_timesteps | 4747264  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=0.70 +/- 0.90
Episode length: 301.20 +/- 56.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4750000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.023     |
|    n_updates            | 11590     |
|    policy_gradient_loss | 1.55e-10  |
|    value_loss           | 0.0622    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1160     |
|    time_elapsed    | 7982     |
|    total_timesteps | 4751360  |
---------------------------------
Eval num_timesteps=4755000, episode_reward=0.30 +/- 0.64
Episode length: 315.60 +/- 75.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4755000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0253    |
|    n_updates            | 11600     |
|    policy_gradient_loss | 2.4e-10   |
|    value_loss           | 0.0691    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1161     |
|    time_elapsed    | 7990     |
|    total_timesteps | 4755456  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.5       |
| time/                   |           |
|    fps                  | 595       |
|    iterations           | 1162      |
|    time_elapsed         | 7996      |
|    total_timesteps      | 4759552   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0289    |
|    n_updates            | 11610     |
|    policy_gradient_loss | -5.6e-10  |
|    value_loss           | 0.0592    |
---------------------------------------
Eval num_timesteps=4760000, episode_reward=0.50 +/- 0.50
Episode length: 300.00 +/- 43.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4760000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0527    |
|    n_updates            | 11620     |
|    policy_gradient_loss | 2.71e-10  |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 595      |
|    iterations      | 1163     |
|    time_elapsed    | 8005     |
|    total_timesteps | 4763648  |
---------------------------------
Eval num_timesteps=4765000, episode_reward=0.60 +/- 0.49
Episode length: 305.20 +/- 51.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4765000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0291    |
|    n_updates            | 11630     |
|    policy_gradient_loss | -4.77e-10 |
|    value_loss           | 0.0581    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 1164     |
|    time_elapsed    | 8014     |
|    total_timesteps | 4767744  |
---------------------------------
Eval num_timesteps=4770000, episode_reward=0.30 +/- 0.46
Episode length: 301.40 +/- 35.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4770000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0336    |
|    n_updates            | 11640     |
|    policy_gradient_loss | -8.28e-10 |
|    value_loss           | 0.0566    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 1165     |
|    time_elapsed    | 8022     |
|    total_timesteps | 4771840  |
---------------------------------
Eval num_timesteps=4775000, episode_reward=0.60 +/- 0.92
Episode length: 294.20 +/- 55.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4775000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.038     |
|    n_updates            | 11650     |
|    policy_gradient_loss | -1.63e-10 |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 1166     |
|    time_elapsed    | 8031     |
|    total_timesteps | 4775936  |
---------------------------------
Eval num_timesteps=4780000, episode_reward=0.10 +/- 0.30
Episode length: 283.00 +/- 60.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 4780000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0157    |
|    n_updates            | 11660     |
|    policy_gradient_loss | -1.8e-10  |
|    value_loss           | 0.0687    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 1167     |
|    time_elapsed    | 8039     |
|    total_timesteps | 4780032  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 293       |
|    ep_rew_mean          | 0.37      |
| time/                   |           |
|    fps                  | 594       |
|    iterations           | 1168      |
|    time_elapsed         | 8045      |
|    total_timesteps      | 4784128   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0322    |
|    n_updates            | 11670     |
|    policy_gradient_loss | 8.48e-10  |
|    value_loss           | 0.0551    |
---------------------------------------
Eval num_timesteps=4785000, episode_reward=0.70 +/- 0.78
Episode length: 329.60 +/- 64.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 330       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4785000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0351    |
|    n_updates            | 11680     |
|    policy_gradient_loss | 6.38e-10  |
|    value_loss           | 0.0587    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 1169     |
|    time_elapsed    | 8054     |
|    total_timesteps | 4788224  |
---------------------------------
Eval num_timesteps=4790000, episode_reward=0.80 +/- 0.75
Episode length: 307.20 +/- 59.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 4790000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0229    |
|    n_updates            | 11690     |
|    policy_gradient_loss | -6.89e-10 |
|    value_loss           | 0.0535    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 1170     |
|    time_elapsed    | 8062     |
|    total_timesteps | 4792320  |
---------------------------------
Eval num_timesteps=4795000, episode_reward=0.50 +/- 0.67
Episode length: 303.40 +/- 38.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4795000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0244    |
|    n_updates            | 11700     |
|    policy_gradient_loss | -6.16e-10 |
|    value_loss           | 0.0545    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 1171     |
|    time_elapsed    | 8072     |
|    total_timesteps | 4796416  |
---------------------------------
Eval num_timesteps=4800000, episode_reward=0.50 +/- 0.50
Episode length: 306.80 +/- 31.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4800000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0244    |
|    n_updates            | 11710     |
|    policy_gradient_loss | -2.48e-10 |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 593      |
|    iterations      | 1172     |
|    time_elapsed    | 8084     |
|    total_timesteps | 4800512  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 296       |
|    ep_rew_mean          | 0.35      |
| time/                   |           |
|    fps                  | 593       |
|    iterations           | 1173      |
|    time_elapsed         | 8092      |
|    total_timesteps      | 4804608   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0169    |
|    n_updates            | 11720     |
|    policy_gradient_loss | -3.1e-10  |
|    value_loss           | 0.0579    |
---------------------------------------
Eval num_timesteps=4805000, episode_reward=0.60 +/- 0.66
Episode length: 342.80 +/- 38.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 343       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4805000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0251    |
|    n_updates            | 11730     |
|    policy_gradient_loss | 4.04e-10  |
|    value_loss           | 0.0534    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 593      |
|    iterations      | 1174     |
|    time_elapsed    | 8103     |
|    total_timesteps | 4808704  |
---------------------------------
Eval num_timesteps=4810000, episode_reward=0.40 +/- 0.66
Episode length: 289.00 +/- 64.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4810000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0248    |
|    n_updates            | 11740     |
|    policy_gradient_loss | -1.42e-10 |
|    value_loss           | 0.0613    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 593      |
|    iterations      | 1175     |
|    time_elapsed    | 8114     |
|    total_timesteps | 4812800  |
---------------------------------
Eval num_timesteps=4815000, episode_reward=0.50 +/- 0.50
Episode length: 316.80 +/- 49.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4815000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000161 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0354    |
|    n_updates            | 11750     |
|    policy_gradient_loss | -7.64e-10 |
|    value_loss           | 0.0589    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 592      |
|    iterations      | 1176     |
|    time_elapsed    | 8125     |
|    total_timesteps | 4816896  |
---------------------------------
Eval num_timesteps=4820000, episode_reward=0.60 +/- 0.66
Episode length: 310.00 +/- 53.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 310           |
|    mean_reward          | 0.6           |
| time/                   |               |
|    total_timesteps      | 4820000       |
| train/                  |               |
|    approx_kl            | 2.0285588e-05 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000125     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0367        |
|    n_updates            | 11760         |
|    policy_gradient_loss | 2.75e-06      |
|    value_loss           | 0.0618        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 592      |
|    iterations      | 1177     |
|    time_elapsed    | 8137     |
|    total_timesteps | 4820992  |
---------------------------------
Eval num_timesteps=4825000, episode_reward=0.40 +/- 0.49
Episode length: 291.60 +/- 46.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4825000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0297    |
|    n_updates            | 11770     |
|    policy_gradient_loss | -4.95e-10 |
|    value_loss           | 0.0676    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 592      |
|    iterations      | 1178     |
|    time_elapsed    | 8148     |
|    total_timesteps | 4825088  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 295       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 592       |
|    iterations           | 1179      |
|    time_elapsed         | 8156      |
|    total_timesteps      | 4829184   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0341    |
|    n_updates            | 11780     |
|    policy_gradient_loss | 3.07e-10  |
|    value_loss           | 0.0626    |
---------------------------------------
Eval num_timesteps=4830000, episode_reward=0.50 +/- 0.50
Episode length: 301.60 +/- 61.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4830000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0132    |
|    n_updates            | 11790     |
|    policy_gradient_loss | -4e-10    |
|    value_loss           | 0.0564    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 591      |
|    iterations      | 1180     |
|    time_elapsed    | 8168     |
|    total_timesteps | 4833280  |
---------------------------------
Eval num_timesteps=4835000, episode_reward=0.50 +/- 0.67
Episode length: 287.80 +/- 39.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4835000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.024     |
|    n_updates            | 11800     |
|    policy_gradient_loss | -3.86e-11 |
|    value_loss           | 0.0647    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 591      |
|    iterations      | 1181     |
|    time_elapsed    | 8178     |
|    total_timesteps | 4837376  |
---------------------------------
Eval num_timesteps=4840000, episode_reward=0.30 +/- 0.46
Episode length: 294.20 +/- 28.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4840000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0224    |
|    n_updates            | 11810     |
|    policy_gradient_loss | -6.47e-10 |
|    value_loss           | 0.0639    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 591      |
|    iterations      | 1182     |
|    time_elapsed    | 8189     |
|    total_timesteps | 4841472  |
---------------------------------
Eval num_timesteps=4845000, episode_reward=0.30 +/- 0.64
Episode length: 272.80 +/- 36.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 273       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4845000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0113    |
|    n_updates            | 11820     |
|    policy_gradient_loss | -4.12e-10 |
|    value_loss           | 0.0605    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 590      |
|    iterations      | 1183     |
|    time_elapsed    | 8200     |
|    total_timesteps | 4845568  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 307       |
|    ep_rew_mean          | 0.56      |
| time/                   |           |
|    fps                  | 590       |
|    iterations           | 1184      |
|    time_elapsed         | 8208      |
|    total_timesteps      | 4849664   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0434    |
|    n_updates            | 11830     |
|    policy_gradient_loss | 1.31e-11  |
|    value_loss           | 0.063     |
---------------------------------------
Eval num_timesteps=4850000, episode_reward=0.00 +/- 0.00
Episode length: 267.20 +/- 21.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 267       |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 4850000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0381    |
|    n_updates            | 11840     |
|    policy_gradient_loss | 3.85e-10  |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 590      |
|    iterations      | 1185     |
|    time_elapsed    | 8219     |
|    total_timesteps | 4853760  |
---------------------------------
Eval num_timesteps=4855000, episode_reward=0.50 +/- 0.67
Episode length: 290.40 +/- 51.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4855000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0389    |
|    n_updates            | 11850     |
|    policy_gradient_loss | 2.89e-10  |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 590      |
|    iterations      | 1186     |
|    time_elapsed    | 8230     |
|    total_timesteps | 4857856  |
---------------------------------
Eval num_timesteps=4860000, episode_reward=0.40 +/- 0.49
Episode length: 303.80 +/- 52.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4860000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0125    |
|    n_updates            | 11860     |
|    policy_gradient_loss | 2.68e-10  |
|    value_loss           | 0.0536    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 589      |
|    iterations      | 1187     |
|    time_elapsed    | 8241     |
|    total_timesteps | 4861952  |
---------------------------------
Eval num_timesteps=4865000, episode_reward=0.60 +/- 0.49
Episode length: 294.80 +/- 32.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4865000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0222    |
|    n_updates            | 11870     |
|    policy_gradient_loss | 8.37e-10  |
|    value_loss           | 0.0624    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 589      |
|    iterations      | 1188     |
|    time_elapsed    | 8252     |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4870000, episode_reward=0.60 +/- 0.92
Episode length: 296.00 +/- 71.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4870000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0237    |
|    n_updates            | 11880     |
|    policy_gradient_loss | -3.27e-10 |
|    value_loss           | 0.0666    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 589      |
|    iterations      | 1189     |
|    time_elapsed    | 8263     |
|    total_timesteps | 4870144  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 589       |
|    iterations           | 1190      |
|    time_elapsed         | 8272      |
|    total_timesteps      | 4874240   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.02      |
|    n_updates            | 11890     |
|    policy_gradient_loss | 5e-10     |
|    value_loss           | 0.0665    |
---------------------------------------
Eval num_timesteps=4875000, episode_reward=0.40 +/- 0.49
Episode length: 305.60 +/- 40.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4875000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0351    |
|    n_updates            | 11900     |
|    policy_gradient_loss | 1.84e-10  |
|    value_loss           | 0.0588    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 588      |
|    iterations      | 1191     |
|    time_elapsed    | 8283     |
|    total_timesteps | 4878336  |
---------------------------------
Eval num_timesteps=4880000, episode_reward=0.20 +/- 0.40
Episode length: 282.60 +/- 37.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4880000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0161    |
|    n_updates            | 11910     |
|    policy_gradient_loss | -4.93e-10 |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 588      |
|    iterations      | 1192     |
|    time_elapsed    | 8293     |
|    total_timesteps | 4882432  |
---------------------------------
Eval num_timesteps=4885000, episode_reward=0.70 +/- 0.90
Episode length: 307.20 +/- 60.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4885000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0241    |
|    n_updates            | 11920     |
|    policy_gradient_loss | -2.4e-10  |
|    value_loss           | 0.0573    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 588      |
|    iterations      | 1193     |
|    time_elapsed    | 8305     |
|    total_timesteps | 4886528  |
---------------------------------
Eval num_timesteps=4890000, episode_reward=0.50 +/- 0.92
Episode length: 307.40 +/- 64.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4890000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0245    |
|    n_updates            | 11930     |
|    policy_gradient_loss | 9.42e-10  |
|    value_loss           | 0.0565    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 588      |
|    iterations      | 1194     |
|    time_elapsed    | 8316     |
|    total_timesteps | 4890624  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 306       |
|    ep_rew_mean          | 0.49      |
| time/                   |           |
|    fps                  | 587       |
|    iterations           | 1195      |
|    time_elapsed         | 8324      |
|    total_timesteps      | 4894720   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0481    |
|    n_updates            | 11940     |
|    policy_gradient_loss | 2.26e-11  |
|    value_loss           | 0.0601    |
---------------------------------------
Eval num_timesteps=4895000, episode_reward=0.60 +/- 0.66
Episode length: 297.80 +/- 68.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4895000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.032     |
|    n_updates            | 11950     |
|    policy_gradient_loss | 1.08e-09  |
|    value_loss           | 0.0624    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 587      |
|    iterations      | 1196     |
|    time_elapsed    | 8335     |
|    total_timesteps | 4898816  |
---------------------------------
Eval num_timesteps=4900000, episode_reward=0.10 +/- 0.30
Episode length: 293.20 +/- 49.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 4900000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0264    |
|    n_updates            | 11960     |
|    policy_gradient_loss | 2.18e-10  |
|    value_loss           | 0.0613    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 587      |
|    iterations      | 1197     |
|    time_elapsed    | 8347     |
|    total_timesteps | 4902912  |
---------------------------------
Eval num_timesteps=4905000, episode_reward=0.60 +/- 0.49
Episode length: 308.80 +/- 64.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 4905000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0339    |
|    n_updates            | 11970     |
|    policy_gradient_loss | -1.18e-11 |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 587      |
|    iterations      | 1198     |
|    time_elapsed    | 8358     |
|    total_timesteps | 4907008  |
---------------------------------
Eval num_timesteps=4910000, episode_reward=0.40 +/- 0.49
Episode length: 307.40 +/- 32.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4910000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0311    |
|    n_updates            | 11980     |
|    policy_gradient_loss | 6.71e-10  |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 586      |
|    iterations      | 1199     |
|    time_elapsed    | 8368     |
|    total_timesteps | 4911104  |
---------------------------------
Eval num_timesteps=4915000, episode_reward=0.50 +/- 0.67
Episode length: 290.80 +/- 57.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 291       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4915000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.04      |
|    n_updates            | 11990     |
|    policy_gradient_loss | -7.02e-10 |
|    value_loss           | 0.0625    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 586      |
|    iterations      | 1200     |
|    time_elapsed    | 8379     |
|    total_timesteps | 4915200  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 586       |
|    iterations           | 1201      |
|    time_elapsed         | 8388      |
|    total_timesteps      | 4919296   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0294    |
|    n_updates            | 12000     |
|    policy_gradient_loss | 6.7e-10   |
|    value_loss           | 0.0611    |
---------------------------------------
Eval num_timesteps=4920000, episode_reward=0.10 +/- 0.30
Episode length: 303.40 +/- 61.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 4920000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0127    |
|    n_updates            | 12010     |
|    policy_gradient_loss | 5.47e-10  |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 586      |
|    iterations      | 1202     |
|    time_elapsed    | 8399     |
|    total_timesteps | 4923392  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=0.30 +/- 0.46
Episode length: 301.20 +/- 50.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4925000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0397    |
|    n_updates            | 12020     |
|    policy_gradient_loss | 1.12e-10  |
|    value_loss           | 0.0601    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 585      |
|    iterations      | 1203     |
|    time_elapsed    | 8410     |
|    total_timesteps | 4927488  |
---------------------------------
Eval num_timesteps=4930000, episode_reward=0.30 +/- 0.46
Episode length: 314.80 +/- 57.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4930000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00671   |
|    n_updates            | 12030     |
|    policy_gradient_loss | -2.44e-10 |
|    value_loss           | 0.0665    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 585      |
|    iterations      | 1204     |
|    time_elapsed    | 8422     |
|    total_timesteps | 4931584  |
---------------------------------
Eval num_timesteps=4935000, episode_reward=0.40 +/- 0.66
Episode length: 309.00 +/- 41.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4935000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0343    |
|    n_updates            | 12040     |
|    policy_gradient_loss | -8.22e-10 |
|    value_loss           | 0.0557    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 585      |
|    iterations      | 1205     |
|    time_elapsed    | 8433     |
|    total_timesteps | 4935680  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 296       |
|    ep_rew_mean          | 0.45      |
| time/                   |           |
|    fps                  | 585       |
|    iterations           | 1206      |
|    time_elapsed         | 8441      |
|    total_timesteps      | 4939776   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0696    |
|    n_updates            | 12050     |
|    policy_gradient_loss | -2.3e-10  |
|    value_loss           | 0.0597    |
---------------------------------------
Eval num_timesteps=4940000, episode_reward=0.30 +/- 0.64
Episode length: 296.20 +/- 53.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4940000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0246    |
|    n_updates            | 12060     |
|    policy_gradient_loss | 5.81e-10  |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 584      |
|    iterations      | 1207     |
|    time_elapsed    | 8452     |
|    total_timesteps | 4943872  |
---------------------------------
Eval num_timesteps=4945000, episode_reward=0.20 +/- 0.40
Episode length: 315.40 +/- 47.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4945000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0218    |
|    n_updates            | 12070     |
|    policy_gradient_loss | -1.24e-09 |
|    value_loss           | 0.0567    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 584      |
|    iterations      | 1208     |
|    time_elapsed    | 8463     |
|    total_timesteps | 4947968  |
---------------------------------
Eval num_timesteps=4950000, episode_reward=0.50 +/- 0.67
Episode length: 325.60 +/- 64.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 326       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4950000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0328    |
|    n_updates            | 12080     |
|    policy_gradient_loss | -9.45e-10 |
|    value_loss           | 0.0547    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 584      |
|    iterations      | 1209     |
|    time_elapsed    | 8475     |
|    total_timesteps | 4952064  |
---------------------------------
Eval num_timesteps=4955000, episode_reward=0.20 +/- 0.40
Episode length: 291.80 +/- 55.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 4955000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0333    |
|    n_updates            | 12090     |
|    policy_gradient_loss | 1.94e-10  |
|    value_loss           | 0.0735    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 584      |
|    iterations      | 1210     |
|    time_elapsed    | 8486     |
|    total_timesteps | 4956160  |
---------------------------------
Eval num_timesteps=4960000, episode_reward=0.40 +/- 0.66
Episode length: 296.60 +/- 40.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4960000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.022     |
|    n_updates            | 12100     |
|    policy_gradient_loss | 2.43e-10  |
|    value_loss           | 0.0665    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 583      |
|    iterations      | 1211     |
|    time_elapsed    | 8497     |
|    total_timesteps | 4960256  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 297       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 583       |
|    iterations           | 1212      |
|    time_elapsed         | 8505      |
|    total_timesteps      | 4964352   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0286    |
|    n_updates            | 12110     |
|    policy_gradient_loss | 5.6e-10   |
|    value_loss           | 0.0604    |
---------------------------------------
Eval num_timesteps=4965000, episode_reward=0.30 +/- 0.46
Episode length: 288.60 +/- 50.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 4965000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0107    |
|    n_updates            | 12120     |
|    policy_gradient_loss | -1.04e-09 |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 583      |
|    iterations      | 1213     |
|    time_elapsed    | 8516     |
|    total_timesteps | 4968448  |
---------------------------------
Eval num_timesteps=4970000, episode_reward=0.70 +/- 0.64
Episode length: 307.60 +/- 49.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4970000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0194    |
|    n_updates            | 12130     |
|    policy_gradient_loss | 1.85e-10  |
|    value_loss           | 0.0619    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 583      |
|    iterations      | 1214     |
|    time_elapsed    | 8527     |
|    total_timesteps | 4972544  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=0.40 +/- 0.49
Episode length: 298.00 +/- 44.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4975000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0452    |
|    n_updates            | 12140     |
|    policy_gradient_loss | -3.33e-10 |
|    value_loss           | 0.0562    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 582      |
|    iterations      | 1215     |
|    time_elapsed    | 8538     |
|    total_timesteps | 4976640  |
---------------------------------
Eval num_timesteps=4980000, episode_reward=0.70 +/- 1.00
Episode length: 313.00 +/- 71.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 4980000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0387    |
|    n_updates            | 12150     |
|    policy_gradient_loss | 5.58e-10  |
|    value_loss           | 0.064     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 582      |
|    iterations      | 1216     |
|    time_elapsed    | 8550     |
|    total_timesteps | 4980736  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.5       |
| time/                   |           |
|    fps                  | 582       |
|    iterations           | 1217      |
|    time_elapsed         | 8558      |
|    total_timesteps      | 4984832   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0187    |
|    n_updates            | 12160     |
|    policy_gradient_loss | -1.65e-10 |
|    value_loss           | 0.0625    |
---------------------------------------
Eval num_timesteps=4985000, episode_reward=0.50 +/- 0.67
Episode length: 292.60 +/- 38.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 4985000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0246    |
|    n_updates            | 12170     |
|    policy_gradient_loss | 3.7e-10   |
|    value_loss           | 0.0634    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 582      |
|    iterations      | 1218     |
|    time_elapsed    | 8569     |
|    total_timesteps | 4988928  |
---------------------------------
Eval num_timesteps=4990000, episode_reward=0.40 +/- 0.49
Episode length: 284.20 +/- 35.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 284       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4990000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0242    |
|    n_updates            | 12180     |
|    policy_gradient_loss | -4.37e-11 |
|    value_loss           | 0.0623    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 581      |
|    iterations      | 1219     |
|    time_elapsed    | 8580     |
|    total_timesteps | 4993024  |
---------------------------------
Eval num_timesteps=4995000, episode_reward=0.40 +/- 0.49
Episode length: 307.40 +/- 53.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 4995000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0297    |
|    n_updates            | 12190     |
|    policy_gradient_loss | -1.23e-10 |
|    value_loss           | 0.0657    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 581      |
|    iterations      | 1220     |
|    time_elapsed    | 8591     |
|    total_timesteps | 4997120  |
---------------------------------
Eval num_timesteps=5000000, episode_reward=0.60 +/- 0.66
Episode length: 298.40 +/- 47.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5000000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.018     |
|    n_updates            | 12200     |
|    policy_gradient_loss | -2.35e-10 |
|    value_loss           | 0.0613    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 581      |
|    iterations      | 1221     |
|    time_elapsed    | 8602     |
|    total_timesteps | 5001216  |
---------------------------------
Eval num_timesteps=5005000, episode_reward=0.50 +/- 0.81
Episode length: 331.80 +/- 41.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 332       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5005000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0325    |
|    n_updates            | 12210     |
|    policy_gradient_loss | 1.44e-09  |
|    value_loss           | 0.0622    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 581      |
|    iterations      | 1222     |
|    time_elapsed    | 8614     |
|    total_timesteps | 5005312  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 294       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 580       |
|    iterations           | 1223      |
|    time_elapsed         | 8622      |
|    total_timesteps      | 5009408   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0447    |
|    n_updates            | 12220     |
|    policy_gradient_loss | 1.06e-10  |
|    value_loss           | 0.0615    |
---------------------------------------
Eval num_timesteps=5010000, episode_reward=0.80 +/- 0.75
Episode length: 305.60 +/- 38.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5010000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0141    |
|    n_updates            | 12230     |
|    policy_gradient_loss | -9.67e-10 |
|    value_loss           | 0.0614    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 580      |
|    iterations      | 1224     |
|    time_elapsed    | 8633     |
|    total_timesteps | 5013504  |
---------------------------------
Eval num_timesteps=5015000, episode_reward=0.40 +/- 0.66
Episode length: 308.60 +/- 60.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5015000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0291    |
|    n_updates            | 12240     |
|    policy_gradient_loss | 4.79e-10  |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 580      |
|    iterations      | 1225     |
|    time_elapsed    | 8644     |
|    total_timesteps | 5017600  |
---------------------------------
Eval num_timesteps=5020000, episode_reward=0.80 +/- 1.08
Episode length: 313.20 +/- 59.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5020000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0365    |
|    n_updates            | 12250     |
|    policy_gradient_loss | -3.58e-10 |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 580      |
|    iterations      | 1226     |
|    time_elapsed    | 8656     |
|    total_timesteps | 5021696  |
---------------------------------
Eval num_timesteps=5025000, episode_reward=0.30 +/- 0.46
Episode length: 300.80 +/- 48.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5025000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0176    |
|    n_updates            | 12260     |
|    policy_gradient_loss | -1.45e-10 |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 579      |
|    iterations      | 1227     |
|    time_elapsed    | 8667     |
|    total_timesteps | 5025792  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.45      |
| time/                   |           |
|    fps                  | 579       |
|    iterations           | 1228      |
|    time_elapsed         | 8675      |
|    total_timesteps      | 5029888   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0404    |
|    n_updates            | 12270     |
|    policy_gradient_loss | -1.46e-10 |
|    value_loss           | 0.0642    |
---------------------------------------
Eval num_timesteps=5030000, episode_reward=0.80 +/- 0.98
Episode length: 316.00 +/- 56.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5030000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0377    |
|    n_updates            | 12280     |
|    policy_gradient_loss | 9e-10     |
|    value_loss           | 0.0578    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 579      |
|    iterations      | 1229     |
|    time_elapsed    | 8686     |
|    total_timesteps | 5033984  |
---------------------------------
Eval num_timesteps=5035000, episode_reward=0.70 +/- 0.78
Episode length: 308.20 +/- 67.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5035000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.011     |
|    n_updates            | 12290     |
|    policy_gradient_loss | -3.03e-10 |
|    value_loss           | 0.0644    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 579      |
|    iterations      | 1230     |
|    time_elapsed    | 8697     |
|    total_timesteps | 5038080  |
---------------------------------
Eval num_timesteps=5040000, episode_reward=0.40 +/- 0.66
Episode length: 307.40 +/- 57.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5040000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0246    |
|    n_updates            | 12300     |
|    policy_gradient_loss | 1.31e-11  |
|    value_loss           | 0.0542    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 578      |
|    iterations      | 1231     |
|    time_elapsed    | 8709     |
|    total_timesteps | 5042176  |
---------------------------------
Eval num_timesteps=5045000, episode_reward=0.30 +/- 0.46
Episode length: 337.40 +/- 56.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 337       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5045000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0158    |
|    n_updates            | 12310     |
|    policy_gradient_loss | 6.62e-11  |
|    value_loss           | 0.0553    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 578      |
|    iterations      | 1232     |
|    time_elapsed    | 8720     |
|    total_timesteps | 5046272  |
---------------------------------
Eval num_timesteps=5050000, episode_reward=0.30 +/- 0.46
Episode length: 311.60 +/- 45.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5050000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.042     |
|    n_updates            | 12320     |
|    policy_gradient_loss | 4.35e-10  |
|    value_loss           | 0.0549    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 578      |
|    iterations      | 1233     |
|    time_elapsed    | 8732     |
|    total_timesteps | 5050368  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 298       |
|    ep_rew_mean          | 0.35      |
| time/                   |           |
|    fps                  | 578       |
|    iterations           | 1234      |
|    time_elapsed         | 8740      |
|    total_timesteps      | 5054464   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0287    |
|    n_updates            | 12330     |
|    policy_gradient_loss | 5.81e-10  |
|    value_loss           | 0.055     |
---------------------------------------
Eval num_timesteps=5055000, episode_reward=0.70 +/- 0.64
Episode length: 322.80 +/- 42.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 323       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5055000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0345    |
|    n_updates            | 12340     |
|    policy_gradient_loss | -1.3e-09  |
|    value_loss           | 0.0533    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.3      |
| time/              |          |
|    fps             | 577      |
|    iterations      | 1235     |
|    time_elapsed    | 8752     |
|    total_timesteps | 5058560  |
---------------------------------
Eval num_timesteps=5060000, episode_reward=0.50 +/- 0.50
Episode length: 323.80 +/- 76.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 324       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5060000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000139 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0289    |
|    n_updates            | 12350     |
|    policy_gradient_loss | 1.01e-09  |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 577      |
|    iterations      | 1236     |
|    time_elapsed    | 8763     |
|    total_timesteps | 5062656  |
---------------------------------
Eval num_timesteps=5065000, episode_reward=0.70 +/- 0.90
Episode length: 327.80 +/- 53.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 328          |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 5065000      |
| train/                  |              |
|    approx_kl            | 1.983426e-08 |
|    clip_fraction        | 0.000171     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000203    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0466       |
|    n_updates            | 12360        |
|    policy_gradient_loss | 1.24e-06     |
|    value_loss           | 0.0534       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.29     |
| time/              |          |
|    fps             | 577      |
|    iterations      | 1237     |
|    time_elapsed    | 8774     |
|    total_timesteps | 5066752  |
---------------------------------
Eval num_timesteps=5070000, episode_reward=0.40 +/- 0.66
Episode length: 298.60 +/- 46.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5070000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0412    |
|    n_updates            | 12370     |
|    policy_gradient_loss | -5.67e-10 |
|    value_loss           | 0.066     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 577      |
|    iterations      | 1238     |
|    time_elapsed    | 8785     |
|    total_timesteps | 5070848  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 296      |
|    ep_rew_mean          | 0.4      |
| time/                   |          |
|    fps                  | 577      |
|    iterations           | 1239     |
|    time_elapsed         | 8794     |
|    total_timesteps      | 5074944  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0487   |
|    n_updates            | 12380    |
|    policy_gradient_loss | 2.13e-10 |
|    value_loss           | 0.0679   |
--------------------------------------
Eval num_timesteps=5075000, episode_reward=0.70 +/- 0.78
Episode length: 325.00 +/- 62.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 325       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5075000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0251    |
|    n_updates            | 12390     |
|    policy_gradient_loss | -5.69e-11 |
|    value_loss           | 0.055     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 576      |
|    iterations      | 1240     |
|    time_elapsed    | 8805     |
|    total_timesteps | 5079040  |
---------------------------------
Eval num_timesteps=5080000, episode_reward=0.60 +/- 0.49
Episode length: 311.00 +/- 51.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5080000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0244    |
|    n_updates            | 12400     |
|    policy_gradient_loss | -1.42e-10 |
|    value_loss           | 0.0661    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 576      |
|    iterations      | 1241     |
|    time_elapsed    | 8817     |
|    total_timesteps | 5083136  |
---------------------------------
Eval num_timesteps=5085000, episode_reward=0.10 +/- 0.30
Episode length: 290.80 +/- 46.53
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 291      |
|    mean_reward          | 0.1      |
| time/                   |          |
|    total_timesteps      | 5085000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 5.96e-08 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0229   |
|    n_updates            | 12410    |
|    policy_gradient_loss | 8.39e-10 |
|    value_loss           | 0.0575   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 576      |
|    iterations      | 1242     |
|    time_elapsed    | 8828     |
|    total_timesteps | 5087232  |
---------------------------------
Eval num_timesteps=5090000, episode_reward=0.50 +/- 0.67
Episode length: 313.20 +/- 39.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5090000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0479    |
|    n_updates            | 12420     |
|    policy_gradient_loss | -1.36e-10 |
|    value_loss           | 0.0545    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 575      |
|    iterations      | 1243     |
|    time_elapsed    | 8839     |
|    total_timesteps | 5091328  |
---------------------------------
Eval num_timesteps=5095000, episode_reward=0.20 +/- 0.40
Episode length: 293.40 +/- 54.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 293      |
|    mean_reward          | 0.2      |
| time/                   |          |
|    total_timesteps      | 5095000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0383   |
|    n_updates            | 12430    |
|    policy_gradient_loss | 5.29e-10 |
|    value_loss           | 0.0606   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 575      |
|    iterations      | 1244     |
|    time_elapsed    | 8850     |
|    total_timesteps | 5095424  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 303      |
|    ep_rew_mean          | 0.39     |
| time/                   |          |
|    fps                  | 575      |
|    iterations           | 1245     |
|    time_elapsed         | 8858     |
|    total_timesteps      | 5099520  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0196   |
|    n_updates            | 12440    |
|    policy_gradient_loss | 2.62e-11 |
|    value_loss           | 0.0531   |
--------------------------------------
Eval num_timesteps=5100000, episode_reward=0.50 +/- 0.67
Episode length: 300.20 +/- 57.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5100000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0237    |
|    n_updates            | 12450     |
|    policy_gradient_loss | -6.37e-10 |
|    value_loss           | 0.0529    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 575      |
|    iterations      | 1246     |
|    time_elapsed    | 8869     |
|    total_timesteps | 5103616  |
---------------------------------
Eval num_timesteps=5105000, episode_reward=0.50 +/- 0.50
Episode length: 305.20 +/- 59.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5105000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0376    |
|    n_updates            | 12460     |
|    policy_gradient_loss | -5.03e-10 |
|    value_loss           | 0.0664    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 575      |
|    iterations      | 1247     |
|    time_elapsed    | 8880     |
|    total_timesteps | 5107712  |
---------------------------------
Eval num_timesteps=5110000, episode_reward=0.80 +/- 0.40
Episode length: 295.80 +/- 41.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5110000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0294    |
|    n_updates            | 12470     |
|    policy_gradient_loss | -1.81e-10 |
|    value_loss           | 0.066     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 574      |
|    iterations      | 1248     |
|    time_elapsed    | 8892     |
|    total_timesteps | 5111808  |
---------------------------------
Eval num_timesteps=5115000, episode_reward=0.80 +/- 0.75
Episode length: 340.00 +/- 73.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 340       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5115000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0372    |
|    n_updates            | 12480     |
|    policy_gradient_loss | 6.12e-10  |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 574      |
|    iterations      | 1249     |
|    time_elapsed    | 8903     |
|    total_timesteps | 5115904  |
---------------------------------
Eval num_timesteps=5120000, episode_reward=0.30 +/- 0.46
Episode length: 290.80 +/- 45.05
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 291      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 5120000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0258   |
|    n_updates            | 12490    |
|    policy_gradient_loss | 1.64e-10 |
|    value_loss           | 0.0585   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 574      |
|    iterations      | 1250     |
|    time_elapsed    | 8914     |
|    total_timesteps | 5120000  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 302      |
|    ep_rew_mean          | 0.48     |
| time/                   |          |
|    fps                  | 574      |
|    iterations           | 1251     |
|    time_elapsed         | 8923     |
|    total_timesteps      | 5124096  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0369   |
|    n_updates            | 12500    |
|    policy_gradient_loss | 1.8e-10  |
|    value_loss           | 0.0614   |
--------------------------------------
Eval num_timesteps=5125000, episode_reward=0.70 +/- 0.64
Episode length: 311.40 +/- 47.46
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 311      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 5125000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0196   |
|    n_updates            | 12510    |
|    policy_gradient_loss | 7.29e-10 |
|    value_loss           | 0.06     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 573      |
|    iterations      | 1252     |
|    time_elapsed    | 8934     |
|    total_timesteps | 5128192  |
---------------------------------
Eval num_timesteps=5130000, episode_reward=0.70 +/- 0.78
Episode length: 324.00 +/- 58.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 324       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5130000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.049     |
|    n_updates            | 12520     |
|    policy_gradient_loss | -3.93e-10 |
|    value_loss           | 0.0661    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 573      |
|    iterations      | 1253     |
|    time_elapsed    | 8945     |
|    total_timesteps | 5132288  |
---------------------------------
Eval num_timesteps=5135000, episode_reward=0.30 +/- 0.46
Episode length: 282.20 +/- 55.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 282       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5135000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0322    |
|    n_updates            | 12530     |
|    policy_gradient_loss | 7.64e-11  |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 573      |
|    iterations      | 1254     |
|    time_elapsed    | 8956     |
|    total_timesteps | 5136384  |
---------------------------------
Eval num_timesteps=5140000, episode_reward=0.30 +/- 0.46
Episode length: 306.00 +/- 47.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5140000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0273    |
|    n_updates            | 12540     |
|    policy_gradient_loss | -4.87e-10 |
|    value_loss           | 0.0584    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 573      |
|    iterations      | 1255     |
|    time_elapsed    | 8967     |
|    total_timesteps | 5140480  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 573       |
|    iterations           | 1256      |
|    time_elapsed         | 8976      |
|    total_timesteps      | 5144576   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0244    |
|    n_updates            | 12550     |
|    policy_gradient_loss | -5.86e-10 |
|    value_loss           | 0.0623    |
---------------------------------------
Eval num_timesteps=5145000, episode_reward=0.70 +/- 0.46
Episode length: 324.40 +/- 37.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 324       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5145000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0287    |
|    n_updates            | 12560     |
|    policy_gradient_loss | -3.86e-10 |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 572      |
|    iterations      | 1257     |
|    time_elapsed    | 8987     |
|    total_timesteps | 5148672  |
---------------------------------
Eval num_timesteps=5150000, episode_reward=0.20 +/- 0.40
Episode length: 295.80 +/- 41.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5150000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0367    |
|    n_updates            | 12570     |
|    policy_gradient_loss | -2.67e-10 |
|    value_loss           | 0.0625    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 572      |
|    iterations      | 1258     |
|    time_elapsed    | 8998     |
|    total_timesteps | 5152768  |
---------------------------------
Eval num_timesteps=5155000, episode_reward=0.30 +/- 0.46
Episode length: 287.20 +/- 44.82
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 287      |
|    mean_reward          | 0.3      |
| time/                   |          |
|    total_timesteps      | 5155000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0334   |
|    n_updates            | 12580    |
|    policy_gradient_loss | 1.92e-10 |
|    value_loss           | 0.0575   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 572      |
|    iterations      | 1259     |
|    time_elapsed    | 9009     |
|    total_timesteps | 5156864  |
---------------------------------
Eval num_timesteps=5160000, episode_reward=0.80 +/- 0.75
Episode length: 289.60 +/- 30.01
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 290      |
|    mean_reward          | 0.8      |
| time/                   |          |
|    total_timesteps      | 5160000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0297   |
|    n_updates            | 12590    |
|    policy_gradient_loss | 8.06e-10 |
|    value_loss           | 0.0636   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 572      |
|    iterations      | 1260     |
|    time_elapsed    | 9020     |
|    total_timesteps | 5160960  |
---------------------------------
Eval num_timesteps=5165000, episode_reward=0.50 +/- 0.81
Episode length: 328.60 +/- 43.64
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 329      |
|    mean_reward          | 0.5      |
| time/                   |          |
|    total_timesteps      | 5165000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0424   |
|    n_updates            | 12600    |
|    policy_gradient_loss | 2.23e-10 |
|    value_loss           | 0.0586   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 571      |
|    iterations      | 1261     |
|    time_elapsed    | 9031     |
|    total_timesteps | 5165056  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 296      |
|    ep_rew_mean          | 0.38     |
| time/                   |          |
|    fps                  | 571      |
|    iterations           | 1262     |
|    time_elapsed         | 9040     |
|    total_timesteps      | 5169152  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.0198   |
|    n_updates            | 12610    |
|    policy_gradient_loss | 5.27e-10 |
|    value_loss           | 0.0563   |
--------------------------------------
Eval num_timesteps=5170000, episode_reward=0.70 +/- 0.64
Episode length: 287.00 +/- 34.99
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 287      |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 5170000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 0        |
|    learning_rate        | 0.01     |
|    loss                 | 0.062    |
|    n_updates            | 12620    |
|    policy_gradient_loss | 5.6e-11  |
|    value_loss           | 0.0559   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 571      |
|    iterations      | 1263     |
|    time_elapsed    | 9051     |
|    total_timesteps | 5173248  |
---------------------------------
Eval num_timesteps=5175000, episode_reward=0.60 +/- 0.49
Episode length: 304.20 +/- 36.42
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 304      |
|    mean_reward          | 0.6      |
| time/                   |          |
|    total_timesteps      | 5175000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.00017 |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.01     |
|    loss                 | 0.0319   |
|    n_updates            | 12630    |
|    policy_gradient_loss | -9.4e-10 |
|    value_loss           | 0.0565   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 571      |
|    iterations      | 1264     |
|    time_elapsed    | 9062     |
|    total_timesteps | 5177344  |
---------------------------------
Eval num_timesteps=5180000, episode_reward=0.50 +/- 0.50
Episode length: 300.80 +/- 62.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5180000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0291    |
|    n_updates            | 12640     |
|    policy_gradient_loss | -1.08e-09 |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 287      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 571      |
|    iterations      | 1265     |
|    time_elapsed    | 9073     |
|    total_timesteps | 5181440  |
---------------------------------
Eval num_timesteps=5185000, episode_reward=0.40 +/- 0.66
Episode length: 310.80 +/- 60.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5185000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.00017  |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0252    |
|    n_updates            | 12650     |
|    policy_gradient_loss | -8.85e-10 |
|    value_loss           | 0.0624    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 570      |
|    iterations      | 1266     |
|    time_elapsed    | 9085     |
|    total_timesteps | 5185536  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 290           |
|    ep_rew_mean          | 0.4           |
| time/                   |               |
|    fps                  | 570           |
|    iterations           | 1267          |
|    time_elapsed         | 9093          |
|    total_timesteps      | 5189632       |
| train/                  |               |
|    approx_kl            | 3.9117556e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000127     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0236        |
|    n_updates            | 12660         |
|    policy_gradient_loss | -3.18e-05     |
|    value_loss           | 0.0651        |
-------------------------------------------
Eval num_timesteps=5190000, episode_reward=0.60 +/- 0.49
Episode length: 303.60 +/- 42.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5190000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0181    |
|    n_updates            | 12670     |
|    policy_gradient_loss | 6.36e-10  |
|    value_loss           | 0.0558    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 570      |
|    iterations      | 1268     |
|    time_elapsed    | 9104     |
|    total_timesteps | 5193728  |
---------------------------------
Eval num_timesteps=5195000, episode_reward=0.30 +/- 0.46
Episode length: 304.00 +/- 62.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5195000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0256    |
|    n_updates            | 12680     |
|    policy_gradient_loss | -8.48e-10 |
|    value_loss           | 0.0549    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 570      |
|    iterations      | 1269     |
|    time_elapsed    | 9115     |
|    total_timesteps | 5197824  |
---------------------------------
Eval num_timesteps=5200000, episode_reward=0.50 +/- 0.50
Episode length: 289.80 +/- 54.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5200000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0287    |
|    n_updates            | 12690     |
|    policy_gradient_loss | 9.6e-11   |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 569      |
|    iterations      | 1270     |
|    time_elapsed    | 9126     |
|    total_timesteps | 5201920  |
---------------------------------
Eval num_timesteps=5205000, episode_reward=0.50 +/- 0.67
Episode length: 293.80 +/- 36.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5205000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0228    |
|    n_updates            | 12700     |
|    policy_gradient_loss | 7.28e-12  |
|    value_loss           | 0.0525    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 569      |
|    iterations      | 1271     |
|    time_elapsed    | 9137     |
|    total_timesteps | 5206016  |
---------------------------------
Eval num_timesteps=5210000, episode_reward=0.90 +/- 1.04
Episode length: 325.60 +/- 79.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 326       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 5210000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0253    |
|    n_updates            | 12710     |
|    policy_gradient_loss | -2.71e-10 |
|    value_loss           | 0.0632    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 569      |
|    iterations      | 1272     |
|    time_elapsed    | 9149     |
|    total_timesteps | 5210112  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.46      |
| time/                   |           |
|    fps                  | 569       |
|    iterations           | 1273      |
|    time_elapsed         | 9157      |
|    total_timesteps      | 5214208   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.026     |
|    n_updates            | 12720     |
|    policy_gradient_loss | -3.88e-10 |
|    value_loss           | 0.0609    |
---------------------------------------
Eval num_timesteps=5215000, episode_reward=0.60 +/- 1.02
Episode length: 305.60 +/- 60.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5215000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0177    |
|    n_updates            | 12730     |
|    policy_gradient_loss | 9.17e-10  |
|    value_loss           | 0.06      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 569      |
|    iterations      | 1274     |
|    time_elapsed    | 9168     |
|    total_timesteps | 5218304  |
---------------------------------
Eval num_timesteps=5220000, episode_reward=0.90 +/- 0.54
Episode length: 335.80 +/- 47.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 336       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 5220000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0304    |
|    n_updates            | 12740     |
|    policy_gradient_loss | 2.73e-10  |
|    value_loss           | 0.0648    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 568      |
|    iterations      | 1275     |
|    time_elapsed    | 9180     |
|    total_timesteps | 5222400  |
---------------------------------
Eval num_timesteps=5225000, episode_reward=0.80 +/- 0.75
Episode length: 315.40 +/- 54.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5225000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0322    |
|    n_updates            | 12750     |
|    policy_gradient_loss | -1.78e-11 |
|    value_loss           | 0.0618    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 568      |
|    iterations      | 1276     |
|    time_elapsed    | 9191     |
|    total_timesteps | 5226496  |
---------------------------------
Eval num_timesteps=5230000, episode_reward=0.70 +/- 0.64
Episode length: 338.00 +/- 51.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 338       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5230000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000122 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.022     |
|    n_updates            | 12760     |
|    policy_gradient_loss | -8.24e-10 |
|    value_loss           | 0.0644    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 568      |
|    iterations      | 1277     |
|    time_elapsed    | 9202     |
|    total_timesteps | 5230592  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 300          |
|    ep_rew_mean          | 0.48         |
| time/                   |              |
|    fps                  | 568          |
|    iterations           | 1278         |
|    time_elapsed         | 9210         |
|    total_timesteps      | 5234688      |
| train/                  |              |
|    approx_kl            | 2.143628e-06 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000116    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0318       |
|    n_updates            | 12770        |
|    policy_gradient_loss | -1.01e-07    |
|    value_loss           | 0.0537       |
------------------------------------------
Eval num_timesteps=5235000, episode_reward=0.40 +/- 0.49
Episode length: 295.40 +/- 48.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5235000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0309    |
|    n_updates            | 12780     |
|    policy_gradient_loss | -3.95e-10 |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 568      |
|    iterations      | 1279     |
|    time_elapsed    | 9221     |
|    total_timesteps | 5238784  |
---------------------------------
Eval num_timesteps=5240000, episode_reward=0.60 +/- 0.66
Episode length: 305.00 +/- 33.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5240000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.05      |
|    n_updates            | 12790     |
|    policy_gradient_loss | -9.82e-10 |
|    value_loss           | 0.0642    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 567      |
|    iterations      | 1280     |
|    time_elapsed    | 9232     |
|    total_timesteps | 5242880  |
---------------------------------
Eval num_timesteps=5245000, episode_reward=0.30 +/- 0.46
Episode length: 327.40 +/- 66.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 327       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5245000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0321    |
|    n_updates            | 12800     |
|    policy_gradient_loss | -2.91e-10 |
|    value_loss           | 0.0622    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 567      |
|    iterations      | 1281     |
|    time_elapsed    | 9244     |
|    total_timesteps | 5246976  |
---------------------------------
Eval num_timesteps=5250000, episode_reward=0.40 +/- 0.66
Episode length: 308.20 +/- 53.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5250000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0318    |
|    n_updates            | 12810     |
|    policy_gradient_loss | -1.06e-10 |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 567      |
|    iterations      | 1282     |
|    time_elapsed    | 9255     |
|    total_timesteps | 5251072  |
---------------------------------
Eval num_timesteps=5255000, episode_reward=0.60 +/- 0.66
Episode length: 297.80 +/- 64.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5255000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.04      |
|    n_updates            | 12820     |
|    policy_gradient_loss | -5.48e-10 |
|    value_loss           | 0.0645    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 567      |
|    iterations      | 1283     |
|    time_elapsed    | 9266     |
|    total_timesteps | 5255168  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 291       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 567       |
|    iterations           | 1284      |
|    time_elapsed         | 9275      |
|    total_timesteps      | 5259264   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0323    |
|    n_updates            | 12830     |
|    policy_gradient_loss | -9.06e-11 |
|    value_loss           | 0.0693    |
---------------------------------------
Eval num_timesteps=5260000, episode_reward=0.30 +/- 0.64
Episode length: 295.20 +/- 64.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5260000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0259    |
|    n_updates            | 12840     |
|    policy_gradient_loss | -1.54e-10 |
|    value_loss           | 0.0628    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 566      |
|    iterations      | 1285     |
|    time_elapsed    | 9286     |
|    total_timesteps | 5263360  |
---------------------------------
Eval num_timesteps=5265000, episode_reward=0.40 +/- 0.49
Episode length: 357.60 +/- 60.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 358       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5265000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.031     |
|    n_updates            | 12850     |
|    policy_gradient_loss | -2.23e-10 |
|    value_loss           | 0.0686    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 290      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 566      |
|    iterations      | 1286     |
|    time_elapsed    | 9297     |
|    total_timesteps | 5267456  |
---------------------------------
Eval num_timesteps=5270000, episode_reward=0.30 +/- 0.64
Episode length: 288.80 +/- 45.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5270000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0362    |
|    n_updates            | 12860     |
|    policy_gradient_loss | -8.37e-11 |
|    value_loss           | 0.0595    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 566      |
|    iterations      | 1287     |
|    time_elapsed    | 9308     |
|    total_timesteps | 5271552  |
---------------------------------
Eval num_timesteps=5275000, episode_reward=0.60 +/- 0.80
Episode length: 331.40 +/- 43.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 331       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5275000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0246    |
|    n_updates            | 12870     |
|    policy_gradient_loss | 7.23e-10  |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 566      |
|    iterations      | 1288     |
|    time_elapsed    | 9319     |
|    total_timesteps | 5275648  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 290       |
|    ep_rew_mean          | 0.42      |
| time/                   |           |
|    fps                  | 565       |
|    iterations           | 1289      |
|    time_elapsed         | 9328      |
|    total_timesteps      | 5279744   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0341    |
|    n_updates            | 12880     |
|    policy_gradient_loss | 9.13e-11  |
|    value_loss           | 0.0655    |
---------------------------------------
Eval num_timesteps=5280000, episode_reward=0.20 +/- 0.40
Episode length: 280.60 +/- 42.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5280000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0133    |
|    n_updates            | 12890     |
|    policy_gradient_loss | 6.56e-10  |
|    value_loss           | 0.0649    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 289      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 565      |
|    iterations      | 1290     |
|    time_elapsed    | 9339     |
|    total_timesteps | 5283840  |
---------------------------------
Eval num_timesteps=5285000, episode_reward=0.70 +/- 0.46
Episode length: 304.80 +/- 40.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5285000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0543    |
|    n_updates            | 12900     |
|    policy_gradient_loss | -1.18e-09 |
|    value_loss           | 0.0591    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 565      |
|    iterations      | 1291     |
|    time_elapsed    | 9350     |
|    total_timesteps | 5287936  |
---------------------------------
Eval num_timesteps=5290000, episode_reward=0.20 +/- 0.40
Episode length: 278.20 +/- 46.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5290000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0288    |
|    n_updates            | 12910     |
|    policy_gradient_loss | -3.88e-10 |
|    value_loss           | 0.0612    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 565      |
|    iterations      | 1292     |
|    time_elapsed    | 9361     |
|    total_timesteps | 5292032  |
---------------------------------
Eval num_timesteps=5295000, episode_reward=0.30 +/- 0.64
Episode length: 303.40 +/- 53.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5295000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0289    |
|    n_updates            | 12920     |
|    policy_gradient_loss | 7.02e-10  |
|    value_loss           | 0.0556    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 565      |
|    iterations      | 1293     |
|    time_elapsed    | 9372     |
|    total_timesteps | 5296128  |
---------------------------------
Eval num_timesteps=5300000, episode_reward=0.50 +/- 0.67
Episode length: 286.40 +/- 52.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5300000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0321    |
|    n_updates            | 12930     |
|    policy_gradient_loss | -2.54e-10 |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 564      |
|    iterations      | 1294     |
|    time_elapsed    | 9383     |
|    total_timesteps | 5300224  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.42      |
| time/                   |           |
|    fps                  | 564       |
|    iterations           | 1295      |
|    time_elapsed         | 9391      |
|    total_timesteps      | 5304320   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0189    |
|    n_updates            | 12940     |
|    policy_gradient_loss | 1.32e-10  |
|    value_loss           | 0.0555    |
---------------------------------------
Eval num_timesteps=5305000, episode_reward=0.40 +/- 0.49
Episode length: 305.20 +/- 49.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5305000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0392    |
|    n_updates            | 12950     |
|    policy_gradient_loss | -1.42e-10 |
|    value_loss           | 0.0539    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 564      |
|    iterations      | 1296     |
|    time_elapsed    | 9402     |
|    total_timesteps | 5308416  |
---------------------------------
Eval num_timesteps=5310000, episode_reward=0.30 +/- 0.46
Episode length: 323.00 +/- 55.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 323       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5310000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0186    |
|    n_updates            | 12960     |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 564      |
|    iterations      | 1297     |
|    time_elapsed    | 9414     |
|    total_timesteps | 5312512  |
---------------------------------
Eval num_timesteps=5315000, episode_reward=0.60 +/- 0.66
Episode length: 316.20 +/- 56.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5315000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0199    |
|    n_updates            | 12970     |
|    policy_gradient_loss | 2.02e-10  |
|    value_loss           | 0.0583    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 564      |
|    iterations      | 1298     |
|    time_elapsed    | 9425     |
|    total_timesteps | 5316608  |
---------------------------------
Eval num_timesteps=5320000, episode_reward=0.30 +/- 0.46
Episode length: 274.80 +/- 47.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 275       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5320000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0328    |
|    n_updates            | 12980     |
|    policy_gradient_loss | 1.03e-10  |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 563      |
|    iterations      | 1299     |
|    time_elapsed    | 9436     |
|    total_timesteps | 5320704  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.41      |
| time/                   |           |
|    fps                  | 563       |
|    iterations           | 1300      |
|    time_elapsed         | 9445      |
|    total_timesteps      | 5324800   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0344    |
|    n_updates            | 12990     |
|    policy_gradient_loss | -9.32e-10 |
|    value_loss           | 0.0577    |
---------------------------------------
Eval num_timesteps=5325000, episode_reward=0.20 +/- 0.40
Episode length: 311.60 +/- 53.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5325000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0222    |
|    n_updates            | 13000     |
|    policy_gradient_loss | 1e-09     |
|    value_loss           | 0.0522    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 563      |
|    iterations      | 1301     |
|    time_elapsed    | 9456     |
|    total_timesteps | 5328896  |
---------------------------------
Eval num_timesteps=5330000, episode_reward=0.40 +/- 0.66
Episode length: 284.20 +/- 27.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 284       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5330000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0177    |
|    n_updates            | 13010     |
|    policy_gradient_loss | 8e-10     |
|    value_loss           | 0.0572    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 563      |
|    iterations      | 1302     |
|    time_elapsed    | 9467     |
|    total_timesteps | 5332992  |
---------------------------------
Eval num_timesteps=5335000, episode_reward=0.40 +/- 0.66
Episode length: 320.40 +/- 54.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 320       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5335000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0208    |
|    n_updates            | 13020     |
|    policy_gradient_loss | -4.34e-10 |
|    value_loss           | 0.0626    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 563      |
|    iterations      | 1303     |
|    time_elapsed    | 9478     |
|    total_timesteps | 5337088  |
---------------------------------
Eval num_timesteps=5340000, episode_reward=0.70 +/- 0.64
Episode length: 307.00 +/- 59.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5340000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0191    |
|    n_updates            | 13030     |
|    policy_gradient_loss | 1.69e-10  |
|    value_loss           | 0.0632    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 562      |
|    iterations      | 1304     |
|    time_elapsed    | 9489     |
|    total_timesteps | 5341184  |
---------------------------------
Eval num_timesteps=5345000, episode_reward=0.60 +/- 0.49
Episode length: 321.20 +/- 77.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 321       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5345000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0424    |
|    n_updates            | 13040     |
|    policy_gradient_loss | 6.53e-10  |
|    value_loss           | 0.0623    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 562      |
|    iterations      | 1305     |
|    time_elapsed    | 9501     |
|    total_timesteps | 5345280  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 299       |
|    ep_rew_mean          | 0.4       |
| time/                   |           |
|    fps                  | 562       |
|    iterations           | 1306      |
|    time_elapsed         | 9509      |
|    total_timesteps      | 5349376   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0355    |
|    n_updates            | 13050     |
|    policy_gradient_loss | 3.77e-10  |
|    value_loss           | 0.0597    |
---------------------------------------
Eval num_timesteps=5350000, episode_reward=0.80 +/- 0.75
Episode length: 328.40 +/- 57.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 328       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5350000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0283    |
|    n_updates            | 13060     |
|    policy_gradient_loss | 1.59e-10  |
|    value_loss           | 0.0523    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 562      |
|    iterations      | 1307     |
|    time_elapsed    | 9521     |
|    total_timesteps | 5353472  |
---------------------------------
Eval num_timesteps=5355000, episode_reward=0.60 +/- 0.49
Episode length: 304.20 +/- 55.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5355000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0508    |
|    n_updates            | 13070     |
|    policy_gradient_loss | -9.31e-11 |
|    value_loss           | 0.0583    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 562      |
|    iterations      | 1308     |
|    time_elapsed    | 9532     |
|    total_timesteps | 5357568  |
---------------------------------
Eval num_timesteps=5360000, episode_reward=0.60 +/- 0.66
Episode length: 302.80 +/- 50.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5360000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0475    |
|    n_updates            | 13080     |
|    policy_gradient_loss | -4.25e-10 |
|    value_loss           | 0.0557    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 561      |
|    iterations      | 1309     |
|    time_elapsed    | 9543     |
|    total_timesteps | 5361664  |
---------------------------------
Eval num_timesteps=5365000, episode_reward=0.20 +/- 0.40
Episode length: 300.20 +/- 38.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5365000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0377    |
|    n_updates            | 13090     |
|    policy_gradient_loss | -5.55e-10 |
|    value_loss           | 0.0651    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 561      |
|    iterations      | 1310     |
|    time_elapsed    | 9554     |
|    total_timesteps | 5365760  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 302       |
|    ep_rew_mean          | 0.45      |
| time/                   |           |
|    fps                  | 561       |
|    iterations           | 1311      |
|    time_elapsed         | 9562      |
|    total_timesteps      | 5369856   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0371    |
|    n_updates            | 13100     |
|    policy_gradient_loss | -8.11e-10 |
|    value_loss           | 0.0693    |
---------------------------------------
Eval num_timesteps=5370000, episode_reward=0.40 +/- 0.66
Episode length: 298.60 +/- 44.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 299       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5370000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00465   |
|    n_updates            | 13110     |
|    policy_gradient_loss | 2.64e-10  |
|    value_loss           | 0.0519    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 561      |
|    iterations      | 1312     |
|    time_elapsed    | 9573     |
|    total_timesteps | 5373952  |
---------------------------------
Eval num_timesteps=5375000, episode_reward=0.40 +/- 0.49
Episode length: 302.60 +/- 52.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5375000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0237    |
|    n_updates            | 13120     |
|    policy_gradient_loss | 1.57e-10  |
|    value_loss           | 0.0634    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 561      |
|    iterations      | 1313     |
|    time_elapsed    | 9585     |
|    total_timesteps | 5378048  |
---------------------------------
Eval num_timesteps=5380000, episode_reward=0.30 +/- 0.46
Episode length: 295.40 +/- 24.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 295       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5380000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0107    |
|    n_updates            | 13130     |
|    policy_gradient_loss | -1.27e-09 |
|    value_loss           | 0.0616    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 560      |
|    iterations      | 1314     |
|    time_elapsed    | 9596     |
|    total_timesteps | 5382144  |
---------------------------------
Eval num_timesteps=5385000, episode_reward=0.40 +/- 0.49
Episode length: 293.60 +/- 43.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5385000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0593    |
|    n_updates            | 13140     |
|    policy_gradient_loss | 9.39e-11  |
|    value_loss           | 0.0549    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 560      |
|    iterations      | 1315     |
|    time_elapsed    | 9607     |
|    total_timesteps | 5386240  |
---------------------------------
Eval num_timesteps=5390000, episode_reward=0.50 +/- 0.67
Episode length: 297.00 +/- 43.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 297       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5390000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0413    |
|    n_updates            | 13150     |
|    policy_gradient_loss | -1.72e-10 |
|    value_loss           | 0.0717    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 560      |
|    iterations      | 1316     |
|    time_elapsed    | 9618     |
|    total_timesteps | 5390336  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 298       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 560       |
|    iterations           | 1317      |
|    time_elapsed         | 9627      |
|    total_timesteps      | 5394432   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0385    |
|    n_updates            | 13160     |
|    policy_gradient_loss | -1.88e-10 |
|    value_loss           | 0.0583    |
---------------------------------------
Eval num_timesteps=5395000, episode_reward=0.70 +/- 0.78
Episode length: 321.20 +/- 59.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 321       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5395000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0303    |
|    n_updates            | 13170     |
|    policy_gradient_loss | -2.77e-10 |
|    value_loss           | 0.0697    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 294      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 560      |
|    iterations      | 1318     |
|    time_elapsed    | 9638     |
|    total_timesteps | 5398528  |
---------------------------------
Eval num_timesteps=5400000, episode_reward=0.50 +/- 0.67
Episode length: 282.00 +/- 41.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 282       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5400000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0575    |
|    n_updates            | 13180     |
|    policy_gradient_loss | 1.54e-10  |
|    value_loss           | 0.0639    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 559      |
|    iterations      | 1319     |
|    time_elapsed    | 9649     |
|    total_timesteps | 5402624  |
---------------------------------
Eval num_timesteps=5405000, episode_reward=0.40 +/- 0.66
Episode length: 314.00 +/- 76.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5405000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000159 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00929   |
|    n_updates            | 13190     |
|    policy_gradient_loss | 1.13e-10  |
|    value_loss           | 0.06      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 559      |
|    iterations      | 1320     |
|    time_elapsed    | 9660     |
|    total_timesteps | 5406720  |
---------------------------------
Eval num_timesteps=5410000, episode_reward=0.50 +/- 0.67
Episode length: 304.60 +/- 30.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 305           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 5410000       |
| train/                  |               |
|    approx_kl            | 1.4675315e-06 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000143     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0559        |
|    n_updates            | 13200         |
|    policy_gradient_loss | 4.12e-06      |
|    value_loss           | 0.0622        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 559      |
|    iterations      | 1321     |
|    time_elapsed    | 9671     |
|    total_timesteps | 5410816  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 559       |
|    iterations           | 1322      |
|    time_elapsed         | 9680      |
|    total_timesteps      | 5414912   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0397    |
|    n_updates            | 13210     |
|    policy_gradient_loss | -6.66e-11 |
|    value_loss           | 0.0607    |
---------------------------------------
Eval num_timesteps=5415000, episode_reward=0.40 +/- 0.49
Episode length: 292.20 +/- 40.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5415000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0214    |
|    n_updates            | 13220     |
|    policy_gradient_loss | 7.71e-11  |
|    value_loss           | 0.0659    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 559      |
|    iterations      | 1323     |
|    time_elapsed    | 9691     |
|    total_timesteps | 5419008  |
---------------------------------
Eval num_timesteps=5420000, episode_reward=0.30 +/- 0.46
Episode length: 266.40 +/- 47.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 266       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5420000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0234    |
|    n_updates            | 13230     |
|    policy_gradient_loss | -9.94e-10 |
|    value_loss           | 0.0537    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 558      |
|    iterations      | 1324     |
|    time_elapsed    | 9702     |
|    total_timesteps | 5423104  |
---------------------------------
Eval num_timesteps=5425000, episode_reward=0.40 +/- 0.49
Episode length: 269.20 +/- 47.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 269       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5425000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0441    |
|    n_updates            | 13240     |
|    policy_gradient_loss | 2.31e-10  |
|    value_loss           | 0.0646    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 558      |
|    iterations      | 1325     |
|    time_elapsed    | 9713     |
|    total_timesteps | 5427200  |
---------------------------------
Eval num_timesteps=5430000, episode_reward=0.60 +/- 0.66
Episode length: 281.40 +/- 35.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5430000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0228    |
|    n_updates            | 13250     |
|    policy_gradient_loss | 2.96e-10  |
|    value_loss           | 0.0636    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 311      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 558      |
|    iterations      | 1326     |
|    time_elapsed    | 9724     |
|    total_timesteps | 5431296  |
---------------------------------
Eval num_timesteps=5435000, episode_reward=0.30 +/- 0.64
Episode length: 316.20 +/- 42.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5435000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0462    |
|    n_updates            | 13260     |
|    policy_gradient_loss | 3.2e-11   |
|    value_loss           | 0.0518    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 558      |
|    iterations      | 1327     |
|    time_elapsed    | 9735     |
|    total_timesteps | 5435392  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 312       |
|    ep_rew_mean          | 0.52      |
| time/                   |           |
|    fps                  | 558       |
|    iterations           | 1328      |
|    time_elapsed         | 9743      |
|    total_timesteps      | 5439488   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0105    |
|    n_updates            | 13270     |
|    policy_gradient_loss | 7.23e-10  |
|    value_loss           | 0.0701    |
---------------------------------------
Eval num_timesteps=5440000, episode_reward=1.00 +/- 1.10
Episode length: 330.60 +/- 65.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 331       |
|    mean_reward          | 1         |
| time/                   |           |
|    total_timesteps      | 5440000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0385    |
|    n_updates            | 13280     |
|    policy_gradient_loss | -5.03e-10 |
|    value_loss           | 0.055     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 313      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 558      |
|    iterations      | 1329     |
|    time_elapsed    | 9754     |
|    total_timesteps | 5443584  |
---------------------------------
Eval num_timesteps=5445000, episode_reward=0.60 +/- 0.66
Episode length: 311.40 +/- 64.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5445000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0488    |
|    n_updates            | 13290     |
|    policy_gradient_loss | 4.05e-10  |
|    value_loss           | 0.0629    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 557      |
|    iterations      | 1330     |
|    time_elapsed    | 9766     |
|    total_timesteps | 5447680  |
---------------------------------
Eval num_timesteps=5450000, episode_reward=0.40 +/- 0.66
Episode length: 269.60 +/- 33.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 270       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5450000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0217    |
|    n_updates            | 13300     |
|    policy_gradient_loss | -9.35e-10 |
|    value_loss           | 0.0611    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 557      |
|    iterations      | 1331     |
|    time_elapsed    | 9776     |
|    total_timesteps | 5451776  |
---------------------------------
Eval num_timesteps=5455000, episode_reward=0.20 +/- 0.40
Episode length: 301.00 +/- 44.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5455000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0185    |
|    n_updates            | 13310     |
|    policy_gradient_loss | -3.72e-10 |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 557      |
|    iterations      | 1332     |
|    time_elapsed    | 9788     |
|    total_timesteps | 5455872  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 557       |
|    iterations           | 1333      |
|    time_elapsed         | 9796      |
|    total_timesteps      | 5459968   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0566    |
|    n_updates            | 13320     |
|    policy_gradient_loss | 2.97e-10  |
|    value_loss           | 0.0587    |
---------------------------------------
Eval num_timesteps=5460000, episode_reward=0.30 +/- 0.64
Episode length: 298.20 +/- 51.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5460000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0253    |
|    n_updates            | 13330     |
|    policy_gradient_loss | -4.06e-10 |
|    value_loss           | 0.0585    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 557      |
|    iterations      | 1334     |
|    time_elapsed    | 9807     |
|    total_timesteps | 5464064  |
---------------------------------
Eval num_timesteps=5465000, episode_reward=0.30 +/- 0.46
Episode length: 295.60 +/- 59.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5465000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0297    |
|    n_updates            | 13340     |
|    policy_gradient_loss | -7.28e-13 |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 556      |
|    iterations      | 1335     |
|    time_elapsed    | 9818     |
|    total_timesteps | 5468160  |
---------------------------------
Eval num_timesteps=5470000, episode_reward=0.50 +/- 0.81
Episode length: 303.60 +/- 41.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5470000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0299    |
|    n_updates            | 13350     |
|    policy_gradient_loss | -5.51e-10 |
|    value_loss           | 0.0621    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 556      |
|    iterations      | 1336     |
|    time_elapsed    | 9829     |
|    total_timesteps | 5472256  |
---------------------------------
Eval num_timesteps=5475000, episode_reward=0.30 +/- 0.46
Episode length: 308.80 +/- 26.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5475000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0361    |
|    n_updates            | 13360     |
|    policy_gradient_loss | 4.64e-10  |
|    value_loss           | 0.0645    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 556      |
|    iterations      | 1337     |
|    time_elapsed    | 9840     |
|    total_timesteps | 5476352  |
---------------------------------
Eval num_timesteps=5480000, episode_reward=0.30 +/- 0.46
Episode length: 273.80 +/- 37.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 274       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5480000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0274    |
|    n_updates            | 13370     |
|    policy_gradient_loss | -1.43e-10 |
|    value_loss           | 0.0603    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 556      |
|    iterations      | 1338     |
|    time_elapsed    | 9851     |
|    total_timesteps | 5480448  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 305       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 556       |
|    iterations           | 1339      |
|    time_elapsed         | 9859      |
|    total_timesteps      | 5484544   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.017     |
|    n_updates            | 13380     |
|    policy_gradient_loss | -7.2e-11  |
|    value_loss           | 0.0565    |
---------------------------------------
Eval num_timesteps=5485000, episode_reward=0.40 +/- 0.49
Episode length: 300.20 +/- 38.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5485000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0306    |
|    n_updates            | 13390     |
|    policy_gradient_loss | 2.1e-10   |
|    value_loss           | 0.0506    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 556      |
|    iterations      | 1340     |
|    time_elapsed    | 9870     |
|    total_timesteps | 5488640  |
---------------------------------
Eval num_timesteps=5490000, episode_reward=0.50 +/- 0.50
Episode length: 306.20 +/- 47.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5490000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0525    |
|    n_updates            | 13400     |
|    policy_gradient_loss | 3.42e-11  |
|    value_loss           | 0.0583    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 555      |
|    iterations      | 1341     |
|    time_elapsed    | 9882     |
|    total_timesteps | 5492736  |
---------------------------------
Eval num_timesteps=5495000, episode_reward=0.60 +/- 0.66
Episode length: 326.20 +/- 42.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 326       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5495000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0416    |
|    n_updates            | 13410     |
|    policy_gradient_loss | -1.12e-10 |
|    value_loss           | 0.0601    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 555      |
|    iterations      | 1342     |
|    time_elapsed    | 9893     |
|    total_timesteps | 5496832  |
---------------------------------
Eval num_timesteps=5500000, episode_reward=0.20 +/- 0.40
Episode length: 278.20 +/- 42.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5500000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0236    |
|    n_updates            | 13420     |
|    policy_gradient_loss | -2.77e-10 |
|    value_loss           | 0.0599    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 555      |
|    iterations      | 1343     |
|    time_elapsed    | 9904     |
|    total_timesteps | 5500928  |
---------------------------------
Eval num_timesteps=5505000, episode_reward=0.60 +/- 0.66
Episode length: 315.00 +/- 38.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5505000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.034     |
|    n_updates            | 13430     |
|    policy_gradient_loss | -2.31e-10 |
|    value_loss           | 0.0716    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 555      |
|    iterations      | 1344     |
|    time_elapsed    | 9915     |
|    total_timesteps | 5505024  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 307       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 555       |
|    iterations           | 1345      |
|    time_elapsed         | 9923      |
|    total_timesteps      | 5509120   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0113    |
|    n_updates            | 13440     |
|    policy_gradient_loss | 1.62e-09  |
|    value_loss           | 0.0601    |
---------------------------------------
Eval num_timesteps=5510000, episode_reward=0.70 +/- 1.00
Episode length: 293.00 +/- 44.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5510000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0549    |
|    n_updates            | 13450     |
|    policy_gradient_loss | -2.5e-10  |
|    value_loss           | 0.0614    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 554      |
|    iterations      | 1346     |
|    time_elapsed    | 9934     |
|    total_timesteps | 5513216  |
---------------------------------
Eval num_timesteps=5515000, episode_reward=0.30 +/- 0.46
Episode length: 277.80 +/- 40.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5515000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0232    |
|    n_updates            | 13460     |
|    policy_gradient_loss | -9.79e-10 |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 554      |
|    iterations      | 1347     |
|    time_elapsed    | 9945     |
|    total_timesteps | 5517312  |
---------------------------------
Eval num_timesteps=5520000, episode_reward=0.50 +/- 0.67
Episode length: 289.60 +/- 52.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5520000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0507    |
|    n_updates            | 13470     |
|    policy_gradient_loss | -3.83e-10 |
|    value_loss           | 0.0615    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 554      |
|    iterations      | 1348     |
|    time_elapsed    | 9957     |
|    total_timesteps | 5521408  |
---------------------------------
Eval num_timesteps=5525000, episode_reward=0.10 +/- 0.30
Episode length: 272.40 +/- 43.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 272       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5525000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0232    |
|    n_updates            | 13480     |
|    policy_gradient_loss | -5.08e-10 |
|    value_loss           | 0.0552    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 554      |
|    iterations      | 1349     |
|    time_elapsed    | 9968     |
|    total_timesteps | 5525504  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 308       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 554       |
|    iterations           | 1350      |
|    time_elapsed         | 9976      |
|    total_timesteps      | 5529600   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0454    |
|    n_updates            | 13490     |
|    policy_gradient_loss | -1.05e-09 |
|    value_loss           | 0.0612    |
---------------------------------------
Eval num_timesteps=5530000, episode_reward=0.20 +/- 0.40
Episode length: 291.80 +/- 69.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5530000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0265    |
|    n_updates            | 13500     |
|    policy_gradient_loss | -1.39e-09 |
|    value_loss           | 0.0623    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 554      |
|    iterations      | 1351     |
|    time_elapsed    | 9987     |
|    total_timesteps | 5533696  |
---------------------------------
Eval num_timesteps=5535000, episode_reward=0.10 +/- 0.30
Episode length: 286.00 +/- 33.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5535000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0234    |
|    n_updates            | 13510     |
|    policy_gradient_loss | -7.57e-10 |
|    value_loss           | 0.0601    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 553      |
|    iterations      | 1352     |
|    time_elapsed    | 9998     |
|    total_timesteps | 5537792  |
---------------------------------
Eval num_timesteps=5540000, episode_reward=0.60 +/- 0.66
Episode length: 307.60 +/- 75.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5540000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0231    |
|    n_updates            | 13520     |
|    policy_gradient_loss | 5.24e-10  |
|    value_loss           | 0.0599    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 553      |
|    iterations      | 1353     |
|    time_elapsed    | 10009    |
|    total_timesteps | 5541888  |
---------------------------------
Eval num_timesteps=5545000, episode_reward=0.30 +/- 0.64
Episode length: 309.60 +/- 43.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5545000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000173 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0406    |
|    n_updates            | 13530     |
|    policy_gradient_loss | 5.86e-10  |
|    value_loss           | 0.0693    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 553      |
|    iterations      | 1354     |
|    time_elapsed    | 10020    |
|    total_timesteps | 5545984  |
---------------------------------
Eval num_timesteps=5550000, episode_reward=1.20 +/- 0.87
Episode length: 301.80 +/- 59.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 302           |
|    mean_reward          | 1.2           |
| time/                   |               |
|    total_timesteps      | 5550000       |
| train/                  |               |
|    approx_kl            | 3.5166042e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000256     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0278        |
|    n_updates            | 13540         |
|    policy_gradient_loss | -1.23e-05     |
|    value_loss           | 0.0672        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 553      |
|    iterations      | 1355     |
|    time_elapsed    | 10031    |
|    total_timesteps | 5550080  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.51      |
| time/                   |           |
|    fps                  | 553       |
|    iterations           | 1356      |
|    time_elapsed         | 10039     |
|    total_timesteps      | 5554176   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.032     |
|    n_updates            | 13550     |
|    policy_gradient_loss | -4.14e-10 |
|    value_loss           | 0.0619    |
---------------------------------------
Eval num_timesteps=5555000, episode_reward=0.10 +/- 0.30
Episode length: 288.80 +/- 28.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5555000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0464    |
|    n_updates            | 13560     |
|    policy_gradient_loss | -5.09e-10 |
|    value_loss           | 0.0695    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 553      |
|    iterations      | 1357     |
|    time_elapsed    | 10050    |
|    total_timesteps | 5558272  |
---------------------------------
Eval num_timesteps=5560000, episode_reward=0.50 +/- 0.67
Episode length: 307.00 +/- 45.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5560000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0184    |
|    n_updates            | 13570     |
|    policy_gradient_loss | 8.95e-11  |
|    value_loss           | 0.0634    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 552      |
|    iterations      | 1358     |
|    time_elapsed    | 10061    |
|    total_timesteps | 5562368  |
---------------------------------
Eval num_timesteps=5565000, episode_reward=0.10 +/- 0.30
Episode length: 283.40 +/- 60.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5565000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000264 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0283    |
|    n_updates            | 13580     |
|    policy_gradient_loss | -6.93e-10 |
|    value_loss           | 0.056     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 552      |
|    iterations      | 1359     |
|    time_elapsed    | 10072    |
|    total_timesteps | 5566464  |
---------------------------------
Eval num_timesteps=5570000, episode_reward=0.30 +/- 0.46
Episode length: 268.00 +/- 44.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 268           |
|    mean_reward          | 0.3           |
| time/                   |               |
|    total_timesteps      | 5570000       |
| train/                  |               |
|    approx_kl            | 3.8622093e-05 |
|    clip_fraction        | 0.00022       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000157     |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.01          |
|    loss                 | 0.0385        |
|    n_updates            | 13590         |
|    policy_gradient_loss | -3.25e-05     |
|    value_loss           | 0.0543        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 552      |
|    iterations      | 1360     |
|    time_elapsed    | 10083    |
|    total_timesteps | 5570560  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 302       |
|    ep_rew_mean          | 0.45      |
| time/                   |           |
|    fps                  | 552       |
|    iterations           | 1361      |
|    time_elapsed         | 10092     |
|    total_timesteps      | 5574656   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0399    |
|    n_updates            | 13600     |
|    policy_gradient_loss | 3.2e-11   |
|    value_loss           | 0.0627    |
---------------------------------------
Eval num_timesteps=5575000, episode_reward=0.70 +/- 0.64
Episode length: 302.00 +/- 60.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5575000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0165    |
|    n_updates            | 13610     |
|    policy_gradient_loss | -8.5e-10  |
|    value_loss           | 0.062     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 552      |
|    iterations      | 1362     |
|    time_elapsed    | 10103    |
|    total_timesteps | 5578752  |
---------------------------------
Eval num_timesteps=5580000, episode_reward=0.40 +/- 0.49
Episode length: 318.80 +/- 45.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 319       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5580000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00801   |
|    n_updates            | 13620     |
|    policy_gradient_loss | 4.13e-10  |
|    value_loss           | 0.0603    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 551      |
|    iterations      | 1363     |
|    time_elapsed    | 10114    |
|    total_timesteps | 5582848  |
---------------------------------
Eval num_timesteps=5585000, episode_reward=0.30 +/- 0.64
Episode length: 304.00 +/- 43.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5585000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0315    |
|    n_updates            | 13630     |
|    policy_gradient_loss | 9.78e-10  |
|    value_loss           | 0.0682    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 551      |
|    iterations      | 1364     |
|    time_elapsed    | 10125    |
|    total_timesteps | 5586944  |
---------------------------------
Eval num_timesteps=5590000, episode_reward=0.80 +/- 0.75
Episode length: 312.20 +/- 51.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 312       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5590000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0207    |
|    n_updates            | 13640     |
|    policy_gradient_loss | -3.04e-10 |
|    value_loss           | 0.0615    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 551      |
|    iterations      | 1365     |
|    time_elapsed    | 10137    |
|    total_timesteps | 5591040  |
---------------------------------
Eval num_timesteps=5595000, episode_reward=0.30 +/- 0.46
Episode length: 289.60 +/- 46.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 290       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5595000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0338    |
|    n_updates            | 13650     |
|    policy_gradient_loss | -1.18e-09 |
|    value_loss           | 0.0609    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 551      |
|    iterations      | 1366     |
|    time_elapsed    | 10148    |
|    total_timesteps | 5595136  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 306       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 551       |
|    iterations           | 1367      |
|    time_elapsed         | 10157     |
|    total_timesteps      | 5599232   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0346    |
|    n_updates            | 13660     |
|    policy_gradient_loss | 7.81e-10  |
|    value_loss           | 0.0584    |
---------------------------------------
Eval num_timesteps=5600000, episode_reward=0.60 +/- 0.66
Episode length: 308.20 +/- 59.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5600000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0189    |
|    n_updates            | 13670     |
|    policy_gradient_loss | -2.22e-10 |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 551      |
|    iterations      | 1368     |
|    time_elapsed    | 10168    |
|    total_timesteps | 5603328  |
---------------------------------
Eval num_timesteps=5605000, episode_reward=0.50 +/- 0.50
Episode length: 349.40 +/- 42.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 349       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5605000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0332    |
|    n_updates            | 13680     |
|    policy_gradient_loss | 3.01e-10  |
|    value_loss           | 0.0601    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 550      |
|    iterations      | 1369     |
|    time_elapsed    | 10180    |
|    total_timesteps | 5607424  |
---------------------------------
Eval num_timesteps=5610000, episode_reward=0.40 +/- 0.49
Episode length: 278.20 +/- 38.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 278       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5610000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000155 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0145    |
|    n_updates            | 13690     |
|    policy_gradient_loss | -3.71e-11 |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 550      |
|    iterations      | 1370     |
|    time_elapsed    | 10191    |
|    total_timesteps | 5611520  |
---------------------------------
Eval num_timesteps=5615000, episode_reward=0.30 +/- 0.64
Episode length: 291.00 +/- 37.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 291          |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 5615000      |
| train/                  |              |
|    approx_kl            | 6.756716e-06 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000125    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0369       |
|    n_updates            | 13700        |
|    policy_gradient_loss | -6.16e-07    |
|    value_loss           | 0.0658       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 550      |
|    iterations      | 1371     |
|    time_elapsed    | 10201    |
|    total_timesteps | 5615616  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 550       |
|    iterations           | 1372      |
|    time_elapsed         | 10210     |
|    total_timesteps      | 5619712   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000165 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0203    |
|    n_updates            | 13710     |
|    policy_gradient_loss | 2.95e-10  |
|    value_loss           | 0.059     |
---------------------------------------
Eval num_timesteps=5620000, episode_reward=0.30 +/- 0.64
Episode length: 313.40 +/- 44.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5620000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000165 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0168    |
|    n_updates            | 13720     |
|    policy_gradient_loss | 8e-12     |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 550      |
|    iterations      | 1373     |
|    time_elapsed    | 10221    |
|    total_timesteps | 5623808  |
---------------------------------
Eval num_timesteps=5625000, episode_reward=0.40 +/- 0.49
Episode length: 276.60 +/- 45.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 277       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5625000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000165 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0297    |
|    n_updates            | 13730     |
|    policy_gradient_loss | 5.82e-12  |
|    value_loss           | 0.0574    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 550      |
|    iterations      | 1374     |
|    time_elapsed    | 10232    |
|    total_timesteps | 5627904  |
---------------------------------
Eval num_timesteps=5630000, episode_reward=0.90 +/- 0.70
Episode length: 346.60 +/- 49.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 347       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 5630000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000165 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0655    |
|    n_updates            | 13740     |
|    policy_gradient_loss | 1.12e-10  |
|    value_loss           | 0.0679    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 549      |
|    iterations      | 1375     |
|    time_elapsed    | 10243    |
|    total_timesteps | 5632000  |
---------------------------------
Eval num_timesteps=5635000, episode_reward=0.50 +/- 0.50
Episode length: 306.80 +/- 59.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 307       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5635000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000165 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0232    |
|    n_updates            | 13750     |
|    policy_gradient_loss | -6.58e-10 |
|    value_loss           | 0.0596    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 549      |
|    iterations      | 1376     |
|    time_elapsed    | 10254    |
|    total_timesteps | 5636096  |
---------------------------------
Eval num_timesteps=5640000, episode_reward=0.60 +/- 0.66
Episode length: 312.80 +/- 61.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5640000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000165 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0182    |
|    n_updates            | 13760     |
|    policy_gradient_loss | -2.33e-11 |
|    value_loss           | 0.0567    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 549      |
|    iterations      | 1377     |
|    time_elapsed    | 10265    |
|    total_timesteps | 5640192  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 304          |
|    ep_rew_mean          | 0.45         |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 1378         |
|    time_elapsed         | 10274        |
|    total_timesteps      | 5644288      |
| train/                  |              |
|    approx_kl            | 6.706487e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000256    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.0383       |
|    n_updates            | 13770        |
|    policy_gradient_loss | 2.42e-08     |
|    value_loss           | 0.0676       |
------------------------------------------
Eval num_timesteps=5645000, episode_reward=0.40 +/- 0.66
Episode length: 296.40 +/- 46.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5645000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0285    |
|    n_updates            | 13780     |
|    policy_gradient_loss | -4.56e-10 |
|    value_loss           | 0.0574    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 549      |
|    iterations      | 1379     |
|    time_elapsed    | 10285    |
|    total_timesteps | 5648384  |
---------------------------------
Eval num_timesteps=5650000, episode_reward=1.00 +/- 0.89
Episode length: 308.00 +/- 53.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 1         |
| time/                   |           |
|    total_timesteps      | 5650000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0119    |
|    n_updates            | 13790     |
|    policy_gradient_loss | 1.48e-10  |
|    value_loss           | 0.0563    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 548      |
|    iterations      | 1380     |
|    time_elapsed    | 10296    |
|    total_timesteps | 5652480  |
---------------------------------
Eval num_timesteps=5655000, episode_reward=0.20 +/- 0.40
Episode length: 268.80 +/- 46.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 269       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5655000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0376    |
|    n_updates            | 13800     |
|    policy_gradient_loss | -2.04e-11 |
|    value_loss           | 0.0519    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 548      |
|    iterations      | 1381     |
|    time_elapsed    | 10307    |
|    total_timesteps | 5656576  |
---------------------------------
Eval num_timesteps=5660000, episode_reward=0.40 +/- 0.49
Episode length: 285.60 +/- 37.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5660000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0354    |
|    n_updates            | 13810     |
|    policy_gradient_loss | 4.44e-11  |
|    value_loss           | 0.0593    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 548      |
|    iterations      | 1382     |
|    time_elapsed    | 10318    |
|    total_timesteps | 5660672  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.32      |
| time/                   |           |
|    fps                  | 548       |
|    iterations           | 1383      |
|    time_elapsed         | 10326     |
|    total_timesteps      | 5664768   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0251    |
|    n_updates            | 13820     |
|    policy_gradient_loss | -6.15e-11 |
|    value_loss           | 0.0597    |
---------------------------------------
Eval num_timesteps=5665000, episode_reward=0.30 +/- 0.64
Episode length: 310.40 +/- 42.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5665000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0354    |
|    n_updates            | 13830     |
|    policy_gradient_loss | 2.84e-10  |
|    value_loss           | 0.0514    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.31     |
| time/              |          |
|    fps             | 548      |
|    iterations      | 1384     |
|    time_elapsed    | 10338    |
|    total_timesteps | 5668864  |
---------------------------------
Eval num_timesteps=5670000, episode_reward=0.30 +/- 0.46
Episode length: 314.40 +/- 43.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5670000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0157    |
|    n_updates            | 13840     |
|    policy_gradient_loss | -2.96e-10 |
|    value_loss           | 0.0596    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.32     |
| time/              |          |
|    fps             | 548      |
|    iterations      | 1385     |
|    time_elapsed    | 10349    |
|    total_timesteps | 5672960  |
---------------------------------
Eval num_timesteps=5675000, episode_reward=0.60 +/- 0.49
Episode length: 313.60 +/- 40.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5675000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0161    |
|    n_updates            | 13850     |
|    policy_gradient_loss | 7.58e-10  |
|    value_loss           | 0.0574    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.35     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 1386     |
|    time_elapsed    | 10360    |
|    total_timesteps | 5677056  |
---------------------------------
Eval num_timesteps=5680000, episode_reward=0.50 +/- 0.50
Episode length: 315.40 +/- 55.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5680000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0347    |
|    n_updates            | 13860     |
|    policy_gradient_loss | 1.32e-10  |
|    value_loss           | 0.0604    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 1387     |
|    time_elapsed    | 10372    |
|    total_timesteps | 5681152  |
---------------------------------
Eval num_timesteps=5685000, episode_reward=0.30 +/- 0.64
Episode length: 315.40 +/- 49.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 315       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5685000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0322    |
|    n_updates            | 13870     |
|    policy_gradient_loss | 4.44e-11  |
|    value_loss           | 0.0579    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 1388     |
|    time_elapsed    | 10383    |
|    total_timesteps | 5685248  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.39      |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 1389      |
|    time_elapsed         | 10391     |
|    total_timesteps      | 5689344   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0234    |
|    n_updates            | 13880     |
|    policy_gradient_loss | 1.04e-10  |
|    value_loss           | 0.0634    |
---------------------------------------
Eval num_timesteps=5690000, episode_reward=0.60 +/- 0.49
Episode length: 287.40 +/- 39.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5690000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.018     |
|    n_updates            | 13890     |
|    policy_gradient_loss | -7.93e-11 |
|    value_loss           | 0.0576    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 1390     |
|    time_elapsed    | 10402    |
|    total_timesteps | 5693440  |
---------------------------------
Eval num_timesteps=5695000, episode_reward=0.30 +/- 0.46
Episode length: 274.00 +/- 42.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 274       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5695000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0169    |
|    n_updates            | 13900     |
|    policy_gradient_loss | 4.43e-10  |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 547      |
|    iterations      | 1391     |
|    time_elapsed    | 10413    |
|    total_timesteps | 5697536  |
---------------------------------
Eval num_timesteps=5700000, episode_reward=0.30 +/- 0.46
Episode length: 286.40 +/- 53.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 286       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5700000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0465    |
|    n_updates            | 13910     |
|    policy_gradient_loss | 2.55e-10  |
|    value_loss           | 0.0569    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 306      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 1392     |
|    time_elapsed    | 10424    |
|    total_timesteps | 5701632  |
---------------------------------
Eval num_timesteps=5705000, episode_reward=0.30 +/- 0.46
Episode length: 274.60 +/- 53.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 275       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5705000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0362    |
|    n_updates            | 13920     |
|    policy_gradient_loss | 3.49e-10  |
|    value_loss           | 0.0594    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 308      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 546      |
|    iterations      | 1393     |
|    time_elapsed    | 10434    |
|    total_timesteps | 5705728  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 308       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 546       |
|    iterations           | 1394      |
|    time_elapsed         | 10443     |
|    total_timesteps      | 5709824   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0156    |
|    n_updates            | 13930     |
|    policy_gradient_loss | 3.38e-10  |
|    value_loss           | 0.0607    |
---------------------------------------
Eval num_timesteps=5710000, episode_reward=0.70 +/- 0.90
Episode length: 303.40 +/- 62.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5710000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0419    |
|    n_updates            | 13940     |
|    policy_gradient_loss | -5.84e-10 |
|    value_loss           | 0.0562    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 1395     |
|    time_elapsed    | 10454    |
|    total_timesteps | 5713920  |
---------------------------------
Eval num_timesteps=5715000, episode_reward=0.20 +/- 0.40
Episode length: 266.00 +/- 42.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 266       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5715000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0587    |
|    n_updates            | 13950     |
|    policy_gradient_loss | -2.5e-10  |
|    value_loss           | 0.0586    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 1396     |
|    time_elapsed    | 10465    |
|    total_timesteps | 5718016  |
---------------------------------
Eval num_timesteps=5720000, episode_reward=0.60 +/- 0.49
Episode length: 311.20 +/- 73.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5720000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0213    |
|    n_updates            | 13960     |
|    policy_gradient_loss | -1.2e-11  |
|    value_loss           | 0.0587    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.4      |
| time/              |          |
|    fps             | 546      |
|    iterations      | 1397     |
|    time_elapsed    | 10476    |
|    total_timesteps | 5722112  |
---------------------------------
Eval num_timesteps=5725000, episode_reward=0.10 +/- 0.30
Episode length: 270.20 +/- 36.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 270       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5725000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0201    |
|    n_updates            | 13970     |
|    policy_gradient_loss | 3.02e-10  |
|    value_loss           | 0.0623    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 546      |
|    iterations      | 1398     |
|    time_elapsed    | 10487    |
|    total_timesteps | 5726208  |
---------------------------------
Eval num_timesteps=5730000, episode_reward=0.40 +/- 0.49
Episode length: 310.00 +/- 65.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5730000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0245    |
|    n_updates            | 13980     |
|    policy_gradient_loss | 3.71e-10  |
|    value_loss           | 0.0615    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 1399     |
|    time_elapsed    | 10498    |
|    total_timesteps | 5730304  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 299       |
|    ep_rew_mean          | 0.42      |
| time/                   |           |
|    fps                  | 545       |
|    iterations           | 1400      |
|    time_elapsed         | 10506     |
|    total_timesteps      | 5734400   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0344    |
|    n_updates            | 13990     |
|    policy_gradient_loss | -1.32e-10 |
|    value_loss           | 0.0611    |
---------------------------------------
Eval num_timesteps=5735000, episode_reward=0.50 +/- 0.81
Episode length: 287.80 +/- 41.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5735000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000214 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0265    |
|    n_updates            | 14000     |
|    policy_gradient_loss | -6.36e-10 |
|    value_loss           | 0.0528    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 1401     |
|    time_elapsed    | 10517    |
|    total_timesteps | 5738496  |
---------------------------------
Eval num_timesteps=5740000, episode_reward=0.50 +/- 0.81
Episode length: 315.40 +/- 57.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 315           |
|    mean_reward          | 0.5           |
| time/                   |               |
|    total_timesteps      | 5740000       |
| train/                  |               |
|    approx_kl            | 1.0477379e-09 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000187     |
|    explained_variance   | 0             |
|    learning_rate        | 0.01          |
|    loss                 | 0.0219        |
|    n_updates            | 14010         |
|    policy_gradient_loss | 6.76e-07      |
|    value_loss           | 0.0719        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 1402     |
|    time_elapsed    | 10529    |
|    total_timesteps | 5742592  |
---------------------------------
Eval num_timesteps=5745000, episode_reward=0.40 +/- 0.66
Episode length: 293.60 +/- 42.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5745000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0491    |
|    n_updates            | 14020     |
|    policy_gradient_loss | 2.98e-11  |
|    value_loss           | 0.0587    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 1403     |
|    time_elapsed    | 10540    |
|    total_timesteps | 5746688  |
---------------------------------
Eval num_timesteps=5750000, episode_reward=0.30 +/- 0.46
Episode length: 291.80 +/- 66.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5750000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.049     |
|    n_updates            | 14030     |
|    policy_gradient_loss | 1.75e-10  |
|    value_loss           | 0.0673    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 545      |
|    iterations      | 1404     |
|    time_elapsed    | 10551    |
|    total_timesteps | 5750784  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.54      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 1405      |
|    time_elapsed         | 10559     |
|    total_timesteps      | 5754880   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0397    |
|    n_updates            | 14040     |
|    policy_gradient_loss | -1.45e-10 |
|    value_loss           | 0.0689    |
---------------------------------------
Eval num_timesteps=5755000, episode_reward=0.40 +/- 0.49
Episode length: 283.40 +/- 41.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5755000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.033     |
|    n_updates            | 14050     |
|    policy_gradient_loss | -7.49e-10 |
|    value_loss           | 0.067     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 1406     |
|    time_elapsed    | 10570    |
|    total_timesteps | 5758976  |
---------------------------------
Eval num_timesteps=5760000, episode_reward=0.40 +/- 0.49
Episode length: 300.80 +/- 54.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5760000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0245    |
|    n_updates            | 14060     |
|    policy_gradient_loss | 3.11e-10  |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 1407     |
|    time_elapsed    | 10581    |
|    total_timesteps | 5763072  |
---------------------------------
Eval num_timesteps=5765000, episode_reward=0.60 +/- 0.49
Episode length: 303.20 +/- 49.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5765000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0331    |
|    n_updates            | 14070     |
|    policy_gradient_loss | -1.02e-09 |
|    value_loss           | 0.0569    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 1408     |
|    time_elapsed    | 10592    |
|    total_timesteps | 5767168  |
---------------------------------
Eval num_timesteps=5770000, episode_reward=0.30 +/- 0.64
Episode length: 293.60 +/- 47.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5770000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0282    |
|    n_updates            | 14080     |
|    policy_gradient_loss | -3.85e-10 |
|    value_loss           | 0.0578    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 298      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 1409     |
|    time_elapsed    | 10603    |
|    total_timesteps | 5771264  |
---------------------------------
Eval num_timesteps=5775000, episode_reward=0.20 +/- 0.40
Episode length: 284.40 +/- 46.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 284       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5775000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0324    |
|    n_updates            | 14090     |
|    policy_gradient_loss | -3.41e-10 |
|    value_loss           | 0.0664    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 544      |
|    iterations      | 1410     |
|    time_elapsed    | 10614    |
|    total_timesteps | 5775360  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 304       |
|    ep_rew_mean          | 0.43      |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 1411      |
|    time_elapsed         | 10623     |
|    total_timesteps      | 5779456   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00569   |
|    n_updates            | 14100     |
|    policy_gradient_loss | 1.58e-10  |
|    value_loss           | 0.0543    |
---------------------------------------
Eval num_timesteps=5780000, episode_reward=0.40 +/- 0.49
Episode length: 281.40 +/- 49.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 281       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5780000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0262    |
|    n_updates            | 14110     |
|    policy_gradient_loss | 4.48e-10  |
|    value_loss           | 0.0563    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1412     |
|    time_elapsed    | 10634    |
|    total_timesteps | 5783552  |
---------------------------------
Eval num_timesteps=5785000, episode_reward=0.00 +/- 0.00
Episode length: 293.40 +/- 55.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 293       |
|    mean_reward          | 0         |
| time/                   |           |
|    total_timesteps      | 5785000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0209    |
|    n_updates            | 14120     |
|    policy_gradient_loss | -1.31e-10 |
|    value_loss           | 0.0637    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1413     |
|    time_elapsed    | 10645    |
|    total_timesteps | 5787648  |
---------------------------------
Eval num_timesteps=5790000, episode_reward=0.70 +/- 0.64
Episode length: 332.20 +/- 66.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 332       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5790000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0377    |
|    n_updates            | 14130     |
|    policy_gradient_loss | -6.69e-10 |
|    value_loss           | 0.0678    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1414     |
|    time_elapsed    | 10656    |
|    total_timesteps | 5791744  |
---------------------------------
Eval num_timesteps=5795000, episode_reward=0.20 +/- 0.40
Episode length: 288.00 +/- 45.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 288       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5795000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0432    |
|    n_updates            | 14140     |
|    policy_gradient_loss | -3.27e-10 |
|    value_loss           | 0.0562    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1415     |
|    time_elapsed    | 10667    |
|    total_timesteps | 5795840  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.44      |
| time/                   |           |
|    fps                  | 543       |
|    iterations           | 1416      |
|    time_elapsed         | 10675     |
|    total_timesteps      | 5799936   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0574    |
|    n_updates            | 14150     |
|    policy_gradient_loss | -3.91e-10 |
|    value_loss           | 0.0596    |
---------------------------------------
Eval num_timesteps=5800000, episode_reward=0.10 +/- 0.30
Episode length: 308.80 +/- 44.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.1       |
| time/                   |           |
|    total_timesteps      | 5800000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0184    |
|    n_updates            | 14160     |
|    policy_gradient_loss | -7.16e-10 |
|    value_loss           | 0.0558    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1417     |
|    time_elapsed    | 10687    |
|    total_timesteps | 5804032  |
---------------------------------
Eval num_timesteps=5805000, episode_reward=0.20 +/- 0.40
Episode length: 303.80 +/- 52.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 304       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5805000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0231    |
|    n_updates            | 14170     |
|    policy_gradient_loss | 2.74e-10  |
|    value_loss           | 0.0649    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1418     |
|    time_elapsed    | 10698    |
|    total_timesteps | 5808128  |
---------------------------------
Eval num_timesteps=5810000, episode_reward=0.90 +/- 0.94
Episode length: 315.80 +/- 57.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 5810000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0187    |
|    n_updates            | 14180     |
|    policy_gradient_loss | 1.56e-10  |
|    value_loss           | 0.0697    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1419     |
|    time_elapsed    | 10709    |
|    total_timesteps | 5812224  |
---------------------------------
Eval num_timesteps=5815000, episode_reward=0.70 +/- 0.64
Episode length: 280.20 +/- 30.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 280       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5815000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.018     |
|    n_updates            | 14190     |
|    policy_gradient_loss | 1.86e-10  |
|    value_loss           | 0.0637    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1420     |
|    time_elapsed    | 10720    |
|    total_timesteps | 5816320  |
---------------------------------
Eval num_timesteps=5820000, episode_reward=0.40 +/- 0.49
Episode length: 320.20 +/- 57.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 320       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5820000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0379    |
|    n_updates            | 14200     |
|    policy_gradient_loss | -8.77e-10 |
|    value_loss           | 0.063     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1421     |
|    time_elapsed    | 10731    |
|    total_timesteps | 5820416  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 298       |
|    ep_rew_mean          | 0.54      |
| time/                   |           |
|    fps                  | 542       |
|    iterations           | 1422      |
|    time_elapsed         | 10740     |
|    total_timesteps      | 5824512   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0528    |
|    n_updates            | 14210     |
|    policy_gradient_loss | -6.14e-10 |
|    value_loss           | 0.0695    |
---------------------------------------
Eval num_timesteps=5825000, episode_reward=0.80 +/- 0.98
Episode length: 334.60 +/- 59.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 335       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5825000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0316    |
|    n_updates            | 14220     |
|    policy_gradient_loss | 4.98e-10  |
|    value_loss           | 0.061     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1423     |
|    time_elapsed    | 10751    |
|    total_timesteps | 5828608  |
---------------------------------
Eval num_timesteps=5830000, episode_reward=0.70 +/- 0.78
Episode length: 321.60 +/- 57.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 322       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5830000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0342    |
|    n_updates            | 14230     |
|    policy_gradient_loss | -1.95e-10 |
|    value_loss           | 0.0529    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1424     |
|    time_elapsed    | 10762    |
|    total_timesteps | 5832704  |
---------------------------------
Eval num_timesteps=5835000, episode_reward=0.20 +/- 0.60
Episode length: 284.00 +/- 38.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 284       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5835000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0336    |
|    n_updates            | 14240     |
|    policy_gradient_loss | 3.78e-10  |
|    value_loss           | 0.0661    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1425     |
|    time_elapsed    | 10773    |
|    total_timesteps | 5836800  |
---------------------------------
Eval num_timesteps=5840000, episode_reward=0.50 +/- 0.50
Episode length: 311.40 +/- 51.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 311       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5840000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0318    |
|    n_updates            | 14250     |
|    policy_gradient_loss | -8.22e-11 |
|    value_loss           | 0.0621    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1426     |
|    time_elapsed    | 10784    |
|    total_timesteps | 5840896  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 541       |
|    iterations           | 1427      |
|    time_elapsed         | 10793     |
|    total_timesteps      | 5844992   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0297    |
|    n_updates            | 14260     |
|    policy_gradient_loss | -7.18e-10 |
|    value_loss           | 0.0628    |
---------------------------------------
Eval num_timesteps=5845000, episode_reward=0.50 +/- 0.67
Episode length: 300.60 +/- 68.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 301       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5845000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0284    |
|    n_updates            | 14270     |
|    policy_gradient_loss | -3.45e-10 |
|    value_loss           | 0.0629    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1428     |
|    time_elapsed    | 10804    |
|    total_timesteps | 5849088  |
---------------------------------
Eval num_timesteps=5850000, episode_reward=0.50 +/- 0.50
Episode length: 306.40 +/- 59.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5850000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00969   |
|    n_updates            | 14280     |
|    policy_gradient_loss | -5.78e-10 |
|    value_loss           | 0.0565    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1429     |
|    time_elapsed    | 10813    |
|    total_timesteps | 5853184  |
---------------------------------
Eval num_timesteps=5855000, episode_reward=0.40 +/- 0.49
Episode length: 291.80 +/- 34.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5855000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0255    |
|    n_updates            | 14290     |
|    policy_gradient_loss | 7.85e-10  |
|    value_loss           | 0.0637    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 295      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1430     |
|    time_elapsed    | 10821    |
|    total_timesteps | 5857280  |
---------------------------------
Eval num_timesteps=5860000, episode_reward=0.30 +/- 0.46
Episode length: 287.40 +/- 59.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5860000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0175    |
|    n_updates            | 14300     |
|    policy_gradient_loss | 3.41e-10  |
|    value_loss           | 0.0691    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 291      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1431     |
|    time_elapsed    | 10828    |
|    total_timesteps | 5861376  |
---------------------------------
Eval num_timesteps=5865000, episode_reward=0.40 +/- 0.49
Episode length: 306.40 +/- 43.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5865000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0184    |
|    n_updates            | 14310     |
|    policy_gradient_loss | 1.91e-10  |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1432     |
|    time_elapsed    | 10835    |
|    total_timesteps | 5865472  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 293       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 541       |
|    iterations           | 1433      |
|    time_elapsed         | 10839     |
|    total_timesteps      | 5869568   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0267    |
|    n_updates            | 14320     |
|    policy_gradient_loss | 3.58e-10  |
|    value_loss           | 0.0586    |
---------------------------------------
Eval num_timesteps=5870000, episode_reward=0.80 +/- 0.75
Episode length: 312.80 +/- 64.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5870000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0218    |
|    n_updates            | 14330     |
|    policy_gradient_loss | 5.88e-10  |
|    value_loss           | 0.0661    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 293      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1434     |
|    time_elapsed    | 10845    |
|    total_timesteps | 5873664  |
---------------------------------
Eval num_timesteps=5875000, episode_reward=0.80 +/- 0.60
Episode length: 315.60 +/- 60.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5875000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.00908   |
|    n_updates            | 14340     |
|    policy_gradient_loss | 8.31e-10  |
|    value_loss           | 0.0567    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 296      |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1435     |
|    time_elapsed    | 10852    |
|    total_timesteps | 5877760  |
---------------------------------
Eval num_timesteps=5880000, episode_reward=0.20 +/- 0.40
Episode length: 289.00 +/- 45.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5880000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0087    |
|    n_updates            | 14350     |
|    policy_gradient_loss | 5.86e-10  |
|    value_loss           | 0.0583    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1436     |
|    time_elapsed    | 10858    |
|    total_timesteps | 5881856  |
---------------------------------
Eval num_timesteps=5885000, episode_reward=0.80 +/- 0.60
Episode length: 313.20 +/- 42.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 313       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5885000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0348    |
|    n_updates            | 14360     |
|    policy_gradient_loss | -3.55e-10 |
|    value_loss           | 0.0596    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1437     |
|    time_elapsed    | 10864    |
|    total_timesteps | 5885952  |
---------------------------------
Eval num_timesteps=5890000, episode_reward=1.10 +/- 0.54
Episode length: 352.20 +/- 45.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 352       |
|    mean_reward          | 1.1       |
| time/                   |           |
|    total_timesteps      | 5890000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.01      |
|    loss                 | 0.0187    |
|    n_updates            | 14370     |
|    policy_gradient_loss | 1.29e-09  |
|    value_loss           | 0.0664    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1438     |
|    time_elapsed    | 10871    |
|    total_timesteps | 5890048  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 301       |
|    ep_rew_mean          | 0.5       |
| time/                   |           |
|    fps                  | 541       |
|    iterations           | 1439      |
|    time_elapsed         | 10876     |
|    total_timesteps      | 5894144   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.031     |
|    n_updates            | 14380     |
|    policy_gradient_loss | -5.76e-10 |
|    value_loss           | 0.0629    |
---------------------------------------
Eval num_timesteps=5895000, episode_reward=0.50 +/- 0.50
Episode length: 308.40 +/- 66.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 308       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5895000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0486    |
|    n_updates            | 14390     |
|    policy_gradient_loss | -9.82e-11 |
|    value_loss           | 0.0564    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 541      |
|    iterations      | 1440     |
|    time_elapsed    | 10882    |
|    total_timesteps | 5898240  |
---------------------------------
Eval num_timesteps=5900000, episode_reward=0.40 +/- 0.49
Episode length: 290.60 +/- 38.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 291       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5900000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0311    |
|    n_updates            | 14400     |
|    policy_gradient_loss | 4.85e-10  |
|    value_loss           | 0.0647    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1441     |
|    time_elapsed    | 10888    |
|    total_timesteps | 5902336  |
---------------------------------
Eval num_timesteps=5905000, episode_reward=0.40 +/- 0.66
Episode length: 298.20 +/- 62.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 298       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5905000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0411    |
|    n_updates            | 14410     |
|    policy_gradient_loss | 9.95e-10  |
|    value_loss           | 0.0628    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1442     |
|    time_elapsed    | 10895    |
|    total_timesteps | 5906432  |
---------------------------------
Eval num_timesteps=5910000, episode_reward=0.30 +/- 0.64
Episode length: 309.20 +/- 52.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 309       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5910000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0142    |
|    n_updates            | 14420     |
|    policy_gradient_loss | -4.98e-10 |
|    value_loss           | 0.0566    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1443     |
|    time_elapsed    | 10901    |
|    total_timesteps | 5910528  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 300       |
|    ep_rew_mean          | 0.45      |
| time/                   |           |
|    fps                  | 542       |
|    iterations           | 1444      |
|    time_elapsed         | 10905     |
|    total_timesteps      | 5914624   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0252    |
|    n_updates            | 14430     |
|    policy_gradient_loss | 4.61e-10  |
|    value_loss           | 0.0558    |
---------------------------------------
Eval num_timesteps=5915000, episode_reward=0.90 +/- 0.70
Episode length: 310.00 +/- 56.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 310       |
|    mean_reward          | 0.9       |
| time/                   |           |
|    total_timesteps      | 5915000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000176 |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.01      |
|    loss                 | 0.0416    |
|    n_updates            | 14440     |
|    policy_gradient_loss | 1.46e-10  |
|    value_loss           | 0.0617    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 300      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1445     |
|    time_elapsed    | 10912    |
|    total_timesteps | 5918720  |
---------------------------------
Eval num_timesteps=5920000, episode_reward=0.70 +/- 0.78
Episode length: 313.20 +/- 53.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 313           |
|    mean_reward          | 0.7           |
| time/                   |               |
|    total_timesteps      | 5920000       |
| train/                  |               |
|    approx_kl            | 1.2478267e-06 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000202     |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.01          |
|    loss                 | 0.0159        |
|    n_updates            | 14450         |
|    policy_gradient_loss | 4.72e-06      |
|    value_loss           | 0.0657        |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.46     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1446     |
|    time_elapsed    | 10918    |
|    total_timesteps | 5922816  |
---------------------------------
Eval num_timesteps=5925000, episode_reward=0.50 +/- 0.81
Episode length: 316.60 +/- 33.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 317       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5925000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000177 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0226    |
|    n_updates            | 14460     |
|    policy_gradient_loss | -4.68e-10 |
|    value_loss           | 0.0569    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1447     |
|    time_elapsed    | 10925    |
|    total_timesteps | 5926912  |
---------------------------------
Eval num_timesteps=5930000, episode_reward=0.70 +/- 0.78
Episode length: 306.00 +/- 69.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 306       |
|    mean_reward          | 0.7       |
| time/                   |           |
|    total_timesteps      | 5930000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000177 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0318    |
|    n_updates            | 14470     |
|    policy_gradient_loss | -2.14e-10 |
|    value_loss           | 0.0607    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1448     |
|    time_elapsed    | 10931    |
|    total_timesteps | 5931008  |
---------------------------------
Eval num_timesteps=5935000, episode_reward=0.50 +/- 0.81
Episode length: 294.20 +/- 47.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 294       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5935000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000177 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0226    |
|    n_updates            | 14480     |
|    policy_gradient_loss | -3.41e-10 |
|    value_loss           | 0.0569    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1449     |
|    time_elapsed    | 10938    |
|    total_timesteps | 5935104  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 306       |
|    ep_rew_mean          | 0.48      |
| time/                   |           |
|    fps                  | 542       |
|    iterations           | 1450      |
|    time_elapsed         | 10942     |
|    total_timesteps      | 5939200   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000177 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0114    |
|    n_updates            | 14490     |
|    policy_gradient_loss | -6.26e-10 |
|    value_loss           | 0.063     |
---------------------------------------
Eval num_timesteps=5940000, episode_reward=0.80 +/- 0.75
Episode length: 345.40 +/- 54.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 345       |
|    mean_reward          | 0.8       |
| time/                   |           |
|    total_timesteps      | 5940000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000177 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0266    |
|    n_updates            | 14500     |
|    policy_gradient_loss | 1.13e-09  |
|    value_loss           | 0.0598    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1451     |
|    time_elapsed    | 10949    |
|    total_timesteps | 5943296  |
---------------------------------
Eval num_timesteps=5945000, episode_reward=0.40 +/- 0.66
Episode length: 285.20 +/- 48.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 285       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5945000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000177 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0146    |
|    n_updates            | 14510     |
|    policy_gradient_loss | -2.58e-10 |
|    value_loss           | 0.0649    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.43     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1452     |
|    time_elapsed    | 10955    |
|    total_timesteps | 5947392  |
---------------------------------
Eval num_timesteps=5950000, episode_reward=0.90 +/- 0.83
Episode length: 349.80 +/- 69.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 350          |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 5950000      |
| train/                  |              |
|    approx_kl            | 9.120864e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000208    |
|    explained_variance   | 0            |
|    learning_rate        | 0.01         |
|    loss                 | 0.035        |
|    n_updates            | 14520        |
|    policy_gradient_loss | 4.88e-08     |
|    value_loss           | 0.0595       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1453     |
|    time_elapsed    | 10961    |
|    total_timesteps | 5951488  |
---------------------------------
Eval num_timesteps=5955000, episode_reward=0.30 +/- 0.64
Episode length: 288.60 +/- 49.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 289       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5955000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0349    |
|    n_updates            | 14530     |
|    policy_gradient_loss | 3.19e-10  |
|    value_loss           | 0.0642    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 542      |
|    iterations      | 1454     |
|    time_elapsed    | 10968    |
|    total_timesteps | 5955584  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.53      |
| time/                   |           |
|    fps                  | 543       |
|    iterations           | 1455      |
|    time_elapsed         | 10972     |
|    total_timesteps      | 5959680   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0351    |
|    n_updates            | 14540     |
|    policy_gradient_loss | 5.46e-10  |
|    value_loss           | 0.0646    |
---------------------------------------
Eval num_timesteps=5960000, episode_reward=0.40 +/- 0.49
Episode length: 291.60 +/- 44.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 292       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5960000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0407    |
|    n_updates            | 14550     |
|    policy_gradient_loss | -9.39e-10 |
|    value_loss           | 0.0698    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1456     |
|    time_elapsed    | 10978    |
|    total_timesteps | 5963776  |
---------------------------------
Eval num_timesteps=5965000, episode_reward=0.40 +/- 0.49
Episode length: 283.40 +/- 46.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 283       |
|    mean_reward          | 0.4       |
| time/                   |           |
|    total_timesteps      | 5965000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0486    |
|    n_updates            | 14560     |
|    policy_gradient_loss | 6.02e-10  |
|    value_loss           | 0.0577    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 301      |
|    ep_rew_mean     | 0.51     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1457     |
|    time_elapsed    | 10985    |
|    total_timesteps | 5967872  |
---------------------------------
Eval num_timesteps=5970000, episode_reward=0.60 +/- 0.80
Episode length: 305.00 +/- 44.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 305       |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 5970000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0326    |
|    n_updates            | 14570     |
|    policy_gradient_loss | -5.53e-11 |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1458     |
|    time_elapsed    | 10991    |
|    total_timesteps | 5971968  |
---------------------------------
Eval num_timesteps=5975000, episode_reward=0.20 +/- 0.40
Episode length: 300.20 +/- 47.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 300       |
|    mean_reward          | 0.2       |
| time/                   |           |
|    total_timesteps      | 5975000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0359    |
|    n_updates            | 14580     |
|    policy_gradient_loss | -1.54e-10 |
|    value_loss           | 0.0602    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1459     |
|    time_elapsed    | 10997    |
|    total_timesteps | 5976064  |
---------------------------------
Eval num_timesteps=5980000, episode_reward=0.30 +/- 0.46
Episode length: 282.00 +/- 53.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 282       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 5980000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0213    |
|    n_updates            | 14590     |
|    policy_gradient_loss | 5.81e-10  |
|    value_loss           | 0.0608    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 305      |
|    ep_rew_mean     | 0.53     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1460     |
|    time_elapsed    | 11004    |
|    total_timesteps | 5980160  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 303       |
|    ep_rew_mean          | 0.47      |
| time/                   |           |
|    fps                  | 543       |
|    iterations           | 1461      |
|    time_elapsed         | 11008     |
|    total_timesteps      | 5984256   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.022     |
|    n_updates            | 14600     |
|    policy_gradient_loss | -5.17e-11 |
|    value_loss           | 0.0591    |
---------------------------------------
Eval num_timesteps=5985000, episode_reward=0.50 +/- 0.92
Episode length: 314.20 +/- 51.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 314       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5985000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0292    |
|    n_updates            | 14610     |
|    policy_gradient_loss | 8.09e-10  |
|    value_loss           | 0.062     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.45     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1462     |
|    time_elapsed    | 11014    |
|    total_timesteps | 5988352  |
---------------------------------
Eval num_timesteps=5990000, episode_reward=0.50 +/- 0.67
Episode length: 301.80 +/- 67.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 302       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5990000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0539    |
|    n_updates            | 14620     |
|    policy_gradient_loss | 3.63e-10  |
|    value_loss           | 0.066     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1463     |
|    time_elapsed    | 11021    |
|    total_timesteps | 5992448  |
---------------------------------
Eval num_timesteps=5995000, episode_reward=0.50 +/- 0.50
Episode length: 316.00 +/- 62.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 316       |
|    mean_reward          | 0.5       |
| time/                   |           |
|    total_timesteps      | 5995000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0274    |
|    n_updates            | 14630     |
|    policy_gradient_loss | -2.26e-10 |
|    value_loss           | 0.0548    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1464     |
|    time_elapsed    | 11027    |
|    total_timesteps | 5996544  |
---------------------------------
Eval num_timesteps=6000000, episode_reward=0.30 +/- 0.46
Episode length: 296.20 +/- 52.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 0.3       |
| time/                   |           |
|    total_timesteps      | 6000000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0         |
|    learning_rate        | 0.01      |
|    loss                 | 0.0392    |
|    n_updates            | 14640     |
|    policy_gradient_loss | 6.35e-10  |
|    value_loss           | 0.0627    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 543      |
|    iterations      | 1465     |
|    time_elapsed    | 11034    |
|    total_timesteps | 6000640  |
---------------------------------
