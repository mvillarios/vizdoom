/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 31       |
|    iterations      | 1        |
|    time_elapsed    | 65       |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 2500          |
| train/                  |               |
|    approx_kl            | 0.00025921472 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | -5            |
|    learning_rate        | 1e-05         |
|    loss                 | 0.000936      |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.000762     |
|    value_loss           | 0.00406       |
-------------------------------------------
Eval num_timesteps=3000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-0.16 +/- 0.24
Episode length: 504.52 +/- 100.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 2        |
|    time_elapsed    | 132      |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 0.0006318106 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -2.99        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00172      |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00131     |
|    value_loss           | 0.00161      |
------------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 3        |
|    time_elapsed    | 199      |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0015970813 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -4.53        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00343      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 0.0011       |
------------------------------------------
Eval num_timesteps=7000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 4        |
|    time_elapsed    | 265      |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0010518165 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -3.35        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0064       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 0.00067      |
------------------------------------------
Eval num_timesteps=9000, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-0.19 +/- 0.17
Episode length: 515.30 +/- 67.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.19 +/- 0.16
Episode length: 516.72 +/- 57.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 5        |
|    time_elapsed    | 331      |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0008668634 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -7.19        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000599    |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 0.00104      |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 6        |
|    time_elapsed    | 398      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 12500         |
| train/                  |               |
|    approx_kl            | 0.00071230903 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | -6.47         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.0113       |
|    n_updates            | 60            |
|    policy_gradient_loss | -0.00158      |
|    value_loss           | 0.000771      |
-------------------------------------------
Eval num_timesteps=13000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-0.16 +/- 0.23
Episode length: 506.00 +/- 93.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 7        |
|    time_elapsed    | 464      |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 14500         |
| train/                  |               |
|    approx_kl            | 0.00042768157 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | -6.52         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.011        |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.000741     |
|    value_loss           | 0.000526      |
-------------------------------------------
Eval num_timesteps=15000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 525      |
|    ep_rew_mean     | -0.21    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 8        |
|    time_elapsed    | 530      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0006870866 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -2.93        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00561      |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 0.000659     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.149   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 9        |
|    time_elapsed    | 596      |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 18500         |
| train/                  |               |
|    approx_kl            | 0.00031365306 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | -0.0851       |
|    learning_rate        | 1e-05         |
|    loss                 | -0.000297     |
|    n_updates            | 90            |
|    policy_gradient_loss | -0.000552     |
|    value_loss           | 0.00881       |
-------------------------------------------
Eval num_timesteps=19000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.155   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 10       |
|    time_elapsed    | 663      |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.004224735 |
|    clip_fraction        | 0.00786     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.79       |
|    explained_variance   | -2.54       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0249     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 0.000975    |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.135   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 11       |
|    time_elapsed    | 746      |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 23000         |
| train/                  |               |
|    approx_kl            | 0.00021979006 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.78         |
|    explained_variance   | 0.0182        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.000876      |
|    n_updates            | 110           |
|    policy_gradient_loss | -0.000401     |
|    value_loss           | 0.0044        |
-------------------------------------------
Eval num_timesteps=23500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 12       |
|    time_elapsed    | 813      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0020488747 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.0408       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00209      |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 0.00462      |
------------------------------------------
Eval num_timesteps=25500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.104   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 13       |
|    time_elapsed    | 878      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0023583353 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.24         |
|    learning_rate        | 1e-05        |
|    loss                 | -7.89e-05    |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 0.00416      |
------------------------------------------
Eval num_timesteps=27500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-0.14 +/- 0.29
Episode length: 494.52 +/- 120.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
New best mean reward!
Eval num_timesteps=28500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.112   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 14       |
|    time_elapsed    | 944      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0011250459 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.224        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00858      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000797    |
|    value_loss           | 0.000659     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.118   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 15       |
|    time_elapsed    | 1010     |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0016738756 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -0.309       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0059      |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 0.000686     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-0.14 +/- 0.29
Episode length: 494.04 +/- 122.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
New best mean reward!
Eval num_timesteps=32000, episode_reward=-0.19 +/- 0.16
Episode length: 517.80 +/- 50.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.124   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 16       |
|    time_elapsed    | 1076     |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0014250043 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -0.148       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00502      |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00124     |
|    value_loss           | 0.000361     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.098   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 17       |
|    time_elapsed    | 1141     |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.16 +/- 0.24
Episode length: 504.60 +/- 99.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0009494781 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.0254       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00296      |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000695    |
|    value_loss           | 0.00801      |
------------------------------------------
Eval num_timesteps=35500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.104   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 18       |
|    time_elapsed    | 1207     |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.007838102 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.216       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00168     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 0.000972    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 19       |
|    time_elapsed    | 1274     |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0048947474 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | -1.36        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0116       |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 0.000747     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-0.14 +/- 0.29
Episode length: 494.20 +/- 121.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 30       |
|    iterations      | 20       |
|    time_elapsed    | 1340     |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 41000        |
| train/                  |              |
|    approx_kl            | 0.0030460441 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.0596       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00427      |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00221     |
|    value_loss           | 0.00444      |
------------------------------------------
Eval num_timesteps=41500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0928  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 21       |
|    time_elapsed    | 1422     |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=-0.19 +/- 0.17
Episode length: 515.60 +/- 65.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.003756118 |
|    clip_fraction        | 0.00469     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.211       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00532     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 0.00382     |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0754  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 22       |
|    time_elapsed    | 1488     |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0070102746 |
|    clip_fraction        | 0.0548       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.371        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00448      |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00356     |
|    value_loss           | 0.00714      |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-0.11 +/- 0.33
Episode length: 483.82 +/- 139.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0699  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 23       |
|    time_elapsed    | 1554     |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 47500        |
| train/                  |              |
|    approx_kl            | 0.0038832605 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.76        |
|    explained_variance   | 0.242        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00225      |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00289     |
|    value_loss           | 0.00395      |
------------------------------------------
Eval num_timesteps=48000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-0.16 +/- 0.24
Episode length: 504.58 +/- 100.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0536  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 24       |
|    time_elapsed    | 1620     |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0063642175 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.74        |
|    explained_variance   | 0.204        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00526      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00401     |
|    value_loss           | 0.00728      |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-0.14 +/- 0.29
Episode length: 494.26 +/- 121.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0307  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 25       |
|    time_elapsed    | 1685     |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=-0.14 +/- 0.29
Episode length: 494.18 +/- 121.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 0.0043904725 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.73        |
|    explained_variance   | 0.208        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000995    |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00262     |
|    value_loss           | 0.00667      |
------------------------------------------
Eval num_timesteps=52000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0307  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 26       |
|    time_elapsed    | 1750     |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.006745329 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | -0.444      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0159     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 0.00102     |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 477      |
|    ep_rew_mean     | -0.0307  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 27       |
|    time_elapsed    | 1816     |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.005041292 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.0703      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0285     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 0.00117     |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-0.16 +/- 0.23
Episode length: 505.94 +/- 93.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 0.00244  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 28       |
|    time_elapsed    | 1882     |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0051512695 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.68        |
|    explained_variance   | 0.162        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0151       |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00237     |
|    value_loss           | 0.00878      |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | 0.00244  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 29       |
|    time_elapsed    | 1948     |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0075672977 |
|    clip_fraction        | 0.0587       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.64        |
|    explained_variance   | -0.263       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0112      |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00964     |
|    value_loss           | 0.000887     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | 0.014    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 30       |
|    time_elapsed    | 2015     |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.012125086 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.124       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0107      |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 0.004       |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-0.14 +/- 0.28
Episode length: 497.10 +/- 111.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 497      |
|    mean_reward     | -0.139   |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | 0.0157   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 31       |
|    time_elapsed    | 2080     |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-0.14 +/- 0.29
Episode length: 494.36 +/- 121.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 494         |
|    mean_reward          | -0.138      |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.013008831 |
|    clip_fraction        | 0.0816      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.139       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00937    |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 0.00752     |
-----------------------------------------
Eval num_timesteps=64000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-0.19 +/- 0.16
Episode length: 517.08 +/- 55.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 459      |
|    ep_rew_mean     | 0.0263   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 32       |
|    time_elapsed    | 2161     |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0025884588 |
|    clip_fraction        | 0.0063       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.279        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00381      |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 0.00405      |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 463      |
|    ep_rew_mean     | 0.0147   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 33       |
|    time_elapsed    | 2227     |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 525        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 68000      |
| train/                  |            |
|    approx_kl            | 0.01152133 |
|    clip_fraction        | 0.052      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.426      |
|    learning_rate        | 1e-05      |
|    loss                 | 0.0177     |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.00215   |
|    value_loss           | 0.000485   |
----------------------------------------
Eval num_timesteps=68500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 463      |
|    ep_rew_mean     | 0.0147   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 34       |
|    time_elapsed    | 2294     |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.006952244 |
|    clip_fraction        | 0.053       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.181       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0061     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0062     |
|    value_loss           | 0.000433    |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | 0.0141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 35       |
|    time_elapsed    | 2359     |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0022396662 |
|    clip_fraction        | 0.00786      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.184        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00104     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 0.00416      |
------------------------------------------
Eval num_timesteps=72500, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | 0.0139   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 36       |
|    time_elapsed    | 2424     |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0058255694 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.126        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00559     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00431     |
|    value_loss           | 0.00385      |
------------------------------------------
Eval num_timesteps=74500, episode_reward=-0.14 +/- 0.29
Episode length: 494.06 +/- 122.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 465      |
|    ep_rew_mean     | 0.0239   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 37       |
|    time_elapsed    | 2490     |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0021112121 |
|    clip_fraction        | 0.00967      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.792       |
|    explained_variance   | 0.176        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0169       |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.000869    |
|    value_loss           | 0.00417      |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | 0.0345   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 38       |
|    time_elapsed    | 2556     |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0036562155 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.821       |
|    explained_variance   | 0.411        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00668     |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00262     |
|    value_loss           | 0.00362      |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-0.16 +/- 0.24
Episode length: 504.36 +/- 101.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | 0.0345   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 39       |
|    time_elapsed    | 2622     |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0021686524 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.667       |
|    explained_variance   | -0.395       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00691     |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 0.000521     |
------------------------------------------
Eval num_timesteps=80500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 470      |
|    ep_rew_mean     | 0.0121   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 40       |
|    time_elapsed    | 2688     |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-0.19 +/- 0.17
Episode length: 515.68 +/- 65.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.002307268 |
|    clip_fraction        | 0.0267      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.466      |
|    explained_variance   | -2.11       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00582    |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00306    |
|    value_loss           | 0.000652    |
-----------------------------------------
Eval num_timesteps=82500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | 0.0227   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 41       |
|    time_elapsed    | 2754     |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-0.16 +/- 0.24
Episode length: 504.46 +/- 100.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0003680379 |
|    clip_fraction        | 0.00835      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.275       |
|    explained_variance   | 0.606        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000956    |
|    n_updates            | 410          |
|    policy_gradient_loss | 0.000281     |
|    value_loss           | 0.00346      |
------------------------------------------
Eval num_timesteps=84500, episode_reward=-0.16 +/- 0.24
Episode length: 504.42 +/- 100.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-0.16 +/- 0.24
Episode length: 504.48 +/- 100.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | 0.0227   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 42       |
|    time_elapsed    | 2835     |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=-0.19 +/- 0.16
Episode length: 516.46 +/- 59.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -0.187       |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 0.0010173317 |
|    clip_fraction        | 0.00288      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.643       |
|    explained_variance   | 0.401        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00454     |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000808    |
|    value_loss           | 0.000656     |
------------------------------------------
Eval num_timesteps=87000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 473      |
|    ep_rew_mean     | 0.011    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 43       |
|    time_elapsed    | 2901     |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0019419712 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.606       |
|    explained_variance   | 0.419        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00675      |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 0.000603     |
------------------------------------------
Eval num_timesteps=89000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 477       |
|    ep_rew_mean     | -0.000831 |
| time/              |           |
|    fps             | 30        |
|    iterations      | 44        |
|    time_elapsed    | 2967      |
|    total_timesteps | 90112     |
----------------------------------
Eval num_timesteps=90500, episode_reward=-0.19 +/- 0.17
Episode length: 515.78 +/- 64.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0026841455 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.569       |
|    explained_variance   | -0.416       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00997     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00265     |
|    value_loss           | 0.000407     |
------------------------------------------
Eval num_timesteps=91000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 477       |
|    ep_rew_mean     | -0.000831 |
| time/              |           |
|    fps             | 30        |
|    iterations      | 45        |
|    time_elapsed    | 3034      |
|    total_timesteps | 92160     |
----------------------------------
Eval num_timesteps=92500, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 504           |
|    mean_reward          | -0.162        |
| time/                   |               |
|    total_timesteps      | 92500         |
| train/                  |               |
|    approx_kl            | 0.00019954852 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.411        |
|    explained_variance   | -0.282        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.000372      |
|    n_updates            | 450           |
|    policy_gradient_loss | -0.000211     |
|    value_loss           | 0.000515      |
-------------------------------------------
Eval num_timesteps=93000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-0.19 +/- 0.17
Episode length: 514.84 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0231  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 46       |
|    time_elapsed    | 3101     |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 94500        |
| train/                  |              |
|    approx_kl            | 0.0007274252 |
|    clip_fraction        | 0.00688      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 0.512        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00243     |
|    n_updates            | 460          |
|    policy_gradient_loss | 0.000408     |
|    value_loss           | 0.00356      |
------------------------------------------
Eval num_timesteps=95000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0231  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 47       |
|    time_elapsed    | 3169     |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0024588646 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.777       |
|    explained_variance   | 0.556        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0128      |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00258     |
|    value_loss           | 0.000582     |
------------------------------------------
Eval num_timesteps=97000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0334  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 48       |
|    time_elapsed    | 3236     |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 0.0034499653 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.656       |
|    explained_variance   | 0.38         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00149     |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.000682    |
|    value_loss           | 0.00392      |
------------------------------------------
Eval num_timesteps=99000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0564  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 49       |
|    time_elapsed    | 3302     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 100500       |
| train/                  |              |
|    approx_kl            | 0.0013141381 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0.784        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0095      |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 0.000387     |
------------------------------------------
Eval num_timesteps=101000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-0.14 +/- 0.29
Episode length: 494.46 +/- 120.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.0462  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 50       |
|    time_elapsed    | 3368     |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 0.0031267141 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.665       |
|    explained_variance   | 0.279        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00127     |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.000433    |
|    value_loss           | 0.00406      |
------------------------------------------
Eval num_timesteps=103000, episode_reward=-0.14 +/- 0.29
Episode length: 494.10 +/- 122.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.0461  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 51       |
|    time_elapsed    | 3433     |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0022946284 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.624       |
|    explained_variance   | 0.321        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.003       |
|    n_updates            | 510          |
|    policy_gradient_loss | 0.000666     |
|    value_loss           | 0.00365      |
------------------------------------------
Eval num_timesteps=105000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-0.19 +/- 0.17
Episode length: 515.66 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.0682  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 52       |
|    time_elapsed    | 3500     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0012281493 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.55        |
|    explained_variance   | -0.298       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00633     |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00214     |
|    value_loss           | 0.000377     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-0.19 +/- 0.15
Episode length: 521.02 +/- 27.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 521      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.0682  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 53       |
|    time_elapsed    | 3582     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0010025752 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.384       |
|    explained_variance   | 0.664        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00739     |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000982    |
|    value_loss           | 0.000433     |
------------------------------------------
Eval num_timesteps=109500, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 494      |
|    ep_rew_mean     | -0.0678  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 54       |
|    time_elapsed    | 3647     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-0.16 +/- 0.24
Episode length: 504.34 +/- 101.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 0.0044414154 |
|    clip_fraction        | 0.0505       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.758       |
|    explained_variance   | 0.0616       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00321      |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 0.00356      |
------------------------------------------
Eval num_timesteps=111500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.0794  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 55       |
|    time_elapsed    | 3713     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 0.0007258444 |
|    clip_fraction        | 0.00757      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.339       |
|    explained_variance   | 0.668        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00676     |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 0.000426     |
------------------------------------------
Eval num_timesteps=113500, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.091   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 56       |
|    time_elapsed    | 3779     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-0.16 +/- 0.24
Episode length: 504.48 +/- 100.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 504         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.002941826 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.442      |
|    explained_variance   | 0.293       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.000837    |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.000616   |
|    value_loss           | 0.000303    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.0895  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 57       |
|    time_elapsed    | 3845     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 504         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.008143582 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.38        |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0213     |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 0.00197     |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.0782  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 58       |
|    time_elapsed    | 3910     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 504           |
|    mean_reward          | -0.162        |
| time/                   |               |
|    total_timesteps      | 119000        |
| train/                  |               |
|    approx_kl            | 0.00044450018 |
|    clip_fraction        | 0.00718       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.289        |
|    explained_variance   | 0.832         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00458       |
|    n_updates            | 580           |
|    policy_gradient_loss | -0.00164      |
|    value_loss           | 0.00273       |
-------------------------------------------
Eval num_timesteps=119500, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.0893  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 59       |
|    time_elapsed    | 3976     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0004674305 |
|    clip_fraction        | 0.00674      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | -0.206       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.003        |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000666    |
|    value_loss           | 0.000275     |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 30       |
|    iterations      | 60       |
|    time_elapsed    | 4043     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.001426819 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.381      |
|    explained_variance   | 0.0348      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00187    |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.00204    |
|    value_loss           | 0.000442    |
-----------------------------------------
Eval num_timesteps=123500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-0.16 +/- 0.24
Episode length: 504.46 +/- 100.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-0.14 +/- 0.29
Episode length: 494.00 +/- 122.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 30       |
|    iterations      | 61       |
|    time_elapsed    | 4108     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.005654192 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.741       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0049      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 0.00036     |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-0.16 +/- 0.24
Episode length: 504.50 +/- 100.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.11    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 62       |
|    time_elapsed    | 4174     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0034746523 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0.384        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000897    |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00378     |
|    value_loss           | 0.000279     |
------------------------------------------
Eval num_timesteps=127500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-0.19 +/- 0.16
Episode length: 517.02 +/- 55.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 63       |
|    time_elapsed    | 4257     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.004008672 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.489       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0203      |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00157    |
|    value_loss           | 0.000351    |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=-0.19 +/- 0.17
Episode length: 515.82 +/- 64.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 64       |
|    time_elapsed    | 4324     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 504        |
|    mean_reward          | -0.162     |
| time/                   |            |
|    total_timesteps      | 131500     |
| train/                  |            |
|    approx_kl            | 0.00253564 |
|    clip_fraction        | 0.00894    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.416     |
|    explained_variance   | 0.286      |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00258    |
|    n_updates            | 640        |
|    policy_gradient_loss | 0.00033    |
|    value_loss           | 0.000258   |
----------------------------------------
Eval num_timesteps=132000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 65       |
|    time_elapsed    | 4390     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-0.19 +/- 0.17
Episode length: 515.86 +/- 63.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0016857651 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.598       |
|    explained_variance   | -1.07        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0105       |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 0.000332     |
------------------------------------------
Eval num_timesteps=134000, episode_reward=-0.14 +/- 0.29
Episode length: 494.16 +/- 122.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.132   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 66       |
|    time_elapsed    | 4457     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-0.16 +/- 0.24
Episode length: 504.46 +/- 100.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0013217083 |
|    clip_fraction        | 0.00557      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.529       |
|    explained_variance   | 0.483        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0105       |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00216     |
|    value_loss           | 0.000362     |
------------------------------------------
Eval num_timesteps=136000, episode_reward=-0.16 +/- 0.23
Episode length: 506.72 +/- 90.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.132   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 67       |
|    time_elapsed    | 4522     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.003333508 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.805       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0081     |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 0.000405    |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=-0.19 +/- 0.16
Episode length: 516.16 +/- 61.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.132   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 68       |
|    time_elapsed    | 4588     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0017911183 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.695       |
|    explained_variance   | -1.88        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00861      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 0.000339     |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 69       |
|    time_elapsed    | 4653     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-0.19 +/- 0.16
Episode length: 518.10 +/- 48.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 518          |
|    mean_reward          | -0.187       |
| time/                   |              |
|    total_timesteps      | 141500       |
| train/                  |              |
|    approx_kl            | 0.0011273687 |
|    clip_fraction        | 0.00918      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.471       |
|    explained_variance   | 0.514        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00267      |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.000477    |
|    value_loss           | 0.0041       |
------------------------------------------
Eval num_timesteps=142000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.121   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 70       |
|    time_elapsed    | 4720     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.002882029 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.767       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00465    |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00098    |
|    value_loss           | 0.000515    |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.132   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 71       |
|    time_elapsed    | 4787     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.003078728 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.624      |
|    explained_variance   | 0.832       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0278      |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 0.000306    |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=-0.19 +/- 0.17
Episode length: 514.84 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.132   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 72       |
|    time_elapsed    | 4852     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.003508114 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.79       |
|    explained_variance   | 0.458       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00431     |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 0.000348    |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.143   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 73       |
|    time_elapsed    | 4934     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0016472125 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.723       |
|    explained_variance   | -0.161       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0067      |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00152     |
|    value_loss           | 0.000306     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=-0.14 +/- 0.29
Episode length: 494.00 +/- 122.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.143   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 74       |
|    time_elapsed    | 4999     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-0.14 +/- 0.28
Episode length: 495.42 +/- 117.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 495          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0013188166 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0.385        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00146      |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.000783    |
|    value_loss           | 0.000518     |
------------------------------------------
Eval num_timesteps=152500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-0.19 +/- 0.16
Episode length: 516.18 +/- 61.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-0.19 +/- 0.17
Episode length: 515.56 +/- 66.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 75       |
|    time_elapsed    | 5064     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0019393471 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.588       |
|    explained_variance   | -0.873       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00246      |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 0.000299     |
------------------------------------------
Eval num_timesteps=154500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 76       |
|    time_elapsed    | 5131     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0023337214 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.45        |
|    explained_variance   | 0.256        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0126       |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.000445    |
|    value_loss           | 0.00366      |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 77       |
|    time_elapsed    | 5199     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-0.16 +/- 0.23
Episode length: 505.46 +/- 95.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0029211727 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.469       |
|    explained_variance   | 0.502        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0124      |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00228     |
|    value_loss           | 0.00323      |
------------------------------------------
Eval num_timesteps=158500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 78       |
|    time_elapsed    | 5266     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 515        |
|    mean_reward          | -0.186     |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.03698896 |
|    clip_fraction        | 0.037      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.622     |
|    explained_variance   | 0.79       |
|    learning_rate        | 1e-05      |
|    loss                 | -0.0322    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.00442   |
|    value_loss           | 0.000248   |
----------------------------------------
Eval num_timesteps=160500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-0.11 +/- 0.33
Episode length: 483.88 +/- 139.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 484      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 79       |
|    time_elapsed    | 5331     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 0.0019816095 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.613       |
|    explained_variance   | 0.637        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000884    |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 0.000252     |
------------------------------------------
Eval num_timesteps=162500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 80       |
|    time_elapsed    | 5398     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0041138856 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 0.678        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00227      |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 0.00017      |
------------------------------------------
Eval num_timesteps=164500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-0.14 +/- 0.28
Episode length: 498.92 +/- 104.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 499      |
|    mean_reward     | -0.14    |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.154   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 81       |
|    time_elapsed    | 5464     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0014355758 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.751       |
|    explained_variance   | 0.254        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00927     |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 0.000278     |
------------------------------------------
Eval num_timesteps=166500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 514      |
|    ep_rew_mean     | -0.166   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 82       |
|    time_elapsed    | 5530     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0015893664 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.349       |
|    explained_variance   | 0.58         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0108       |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 0.00033      |
------------------------------------------
Eval num_timesteps=168500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.177   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 83       |
|    time_elapsed    | 5596     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0024711567 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.702       |
|    explained_variance   | -1.02        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00888     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00362     |
|    value_loss           | 0.000369     |
------------------------------------------
Eval num_timesteps=170500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.177   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 84       |
|    time_elapsed    | 5679     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0027912718 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.761       |
|    explained_variance   | 0.301        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.034       |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00407     |
|    value_loss           | 0.000217     |
------------------------------------------
Eval num_timesteps=173000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 85       |
|    time_elapsed    | 5744     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-0.16 +/- 0.23
Episode length: 505.68 +/- 94.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 0.0022240982 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.507       |
|    explained_variance   | 0.243        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0256       |
|    n_updates            | 850          |
|    policy_gradient_loss | 0.000679     |
|    value_loss           | 0.00663      |
------------------------------------------
Eval num_timesteps=175000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 86       |
|    time_elapsed    | 5810     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-0.11 +/- 0.33
Episode length: 483.66 +/- 140.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 484          |
|    mean_reward          | -0.113       |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0015036168 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.485       |
|    explained_variance   | 0.454        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00828     |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 0.000429     |
------------------------------------------
New best mean reward!
Eval num_timesteps=177000, episode_reward=-0.14 +/- 0.29
Episode length: 494.06 +/- 122.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 87       |
|    time_elapsed    | 5874     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0032518134 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.398       |
|    explained_variance   | 0.367        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00372     |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 0.00253      |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 88       |
|    time_elapsed    | 5941     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0015918083 |
|    clip_fraction        | 0.00728      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.445       |
|    explained_variance   | 0.708        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00874      |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 0.00108      |
------------------------------------------
Eval num_timesteps=181000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 89       |
|    time_elapsed    | 6008     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 0.0016982772 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.603       |
|    explained_variance   | 0.473        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0149       |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.000701    |
|    value_loss           | 0.000293     |
------------------------------------------
Eval num_timesteps=183000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-0.16 +/- 0.24
Episode length: 504.48 +/- 100.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 90       |
|    time_elapsed    | 6073     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-0.16 +/- 0.24
Episode length: 504.54 +/- 100.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 505           |
|    mean_reward          | -0.162        |
| time/                   |               |
|    total_timesteps      | 184500        |
| train/                  |               |
|    approx_kl            | 0.00040991456 |
|    clip_fraction        | 0.0111        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.22         |
|    explained_variance   | -0.957        |
|    learning_rate        | 1e-05         |
|    loss                 | 0.000734      |
|    n_updates            | 900           |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 0.000295      |
-------------------------------------------
Eval num_timesteps=185000, episode_reward=-0.16 +/- 0.24
Episode length: 504.50 +/- 100.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 91       |
|    time_elapsed    | 6139     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.005008175 |
|    clip_fraction        | 0.0424      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.489      |
|    explained_variance   | 0.537       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.000151    |
|    n_updates            | 910         |
|    policy_gradient_loss | -0.00307    |
|    value_loss           | 0.00333     |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-0.19 +/- 0.16
Episode length: 518.22 +/- 47.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 92       |
|    time_elapsed    | 6206     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.019776437 |
|    clip_fraction        | 0.0232      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.749       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0292     |
|    n_updates            | 920         |
|    policy_gradient_loss | 0.00345     |
|    value_loss           | 0.000352    |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-0.19 +/- 0.16
Episode length: 516.80 +/- 57.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-0.16 +/- 0.24
Episode length: 504.36 +/- 101.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 93       |
|    time_elapsed    | 6271     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-0.14 +/- 0.29
Episode length: 494.04 +/- 122.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0030897215 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.811       |
|    explained_variance   | 0.673        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00413      |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00244     |
|    value_loss           | 0.000267     |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 94       |
|    time_elapsed    | 6352     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0030882363 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.745       |
|    explained_variance   | -0.129       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0107      |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00262     |
|    value_loss           | 0.000616     |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 95       |
|    time_elapsed    | 6418     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-0.16 +/- 0.23
Episode length: 505.34 +/- 96.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0011856414 |
|    clip_fraction        | 0.00859      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.623       |
|    explained_variance   | -0.223       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0165      |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 0.000511     |
------------------------------------------
Eval num_timesteps=195500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-0.14 +/- 0.29
Episode length: 494.40 +/- 121.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 96       |
|    time_elapsed    | 6483     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0034918594 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.692       |
|    explained_variance   | -0.481       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00819     |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 0.00046      |
------------------------------------------
Eval num_timesteps=197500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-0.19 +/- 0.16
Episode length: 519.10 +/- 41.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 97       |
|    time_elapsed    | 6549     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 199000        |
| train/                  |               |
|    approx_kl            | 0.00090165273 |
|    clip_fraction        | 0.00767       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.819        |
|    explained_variance   | 0.344         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00151       |
|    n_updates            | 970           |
|    policy_gradient_loss | -0.00139      |
|    value_loss           | 0.000473      |
-------------------------------------------
Eval num_timesteps=199500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.128   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 98       |
|    time_elapsed    | 6616     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0025744713 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.625       |
|    explained_variance   | 0.33         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0114      |
|    n_updates            | 980          |
|    policy_gradient_loss | 0.00124      |
|    value_loss           | 0.00345      |
------------------------------------------
Eval num_timesteps=201500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.128   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 99       |
|    time_elapsed    | 6682     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.0010144331 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.514       |
|    explained_variance   | 0.499        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00181      |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.000679    |
|    value_loss           | 0.000335     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-0.19 +/- 0.17
Episode length: 515.98 +/- 63.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 100      |
|    time_elapsed    | 6748     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0010336956 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.274       |
|    explained_variance   | -0.362       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00663     |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.000639     |
------------------------------------------
Eval num_timesteps=205500, episode_reward=-0.19 +/- 0.16
Episode length: 517.04 +/- 55.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 101      |
|    time_elapsed    | 6815     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-0.16 +/- 0.24
Episode length: 504.58 +/- 100.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0014066861 |
|    clip_fraction        | 0.00703      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.44        |
|    explained_variance   | 0.36         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0038       |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.000632    |
|    value_loss           | 0.00039      |
------------------------------------------
Eval num_timesteps=207500, episode_reward=-0.16 +/- 0.24
Episode length: 504.46 +/- 100.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 102      |
|    time_elapsed    | 6880     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0019141192 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.463       |
|    explained_variance   | 0.62         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00094     |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.000959    |
|    value_loss           | 0.00393      |
------------------------------------------
Eval num_timesteps=209500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 103      |
|    time_elapsed    | 6947     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0018827126 |
|    clip_fraction        | 0.00747      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.742        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00795      |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 0.000545     |
------------------------------------------
Eval num_timesteps=211500, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 104      |
|    time_elapsed    | 7013     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0055102557 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.365       |
|    explained_variance   | 0.21         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0193      |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00187     |
|    value_loss           | 0.000206     |
------------------------------------------
Eval num_timesteps=213500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-0.16 +/- 0.24
Episode length: 504.44 +/- 100.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 105      |
|    time_elapsed    | 7096     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.004190349 |
|    clip_fraction        | 0.0297      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.0387      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0158     |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 0.00039     |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 106      |
|    time_elapsed    | 7161     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-0.19 +/- 0.17
Episode length: 514.84 +/- 71.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 0.0015350388 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.459       |
|    explained_variance   | -0.0212      |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00734     |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 0.000296     |
------------------------------------------
Eval num_timesteps=218000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 107      |
|    time_elapsed    | 7227     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 219500       |
| train/                  |              |
|    approx_kl            | 0.0036725246 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.665       |
|    explained_variance   | 0.516        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0321      |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 0.000427     |
------------------------------------------
Eval num_timesteps=220000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.128   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 108      |
|    time_elapsed    | 7293     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0022835047 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.0142       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00277     |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.00262     |
|    value_loss           | 0.00432      |
------------------------------------------
Eval num_timesteps=222000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 499      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 109      |
|    time_elapsed    | 7359     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 0.0016416557 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.688       |
|    explained_variance   | -0.715       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0081      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 0.000635     |
------------------------------------------
Eval num_timesteps=224000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.152   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 110      |
|    time_elapsed    | 7426     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-0.16 +/- 0.23
Episode length: 505.70 +/- 94.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 506          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 0.0045075966 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.716       |
|    explained_variance   | 0.293        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000745    |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00281     |
|    value_loss           | 0.000308     |
------------------------------------------
Eval num_timesteps=226000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.152   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 111      |
|    time_elapsed    | 7492     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 0.0010824865 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.773       |
|    explained_variance   | -1.08        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000202    |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.000648    |
|    value_loss           | 0.000423     |
------------------------------------------
Eval num_timesteps=228000, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.164   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 112      |
|    time_elapsed    | 7558     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-0.14 +/- 0.29
Episode length: 494.10 +/- 122.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 0.0011909227 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.583       |
|    explained_variance   | 0.0542       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00319     |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 0.000465     |
------------------------------------------
Eval num_timesteps=230000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.152   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 113      |
|    time_elapsed    | 7624     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 0.0018500949 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.396       |
|    explained_variance   | 0.263        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00168     |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.00142     |
|    value_loss           | 0.00425      |
------------------------------------------
Eval num_timesteps=232000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-0.14 +/- 0.29
Episode length: 494.34 +/- 121.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.152   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 114      |
|    time_elapsed    | 7690     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-0.19 +/- 0.15
Episode length: 521.50 +/- 24.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 522         |
|    mean_reward          | -0.189      |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.000840096 |
|    clip_fraction        | 0.00723     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | -0.0181     |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00718    |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 0.000346    |
-----------------------------------------
Eval num_timesteps=234000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.152   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 115      |
|    time_elapsed    | 7772     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0023066867 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.744       |
|    explained_variance   | -0.772       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0177      |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00356     |
|    value_loss           | 0.000415     |
------------------------------------------
Eval num_timesteps=236500, episode_reward=-0.16 +/- 0.23
Episode length: 507.26 +/- 88.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-0.16 +/- 0.24
Episode length: 504.52 +/- 100.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-0.16 +/- 0.24
Episode length: 504.46 +/- 100.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.163   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 116      |
|    time_elapsed    | 7838     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 0.0009540214 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.305       |
|    explained_variance   | 0.0601       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00828     |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 0.000388     |
------------------------------------------
Eval num_timesteps=238500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-0.19 +/- 0.17
Episode length: 514.84 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.163   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 117      |
|    time_elapsed    | 7904     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0022862656 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.745       |
|    explained_variance   | 0.443        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00399     |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00172     |
|    value_loss           | 0.000536     |
------------------------------------------
Eval num_timesteps=240500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-0.19 +/- 0.15
Episode length: 519.88 +/- 35.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.163   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 118      |
|    time_elapsed    | 7970     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-0.16 +/- 0.24
Episode length: 504.62 +/- 99.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0009437597 |
|    clip_fraction        | 0.00688      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.498       |
|    explained_variance   | -0.208       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00735     |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 0.000431     |
------------------------------------------
Eval num_timesteps=242500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.163   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 119      |
|    time_elapsed    | 8035     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-0.19 +/- 0.17
Episode length: 515.64 +/- 65.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0008710869 |
|    clip_fraction        | 0.00845      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | -0.367       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00522      |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00166     |
|    value_loss           | 0.00036      |
------------------------------------------
Eval num_timesteps=244500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-0.16 +/- 0.24
Episode length: 504.42 +/- 100.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 120      |
|    time_elapsed    | 8102     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0013106351 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.694       |
|    explained_variance   | 0.179        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0117       |
|    n_updates            | 1200         |
|    policy_gradient_loss | -9.71e-05    |
|    value_loss           | 0.00721      |
------------------------------------------
Eval num_timesteps=246500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-0.16 +/- 0.23
Episode length: 506.58 +/- 90.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 121      |
|    time_elapsed    | 8167     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0042511867 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.587       |
|    explained_variance   | 0.5          |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00469      |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.00365      |
------------------------------------------
Eval num_timesteps=248500, episode_reward=-0.16 +/- 0.24
Episode length: 504.60 +/- 99.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 122      |
|    time_elapsed    | 8233     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 0.00047229702 |
|    clip_fraction        | 0.0061        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.388        |
|    explained_variance   | 0.608         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00427       |
|    n_updates            | 1220          |
|    policy_gradient_loss | -0.000526     |
|    value_loss           | 0.0037        |
-------------------------------------------
Eval num_timesteps=250500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-0.14 +/- 0.29
Episode length: 494.18 +/- 121.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 123      |
|    time_elapsed    | 8300     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0035743592 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.435       |
|    explained_variance   | 0.605        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00832     |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00187     |
|    value_loss           | 0.00573      |
------------------------------------------
Eval num_timesteps=252500, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-0.19 +/- 0.16
Episode length: 518.54 +/- 45.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 124      |
|    time_elapsed    | 8365     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-0.19 +/- 0.16
Episode length: 518.16 +/- 47.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 518         |
|    mean_reward          | -0.187      |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.001666853 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.543      |
|    explained_variance   | -1.41       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0105     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0027     |
|    value_loss           | 0.000728    |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=-0.19 +/- 0.17
Episode length: 515.70 +/- 65.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 125      |
|    time_elapsed    | 8448     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.001696974 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.241       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0175     |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 0.000362    |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.117   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 126      |
|    time_elapsed    | 8515     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 0.0023316303 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.636       |
|    explained_variance   | 0.594        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00976     |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00245     |
|    value_loss           | 0.000467     |
------------------------------------------
Eval num_timesteps=259000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-0.19 +/- 0.14
Episode length: 523.62 +/- 9.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 524      |
|    mean_reward     | -0.189   |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-0.14 +/- 0.29
Episode length: 494.18 +/- 121.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 127      |
|    time_elapsed    | 8580     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=-0.16 +/- 0.24
Episode length: 504.64 +/- 99.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 0.0023535967 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.862       |
|    explained_variance   | 0.361        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00234      |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 0.00393      |
------------------------------------------
Eval num_timesteps=261000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-0.16 +/- 0.24
Episode length: 504.34 +/- 101.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 128      |
|    time_elapsed    | 8646     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 262500       |
| train/                  |              |
|    approx_kl            | 0.0033327783 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.594       |
|    explained_variance   | -0.458       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0143      |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00271     |
|    value_loss           | 0.000386     |
------------------------------------------
Eval num_timesteps=263000, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.095   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 129      |
|    time_elapsed    | 8712     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.004188502 |
|    clip_fraction        | 0.0173      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.753      |
|    explained_variance   | 0.332       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00481     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 0.00383     |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-0.19 +/- 0.17
Episode length: 514.84 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.095   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 130      |
|    time_elapsed    | 8779     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0016823406 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.563       |
|    explained_variance   | -0.179       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00465     |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 0.000359     |
------------------------------------------
Eval num_timesteps=267000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.095   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 131      |
|    time_elapsed    | 8846     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 0.0024788147 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.762       |
|    explained_variance   | 0.73         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00687      |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00245     |
|    value_loss           | 0.000704     |
------------------------------------------
Eval num_timesteps=269000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.095   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 132      |
|    time_elapsed    | 8913     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-0.19 +/- 0.17
Episode length: 515.70 +/- 65.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.010901736 |
|    clip_fraction        | 0.0529      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.366      |
|    explained_variance   | 0.0111      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0249     |
|    n_updates            | 1320        |
|    policy_gradient_loss | -0.0086     |
|    value_loss           | 0.00342     |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-0.16 +/- 0.24
Episode length: 504.36 +/- 101.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.095   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 133      |
|    time_elapsed    | 8978     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=-0.19 +/- 0.17
Episode length: 515.98 +/- 63.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 0.0037032736 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.847       |
|    explained_variance   | 0.787        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.012       |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.0026      |
|    value_loss           | 0.000722     |
------------------------------------------
Eval num_timesteps=273000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.095   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 134      |
|    time_elapsed    | 9046     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 274500       |
| train/                  |              |
|    approx_kl            | 0.0011289804 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0.648        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0126      |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 0.00123      |
------------------------------------------
Eval num_timesteps=275000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0834  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 135      |
|    time_elapsed    | 9113     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.004692183 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.867      |
|    explained_variance   | 0.355       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00114     |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00288    |
|    value_loss           | 0.00351     |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0834  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 136      |
|    time_elapsed    | 9196     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 279000       |
| train/                  |              |
|    approx_kl            | 0.0011074147 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.659       |
|    explained_variance   | -0.676       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000259     |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 0.0005       |
------------------------------------------
Eval num_timesteps=279500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0831  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 137      |
|    time_elapsed    | 9262     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0009934759 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.297       |
|    explained_variance   | 0.206        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00377      |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.000726    |
|    value_loss           | 0.0026       |
------------------------------------------
Eval num_timesteps=281500, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0831  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 138      |
|    time_elapsed    | 9328     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 0.0038725224 |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.823       |
|    explained_variance   | 0.509        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0046       |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.0043      |
|    value_loss           | 0.00105      |
------------------------------------------
Eval num_timesteps=283500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-0.16 +/- 0.24
Episode length: 504.52 +/- 100.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-0.19 +/- 0.17
Episode length: 515.78 +/- 64.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0831  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 139      |
|    time_elapsed    | 9394     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | 0.00075500016 |
|    clip_fraction        | 0.00605       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.627        |
|    explained_variance   | 0.24          |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00326      |
|    n_updates            | 1390          |
|    policy_gradient_loss | -0.000354     |
|    value_loss           | 0.000773      |
-------------------------------------------
Eval num_timesteps=285500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-0.16 +/- 0.24
Episode length: 504.34 +/- 101.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0831  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 140      |
|    time_elapsed    | 9460     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0020880708 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.366       |
|    explained_variance   | -0.812       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0057       |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 0.00053      |
------------------------------------------
Eval num_timesteps=287500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0831  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 141      |
|    time_elapsed    | 9528     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.003621272 |
|    clip_fraction        | 0.0194      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.724      |
|    explained_variance   | -0.223      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0156     |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00227    |
|    value_loss           | 0.000435    |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -0.0711  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 142      |
|    time_elapsed    | 9594     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0052801305 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.858       |
|    explained_variance   | 0.444        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.021       |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00246     |
|    value_loss           | 0.00319      |
------------------------------------------
Eval num_timesteps=291500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.0816  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 143      |
|    time_elapsed    | 9661     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 0.0016828899 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.331       |
|    explained_variance   | 0.379        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00269     |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 0.000703     |
------------------------------------------
Eval num_timesteps=293500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-0.16 +/- 0.24
Episode length: 504.34 +/- 101.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0936  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 144      |
|    time_elapsed    | 9726     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0024859211 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.397       |
|    explained_variance   | 0.526        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00255      |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 0.000437     |
------------------------------------------
Eval num_timesteps=295500, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 145      |
|    time_elapsed    | 9793     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 0.0009621719 |
|    clip_fraction        | 0.00981      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.514       |
|    explained_variance   | -2.32        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.007       |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 0.000675     |
------------------------------------------
Eval num_timesteps=297500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.117   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 146      |
|    time_elapsed    | 9875     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0028876858 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.691       |
|    explained_variance   | -1.39        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00374     |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 0.000323     |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.128   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 147      |
|    time_elapsed    | 9942     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 301500       |
| train/                  |              |
|    approx_kl            | 0.0025872013 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.972       |
|    explained_variance   | -0.369       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0159      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 0.00028      |
------------------------------------------
Eval num_timesteps=302000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-0.16 +/- 0.24
Episode length: 504.52 +/- 100.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 148      |
|    time_elapsed    | 10008    |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 303500       |
| train/                  |              |
|    approx_kl            | 0.0024191816 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.666       |
|    explained_variance   | 0.711        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00833     |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 0.000681     |
------------------------------------------
Eval num_timesteps=304000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 149      |
|    time_elapsed    | 10075    |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 305500       |
| train/                  |              |
|    approx_kl            | 0.0026230072 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.758       |
|    explained_variance   | 0.641        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000524     |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.0013      |
|    value_loss           | 0.000378     |
------------------------------------------
Eval num_timesteps=306000, episode_reward=-0.14 +/- 0.29
Episode length: 494.06 +/- 122.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-0.16 +/- 0.24
Episode length: 504.36 +/- 101.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 150      |
|    time_elapsed    | 10140    |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=-0.19 +/- 0.16
Episode length: 516.10 +/- 62.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 516         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 307500      |
| train/                  |             |
|    approx_kl            | 0.003209881 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.72       |
|    explained_variance   | -1.2        |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0128     |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 0.000431    |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=-0.14 +/- 0.29
Episode length: 494.00 +/- 122.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-0.14 +/- 0.29
Episode length: 494.92 +/- 119.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -0.14    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 151      |
|    time_elapsed    | 10205    |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 309500       |
| train/                  |              |
|    approx_kl            | 0.0011916263 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | 0.152        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00202      |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.00188     |
|    value_loss           | 0.000527     |
------------------------------------------
Eval num_timesteps=310000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.152   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 152      |
|    time_elapsed    | 10271    |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 311500       |
| train/                  |              |
|    approx_kl            | 0.0033687428 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.746       |
|    explained_variance   | -1.51        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00725      |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 0.000427     |
------------------------------------------
Eval num_timesteps=312000, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=-0.14 +/- 0.29
Episode length: 494.26 +/- 121.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.152   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 153      |
|    time_elapsed    | 10336    |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 313500        |
| train/                  |               |
|    approx_kl            | 0.00083882594 |
|    clip_fraction        | 0.00625       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.226        |
|    explained_variance   | 0.207         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.0065       |
|    n_updates            | 1530          |
|    policy_gradient_loss | -0.0014       |
|    value_loss           | 0.000489      |
-------------------------------------------
Eval num_timesteps=314000, episode_reward=-0.14 +/- 0.29
Episode length: 493.98 +/- 122.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-0.19 +/- 0.17
Episode length: 515.72 +/- 64.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.162   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 154      |
|    time_elapsed    | 10402    |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=-0.16 +/- 0.23
Episode length: 509.88 +/- 78.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 510          |
|    mean_reward          | -0.164       |
| time/                   |              |
|    total_timesteps      | 315500       |
| train/                  |              |
|    approx_kl            | 0.0010216607 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.381       |
|    explained_variance   | -0.417       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00678      |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 0.000459     |
------------------------------------------
Eval num_timesteps=316000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.162   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 155      |
|    time_elapsed    | 10469    |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=-0.17 +/- 0.22
Episode length: 512.86 +/- 73.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 513         |
|    mean_reward          | -0.165      |
| time/                   |             |
|    total_timesteps      | 317500      |
| train/                  |             |
|    approx_kl            | 0.002361461 |
|    clip_fraction        | 0.0216      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.649      |
|    explained_variance   | 0.213       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00474    |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 0.0006      |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=-0.16 +/- 0.24
Episode length: 504.60 +/- 99.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.162   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 156      |
|    time_elapsed    | 10535    |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 0.0022029963 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.373       |
|    explained_variance   | -3.94        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00281      |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00319     |
|    value_loss           | 0.000529     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 511      |
|    ep_rew_mean     | -0.174   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 157      |
|    time_elapsed    | 10617    |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0018980413 |
|    clip_fraction        | 0.00811      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.441       |
|    explained_variance   | -1.35        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00191      |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.000938    |
|    value_loss           | 0.000342     |
------------------------------------------
Eval num_timesteps=322500, episode_reward=-0.16 +/- 0.24
Episode length: 504.42 +/- 100.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=-0.19 +/- 0.17
Episode length: 515.58 +/- 65.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.163   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 158      |
|    time_elapsed    | 10683    |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 324000       |
| train/                  |              |
|    approx_kl            | 0.0016066416 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.843       |
|    explained_variance   | 0.158        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00494     |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.000526    |
|    value_loss           | 0.00428      |
------------------------------------------
Eval num_timesteps=324500, episode_reward=-0.19 +/- 0.17
Episode length: 515.88 +/- 63.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.163   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 159      |
|    time_elapsed    | 10749    |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.004956565 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | -0.258      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00382    |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00523    |
|    value_loss           | 0.000383    |
-----------------------------------------
Eval num_timesteps=326500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.175   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 160      |
|    time_elapsed    | 10818    |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0016537377 |
|    clip_fraction        | 0.00796      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.474       |
|    explained_variance   | -1.15        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0127      |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00169     |
|    value_loss           | 0.000447     |
------------------------------------------
Eval num_timesteps=328500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 513      |
|    ep_rew_mean     | -0.175   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 161      |
|    time_elapsed    | 10884    |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0016131533 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.473       |
|    explained_variance   | -2.26        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00279     |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00264     |
|    value_loss           | 0.000331     |
------------------------------------------
Eval num_timesteps=330500, episode_reward=-0.16 +/- 0.23
Episode length: 505.20 +/- 97.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=-0.16 +/- 0.23
Episode length: 505.82 +/- 94.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 162      |
|    time_elapsed    | 10949    |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0030818498 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.73        |
|    explained_variance   | 0.17         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00326      |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 0.00421      |
------------------------------------------
Eval num_timesteps=332500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=-0.14 +/- 0.29
Episode length: 494.28 +/- 121.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 163      |
|    time_elapsed    | 11014    |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 0.0039553773 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.835       |
|    explained_variance   | -1.86        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00972     |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 0.000218     |
------------------------------------------
Eval num_timesteps=334500, episode_reward=-0.16 +/- 0.24
Episode length: 504.48 +/- 100.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 164      |
|    time_elapsed    | 11080    |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0014741987 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.358       |
|    explained_variance   | 0.0903       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00745     |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 0.000372     |
------------------------------------------
Eval num_timesteps=336500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=-0.16 +/- 0.24
Episode length: 504.34 +/- 101.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 516      |
|    ep_rew_mean     | -0.176   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 165      |
|    time_elapsed    | 11145    |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0011729037 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.356       |
|    explained_variance   | -1.56        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000494     |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00257     |
|    value_loss           | 0.000303     |
------------------------------------------
Eval num_timesteps=338500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=-0.16 +/- 0.24
Episode length: 504.40 +/- 100.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=-0.14 +/- 0.28
Episode length: 499.36 +/- 102.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 499      |
|    mean_reward     | -0.14    |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 515      |
|    ep_rew_mean     | -0.166   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 166      |
|    time_elapsed    | 11211    |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.003728766 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | 0.161       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00193    |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00066    |
|    value_loss           | 0.0039      |
-----------------------------------------
Eval num_timesteps=340500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | -0.178   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 167      |
|    time_elapsed    | 11293    |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 342500       |
| train/                  |              |
|    approx_kl            | 0.0004799176 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | 0.271        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00417     |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.000996    |
|    value_loss           | 0.000254     |
------------------------------------------
Eval num_timesteps=343000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | -0.178   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 168      |
|    time_elapsed    | 11360    |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 344500      |
| train/                  |             |
|    approx_kl            | 0.002657792 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.798      |
|    explained_variance   | 0.356       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.000559    |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00172    |
|    value_loss           | 0.000243    |
-----------------------------------------
Eval num_timesteps=345000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | -0.178   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 169      |
|    time_elapsed    | 11427    |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 346500        |
| train/                  |               |
|    approx_kl            | 0.00054598146 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.41         |
|    explained_variance   | -1.25         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.0126       |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.000618     |
|    value_loss           | 0.000314      |
-------------------------------------------
Eval num_timesteps=347000, episode_reward=-0.14 +/- 0.29
Episode length: 494.00 +/- 122.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 519      |
|    ep_rew_mean     | -0.167   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 170      |
|    time_elapsed    | 11492    |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 0.0015611451 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.313       |
|    explained_variance   | 0.356        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0129      |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.000438    |
|    value_loss           | 0.00366      |
------------------------------------------
Eval num_timesteps=349000, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=-0.16 +/- 0.24
Episode length: 504.48 +/- 100.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.157   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 171      |
|    time_elapsed    | 11557    |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=-0.16 +/- 0.24
Episode length: 504.52 +/- 100.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 505           |
|    mean_reward          | -0.162        |
| time/                   |               |
|    total_timesteps      | 350500        |
| train/                  |               |
|    approx_kl            | 0.00076055224 |
|    clip_fraction        | 0.00483       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.327        |
|    explained_variance   | 0.545         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00476       |
|    n_updates            | 1710          |
|    policy_gradient_loss | -0.00137      |
|    value_loss           | 0.00388       |
-------------------------------------------
Eval num_timesteps=351000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.157   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 172      |
|    time_elapsed    | 11624    |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 352500       |
| train/                  |              |
|    approx_kl            | 0.0029672002 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.401       |
|    explained_variance   | 0.626        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00381     |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00382     |
|    value_loss           | 0.00041      |
------------------------------------------
Eval num_timesteps=353000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 517      |
|    ep_rew_mean     | -0.157   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 173      |
|    time_elapsed    | 11691    |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 354500       |
| train/                  |              |
|    approx_kl            | 0.0046100575 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.771       |
|    explained_variance   | 0.673        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00577     |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.00268     |
|    value_loss           | 0.000418     |
------------------------------------------
Eval num_timesteps=355000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=-0.16 +/- 0.24
Episode length: 504.60 +/- 99.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 174      |
|    time_elapsed    | 11757    |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 356500       |
| train/                  |              |
|    approx_kl            | 0.0039047673 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.599       |
|    explained_variance   | 0.263        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00265     |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 0.00309      |
------------------------------------------
Eval num_timesteps=357000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 175      |
|    time_elapsed    | 11823    |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 515        |
|    mean_reward          | -0.186     |
| time/                   |            |
|    total_timesteps      | 358500     |
| train/                  |            |
|    approx_kl            | 0.00536481 |
|    clip_fraction        | 0.0238     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.845     |
|    explained_variance   | 0.226      |
|    learning_rate        | 1e-05      |
|    loss                 | 9.59e-06   |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.00298   |
|    value_loss           | 0.000228   |
----------------------------------------
Eval num_timesteps=359000, episode_reward=-0.16 +/- 0.24
Episode length: 504.42 +/- 100.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 176      |
|    time_elapsed    | 11889    |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=-0.16 +/- 0.23
Episode length: 505.44 +/- 95.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 505           |
|    mean_reward          | -0.162        |
| time/                   |               |
|    total_timesteps      | 360500        |
| train/                  |               |
|    approx_kl            | 0.00080445374 |
|    clip_fraction        | 0.0118        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.21         |
|    explained_variance   | 0.132         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.000893     |
|    n_updates            | 1760          |
|    policy_gradient_loss | -0.00178      |
|    value_loss           | 0.000338      |
-------------------------------------------
Eval num_timesteps=361000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 177      |
|    time_elapsed    | 11955    |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=-0.19 +/- 0.16
Episode length: 517.52 +/- 52.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 518          |
|    mean_reward          | -0.187       |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0015999031 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.524       |
|    explained_variance   | 0.687        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00794     |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 0.000193     |
------------------------------------------
Eval num_timesteps=363000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 178      |
|    time_elapsed    | 12038    |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=-0.16 +/- 0.24
Episode length: 504.34 +/- 101.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 504          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 0.0022471994 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.897        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0107       |
|    n_updates            | 1780         |
|    policy_gradient_loss | 0.00066      |
|    value_loss           | 0.000468     |
------------------------------------------
Eval num_timesteps=365500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 512      |
|    ep_rew_mean     | -0.145   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 179      |
|    time_elapsed    | 12104    |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.008555804 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.758       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0318      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.002      |
|    value_loss           | 0.000332    |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.133   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 180      |
|    time_elapsed    | 12171    |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=-0.19 +/- 0.17
Episode length: 514.76 +/- 71.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.005583458 |
|    clip_fraction        | 0.0236      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.211       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0057     |
|    n_updates            | 1800        |
|    policy_gradient_loss | -0.00259    |
|    value_loss           | 0.0032      |
-----------------------------------------
Eval num_timesteps=369500, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.133   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 181      |
|    time_elapsed    | 12237    |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.006680517 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.505       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0128      |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00639    |
|    value_loss           | 0.00021     |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-0.14 +/- 0.29
Episode length: 495.00 +/- 118.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 507      |
|    ep_rew_mean     | -0.133   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 182      |
|    time_elapsed    | 12303    |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 373000       |
| train/                  |              |
|    approx_kl            | 0.0017446377 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.4         |
|    explained_variance   | -0.888       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00286      |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 0.000281     |
------------------------------------------
Eval num_timesteps=373500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.144   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 183      |
|    time_elapsed    | 12370    |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0014037935 |
|    clip_fraction        | 0.00591      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.498       |
|    explained_variance   | -0.0449      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00619      |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 0.000363     |
------------------------------------------
Eval num_timesteps=375500, episode_reward=-0.16 +/- 0.24
Episode length: 504.60 +/- 99.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.144   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 184      |
|    time_elapsed    | 12435    |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.004535119 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.808       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00822    |
|    n_updates            | 1840        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 0.000595    |
-----------------------------------------
Eval num_timesteps=377500, episode_reward=-0.16 +/- 0.24
Episode length: 504.42 +/- 100.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 509      |
|    ep_rew_mean     | -0.144   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 185      |
|    time_elapsed    | 12501    |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 515       |
|    mean_reward          | -0.186    |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 0.0392668 |
|    clip_fraction        | 0.0226    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.132    |
|    explained_variance   | 0.696     |
|    learning_rate        | 1e-05     |
|    loss                 | 0.0451    |
|    n_updates            | 1850      |
|    policy_gradient_loss | 0.000311  |
|    value_loss           | 0.000321  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-0.12 +/- 0.32
Episode length: 487.92 +/- 128.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 488      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.132   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 186      |
|    time_elapsed    | 12566    |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 381000       |
| train/                  |              |
|    approx_kl            | 0.0045846943 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.723       |
|    explained_variance   | 0.252        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00169     |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00351     |
|    value_loss           | 0.00296      |
------------------------------------------
Eval num_timesteps=381500, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=-0.19 +/- 0.16
Episode length: 518.80 +/- 43.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 187      |
|    time_elapsed    | 12632    |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=-0.19 +/- 0.17
Episode length: 515.66 +/- 65.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 516        |
|    mean_reward          | -0.186     |
| time/                   |            |
|    total_timesteps      | 383000     |
| train/                  |            |
|    approx_kl            | 0.07477063 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.478     |
|    explained_variance   | 0.78       |
|    learning_rate        | 1e-05      |
|    loss                 | -0.0257    |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.00725   |
|    value_loss           | 0.000494   |
----------------------------------------
Eval num_timesteps=383500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 506      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 188      |
|    time_elapsed    | 12715    |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 385500       |
| train/                  |              |
|    approx_kl            | 0.0015191981 |
|    clip_fraction        | 0.00791      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.75        |
|    explained_variance   | 0.503        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00377     |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 0.00132      |
------------------------------------------
Eval num_timesteps=386000, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.131   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 189      |
|    time_elapsed    | 12782    |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 0.0050585847 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.911       |
|    explained_variance   | 0.299        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0136      |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 0.00422      |
------------------------------------------
Eval num_timesteps=388000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 190      |
|    time_elapsed    | 12848    |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0036388475 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.72        |
|    explained_variance   | 0.477        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00471      |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00378     |
|    value_loss           | 0.00421      |
------------------------------------------
Eval num_timesteps=390000, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 491      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 191      |
|    time_elapsed    | 12914    |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 391500       |
| train/                  |              |
|    approx_kl            | 0.0028687227 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.775       |
|    explained_variance   | 0.484        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000688     |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.00246     |
|    value_loss           | 0.00776      |
------------------------------------------
Eval num_timesteps=392000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 488      |
|    ep_rew_mean     | -0.0953  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 192      |
|    time_elapsed    | 12981    |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 393500       |
| train/                  |              |
|    approx_kl            | 0.0043105874 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.756       |
|    explained_variance   | 0.592        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0198      |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.006       |
|    value_loss           | 0.00333      |
------------------------------------------
Eval num_timesteps=394000, episode_reward=-0.16 +/- 0.24
Episode length: 504.58 +/- 100.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0836  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 193      |
|    time_elapsed    | 13046    |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 395500       |
| train/                  |              |
|    approx_kl            | 0.0029688429 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | 0.514        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00293      |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 0.00369      |
------------------------------------------
Eval num_timesteps=396000, episode_reward=-0.16 +/- 0.24
Episode length: 504.48 +/- 100.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-0.16 +/- 0.23
Episode length: 505.96 +/- 93.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=-0.16 +/- 0.24
Episode length: 504.50 +/- 100.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0829  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 194      |
|    time_elapsed    | 13112    |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 397500       |
| train/                  |              |
|    approx_kl            | 0.0014560157 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.614       |
|    explained_variance   | 0.508        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000801    |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00163     |
|    value_loss           | 0.00393      |
------------------------------------------
Eval num_timesteps=398000, episode_reward=-0.16 +/- 0.24
Episode length: 504.46 +/- 100.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-0.16 +/- 0.23
Episode length: 506.26 +/- 92.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0935  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 195      |
|    time_elapsed    | 13179    |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 399500       |
| train/                  |              |
|    approx_kl            | 0.0010373851 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.446       |
|    explained_variance   | 0.563        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00969     |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 0.000737     |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-0.19 +/- 0.17
Episode length: 515.94 +/- 63.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=-0.16 +/- 0.23
Episode length: 506.32 +/- 92.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-0.19 +/- 0.17
Episode length: 514.84 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0935  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 196      |
|    time_elapsed    | 13245    |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=-0.14 +/- 0.28
Episode length: 495.98 +/- 115.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 401500       |
| train/                  |              |
|    approx_kl            | 0.0010698189 |
|    clip_fraction        | 0.00557      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.8          |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00589     |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.000319     |
------------------------------------------
Eval num_timesteps=402000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=-0.16 +/- 0.24
Episode length: 504.56 +/- 100.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0935  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 197      |
|    time_elapsed    | 13311    |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 0.0047656535 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.844       |
|    explained_variance   | 0.724        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0124       |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 0.000424     |
------------------------------------------
Eval num_timesteps=404000, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=-0.16 +/- 0.24
Episode length: 504.46 +/- 100.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.105   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 198      |
|    time_elapsed    | 13392    |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=-0.14 +/- 0.29
Episode length: 494.04 +/- 122.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0012330447 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.0227       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00101     |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.00125     |
|    value_loss           | 0.000444     |
------------------------------------------
Eval num_timesteps=406500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=-0.19 +/- 0.15
Episode length: 523.20 +/- 12.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 523      |
|    mean_reward     | -0.189   |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.105   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 199      |
|    time_elapsed    | 13458    |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 408000       |
| train/                  |              |
|    approx_kl            | 0.0029075951 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.818       |
|    explained_variance   | -0.615       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0113      |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00396     |
|    value_loss           | 0.000401     |
------------------------------------------
Eval num_timesteps=408500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0936  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 200      |
|    time_elapsed    | 13523    |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0021745597 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.607       |
|    explained_variance   | 0.327        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0165       |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.00353     |
|    value_loss           | 0.00383      |
------------------------------------------
Eval num_timesteps=410500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=-0.16 +/- 0.24
Episode length: 504.56 +/- 100.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0833  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 201      |
|    time_elapsed    | 13589    |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=-0.19 +/- 0.17
Episode length: 515.78 +/- 64.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0047608167 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.94        |
|    explained_variance   | 0.324        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0189      |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.0053      |
|    value_loss           | 0.00324      |
------------------------------------------
Eval num_timesteps=412500, episode_reward=-0.14 +/- 0.29
Episode length: 494.18 +/- 121.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 479      |
|    ep_rew_mean     | -0.0716  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 202      |
|    time_elapsed    | 13655    |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=-0.19 +/- 0.17
Episode length: 514.82 +/- 71.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 0.0026062534 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.8         |
|    explained_variance   | 0.585        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00154      |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.00251     |
|    value_loss           | 0.00385      |
------------------------------------------
Eval num_timesteps=414500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0836  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 203      |
|    time_elapsed    | 13721    |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0028934453 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.673       |
|    explained_variance   | 0.588        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0227      |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00242     |
|    value_loss           | 0.000391     |
------------------------------------------
Eval num_timesteps=416500, episode_reward=-0.16 +/- 0.24
Episode length: 504.36 +/- 101.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=-0.14 +/- 0.29
Episode length: 494.04 +/- 122.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0836  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 204      |
|    time_elapsed    | 13786    |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=-0.19 +/- 0.17
Episode length: 515.78 +/- 64.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0035161395 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.681       |
|    explained_variance   | 0.532        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00574      |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00268     |
|    value_loss           | 0.000306     |
------------------------------------------
Eval num_timesteps=418500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0836  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 205      |
|    time_elapsed    | 13853    |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0011257114 |
|    clip_fraction        | 0.00752      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.653       |
|    explained_variance   | -0.264       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0118      |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 0.000376     |
------------------------------------------
Eval num_timesteps=420500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0836  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 206      |
|    time_elapsed    | 13919    |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=-0.14 +/- 0.29
Episode length: 494.02 +/- 122.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 494          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 422000       |
| train/                  |              |
|    approx_kl            | 0.0007440986 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.402       |
|    explained_variance   | -0.657       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00444     |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.00122     |
|    value_loss           | 0.000326     |
------------------------------------------
Eval num_timesteps=422500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0836  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 207      |
|    time_elapsed    | 13985    |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 515           |
|    mean_reward          | -0.186        |
| time/                   |               |
|    total_timesteps      | 424000        |
| train/                  |               |
|    approx_kl            | 0.00031472964 |
|    clip_fraction        | 0.00723       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.26         |
|    explained_variance   | -1.71         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.000368      |
|    n_updates            | 2070          |
|    policy_gradient_loss | -0.000732     |
|    value_loss           | 0.000317      |
-------------------------------------------
Eval num_timesteps=424500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=-0.19 +/- 0.17
Episode length: 514.72 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=-0.14 +/- 0.29
Episode length: 494.38 +/- 121.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 494      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0836  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 208      |
|    time_elapsed    | 14051    |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0037845057 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.561       |
|    explained_variance   | 0.716        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0209       |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 0.00087      |
------------------------------------------
Eval num_timesteps=426500, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=-0.16 +/- 0.24
Episode length: 504.46 +/- 100.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 484      |
|    ep_rew_mean     | -0.0836  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 209      |
|    time_elapsed    | 14133    |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 428500      |
| train/                  |             |
|    approx_kl            | 0.003256668 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.735      |
|    explained_variance   | -0.775      |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0126      |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00408    |
|    value_loss           | 0.00047     |
-----------------------------------------
Eval num_timesteps=429000, episode_reward=-0.19 +/- 0.17
Episode length: 514.88 +/- 70.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0957  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 210      |
|    time_elapsed    | 14198    |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 0.0014255205 |
|    clip_fraction        | 0.00742      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.267        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00398     |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 0.000526     |
------------------------------------------
Eval num_timesteps=431000, episode_reward=-0.16 +/- 0.24
Episode length: 504.54 +/- 100.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=-0.16 +/- 0.24
Episode length: 504.60 +/- 99.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0844  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 211      |
|    time_elapsed    | 14264    |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=-0.16 +/- 0.24
Episode length: 504.68 +/- 99.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 432500       |
| train/                  |              |
|    approx_kl            | 0.0025174473 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.402       |
|    explained_variance   | 0.133        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0138       |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 0.00367      |
------------------------------------------
Eval num_timesteps=433000, episode_reward=-0.19 +/- 0.17
Episode length: 515.76 +/- 64.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=-0.19 +/- 0.16
Episode length: 517.32 +/- 53.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=-0.19 +/- 0.17
Episode length: 515.86 +/- 63.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.073   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 212      |
|    time_elapsed    | 14329    |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 434500      |
| train/                  |             |
|    approx_kl            | 0.006473063 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.398       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00715    |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00386    |
|    value_loss           | 0.00385     |
-----------------------------------------
Eval num_timesteps=435000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=-0.19 +/- 0.17
Episode length: 516.06 +/- 62.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=-0.16 +/- 0.24
Episode length: 504.56 +/- 100.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0848  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 213      |
|    time_elapsed    | 14395    |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 436500       |
| train/                  |              |
|    approx_kl            | 0.0044591846 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.21        |
|    explained_variance   | 0.864        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0101       |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 0.000574     |
------------------------------------------
Eval num_timesteps=437000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=-0.16 +/- 0.24
Episode length: 504.52 +/- 100.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 505      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0854  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 214      |
|    time_elapsed    | 14461    |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 438500       |
| train/                  |              |
|    approx_kl            | 0.0013554547 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.526       |
|    explained_variance   | 0.497        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00525     |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.000923    |
|    value_loss           | 0.00322      |
------------------------------------------
Eval num_timesteps=439000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=-0.16 +/- 0.24
Episode length: 504.44 +/- 100.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.108   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 215      |
|    time_elapsed    | 14527    |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 440500        |
| train/                  |               |
|    approx_kl            | 0.00094024464 |
|    clip_fraction        | 0.00908       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.176        |
|    explained_variance   | 0.357         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.0143       |
|    n_updates            | 2150          |
|    policy_gradient_loss | -0.0013       |
|    value_loss           | 0.000467      |
-------------------------------------------
Eval num_timesteps=441000, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=-0.19 +/- 0.15
Episode length: 519.74 +/- 36.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 520      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 216      |
|    time_elapsed    | 14593    |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 442500      |
| train/                  |             |
|    approx_kl            | 0.005468967 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.679       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00619     |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 0.00282     |
-----------------------------------------
Eval num_timesteps=443000, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 217      |
|    time_elapsed    | 14659    |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 444500       |
| train/                  |              |
|    approx_kl            | 0.0037479394 |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.566       |
|    explained_variance   | 0.382        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00309     |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.0042      |
|    value_loss           | 0.00353      |
------------------------------------------
Eval num_timesteps=445000, episode_reward=-0.19 +/- 0.17
Episode length: 514.74 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=-0.19 +/- 0.17
Episode length: 514.80 +/- 71.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=-0.19 +/- 0.17
Episode length: 514.68 +/- 72.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 218      |
|    time_elapsed    | 14726    |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 446500       |
| train/                  |              |
|    approx_kl            | 0.0009387642 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 0.539        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00629      |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 0.00332      |
------------------------------------------
Eval num_timesteps=447000, episode_reward=-0.16 +/- 0.24
Episode length: 504.50 +/- 100.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=-0.14 +/- 0.28
Episode length: 497.84 +/- 109.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | -0.139   |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=-0.14 +/- 0.28
Episode length: 495.44 +/- 117.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 219      |
|    time_elapsed    | 14806    |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=-0.19 +/- 0.17
Episode length: 515.32 +/- 67.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.007856971 |
|    clip_fraction        | 0.0667      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.55        |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0159     |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0077     |
|    value_loss           | 0.00111     |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 220      |
|    time_elapsed    | 14873    |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=-0.17 +/- 0.22
Episode length: 513.22 +/- 63.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 513          |
|    mean_reward          | -0.165       |
| time/                   |              |
|    total_timesteps      | 451000       |
| train/                  |              |
|    approx_kl            | 0.0023990313 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.506       |
|    explained_variance   | 0.664        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00473     |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 0.00106      |
------------------------------------------
Eval num_timesteps=451500, episode_reward=-0.16 +/- 0.24
Episode length: 504.32 +/- 101.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=-0.19 +/- 0.17
Episode length: 514.78 +/- 71.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=-0.16 +/- 0.23
Episode length: 505.58 +/- 95.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 221      |
|    time_elapsed    | 14939    |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=-0.14 +/- 0.28
Episode length: 495.94 +/- 115.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0021026994 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.575        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0128       |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00296     |
|    value_loss           | 0.000675     |
------------------------------------------
Eval num_timesteps=453500, episode_reward=-0.16 +/- 0.23
Episode length: 505.76 +/- 94.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=-0.16 +/- 0.23
Episode length: 505.68 +/- 94.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 506      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=-0.19 +/- 0.17
Episode length: 515.94 +/- 63.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 222      |
|    time_elapsed    | 15003    |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=-0.14 +/- 0.28
Episode length: 495.86 +/- 115.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 496          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0005993458 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.356       |
|    explained_variance   | -1.11        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00888     |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.000447     |
------------------------------------------
Eval num_timesteps=455500, episode_reward=-0.16 +/- 0.23
Episode length: 506.56 +/- 90.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 507      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=-0.19 +/- 0.16
Episode length: 516.94 +/- 56.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 517      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=-0.19 +/- 0.17
Episode length: 514.78 +/- 71.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 223      |
|    time_elapsed    | 15068    |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.002562277 |
|    clip_fraction        | 0.0119      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.795      |
|    explained_variance   | -0.0156     |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0069     |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00285    |
|    value_loss           | 0.000681    |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=-0.19 +/- 0.17
Episode length: 514.78 +/- 71.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 224      |
|    time_elapsed    | 15134    |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0007911307 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.191       |
|    explained_variance   | 0.403        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0159      |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00245     |
|    value_loss           | 0.000457     |
------------------------------------------
Eval num_timesteps=459500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 225      |
|    time_elapsed    | 15202    |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 461000        |
| train/                  |               |
|    approx_kl            | 0.00090275076 |
|    clip_fraction        | 0.00664       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.266        |
|    explained_variance   | -0.832        |
|    learning_rate        | 1e-05         |
|    loss                 | -0.000785     |
|    n_updates            | 2250          |
|    policy_gradient_loss | -0.000935     |
|    value_loss           | 0.000429      |
-------------------------------------------
Eval num_timesteps=461500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 226      |
|    time_elapsed    | 15269    |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 463000      |
| train/                  |             |
|    approx_kl            | 0.009910591 |
|    clip_fraction        | 0.0864      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | -2.74       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0329     |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00997    |
|    value_loss           | 0.000311    |
-----------------------------------------
Eval num_timesteps=463500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 227      |
|    time_elapsed    | 15336    |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0021437854 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.486       |
|    explained_variance   | -0.431       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00654     |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 0.000683     |
------------------------------------------
Eval num_timesteps=465500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 228      |
|    time_elapsed    | 15402    |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0021644575 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.694       |
|    explained_variance   | -0.068       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.012       |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.00253     |
|    value_loss           | 0.000545     |
------------------------------------------
Eval num_timesteps=467500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 229      |
|    time_elapsed    | 15469    |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0027833716 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.899       |
|    explained_variance   | -2.81        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00443      |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.0045      |
|    value_loss           | 0.000507     |
------------------------------------------
Eval num_timesteps=469500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 230      |
|    time_elapsed    | 15553    |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.008309442 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | -2.07       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.022      |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00525    |
|    value_loss           | 0.000412    |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 231      |
|    time_elapsed    | 15620    |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 473500       |
| train/                  |              |
|    approx_kl            | 0.0024110104 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.697       |
|    explained_variance   | 0.491        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0106       |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.00246     |
|    value_loss           | 0.000575     |
------------------------------------------
Eval num_timesteps=474000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 232      |
|    time_elapsed    | 15688    |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 475500       |
| train/                  |              |
|    approx_kl            | 0.0026560444 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.506       |
|    explained_variance   | -1.87        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00527     |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 0.000379     |
------------------------------------------
Eval num_timesteps=476000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 233      |
|    time_elapsed    | 15755    |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 477500       |
| train/                  |              |
|    approx_kl            | 0.0015066883 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.367       |
|    explained_variance   | 0.224        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0189      |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.00244     |
|    value_loss           | 0.000271     |
------------------------------------------
Eval num_timesteps=478000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 234      |
|    time_elapsed    | 15822    |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 479500       |
| train/                  |              |
|    approx_kl            | 0.0049539986 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.844       |
|    explained_variance   | -0.0138      |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0114       |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.00341     |
|    value_loss           | 0.00425      |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.129   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 235      |
|    time_elapsed    | 15889    |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 481500       |
| train/                  |              |
|    approx_kl            | 0.0036619408 |
|    clip_fraction        | 0.00991      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.934       |
|    explained_variance   | -0.806       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00537      |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.00265     |
|    value_loss           | 0.000535     |
------------------------------------------
Eval num_timesteps=482000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 236      |
|    time_elapsed    | 15956    |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 483500       |
| train/                  |              |
|    approx_kl            | 0.0032185297 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.682       |
|    explained_variance   | 0.0652       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00916     |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.00393     |
|    value_loss           | 0.00463      |
------------------------------------------
Eval num_timesteps=484000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 237      |
|    time_elapsed    | 16023    |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 485500       |
| train/                  |              |
|    approx_kl            | 0.0059218993 |
|    clip_fraction        | 0.0445       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.916       |
|    explained_variance   | 0.381        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0153      |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 0.000491     |
------------------------------------------
Eval num_timesteps=486000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.117   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 238      |
|    time_elapsed    | 16090    |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 487500       |
| train/                  |              |
|    approx_kl            | 0.0048027323 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.774       |
|    explained_variance   | 0.137        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0121      |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 0.0104       |
------------------------------------------
Eval num_timesteps=488000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.117   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 239      |
|    time_elapsed    | 16157    |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 489500      |
| train/                  |             |
|    approx_kl            | 0.008844381 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | -0.443      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00402    |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00674    |
|    value_loss           | 0.000967    |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.118   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 240      |
|    time_elapsed    | 16240    |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 492000       |
| train/                  |              |
|    approx_kl            | 0.0033648056 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.862       |
|    explained_variance   | 0.0499       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0107      |
|    n_updates            | 2400         |
|    policy_gradient_loss | -0.00389     |
|    value_loss           | 0.00407      |
------------------------------------------
Eval num_timesteps=492500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -0.106   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 241      |
|    time_elapsed    | 16307    |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0045662965 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.135        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00521     |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.00332     |
|    value_loss           | 0.0041       |
------------------------------------------
Eval num_timesteps=494500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.118   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 242      |
|    time_elapsed    | 16373    |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.000420368 |
|    clip_fraction        | 0.00205     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.341      |
|    explained_variance   | 0.343       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00732    |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.000916   |
|    value_loss           | 0.000574    |
-----------------------------------------
Eval num_timesteps=496500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 496      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 243      |
|    time_elapsed    | 16440    |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.007183647 |
|    clip_fraction        | 0.0367      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.251       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0193     |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 0.00411     |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 244      |
|    time_elapsed    | 16507    |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0030755682 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.802       |
|    explained_variance   | 0.0848       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0136      |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 0.00434      |
------------------------------------------
Eval num_timesteps=500500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 245      |
|    time_elapsed    | 16574    |
|    total_timesteps | 501760   |
---------------------------------
Eval num_timesteps=502000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 502000      |
| train/                  |             |
|    approx_kl            | 0.010688899 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.893      |
|    explained_variance   | -0.556      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00752    |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.0044     |
|    value_loss           | 0.000504    |
-----------------------------------------
Eval num_timesteps=502500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 246      |
|    time_elapsed    | 16641    |
|    total_timesteps | 503808   |
---------------------------------
Eval num_timesteps=504000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 504000      |
| train/                  |             |
|    approx_kl            | 0.006951756 |
|    clip_fraction        | 0.0179      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.295       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00377     |
|    n_updates            | 2460        |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 0.000381    |
-----------------------------------------
Eval num_timesteps=504500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 247      |
|    time_elapsed    | 16708    |
|    total_timesteps | 505856   |
---------------------------------
Eval num_timesteps=506000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 506000        |
| train/                  |               |
|    approx_kl            | 0.00021806164 |
|    clip_fraction        | 0.00439       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.211        |
|    explained_variance   | 0.157         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00288      |
|    n_updates            | 2470          |
|    policy_gradient_loss | -0.000966     |
|    value_loss           | 0.000521      |
-------------------------------------------
Eval num_timesteps=506500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 248      |
|    time_elapsed    | 16775    |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=508000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 508000       |
| train/                  |              |
|    approx_kl            | 0.0053411317 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.979       |
|    explained_variance   | 0.494        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0012       |
|    n_updates            | 2480         |
|    policy_gradient_loss | -0.00533     |
|    value_loss           | 0.000276     |
------------------------------------------
Eval num_timesteps=508500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 249      |
|    time_elapsed    | 16842    |
|    total_timesteps | 509952   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 510000       |
| train/                  |              |
|    approx_kl            | 0.0015991082 |
|    clip_fraction        | 0.00771      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.555       |
|    explained_variance   | 0.0521       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00879     |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.00383     |
|    value_loss           | 0.000452     |
------------------------------------------
Eval num_timesteps=510500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 250      |
|    time_elapsed    | 16925    |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=512500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 512500       |
| train/                  |              |
|    approx_kl            | 0.0020934003 |
|    clip_fraction        | 0.00718      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.509       |
|    explained_variance   | 0.703        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00592      |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 0.000341     |
------------------------------------------
Eval num_timesteps=513000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 251      |
|    time_elapsed    | 16992    |
|    total_timesteps | 514048   |
---------------------------------
Eval num_timesteps=514500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 514500       |
| train/                  |              |
|    approx_kl            | 0.0017590723 |
|    clip_fraction        | 0.00723      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.23         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00418     |
|    n_updates            | 2510         |
|    policy_gradient_loss | -0.000835    |
|    value_loss           | 0.000306     |
------------------------------------------
Eval num_timesteps=515000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 493      |
|    ep_rew_mean     | -0.107   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 252      |
|    time_elapsed    | 17060    |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=516500, episode_reward=-0.19 +/- 0.17
Episode length: 515.48 +/- 66.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 516500       |
| train/                  |              |
|    approx_kl            | 0.0016865155 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.876       |
|    explained_variance   | -1.09        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0273      |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 0.000324     |
------------------------------------------
Eval num_timesteps=517000, episode_reward=-0.19 +/- 0.17
Episode length: 515.66 +/- 65.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=-0.14 +/- 0.28
Episode length: 498.36 +/- 106.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 498      |
|    mean_reward     | -0.139   |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0954  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 253      |
|    time_elapsed    | 17125    |
|    total_timesteps | 518144   |
---------------------------------
Eval num_timesteps=518500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 518500       |
| train/                  |              |
|    approx_kl            | 0.0015630608 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.572       |
|    explained_variance   | 0.0689       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00724      |
|    n_updates            | 2530         |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 0.00424      |
------------------------------------------
Eval num_timesteps=519000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0726  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 254      |
|    time_elapsed    | 17192    |
|    total_timesteps | 520192   |
---------------------------------
Eval num_timesteps=520500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 520500       |
| train/                  |              |
|    approx_kl            | 0.0010732044 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.292       |
|    explained_variance   | 0.186        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0122      |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 0.00658      |
------------------------------------------
Eval num_timesteps=521000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0726  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 255      |
|    time_elapsed    | 17259    |
|    total_timesteps | 522240   |
---------------------------------
Eval num_timesteps=522500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 522500      |
| train/                  |             |
|    approx_kl            | 0.002866975 |
|    clip_fraction        | 0.0163      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.524      |
|    explained_variance   | 0.77        |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00173    |
|    n_updates            | 2550        |
|    policy_gradient_loss | -0.00278    |
|    value_loss           | 0.000496    |
-----------------------------------------
Eval num_timesteps=523000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 482      |
|    ep_rew_mean     | -0.0726  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 256      |
|    time_elapsed    | 17326    |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=524500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 524500       |
| train/                  |              |
|    approx_kl            | 0.0045975135 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.928       |
|    explained_variance   | 0.381        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0232      |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.00531     |
|    value_loss           | 0.000647     |
------------------------------------------
Eval num_timesteps=525000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0838  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 257      |
|    time_elapsed    | 17393    |
|    total_timesteps | 526336   |
---------------------------------
Eval num_timesteps=526500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 526500       |
| train/                  |              |
|    approx_kl            | 0.0040309103 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.741       |
|    explained_variance   | -0.104       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0142       |
|    n_updates            | 2570         |
|    policy_gradient_loss | -0.00536     |
|    value_loss           | 0.000621     |
------------------------------------------
Eval num_timesteps=527000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0838  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 258      |
|    time_elapsed    | 17461    |
|    total_timesteps | 528384   |
---------------------------------
Eval num_timesteps=528500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 528500       |
| train/                  |              |
|    approx_kl            | 0.0016057114 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.596        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0026       |
|    n_updates            | 2580         |
|    policy_gradient_loss | -0.00304     |
|    value_loss           | 0.000227     |
------------------------------------------
Eval num_timesteps=529000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0838  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 259      |
|    time_elapsed    | 17528    |
|    total_timesteps | 530432   |
---------------------------------
Eval num_timesteps=530500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 530500       |
| train/                  |              |
|    approx_kl            | 0.0024896942 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.43        |
|    explained_variance   | 0.662        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0251      |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.0034      |
|    value_loss           | 0.000286     |
------------------------------------------
Eval num_timesteps=531000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0942  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 260      |
|    time_elapsed    | 17595    |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=532500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0002519028 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | -2.94        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00293      |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 0.000279     |
------------------------------------------
Eval num_timesteps=533000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -0.0942  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 261      |
|    time_elapsed    | 17679    |
|    total_timesteps | 534528   |
---------------------------------
Eval num_timesteps=535000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0013323993 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.136       |
|    explained_variance   | -1.84        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00575     |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.000427    |
|    value_loss           | 0.000298     |
------------------------------------------
Eval num_timesteps=535500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.118   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 262      |
|    time_elapsed    | 17745    |
|    total_timesteps | 536576   |
---------------------------------
Eval num_timesteps=537000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 537000       |
| train/                  |              |
|    approx_kl            | 0.0024399203 |
|    clip_fraction        | 0.00493      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.877       |
|    explained_variance   | -0.786       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00526     |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.00368     |
|    value_loss           | 0.000365     |
------------------------------------------
Eval num_timesteps=537500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 263      |
|    time_elapsed    | 17812    |
|    total_timesteps | 538624   |
---------------------------------
Eval num_timesteps=539000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 539000       |
| train/                  |              |
|    approx_kl            | 0.0023018438 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.623       |
|    explained_variance   | 0.13         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0046       |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.00386     |
|    value_loss           | 0.00404      |
------------------------------------------
Eval num_timesteps=539500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 264      |
|    time_elapsed    | 17880    |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=541000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 541000       |
| train/                  |              |
|    approx_kl            | 0.0008720871 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0.29         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0114      |
|    n_updates            | 2640         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 0.00306      |
------------------------------------------
Eval num_timesteps=541500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.131   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 265      |
|    time_elapsed    | 17948    |
|    total_timesteps | 542720   |
---------------------------------
Eval num_timesteps=543000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 543000      |
| train/                  |             |
|    approx_kl            | 0.002032748 |
|    clip_fraction        | 0.00801     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | 0.682       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0111     |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00286    |
|    value_loss           | 0.000681    |
-----------------------------------------
Eval num_timesteps=543500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=-0.19 +/- 0.17
Episode length: 514.86 +/- 70.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.131   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 266      |
|    time_elapsed    | 18014    |
|    total_timesteps | 544768   |
---------------------------------
Eval num_timesteps=545000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 0.0049798465 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.41        |
|    explained_variance   | 0.422        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00192      |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.00387     |
|    value_loss           | 0.000477     |
------------------------------------------
Eval num_timesteps=545500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 267      |
|    time_elapsed    | 18082    |
|    total_timesteps | 546816   |
---------------------------------
Eval num_timesteps=547000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 547000       |
| train/                  |              |
|    approx_kl            | 0.0038236189 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.0399       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.013       |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00556     |
|    value_loss           | 0.00407      |
------------------------------------------
Eval num_timesteps=547500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -0.142   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 268      |
|    time_elapsed    | 18149    |
|    total_timesteps | 548864   |
---------------------------------
Eval num_timesteps=549000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 549000      |
| train/                  |             |
|    approx_kl            | 0.003985841 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | -0.253      |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0229     |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.00369    |
|    value_loss           | 0.000335    |
-----------------------------------------
Eval num_timesteps=549500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -0.131   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 269      |
|    time_elapsed    | 18217    |
|    total_timesteps | 550912   |
---------------------------------
Eval num_timesteps=551000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 551000      |
| train/                  |             |
|    approx_kl            | 0.002876956 |
|    clip_fraction        | 0.00864     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.276       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0247     |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00297    |
|    value_loss           | 0.00369     |
-----------------------------------------
Eval num_timesteps=551500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 270      |
|    time_elapsed    | 18284    |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=553000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 525           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 553000        |
| train/                  |               |
|    approx_kl            | 0.00061944395 |
|    clip_fraction        | 0.00884       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.121        |
|    explained_variance   | 0.404         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00093       |
|    n_updates            | 2700          |
|    policy_gradient_loss | -0.00249      |
|    value_loss           | 0.00307       |
-------------------------------------------
Eval num_timesteps=553500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 271      |
|    time_elapsed    | 18367    |
|    total_timesteps | 555008   |
---------------------------------
Eval num_timesteps=555500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 555500       |
| train/                  |              |
|    approx_kl            | 0.0021714345 |
|    clip_fraction        | 0.00933      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.236        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0012       |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 0.000519     |
------------------------------------------
Eval num_timesteps=556000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 272      |
|    time_elapsed    | 18434    |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=557500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 557500       |
| train/                  |              |
|    approx_kl            | 0.0052341092 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.585       |
|    explained_variance   | 0.713        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0186      |
|    n_updates            | 2720         |
|    policy_gradient_loss | -0.00518     |
|    value_loss           | 0.000513     |
------------------------------------------
Eval num_timesteps=558000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=-0.19 +/- 0.17
Episode length: 515.28 +/- 68.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 273      |
|    time_elapsed    | 18501    |
|    total_timesteps | 559104   |
---------------------------------
Eval num_timesteps=559500, episode_reward=-0.19 +/- 0.17
Episode length: 515.46 +/- 66.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 559500       |
| train/                  |              |
|    approx_kl            | 0.0032382803 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | 0.479        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0066       |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.00401     |
|    value_loss           | 0.000568     |
------------------------------------------
Eval num_timesteps=560000, episode_reward=-0.19 +/- 0.16
Episode length: 519.02 +/- 41.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 519      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 274      |
|    time_elapsed    | 18568    |
|    total_timesteps | 561152   |
---------------------------------
Eval num_timesteps=561500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 561500      |
| train/                  |             |
|    approx_kl            | 0.003678977 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.25        |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00351    |
|    n_updates            | 2740        |
|    policy_gradient_loss | -0.00586    |
|    value_loss           | 0.000293    |
-----------------------------------------
Eval num_timesteps=562000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 275      |
|    time_elapsed    | 18635    |
|    total_timesteps | 563200   |
---------------------------------
Eval num_timesteps=563500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 563500       |
| train/                  |              |
|    approx_kl            | 0.0015039132 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.41        |
|    explained_variance   | 0.46         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0113       |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00273     |
|    value_loss           | 0.000332     |
------------------------------------------
Eval num_timesteps=564000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 276      |
|    time_elapsed    | 18702    |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=565500, episode_reward=-0.19 +/- 0.17
Episode length: 515.90 +/- 63.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 516          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 565500       |
| train/                  |              |
|    approx_kl            | 0.0009234704 |
|    clip_fraction        | 0.00859      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0.617        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.03        |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.00363     |
|    value_loss           | 0.000328     |
------------------------------------------
Eval num_timesteps=566000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 497      |
|    ep_rew_mean     | -0.119   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 277      |
|    time_elapsed    | 18769    |
|    total_timesteps | 567296   |
---------------------------------
Eval num_timesteps=567500, episode_reward=-0.19 +/- 0.17
Episode length: 515.48 +/- 66.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 515           |
|    mean_reward          | -0.186        |
| time/                   |               |
|    total_timesteps      | 567500        |
| train/                  |               |
|    approx_kl            | 0.00053401565 |
|    clip_fraction        | 0.00801       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0889       |
|    explained_variance   | -2.08         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00183      |
|    n_updates            | 2770          |
|    policy_gradient_loss | -0.00138      |
|    value_loss           | 0.000197      |
-------------------------------------------
Eval num_timesteps=568000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 501      |
|    ep_rew_mean     | -0.13    |
| time/              |          |
|    fps             | 30       |
|    iterations      | 278      |
|    time_elapsed    | 18835    |
|    total_timesteps | 569344   |
---------------------------------
Eval num_timesteps=569500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 569500       |
| train/                  |              |
|    approx_kl            | 0.0026659265 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.466       |
|    explained_variance   | -0.283       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00014     |
|    n_updates            | 2780         |
|    policy_gradient_loss | -0.00449     |
|    value_loss           | 0.000219     |
------------------------------------------
Eval num_timesteps=570000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 508      |
|    ep_rew_mean     | -0.153   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 279      |
|    time_elapsed    | 18902    |
|    total_timesteps | 571392   |
---------------------------------
Eval num_timesteps=571500, episode_reward=-0.19 +/- 0.17
Episode length: 514.70 +/- 72.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 515          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 571500       |
| train/                  |              |
|    approx_kl            | 0.0054505365 |
|    clip_fraction        | 0.0503       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.908       |
|    explained_variance   | -1.54        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0198      |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.00619     |
|    value_loss           | 0.000231     |
------------------------------------------
Eval num_timesteps=572000, episode_reward=-0.19 +/- 0.17
Episode length: 515.28 +/- 68.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=-0.19 +/- 0.17
Episode length: 515.54 +/- 66.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=-0.14 +/- 0.29
Episode length: 494.58 +/- 120.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 503      |
|    ep_rew_mean     | -0.141   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 280      |
|    time_elapsed    | 18967    |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=573500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 573500       |
| train/                  |              |
|    approx_kl            | 0.0034352294 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.458       |
|    explained_variance   | 0.013        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.018       |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.00687     |
|    value_loss           | 0.00403      |
------------------------------------------
Eval num_timesteps=574000, episode_reward=-0.19 +/- 0.17
Episode length: 515.86 +/- 63.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 516      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=-0.19 +/- 0.17
Episode length: 515.32 +/- 67.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.118   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 281      |
|    time_elapsed    | 19034    |
|    total_timesteps | 575488   |
---------------------------------
Eval num_timesteps=575500, episode_reward=-0.19 +/- 0.17
Episode length: 515.32 +/- 67.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 515         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 575500      |
| train/                  |             |
|    approx_kl            | 0.002074991 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.902      |
|    explained_variance   | 0.263       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00875    |
|    n_updates            | 2810        |
|    policy_gradient_loss | -0.00447    |
|    value_loss           | 0.00606     |
-----------------------------------------
Eval num_timesteps=576000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 495      |
|    ep_rew_mean     | -0.118   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 282      |
|    time_elapsed    | 19117    |
|    total_timesteps | 577536   |
---------------------------------
Eval num_timesteps=578000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0020778012 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.269       |
|    explained_variance   | 0.249        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0115       |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00459     |
|    value_loss           | 0.000548     |
------------------------------------------
Eval num_timesteps=578500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.095   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 283      |
|    time_elapsed    | 19185    |
|    total_timesteps | 579584   |
---------------------------------
Eval num_timesteps=580000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 580000       |
| train/                  |              |
|    approx_kl            | 0.0023205043 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.209       |
|    explained_variance   | 0.594        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00495      |
|    n_updates            | 2830         |
|    policy_gradient_loss | -0.00685     |
|    value_loss           | 0.0057       |
------------------------------------------
Eval num_timesteps=580500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.095   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 284      |
|    time_elapsed    | 19252    |
|    total_timesteps | 581632   |
---------------------------------
Eval num_timesteps=582000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0045054727 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.469       |
|    explained_variance   | 0.649        |
|    learning_rate        | 1e-05        |
|    loss                 | 5.4e-05      |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.00785     |
|    value_loss           | 0.00081      |
------------------------------------------
Eval num_timesteps=582500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.083   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 285      |
|    time_elapsed    | 19319    |
|    total_timesteps | 583680   |
---------------------------------
Eval num_timesteps=584000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 584000       |
| train/                  |              |
|    approx_kl            | 0.0050506582 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.752       |
|    explained_variance   | 0.611        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00166      |
|    n_updates            | 2850         |
|    policy_gradient_loss | -0.0082      |
|    value_loss           | 0.00327      |
------------------------------------------
Eval num_timesteps=584500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.083   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 286      |
|    time_elapsed    | 19386    |
|    total_timesteps | 585728   |
---------------------------------
Eval num_timesteps=586000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 586000       |
| train/                  |              |
|    approx_kl            | 0.0012226057 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.214       |
|    explained_variance   | 0.714        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00308     |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.000859     |
------------------------------------------
Eval num_timesteps=586500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.083   |
| time/              |          |
|    fps             | 30       |
|    iterations      | 287      |
|    time_elapsed    | 19453    |
|    total_timesteps | 587776   |
---------------------------------
Eval num_timesteps=588000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0047856336 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.774       |
|    explained_variance   | 0.382        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0116      |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.00419     |
|    value_loss           | 0.00412      |
------------------------------------------
Eval num_timesteps=588500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0945  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 288      |
|    time_elapsed    | 19520    |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=590000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0031572706 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.39        |
|    explained_variance   | 0.268        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0083      |
|    n_updates            | 2880         |
|    policy_gradient_loss | -0.00361     |
|    value_loss           | 0.000313     |
------------------------------------------
Eval num_timesteps=590500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0831  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 289      |
|    time_elapsed    | 19587    |
|    total_timesteps | 591872   |
---------------------------------
Eval num_timesteps=592000, episode_reward=-0.16 +/- 0.23
Episode length: 505.16 +/- 97.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 505          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 592000       |
| train/                  |              |
|    approx_kl            | 0.0055437107 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.928       |
|    explained_variance   | 0.263        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00375     |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00408     |
|    value_loss           | 0.00338      |
------------------------------------------
Eval num_timesteps=592500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=-0.14 +/- 0.28
Episode length: 495.06 +/- 118.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 495      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 483      |
|    ep_rew_mean     | -0.0831  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 290      |
|    time_elapsed    | 19652    |
|    total_timesteps | 593920   |
---------------------------------
Eval num_timesteps=594000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 594000       |
| train/                  |              |
|    approx_kl            | 0.0024756128 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0.695        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00201      |
|    n_updates            | 2900         |
|    policy_gradient_loss | -0.00269     |
|    value_loss           | 0.000747     |
------------------------------------------
Eval num_timesteps=594500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -0.0947  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 291      |
|    time_elapsed    | 19719    |
|    total_timesteps | 595968   |
---------------------------------
Eval num_timesteps=596000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 525         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 596000      |
| train/                  |             |
|    approx_kl            | 0.003696585 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.371      |
|    explained_variance   | 0.78        |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0172     |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 0.00028     |
-----------------------------------------
Eval num_timesteps=596500, episode_reward=-0.16 +/- 0.24
Episode length: 504.34 +/- 101.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=-0.16 +/- 0.24
Episode length: 504.44 +/- 100.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 486      |
|    ep_rew_mean     | -0.0843  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 292      |
|    time_elapsed    | 19800    |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=598500, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 525          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 598500       |
| train/                  |              |
|    approx_kl            | 0.0042537986 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.627       |
|    explained_variance   | 0.163        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0133       |
|    n_updates            | 2920         |
|    policy_gradient_loss | -0.00593     |
|    value_loss           | 0.00376      |
------------------------------------------
Eval num_timesteps=599000, episode_reward=-0.21 +/- 0.00
Episode length: 525.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 525      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=-0.19 +/- 0.17
Episode length: 514.66 +/- 72.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 515      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-0.16 +/- 0.24
Episode length: 504.38 +/- 101.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 504      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 489      |
|    ep_rew_mean     | -0.0955  |
| time/              |          |
|    fps             | 30       |
|    iterations      | 293      |
|    time_elapsed    | 19867    |
|    total_timesteps | 600064   |
---------------------------------
