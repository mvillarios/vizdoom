/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-547.78 +/- 81.73
Episode length: 111.34 +/- 67.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | -548     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-575.57 +/- 42.33
Episode length: 103.44 +/- 44.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -576     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-560.63 +/- 48.27
Episode length: 111.84 +/- 57.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-566.17 +/- 32.97
Episode length: 101.34 +/- 34.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | -410     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 1        |
|    time_elapsed    | 23       |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=1800.09 +/- 480.61
Episode length: 42.28 +/- 2.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.3       |
|    mean_reward          | 1.8e+03    |
| time/                   |            |
|    total_timesteps      | 2500       |
| train/                  |            |
|    approx_kl            | 0.01049531 |
|    clip_fraction        | 0.0833     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -2.07      |
|    explained_variance   | -0.000132  |
|    learning_rate        | 0.0001     |
|    loss                 | 762        |
|    n_updates            | 4          |
|    policy_gradient_loss | -0.00853   |
|    value_loss           | 1.85e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=1875.77 +/- 404.86
Episode length: 42.54 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
New best mean reward!
Eval num_timesteps=3500, episode_reward=1773.62 +/- 531.10
Episode length: 42.18 +/- 2.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=1692.91 +/- 614.38
Episode length: 41.68 +/- 2.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | -322     |
| time/              |          |
|    fps             | 119      |
|    iterations      | 2        |
|    time_elapsed    | 34       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=1682.68 +/- 558.66
Episode length: 41.74 +/- 2.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.7        |
|    mean_reward          | 1.68e+03    |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.004038698 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.000157    |
|    learning_rate        | 0.0001      |
|    loss                 | 584         |
|    n_updates            | 5           |
|    policy_gradient_loss | -0.00693    |
|    value_loss           | 1.64e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=1781.89 +/- 496.78
Episode length: 41.88 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=1719.83 +/- 565.71
Episode length: 42.02 +/- 2.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=1835.42 +/- 428.35
Episode length: 42.24 +/- 2.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 147      |
|    iterations      | 3        |
|    time_elapsed    | 41       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=1738.02 +/- 539.36
Episode length: 41.98 +/- 2.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.004759142 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | 0.000672    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.22e+03    |
|    n_updates            | 6           |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 2.71e+03    |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=1735.42 +/- 550.53
Episode length: 42.04 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=1638.58 +/- 599.60
Episode length: 41.58 +/- 2.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=1626.73 +/- 644.95
Episode length: 41.96 +/- 3.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | -251     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 4        |
|    time_elapsed    | 49       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=8500, episode_reward=1809.13 +/- 500.45
Episode length: 42.08 +/- 2.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.81e+03     |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0065401974 |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.93        |
|    explained_variance   | 0.000998     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.39e+03     |
|    n_updates            | 7            |
|    policy_gradient_loss | -0.00464     |
|    value_loss           | 3.3e+03      |
------------------------------------------
Eval num_timesteps=9000, episode_reward=1812.03 +/- 487.18
Episode length: 41.98 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=1657.57 +/- 579.96
Episode length: 41.60 +/- 2.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1654.21 +/- 608.37
Episode length: 41.76 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=10500, episode_reward=1664.72 +/- 563.59
Episode length: 42.14 +/- 2.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.66e+03     |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0030890885 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.86        |
|    explained_variance   | 0.000208     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.05e+03     |
|    n_updates            | 8            |
|    policy_gradient_loss | -0.00223     |
|    value_loss           | 6.7e+03      |
------------------------------------------
Eval num_timesteps=11000, episode_reward=1538.43 +/- 627.36
Episode length: 41.70 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=1830.52 +/- 408.58
Episode length: 42.54 +/- 1.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=1723.14 +/- 544.02
Episode length: 41.76 +/- 2.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | -43.9    |
| time/              |          |
|    fps             | 191      |
|    iterations      | 6        |
|    time_elapsed    | 64       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=12500, episode_reward=1695.79 +/- 589.73
Episode length: 41.52 +/- 3.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.5         |
|    mean_reward          | 1.7e+03      |
| time/                   |              |
|    total_timesteps      | 12500        |
| train/                  |              |
|    approx_kl            | 0.0033802465 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.72        |
|    explained_variance   | -9.35e-05    |
|    learning_rate        | 0.0001       |
|    loss                 | 2.02e+03     |
|    n_updates            | 9            |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 4.65e+03     |
------------------------------------------
Eval num_timesteps=13000, episode_reward=1775.55 +/- 497.43
Episode length: 41.90 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=1657.99 +/- 605.42
Episode length: 41.48 +/- 2.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=1830.09 +/- 466.03
Episode length: 42.10 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.8     |
|    ep_rew_mean     | 70       |
| time/              |          |
|    fps             | 200      |
|    iterations      | 7        |
|    time_elapsed    | 71       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=14500, episode_reward=1767.40 +/- 519.59
Episode length: 42.42 +/- 2.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.4        |
|    mean_reward          | 1.77e+03    |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.003838701 |
|    clip_fraction        | 0.0216      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | 0.000956    |
|    learning_rate        | 0.0001      |
|    loss                 | 3.39e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 5.26e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=1607.20 +/- 611.38
Episode length: 41.42 +/- 2.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=1669.20 +/- 565.36
Episode length: 41.82 +/- 2.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=1781.01 +/- 501.91
Episode length: 42.18 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.8     |
|    ep_rew_mean     | 123      |
| time/              |          |
|    fps             | 206      |
|    iterations      | 8        |
|    time_elapsed    | 79       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=16500, episode_reward=1559.33 +/- 642.89
Episode length: 41.36 +/- 3.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.4        |
|    mean_reward          | 1.56e+03    |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.005236402 |
|    clip_fraction        | 0.0611      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.000641    |
|    learning_rate        | 0.0001      |
|    loss                 | 3.52e+03    |
|    n_updates            | 11          |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 8.37e+03    |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=1804.69 +/- 436.26
Episode length: 42.34 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=1522.49 +/- 670.75
Episode length: 41.32 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=1745.36 +/- 552.68
Episode length: 41.92 +/- 2.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | 279      |
| time/              |          |
|    fps             | 212      |
|    iterations      | 9        |
|    time_elapsed    | 86       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=1903.45 +/- 310.88
Episode length: 42.52 +/- 1.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.5       |
|    mean_reward          | 1.9e+03    |
| time/                   |            |
|    total_timesteps      | 18500      |
| train/                  |            |
|    approx_kl            | 0.00822373 |
|    clip_fraction        | 0.099      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 2.36e-05   |
|    learning_rate        | 0.0001     |
|    loss                 | 7.62e+03   |
|    n_updates            | 12         |
|    policy_gradient_loss | -0.00578   |
|    value_loss           | 2.09e+04   |
----------------------------------------
New best mean reward!
Eval num_timesteps=19000, episode_reward=1649.58 +/- 616.19
Episode length: 41.72 +/- 2.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=1841.25 +/- 413.88
Episode length: 42.54 +/- 1.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=1698.96 +/- 567.54
Episode length: 41.90 +/- 2.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | 413      |
| time/              |          |
|    fps             | 217      |
|    iterations      | 10       |
|    time_elapsed    | 94       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=1723.20 +/- 542.80
Episode length: 42.08 +/- 2.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.72e+03    |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.006332714 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.88       |
|    explained_variance   | 8.5e-05     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.5e+04     |
|    n_updates            | 13          |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 2.5e+04     |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=1766.90 +/- 503.12
Episode length: 42.44 +/- 2.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=1732.95 +/- 542.25
Episode length: 42.00 +/- 2.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=1830.20 +/- 420.55
Episode length: 42.32 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=1893.65 +/- 404.75
Episode length: 42.48 +/- 1.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.7     |
|    ep_rew_mean     | 478      |
| time/              |          |
|    fps             | 218      |
|    iterations      | 11       |
|    time_elapsed    | 103      |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=1773.87 +/- 458.61
Episode length: 41.96 +/- 2.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42         |
|    mean_reward          | 1.77e+03   |
| time/                   |            |
|    total_timesteps      | 23000      |
| train/                  |            |
|    approx_kl            | 0.00489496 |
|    clip_fraction        | 0.0759     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.705     |
|    explained_variance   | -4.1e-05   |
|    learning_rate        | 0.0001     |
|    loss                 | 7.09e+03   |
|    n_updates            | 14         |
|    policy_gradient_loss | -0.00559   |
|    value_loss           | 2.43e+04   |
----------------------------------------
Eval num_timesteps=23500, episode_reward=1698.65 +/- 578.77
Episode length: 41.78 +/- 2.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=1706.02 +/- 543.94
Episode length: 41.74 +/- 2.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=1709.09 +/- 564.66
Episode length: 41.74 +/- 3.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | 636      |
| time/              |          |
|    fps             | 222      |
|    iterations      | 12       |
|    time_elapsed    | 110      |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=1682.72 +/- 615.19
Episode length: 41.46 +/- 3.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.5         |
|    mean_reward          | 1.68e+03     |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0057315365 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 7.52e-05     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.1e+04      |
|    n_updates            | 15           |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 6.14e+04     |
------------------------------------------
Eval num_timesteps=25500, episode_reward=1821.93 +/- 454.67
Episode length: 42.38 +/- 1.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=1890.58 +/- 399.50
Episode length: 42.28 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=1922.94 +/- 361.15
Episode length: 42.44 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.5     |
|    ep_rew_mean     | 922      |
| time/              |          |
|    fps             | 225      |
|    iterations      | 13       |
|    time_elapsed    | 117      |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=27000, episode_reward=1701.19 +/- 554.59
Episode length: 42.00 +/- 1.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 1.7e+03      |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0032233556 |
|    clip_fraction        | 0.0441       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.391       |
|    explained_variance   | 0.0002       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.32e+04     |
|    n_updates            | 16           |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 6.8e+04      |
------------------------------------------
Eval num_timesteps=27500, episode_reward=1845.15 +/- 426.49
Episode length: 42.34 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=1814.98 +/- 481.22
Episode length: 42.24 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=1814.63 +/- 462.75
Episode length: 42.56 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.2     |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 228      |
|    iterations      | 14       |
|    time_elapsed    | 125      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=1779.66 +/- 533.61
Episode length: 42.30 +/- 2.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.78e+03    |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.012976815 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.000513    |
|    learning_rate        | 0.0001      |
|    loss                 | 3.06e+04    |
|    n_updates            | 19          |
|    policy_gradient_loss | 7.57e-05    |
|    value_loss           | 8e+04       |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=1814.22 +/- 456.00
Episode length: 42.36 +/- 1.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=1696.21 +/- 553.00
Episode length: 42.24 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=1843.87 +/- 472.24
Episode length: 42.00 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.7     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 230      |
|    iterations      | 15       |
|    time_elapsed    | 133      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=1618.48 +/- 654.64
Episode length: 41.10 +/- 2.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.1         |
|    mean_reward          | 1.62e+03     |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0027515623 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.000631     |
|    learning_rate        | 0.0001       |
|    loss                 | 6.8e+04      |
|    n_updates            | 25           |
|    policy_gradient_loss | -0.000183    |
|    value_loss           | 9.24e+04     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=1780.06 +/- 525.80
Episode length: 42.14 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=1783.33 +/- 547.75
Episode length: 42.04 +/- 2.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=1727.48 +/- 567.67
Episode length: 41.72 +/- 2.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 232      |
|    iterations      | 16       |
|    time_elapsed    | 140      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=1794.94 +/- 485.40
Episode length: 42.66 +/- 1.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.7        |
|    mean_reward          | 1.79e+03    |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.002266031 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.00139     |
|    learning_rate        | 0.0001      |
|    loss                 | 7.85e+04    |
|    n_updates            | 33          |
|    policy_gradient_loss | -0.000493   |
|    value_loss           | 1.16e+05    |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=1843.18 +/- 455.41
Episode length: 42.60 +/- 1.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=1778.37 +/- 544.96
Episode length: 42.14 +/- 2.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=1826.55 +/- 460.24
Episode length: 42.62 +/- 1.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 1.63e+03 |
| time/              |          |
|    fps             | 234      |
|    iterations      | 17       |
|    time_elapsed    | 148      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=35000, episode_reward=1714.88 +/- 559.97
Episode length: 41.90 +/- 1.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.004799382 |
|    clip_fraction        | 0.00376     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0506     |
|    explained_variance   | 0.00205     |
|    learning_rate        | 0.0001      |
|    loss                 | 6.85e+04    |
|    n_updates            | 41          |
|    policy_gradient_loss | 0.000393    |
|    value_loss           | 1.27e+05    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=1771.28 +/- 542.61
Episode length: 42.02 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=1839.89 +/- 414.23
Episode length: 42.56 +/- 1.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=1901.27 +/- 357.38
Episode length: 42.76 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 1.68e+03 |
| time/              |          |
|    fps             | 235      |
|    iterations      | 18       |
|    time_elapsed    | 156      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=37000, episode_reward=1767.51 +/- 510.51
Episode length: 41.80 +/- 2.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.77e+03    |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.005899848 |
|    clip_fraction        | 0.0098      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.00232     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.81e+04    |
|    n_updates            | 45          |
|    policy_gradient_loss | -0.000926   |
|    value_loss           | 1.25e+05    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=1731.46 +/- 539.52
Episode length: 41.76 +/- 2.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=1796.53 +/- 482.25
Episode length: 42.46 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=1783.95 +/- 489.27
Episode length: 42.02 +/- 1.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.6     |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 236      |
|    iterations      | 19       |
|    time_elapsed    | 164      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=39000, episode_reward=1596.78 +/- 638.68
Episode length: 41.58 +/- 2.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 1.6e+03     |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.012778538 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.408      |
|    explained_variance   | 0.0015      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.18e+04    |
|    n_updates            | 46          |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 4.93e+04    |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=1868.92 +/- 397.54
Episode length: 42.38 +/- 1.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=1738.28 +/- 521.20
Episode length: 41.82 +/- 2.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=1852.98 +/- 400.83
Episode length: 42.32 +/- 2.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.5     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 238      |
|    iterations      | 20       |
|    time_elapsed    | 171      |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=41000, episode_reward=1788.58 +/- 452.71
Episode length: 41.92 +/- 1.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.79e+03    |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.008662882 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.00345     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.17e+04    |
|    n_updates            | 47          |
|    policy_gradient_loss | -0.00102    |
|    value_loss           | 1.01e+05    |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=1692.03 +/- 573.76
Episode length: 41.90 +/- 3.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=1624.17 +/- 622.39
Episode length: 41.46 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=1813.16 +/- 472.74
Episode length: 42.04 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=1733.82 +/- 574.19
Episode length: 42.28 +/- 1.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.6     |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    fps             | 238      |
|    iterations      | 21       |
|    time_elapsed    | 180      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=43500, episode_reward=1730.38 +/- 535.72
Episode length: 41.88 +/- 1.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.73e+03     |
| time/                   |              |
|    total_timesteps      | 43500        |
| train/                  |              |
|    approx_kl            | 0.0029838225 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0939      |
|    explained_variance   | 0.0049       |
|    learning_rate        | 0.0001       |
|    loss                 | 5.52e+04     |
|    n_updates            | 52           |
|    policy_gradient_loss | 0.000985     |
|    value_loss           | 1.27e+05     |
------------------------------------------
Eval num_timesteps=44000, episode_reward=1905.20 +/- 339.43
Episode length: 42.72 +/- 1.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=1865.11 +/- 411.51
Episode length: 42.30 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=1768.25 +/- 560.36
Episode length: 41.74 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    fps             | 239      |
|    iterations      | 22       |
|    time_elapsed    | 188      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=45500, episode_reward=1767.79 +/- 514.21
Episode length: 42.22 +/- 2.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.77e+03    |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.002278799 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.00493     |
|    learning_rate        | 0.0001      |
|    loss                 | 7.36e+04    |
|    n_updates            | 54          |
|    policy_gradient_loss | 6.09e-05    |
|    value_loss           | 1.11e+05    |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=1887.14 +/- 395.41
Episode length: 42.10 +/- 1.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=1715.83 +/- 555.64
Episode length: 41.54 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=1781.32 +/- 491.18
Episode length: 42.16 +/- 2.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.5     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 240      |
|    iterations      | 23       |
|    time_elapsed    | 195      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=47500, episode_reward=1809.82 +/- 476.20
Episode length: 42.20 +/- 1.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.007562807 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.339      |
|    explained_variance   | 0.00383     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.63e+04    |
|    n_updates            | 55          |
|    policy_gradient_loss | 0.00645     |
|    value_loss           | 6.74e+04    |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=1779.33 +/- 499.35
Episode length: 42.10 +/- 2.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=1746.97 +/- 504.87
Episode length: 42.02 +/- 2.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=1725.60 +/- 527.49
Episode length: 41.94 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.5     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 241      |
|    iterations      | 24       |
|    time_elapsed    | 203      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=49500, episode_reward=1818.61 +/- 484.56
Episode length: 42.14 +/- 1.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.82e+03    |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.002481889 |
|    clip_fraction        | 0.0149      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.00662     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.1e+04     |
|    n_updates            | 56          |
|    policy_gradient_loss | 0.00441     |
|    value_loss           | 9.67e+04    |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=1778.14 +/- 507.50
Episode length: 41.94 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=1823.30 +/- 460.36
Episode length: 42.22 +/- 2.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=1734.67 +/- 531.61
Episode length: 42.34 +/- 1.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 242      |
|    iterations      | 25       |
|    time_elapsed    | 210      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=51500, episode_reward=1738.19 +/- 535.05
Episode length: 41.98 +/- 2.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 1.74e+03     |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 0.0060933884 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.209       |
|    explained_variance   | 0.00689      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.25e+04     |
|    n_updates            | 57           |
|    policy_gradient_loss | 0.00415      |
|    value_loss           | 7.01e+04     |
------------------------------------------
Eval num_timesteps=52000, episode_reward=1811.25 +/- 481.48
Episode length: 41.94 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=1776.93 +/- 484.54
Episode length: 42.22 +/- 2.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=1675.88 +/- 578.83
Episode length: 42.22 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.9     |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    fps             | 243      |
|    iterations      | 26       |
|    time_elapsed    | 218      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=53500, episode_reward=1713.70 +/- 543.37
Episode length: 41.76 +/- 2.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 1.71e+03     |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 0.0043561338 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.0113       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.29e+04     |
|    n_updates            | 58           |
|    policy_gradient_loss | 0.00189      |
|    value_loss           | 1.1e+05      |
------------------------------------------
Eval num_timesteps=54000, episode_reward=1732.35 +/- 556.54
Episode length: 42.04 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=1721.85 +/- 566.37
Episode length: 42.02 +/- 1.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=1761.98 +/- 536.29
Episode length: 41.76 +/- 2.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44       |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    fps             | 244      |
|    iterations      | 27       |
|    time_elapsed    | 225      |
|    total_timesteps | 55296    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=55500, episode_reward=1787.04 +/- 513.39
Episode length: 42.20 +/- 1.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.79e+03    |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.007854764 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.00728     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.7e+04     |
|    n_updates            | 59          |
|    policy_gradient_loss | 0.0188      |
|    value_loss           | 3.9e+04     |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=1692.93 +/- 569.48
Episode length: 41.60 +/- 2.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=1738.52 +/- 509.20
Episode length: 42.02 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=1802.48 +/- 482.25
Episode length: 42.10 +/- 2.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.6     |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    fps             | 245      |
|    iterations      | 28       |
|    time_elapsed    | 233      |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=57500, episode_reward=1639.07 +/- 637.58
Episode length: 41.58 +/- 3.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 1.64e+03    |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.010840763 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.527      |
|    explained_variance   | 0.00381     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.52e+04    |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00986     |
|    value_loss           | 3.49e+04    |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=1853.02 +/- 465.08
Episode length: 42.24 +/- 2.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=1779.15 +/- 521.35
Episode length: 41.84 +/- 2.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=1782.55 +/- 489.93
Episode length: 42.08 +/- 1.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | 641      |
| time/              |          |
|    fps             | 246      |
|    iterations      | 29       |
|    time_elapsed    | 240      |
|    total_timesteps | 59392    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=59500, episode_reward=1821.07 +/- 477.87
Episode length: 42.46 +/- 2.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.5        |
|    mean_reward          | 1.82e+03    |
| time/                   |             |
|    total_timesteps      | 59500       |
| train/                  |             |
|    approx_kl            | 0.006770633 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | -0.000149   |
|    learning_rate        | 0.0001      |
|    loss                 | 3.96e+03    |
|    n_updates            | 61          |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 1.92e+04    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=1789.56 +/- 504.72
Episode length: 42.18 +/- 2.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=1884.82 +/- 334.49
Episode length: 42.38 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=1819.71 +/- 458.62
Episode length: 42.12 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | 666      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 30       |
|    time_elapsed    | 248      |
|    total_timesteps | 61440    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=61500, episode_reward=1728.28 +/- 548.91
Episode length: 41.96 +/- 2.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.73e+03    |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.014971784 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.00999     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.53e+04    |
|    n_updates            | 62          |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 3.41e+04    |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=1812.34 +/- 457.14
Episode length: 42.14 +/- 1.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=1903.22 +/- 388.83
Episode length: 42.42 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=1706.62 +/- 549.95
Episode length: 41.80 +/- 2.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48       |
|    ep_rew_mean     | 997      |
| time/              |          |
|    fps             | 248      |
|    iterations      | 31       |
|    time_elapsed    | 255      |
|    total_timesteps | 63488    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=63500, episode_reward=1691.95 +/- 619.80
Episode length: 41.60 +/- 3.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 1.69e+03    |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.005307395 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.0105      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.6e+04     |
|    n_updates            | 63          |
|    policy_gradient_loss | -0.000726   |
|    value_loss           | 5.1e+04     |
-----------------------------------------
Eval num_timesteps=64000, episode_reward=1739.25 +/- 506.19
Episode length: 42.30 +/- 2.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=1655.25 +/- 577.73
Episode length: 41.36 +/- 2.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=1720.46 +/- 555.84
Episode length: 41.64 +/- 2.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=1744.60 +/- 523.29
Episode length: 41.74 +/- 2.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45       |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 247      |
|    iterations      | 32       |
|    time_elapsed    | 264      |
|    total_timesteps | 65536    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=66000, episode_reward=1759.94 +/- 503.39
Episode length: 42.28 +/- 2.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.3         |
|    mean_reward          | 1.76e+03     |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0040574507 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.232       |
|    explained_variance   | 0.013        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.93e+04     |
|    n_updates            | 64           |
|    policy_gradient_loss | 0.0056       |
|    value_loss           | 5.8e+04      |
------------------------------------------
Eval num_timesteps=66500, episode_reward=1867.83 +/- 413.64
Episode length: 42.24 +/- 1.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=1735.70 +/- 547.61
Episode length: 41.94 +/- 2.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=1803.93 +/- 513.74
Episode length: 41.96 +/- 3.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 248      |
|    iterations      | 33       |
|    time_elapsed    | 272      |
|    total_timesteps | 67584    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=68000, episode_reward=1896.12 +/- 387.83
Episode length: 42.60 +/- 1.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.6         |
|    mean_reward          | 1.9e+03      |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0077494634 |
|    clip_fraction        | 0.0508       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.393       |
|    explained_variance   | 0.0163       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.89e+04     |
|    n_updates            | 65           |
|    policy_gradient_loss | 0.0135       |
|    value_loss           | 5.37e+04     |
------------------------------------------
Eval num_timesteps=68500, episode_reward=1628.97 +/- 600.23
Episode length: 41.76 +/- 2.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=1799.43 +/- 460.47
Episode length: 42.30 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=1904.16 +/- 336.71
Episode length: 42.68 +/- 1.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | 1e+03    |
| time/              |          |
|    fps             | 249      |
|    iterations      | 34       |
|    time_elapsed    | 279      |
|    total_timesteps | 69632    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=70000, episode_reward=1658.72 +/- 609.16
Episode length: 41.74 +/- 2.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.7         |
|    mean_reward          | 1.66e+03     |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0053285053 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.00595      |
|    learning_rate        | 0.0001       |
|    loss                 | 8.01e+03     |
|    n_updates            | 66           |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 2.76e+04     |
------------------------------------------
Eval num_timesteps=70500, episode_reward=1774.56 +/- 502.98
Episode length: 41.60 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=1695.95 +/- 549.70
Episode length: 42.16 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=1741.58 +/- 504.92
Episode length: 41.98 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | 945      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 35       |
|    time_elapsed    | 287      |
|    total_timesteps | 71680    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=72000, episode_reward=1786.40 +/- 494.53
Episode length: 41.96 +/- 2.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 1.79e+03     |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0062580984 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.283       |
|    explained_variance   | 0.0157       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.77e+04     |
|    n_updates            | 67           |
|    policy_gradient_loss | -0.00577     |
|    value_loss           | 4.36e+04     |
------------------------------------------
Eval num_timesteps=72500, episode_reward=1714.13 +/- 559.90
Episode length: 42.24 +/- 2.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=1804.77 +/- 491.89
Episode length: 41.82 +/- 3.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=1848.11 +/- 430.23
Episode length: 42.04 +/- 1.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 250      |
|    iterations      | 36       |
|    time_elapsed    | 294      |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=74000, episode_reward=1794.90 +/- 513.49
Episode length: 41.62 +/- 2.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 1.79e+03    |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.003556945 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.0248      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.17e+04    |
|    n_updates            | 68          |
|    policy_gradient_loss | 0.00324     |
|    value_loss           | 4.66e+04    |
-----------------------------------------
Eval num_timesteps=74500, episode_reward=1749.78 +/- 545.13
Episode length: 41.58 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=1783.30 +/- 505.29
Episode length: 42.04 +/- 1.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=1592.70 +/- 610.68
Episode length: 41.72 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.59e+03 |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 250      |
|    iterations      | 37       |
|    time_elapsed    | 302      |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=76000, episode_reward=1668.46 +/- 597.01
Episode length: 41.78 +/- 2.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.8       |
|    mean_reward          | 1.67e+03   |
| time/                   |            |
|    total_timesteps      | 76000      |
| train/                  |            |
|    approx_kl            | 0.00513634 |
|    clip_fraction        | 0.0647     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.0179     |
|    learning_rate        | 0.0001     |
|    loss                 | 2.72e+04   |
|    n_updates            | 69         |
|    policy_gradient_loss | 0.00215    |
|    value_loss           | 3.63e+04   |
----------------------------------------
Eval num_timesteps=76500, episode_reward=1700.55 +/- 584.46
Episode length: 42.00 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=1873.19 +/- 409.81
Episode length: 42.20 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=1678.02 +/- 601.90
Episode length: 41.24 +/- 3.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.1     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 38       |
|    time_elapsed    | 309      |
|    total_timesteps | 77824    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=78000, episode_reward=1903.91 +/- 365.98
Episode length: 42.18 +/- 1.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 1.9e+03      |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0053557213 |
|    clip_fraction        | 0.041        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.162       |
|    explained_variance   | 0.0186       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.18e+04     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00488     |
|    value_loss           | 3.88e+04     |
------------------------------------------
Eval num_timesteps=78500, episode_reward=1849.09 +/- 426.88
Episode length: 42.20 +/- 1.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=1812.82 +/- 465.32
Episode length: 42.24 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=1752.88 +/- 524.40
Episode length: 42.16 +/- 2.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.3     |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    fps             | 251      |
|    iterations      | 39       |
|    time_elapsed    | 317      |
|    total_timesteps | 79872    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=80000, episode_reward=1743.71 +/- 539.54
Episode length: 41.94 +/- 2.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.016289905 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.031       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.57e+04    |
|    n_updates            | 71          |
|    policy_gradient_loss | 0.0077      |
|    value_loss           | 6.21e+04    |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=1696.09 +/- 551.92
Episode length: 42.00 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=1816.82 +/- 487.98
Episode length: 41.86 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=1876.76 +/- 407.66
Episode length: 42.24 +/- 2.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 252      |
|    iterations      | 40       |
|    time_elapsed    | 324      |
|    total_timesteps | 81920    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=82000, episode_reward=1715.64 +/- 579.00
Episode length: 41.92 +/- 3.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.72e+03    |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.010788707 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.406      |
|    explained_variance   | 0.016       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.54e+04    |
|    n_updates            | 72          |
|    policy_gradient_loss | -0.000781   |
|    value_loss           | 4.12e+04    |
-----------------------------------------
Eval num_timesteps=82500, episode_reward=1683.74 +/- 594.53
Episode length: 42.14 +/- 2.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=1857.54 +/- 371.60
Episode length: 42.46 +/- 1.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=1761.31 +/- 535.90
Episode length: 42.08 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | 893      |
| time/              |          |
|    fps             | 252      |
|    iterations      | 41       |
|    time_elapsed    | 332      |
|    total_timesteps | 83968    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=84000, episode_reward=-570.98 +/- 30.28
Episode length: 93.30 +/- 22.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93.3       |
|    mean_reward          | -571       |
| time/                   |            |
|    total_timesteps      | 84000      |
| train/                  |            |
|    approx_kl            | 0.03214986 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.597     |
|    explained_variance   | 0.00156    |
|    learning_rate        | 0.0001     |
|    loss                 | 1.64e+04   |
|    n_updates            | 73         |
|    policy_gradient_loss | 0.0279     |
|    value_loss           | 3.36e+04   |
----------------------------------------
Eval num_timesteps=84500, episode_reward=-559.78 +/- 41.95
Episode length: 94.34 +/- 26.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.3     |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-569.58 +/- 33.63
Episode length: 98.76 +/- 27.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-564.58 +/- 33.29
Episode length: 98.08 +/- 24.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.1     |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-575.98 +/- 24.82
Episode length: 105.28 +/- 25.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | -576     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.2     |
|    ep_rew_mean     | 444      |
| time/              |          |
|    fps             | 245      |
|    iterations      | 42       |
|    time_elapsed    | 350      |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=86500, episode_reward=-559.78 +/- 38.83
Episode length: 95.48 +/- 25.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.5        |
|    mean_reward          | -560        |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.018428113 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.619      |
|    explained_variance   | -0.00627    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.59e+04    |
|    n_updates            | 74          |
|    policy_gradient_loss | 0.0123      |
|    value_loss           | 3.31e+04    |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=-559.78 +/- 39.89
Episode length: 104.14 +/- 27.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-573.18 +/- 30.14
Episode length: 101.00 +/- 25.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -573     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-570.38 +/- 30.14
Episode length: 97.02 +/- 25.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | 112      |
| time/              |          |
|    fps             | 240      |
|    iterations      | 43       |
|    time_elapsed    | 365      |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=88500, episode_reward=-566.58 +/- 34.08
Episode length: 101.78 +/- 27.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | -567        |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.010159569 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.00288     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.85e+04    |
|    n_updates            | 75          |
|    policy_gradient_loss | -0.00129    |
|    value_loss           | 3.78e+04    |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=-574.58 +/- 27.20
Episode length: 103.58 +/- 28.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-571.58 +/- 28.79
Episode length: 99.48 +/- 26.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-567.58 +/- 37.38
Episode length: 95.50 +/- 24.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 44       |
|    time_elapsed    | 380      |
|    total_timesteps | 90112    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=90500, episode_reward=-565.58 +/- 37.25
Episode length: 99.84 +/- 22.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.8         |
|    mean_reward          | -566         |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0051144464 |
|    clip_fraction        | 0.0579       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.431       |
|    explained_variance   | 0.00361      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.84e+04     |
|    n_updates            | 76           |
|    policy_gradient_loss | 0.00503      |
|    value_loss           | 3.47e+04     |
------------------------------------------
Eval num_timesteps=91000, episode_reward=-564.18 +/- 33.39
Episode length: 99.20 +/- 29.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-564.38 +/- 32.40
Episode length: 98.22 +/- 24.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-569.78 +/- 31.17
Episode length: 99.06 +/- 23.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.7     |
|    ep_rew_mean     | -381     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 45       |
|    time_elapsed    | 395      |
|    total_timesteps | 92160    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=92500, episode_reward=-560.98 +/- 37.59
Episode length: 90.28 +/- 26.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.3        |
|    mean_reward          | -561        |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.004315647 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.333      |
|    explained_variance   | 0.00684     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.02e+04    |
|    n_updates            | 77          |
|    policy_gradient_loss | -0.00134    |
|    value_loss           | 2e+04       |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=-561.18 +/- 34.77
Episode length: 97.52 +/- 28.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-568.38 +/- 34.79
Episode length: 96.82 +/- 25.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-571.98 +/- 27.28
Episode length: 101.36 +/- 27.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | -509     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 46       |
|    time_elapsed    | 410      |
|    total_timesteps | 94208    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=94500, episode_reward=-571.38 +/- 26.47
Episode length: 99.22 +/- 24.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.2        |
|    mean_reward          | -571        |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 0.006102857 |
|    clip_fraction        | 0.0917      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.468      |
|    explained_variance   | 0.0128      |
|    learning_rate        | 0.0001      |
|    loss                 | 7.12e+03    |
|    n_updates            | 78          |
|    policy_gradient_loss | 0.00729     |
|    value_loss           | 1.57e+04    |
-----------------------------------------
Eval num_timesteps=95000, episode_reward=-567.38 +/- 33.71
Episode length: 97.84 +/- 25.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-573.18 +/- 28.85
Episode length: 97.44 +/- 24.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.4     |
|    mean_reward     | -573     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-558.78 +/- 35.27
Episode length: 101.38 +/- 25.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -559     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.1     |
|    ep_rew_mean     | -560     |
| time/              |          |
|    fps             | 226      |
|    iterations      | 47       |
|    time_elapsed    | 425      |
|    total_timesteps | 96256    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=96500, episode_reward=-573.58 +/- 30.70
Episode length: 94.42 +/- 20.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.4        |
|    mean_reward          | -574        |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.010993249 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.623      |
|    explained_variance   | 0.0041      |
|    learning_rate        | 0.0001      |
|    loss                 | 6.67e+03    |
|    n_updates            | 79          |
|    policy_gradient_loss | -0.00921    |
|    value_loss           | 1.16e+04    |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=-562.18 +/- 35.66
Episode length: 103.32 +/- 24.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -562     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-567.38 +/- 32.31
Episode length: 98.34 +/- 23.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-567.58 +/- 34.20
Episode length: 100.26 +/- 26.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.5     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 48       |
|    time_elapsed    | 440      |
|    total_timesteps | 98304    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=98500, episode_reward=1628.78 +/- 582.79
Episode length: 41.52 +/- 2.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.5       |
|    mean_reward          | 1.63e+03   |
| time/                   |            |
|    total_timesteps      | 98500      |
| train/                  |            |
|    approx_kl            | 0.02081519 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.721     |
|    explained_variance   | -0.0106    |
|    learning_rate        | 0.0001     |
|    loss                 | 6.53e+03   |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 1.45e+04   |
----------------------------------------
Eval num_timesteps=99000, episode_reward=1817.12 +/- 462.30
Episode length: 42.30 +/- 1.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=1752.07 +/- 519.16
Episode length: 42.08 +/- 1.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=1679.48 +/- 539.15
Episode length: 42.08 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | -368     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 49       |
|    time_elapsed    | 448      |
|    total_timesteps | 100352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=100500, episode_reward=1705.54 +/- 606.53
Episode length: 41.84 +/- 3.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 100500      |
| train/                  |             |
|    approx_kl            | 0.025088161 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.657      |
|    explained_variance   | -0.00478    |
|    learning_rate        | 0.0001      |
|    loss                 | 6.51e+03    |
|    n_updates            | 81          |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 1.18e+04    |
-----------------------------------------
Eval num_timesteps=101000, episode_reward=1711.83 +/- 533.69
Episode length: 42.26 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=1762.96 +/- 517.66
Episode length: 42.36 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=1868.40 +/- 381.94
Episode length: 42.60 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.3     |
|    ep_rew_mean     | 121      |
| time/              |          |
|    fps             | 224      |
|    iterations      | 50       |
|    time_elapsed    | 455      |
|    total_timesteps | 102400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=102500, episode_reward=1798.11 +/- 491.14
Episode length: 42.14 +/- 2.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.8e+03     |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.011438121 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.00207     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.04e+04    |
|    n_updates            | 82          |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 2.33e+04    |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=1795.45 +/- 506.07
Episode length: 42.46 +/- 1.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=1804.42 +/- 474.58
Episode length: 41.66 +/- 2.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=1815.03 +/- 474.86
Episode length: 41.74 +/- 2.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 225      |
|    iterations      | 51       |
|    time_elapsed    | 463      |
|    total_timesteps | 104448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=104500, episode_reward=1789.15 +/- 513.33
Episode length: 42.06 +/- 2.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.79e+03     |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0070189647 |
|    clip_fraction        | 0.0573       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.274       |
|    explained_variance   | 0.0117       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.2e+04      |
|    n_updates            | 83           |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 6.86e+04     |
------------------------------------------
Eval num_timesteps=105000, episode_reward=1704.48 +/- 582.30
Episode length: 41.72 +/- 1.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=1850.83 +/- 466.17
Episode length: 42.32 +/- 2.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=1636.08 +/- 619.72
Episode length: 41.28 +/- 2.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.8     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 52       |
|    time_elapsed    | 470      |
|    total_timesteps | 106496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=106500, episode_reward=1873.57 +/- 375.56
Episode length: 42.12 +/- 2.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.87e+03     |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0028626684 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | 0.0153       |
|    learning_rate        | 0.0001       |
|    loss                 | 5.76e+04     |
|    n_updates            | 84           |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 8.07e+04     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=1792.99 +/- 490.45
Episode length: 42.06 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=1872.03 +/- 427.37
Episode length: 42.30 +/- 2.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=1774.71 +/- 506.46
Episode length: 42.08 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=1726.70 +/- 523.23
Episode length: 41.76 +/- 1.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.9     |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    fps             | 226      |
|    iterations      | 53       |
|    time_elapsed    | 479      |
|    total_timesteps | 108544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=109000, episode_reward=1752.33 +/- 533.46
Episode length: 41.64 +/- 2.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 1.75e+03     |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0075083943 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.0164       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.41e+04     |
|    n_updates            | 85           |
|    policy_gradient_loss | 0.00217      |
|    value_loss           | 7.83e+04     |
------------------------------------------
Eval num_timesteps=109500, episode_reward=1817.79 +/- 518.45
Episode length: 42.02 +/- 1.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=1768.44 +/- 485.23
Episode length: 42.28 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=1748.11 +/- 505.96
Episode length: 42.16 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 54       |
|    time_elapsed    | 487      |
|    total_timesteps | 110592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=111000, episode_reward=1838.11 +/- 433.59
Episode length: 42.14 +/- 1.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.84e+03     |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 0.0019245102 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.108       |
|    explained_variance   | 0.0215       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.01e+04     |
|    n_updates            | 86           |
|    policy_gradient_loss | 0.000176     |
|    value_loss           | 8.3e+04      |
------------------------------------------
Eval num_timesteps=111500, episode_reward=1701.23 +/- 591.53
Episode length: 42.12 +/- 2.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=1786.47 +/- 500.43
Episode length: 42.08 +/- 2.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=1743.61 +/- 559.57
Episode length: 41.92 +/- 2.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.1     |
|    ep_rew_mean     | 1.64e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 55       |
|    time_elapsed    | 494      |
|    total_timesteps | 112640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=113000, episode_reward=1760.15 +/- 514.44
Episode length: 41.88 +/- 2.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.76e+03    |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.009998685 |
|    clip_fraction        | 0.00347     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0814     |
|    explained_variance   | 0.0209      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.59e+04    |
|    n_updates            | 87          |
|    policy_gradient_loss | 0.00615     |
|    value_loss           | 7.74e+04    |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=1775.34 +/- 500.22
Episode length: 41.92 +/- 2.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=1689.62 +/- 603.61
Episode length: 41.80 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=1889.58 +/- 389.86
Episode length: 42.68 +/- 1.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.9     |
|    ep_rew_mean     | 1.59e+03 |
| time/              |          |
|    fps             | 228      |
|    iterations      | 56       |
|    time_elapsed    | 502      |
|    total_timesteps | 114688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=115000, episode_reward=1744.46 +/- 520.01
Episode length: 41.84 +/- 2.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.013461257 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.304      |
|    explained_variance   | 0.0209      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.49e+04    |
|    n_updates            | 88          |
|    policy_gradient_loss | 0.0211      |
|    value_loss           | 6.01e+04    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=1754.42 +/- 543.52
Episode length: 41.36 +/- 2.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=1731.88 +/- 513.01
Episode length: 41.98 +/- 2.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=1704.56 +/- 535.25
Episode length: 42.16 +/- 2.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 229      |
|    iterations      | 57       |
|    time_elapsed    | 509      |
|    total_timesteps | 116736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=117000, episode_reward=1730.40 +/- 539.42
Episode length: 41.90 +/- 2.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.73e+03    |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.014994556 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.0105      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.51e+04    |
|    n_updates            | 89          |
|    policy_gradient_loss | 0.0213      |
|    value_loss           | 3.11e+04    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=1873.91 +/- 374.49
Episode length: 42.62 +/- 1.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=1659.71 +/- 593.59
Episode length: 41.44 +/- 2.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=1803.32 +/- 472.10
Episode length: 42.20 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | 669      |
| time/              |          |
|    fps             | 229      |
|    iterations      | 58       |
|    time_elapsed    | 517      |
|    total_timesteps | 118784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=119000, episode_reward=1776.37 +/- 475.93
Episode length: 42.16 +/- 1.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 1.78e+03     |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0126324855 |
|    clip_fraction        | 0.135        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.608       |
|    explained_variance   | 0.00775      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.22e+04     |
|    n_updates            | 90           |
|    policy_gradient_loss | 0.00373      |
|    value_loss           | 2.28e+04     |
------------------------------------------
Eval num_timesteps=119500, episode_reward=1844.45 +/- 453.02
Episode length: 42.14 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=1675.46 +/- 587.18
Episode length: 42.08 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=1778.12 +/- 535.25
Episode length: 41.74 +/- 2.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | 511      |
| time/              |          |
|    fps             | 230      |
|    iterations      | 59       |
|    time_elapsed    | 524      |
|    total_timesteps | 120832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=121000, episode_reward=1885.82 +/- 384.91
Episode length: 42.66 +/- 1.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.7        |
|    mean_reward          | 1.89e+03    |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.013809878 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.334      |
|    explained_variance   | 0.00969     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.99e+04    |
|    n_updates            | 91          |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 3.58e+04    |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=1823.25 +/- 491.67
Episode length: 41.88 +/- 2.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=1869.79 +/- 406.31
Episode length: 42.44 +/- 1.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=1743.02 +/- 510.02
Episode length: 42.02 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 230      |
|    iterations      | 60       |
|    time_elapsed    | 532      |
|    total_timesteps | 122880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=123000, episode_reward=1794.94 +/- 475.51
Episode length: 42.20 +/- 1.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 1.79e+03     |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0074008266 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.017        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.1e+04      |
|    n_updates            | 92           |
|    policy_gradient_loss | 0.00185      |
|    value_loss           | 5.22e+04     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=1759.47 +/- 504.84
Episode length: 42.28 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=1777.52 +/- 493.95
Episode length: 42.36 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=1862.90 +/- 407.07
Episode length: 42.52 +/- 1.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45       |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 231      |
|    iterations      | 61       |
|    time_elapsed    | 539      |
|    total_timesteps | 124928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=125000, episode_reward=1714.26 +/- 569.00
Episode length: 41.98 +/- 1.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.004247744 |
|    clip_fraction        | 0.0268      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.0228      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.59e+04    |
|    n_updates            | 93          |
|    policy_gradient_loss | 0.00572     |
|    value_loss           | 6.16e+04    |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=1545.07 +/- 645.59
Episode length: 41.46 +/- 2.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=1789.68 +/- 484.34
Episode length: 41.90 +/- 1.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=1740.38 +/- 590.67
Episode length: 41.54 +/- 3.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.7     |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    fps             | 232      |
|    iterations      | 62       |
|    time_elapsed    | 546      |
|    total_timesteps | 126976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=127000, episode_reward=1722.94 +/- 549.54
Episode length: 41.94 +/- 2.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.72e+03    |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.013164806 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.088      |
|    explained_variance   | 0.0321      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.06e+04    |
|    n_updates            | 94          |
|    policy_gradient_loss | 0.00343     |
|    value_loss           | 6.21e+04    |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=1729.15 +/- 526.40
Episode length: 42.06 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=1755.62 +/- 545.80
Episode length: 42.02 +/- 2.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=1768.13 +/- 484.23
Episode length: 42.18 +/- 2.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=1875.97 +/- 385.39
Episode length: 42.40 +/- 1.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.4     |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 232      |
|    iterations      | 63       |
|    time_elapsed    | 555      |
|    total_timesteps | 129024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=129500, episode_reward=1796.11 +/- 485.81
Episode length: 42.16 +/- 1.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.8e+03     |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.031327453 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.0237      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.48e+04    |
|    n_updates            | 95          |
|    policy_gradient_loss | 0.0292      |
|    value_loss           | 4.13e+04    |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=1626.39 +/- 639.18
Episode length: 42.06 +/- 3.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=1771.32 +/- 490.98
Episode length: 42.00 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=1610.18 +/- 649.53
Episode length: 41.48 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 232      |
|    iterations      | 64       |
|    time_elapsed    | 563      |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=131500, episode_reward=-567.18 +/- 28.82
Episode length: 101.20 +/- 26.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | -567        |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.036124986 |
|    clip_fraction        | 0.398       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.604      |
|    explained_variance   | -0.00661    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.14e+04    |
|    n_updates            | 96          |
|    policy_gradient_loss | 0.0632      |
|    value_loss           | 2.67e+04    |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=-568.58 +/- 34.40
Episode length: 103.98 +/- 24.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-566.98 +/- 38.48
Episode length: 102.44 +/- 31.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-562.38 +/- 38.97
Episode length: 99.52 +/- 18.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | -562     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 62.2     |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 230      |
|    iterations      | 65       |
|    time_elapsed    | 578      |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=133500, episode_reward=-558.38 +/- 34.62
Episode length: 100.22 +/- 27.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | -558        |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.021351729 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.537      |
|    explained_variance   | -0.000881   |
|    learning_rate        | 0.0001      |
|    loss                 | 1.72e+04    |
|    n_updates            | 97          |
|    policy_gradient_loss | 0.0142      |
|    value_loss           | 3.86e+04    |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=-567.98 +/- 35.04
Episode length: 103.32 +/- 19.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-568.98 +/- 38.07
Episode length: 99.82 +/- 26.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-567.18 +/- 33.50
Episode length: 100.56 +/- 22.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.8     |
|    ep_rew_mean     | -48.1    |
| time/              |          |
|    fps             | 227      |
|    iterations      | 66       |
|    time_elapsed    | 593      |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=135500, episode_reward=-574.18 +/- 36.37
Episode length: 106.24 +/- 28.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | -574        |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.012749419 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.396      |
|    explained_variance   | 0.00459     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.67e+04    |
|    n_updates            | 98          |
|    policy_gradient_loss | 0.0116      |
|    value_loss           | 3.54e+04    |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=-566.18 +/- 40.42
Episode length: 98.90 +/- 23.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-561.18 +/- 44.69
Episode length: 94.78 +/- 28.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-573.58 +/- 28.74
Episode length: 99.62 +/- 24.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.3     |
|    ep_rew_mean     | -332     |
| time/              |          |
|    fps             | 225      |
|    iterations      | 67       |
|    time_elapsed    | 609      |
|    total_timesteps | 137216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=137500, episode_reward=-574.98 +/- 31.32
Episode length: 103.24 +/- 26.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 103        |
|    mean_reward          | -575       |
| time/                   |            |
|    total_timesteps      | 137500     |
| train/                  |            |
|    approx_kl            | 0.00805264 |
|    clip_fraction        | 0.0513     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.296     |
|    explained_variance   | 0.0113     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.68e+04   |
|    n_updates            | 99         |
|    policy_gradient_loss | 0.0195     |
|    value_loss           | 3.47e+04   |
----------------------------------------
Eval num_timesteps=138000, episode_reward=-568.78 +/- 27.28
Episode length: 104.66 +/- 23.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-566.58 +/- 37.17
Episode length: 99.02 +/- 30.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-573.78 +/- 30.74
Episode length: 99.10 +/- 31.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91       |
|    ep_rew_mean     | -458     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 68       |
|    time_elapsed    | 624      |
|    total_timesteps | 139264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=139500, episode_reward=-564.38 +/- 43.01
Episode length: 98.54 +/- 27.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.5         |
|    mean_reward          | -564         |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0119065475 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.544       |
|    explained_variance   | 0.00867      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.37e+04     |
|    n_updates            | 100          |
|    policy_gradient_loss | 0.000133     |
|    value_loss           | 2.86e+04     |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-571.38 +/- 33.84
Episode length: 102.16 +/- 26.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-565.78 +/- 33.01
Episode length: 97.96 +/- 29.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98       |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-570.78 +/- 31.13
Episode length: 98.22 +/- 27.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.9     |
|    ep_rew_mean     | -552     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 69       |
|    time_elapsed    | 639      |
|    total_timesteps | 141312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=141500, episode_reward=-567.58 +/- 31.01
Episode length: 100.86 +/- 26.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | -568        |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.012194054 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.641      |
|    explained_variance   | -0.0133     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.24e+04    |
|    n_updates            | 101         |
|    policy_gradient_loss | 0.0284      |
|    value_loss           | 2.46e+04    |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=-574.58 +/- 28.28
Episode length: 97.78 +/- 23.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-562.18 +/- 38.41
Episode length: 97.54 +/- 23.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | -562     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-560.78 +/- 37.05
Episode length: 98.58 +/- 22.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.3     |
|    ep_rew_mean     | -491     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 70       |
|    time_elapsed    | 654      |
|    total_timesteps | 143360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=143500, episode_reward=1795.31 +/- 518.96
Episode length: 41.86 +/- 2.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.8e+03     |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.010219961 |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.661      |
|    explained_variance   | -0.0148     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.67e+04    |
|    n_updates            | 102         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 3.17e+04    |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=1875.00 +/- 403.53
Episode length: 42.22 +/- 2.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=1678.49 +/- 605.78
Episode length: 42.28 +/- 2.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=1706.95 +/- 541.22
Episode length: 42.16 +/- 2.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.2     |
|    ep_rew_mean     | -325     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 71       |
|    time_elapsed    | 662      |
|    total_timesteps | 145408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=145500, episode_reward=1853.53 +/- 419.50
Episode length: 42.04 +/- 1.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42         |
|    mean_reward          | 1.85e+03   |
| time/                   |            |
|    total_timesteps      | 145500     |
| train/                  |            |
|    approx_kl            | 0.02409395 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.588     |
|    explained_variance   | -0.000874  |
|    learning_rate        | 0.0001     |
|    loss                 | 9.63e+03   |
|    n_updates            | 103        |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 2.2e+04    |
----------------------------------------
Eval num_timesteps=146000, episode_reward=1701.90 +/- 537.77
Episode length: 41.96 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=1624.83 +/- 629.22
Episode length: 41.78 +/- 2.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=1742.77 +/- 569.30
Episode length: 41.60 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 64       |
|    ep_rew_mean     | 159      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 72       |
|    time_elapsed    | 669      |
|    total_timesteps | 147456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=147500, episode_reward=1807.50 +/- 466.71
Episode length: 42.46 +/- 1.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.5        |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.008757325 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.407      |
|    explained_variance   | 0.0113      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.15e+04    |
|    n_updates            | 104         |
|    policy_gradient_loss | -0.00962    |
|    value_loss           | 2.83e+04    |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=1744.47 +/- 519.67
Episode length: 42.20 +/- 1.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=1695.90 +/- 573.86
Episode length: 42.24 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=1659.56 +/- 593.29
Episode length: 41.72 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=1697.97 +/- 555.47
Episode length: 41.96 +/- 2.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.7     |
|    ep_rew_mean     | 677      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 73       |
|    time_elapsed    | 678      |
|    total_timesteps | 149504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=150000, episode_reward=1845.05 +/- 435.82
Episode length: 42.38 +/- 1.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.4        |
|    mean_reward          | 1.85e+03    |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.005396288 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.349      |
|    explained_variance   | 0.0148      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.44e+04    |
|    n_updates            | 105         |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 4.07e+04    |
-----------------------------------------
Eval num_timesteps=150500, episode_reward=1804.78 +/- 487.91
Episode length: 42.22 +/- 3.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=1784.53 +/- 463.10
Episode length: 41.90 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=1678.02 +/- 590.78
Episode length: 41.96 +/- 2.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 74       |
|    time_elapsed    | 686      |
|    total_timesteps | 151552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.09
Eval num_timesteps=152000, episode_reward=1799.97 +/- 450.27
Episode length: 42.40 +/- 1.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 1.8e+03      |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0128106745 |
|    clip_fraction        | 0.0724       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.279       |
|    explained_variance   | 0.0159       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.8e+04      |
|    n_updates            | 106          |
|    policy_gradient_loss | 0.005        |
|    value_loss           | 5.96e+04     |
------------------------------------------
Eval num_timesteps=152500, episode_reward=1733.52 +/- 547.68
Episode length: 41.84 +/- 2.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=1778.48 +/- 495.73
Episode length: 41.80 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=1771.03 +/- 500.16
Episode length: 42.10 +/- 2.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 75       |
|    time_elapsed    | 693      |
|    total_timesteps | 153600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=154000, episode_reward=1874.61 +/- 428.10
Episode length: 42.50 +/- 1.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.5        |
|    mean_reward          | 1.87e+03    |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.012016866 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.622      |
|    explained_variance   | 0.00269     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.24e+04    |
|    n_updates            | 107         |
|    policy_gradient_loss | 0.0241      |
|    value_loss           | 1.64e+04    |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=1925.75 +/- 292.92
Episode length: 42.44 +/- 1.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
New best mean reward!
Eval num_timesteps=155000, episode_reward=1887.51 +/- 393.39
Episode length: 42.20 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=1750.59 +/- 515.93
Episode length: 42.02 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.1     |
|    ep_rew_mean     | 511      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 76       |
|    time_elapsed    | 701      |
|    total_timesteps | 155648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=156000, episode_reward=1660.78 +/- 590.05
Episode length: 41.34 +/- 3.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.3        |
|    mean_reward          | 1.66e+03    |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.010112611 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.675      |
|    explained_variance   | -0.00789    |
|    learning_rate        | 0.0001      |
|    loss                 | 6.36e+03    |
|    n_updates            | 108         |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 1.36e+04    |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=1685.89 +/- 570.95
Episode length: 42.18 +/- 1.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=1759.66 +/- 537.02
Episode length: 42.16 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=1778.48 +/- 493.61
Episode length: 42.14 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.4     |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 222      |
|    iterations      | 77       |
|    time_elapsed    | 708      |
|    total_timesteps | 157696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=158000, episode_reward=1738.06 +/- 526.15
Episode length: 41.50 +/- 2.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.5        |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.010536616 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.468      |
|    explained_variance   | 0.0103      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.52e+04    |
|    n_updates            | 109         |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 3.16e+04    |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=1672.29 +/- 566.42
Episode length: 42.06 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=1665.22 +/- 628.43
Episode length: 41.28 +/- 3.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=1825.47 +/- 440.19
Episode length: 42.28 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | 681      |
| time/              |          |
|    fps             | 223      |
|    iterations      | 78       |
|    time_elapsed    | 716      |
|    total_timesteps | 159744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=160000, episode_reward=1764.96 +/- 508.01
Episode length: 42.26 +/- 1.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.76e+03    |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.005436759 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.0118      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.79e+04    |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00138     |
|    value_loss           | 3.62e+04    |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=1861.03 +/- 403.67
Episode length: 42.28 +/- 1.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=1782.33 +/- 514.42
Episode length: 42.00 +/- 2.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=1797.09 +/- 477.79
Episode length: 41.92 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46       |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    fps             | 223      |
|    iterations      | 79       |
|    time_elapsed    | 723      |
|    total_timesteps | 161792   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=162000, episode_reward=1589.81 +/- 633.46
Episode length: 41.66 +/- 2.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.7       |
|    mean_reward          | 1.59e+03   |
| time/                   |            |
|    total_timesteps      | 162000     |
| train/                  |            |
|    approx_kl            | 0.00967385 |
|    clip_fraction        | 0.032      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.216     |
|    explained_variance   | 0.0149     |
|    learning_rate        | 0.0001     |
|    loss                 | 2.21e+04   |
|    n_updates            | 113        |
|    policy_gradient_loss | -0.00318   |
|    value_loss           | 6.56e+04   |
----------------------------------------
Eval num_timesteps=162500, episode_reward=1692.55 +/- 537.89
Episode length: 41.92 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=1772.61 +/- 498.85
Episode length: 42.02 +/- 1.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=1839.33 +/- 471.93
Episode length: 42.08 +/- 1.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.6     |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 80       |
|    time_elapsed    | 731      |
|    total_timesteps | 163840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=164000, episode_reward=1754.18 +/- 497.04
Episode length: 42.34 +/- 1.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.3         |
|    mean_reward          | 1.75e+03     |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0043442086 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.0168       |
|    learning_rate        | 0.0001       |
|    loss                 | 5e+04        |
|    n_updates            | 114          |
|    policy_gradient_loss | 0.000639     |
|    value_loss           | 6.89e+04     |
------------------------------------------
Eval num_timesteps=164500, episode_reward=1842.04 +/- 435.29
Episode length: 42.34 +/- 2.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=1729.29 +/- 571.12
Episode length: 42.08 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=1869.81 +/- 411.37
Episode length: 42.44 +/- 2.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.3     |
|    ep_rew_mean     | 1.5e+03  |
| time/              |          |
|    fps             | 224      |
|    iterations      | 81       |
|    time_elapsed    | 738      |
|    total_timesteps | 165888   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=166000, episode_reward=1834.58 +/- 470.90
Episode length: 42.08 +/- 1.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.83e+03     |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0019755706 |
|    clip_fraction        | 0.00415      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0755      |
|    explained_variance   | 0.0191       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.89e+04     |
|    n_updates            | 116          |
|    policy_gradient_loss | 0.000739     |
|    value_loss           | 8.3e+04      |
------------------------------------------
Eval num_timesteps=166500, episode_reward=1814.03 +/- 457.82
Episode length: 42.04 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=1720.61 +/- 569.70
Episode length: 41.94 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=1566.49 +/- 620.28
Episode length: 41.36 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.2     |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    fps             | 224      |
|    iterations      | 82       |
|    time_elapsed    | 746      |
|    total_timesteps | 167936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=168000, episode_reward=1839.33 +/- 369.93
Episode length: 42.30 +/- 1.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.84e+03    |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.008157661 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.278      |
|    explained_variance   | 0.0219      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.45e+04    |
|    n_updates            | 117         |
|    policy_gradient_loss | 0.0165      |
|    value_loss           | 6.35e+04    |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=1804.87 +/- 460.23
Episode length: 42.14 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=1747.44 +/- 504.68
Episode length: 42.18 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=1733.31 +/- 536.06
Episode length: 42.38 +/- 1.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 83       |
|    time_elapsed    | 754      |
|    total_timesteps | 169984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=170000, episode_reward=1778.68 +/- 508.78
Episode length: 42.12 +/- 1.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.78e+03     |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0058681853 |
|    clip_fraction        | 0.084        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.329       |
|    explained_variance   | 0.0169       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.66e+04     |
|    n_updates            | 118          |
|    policy_gradient_loss | 0.0139       |
|    value_loss           | 4.79e+04     |
------------------------------------------
Eval num_timesteps=170500, episode_reward=1786.87 +/- 476.75
Episode length: 42.24 +/- 1.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=1614.70 +/- 619.99
Episode length: 41.40 +/- 2.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=1675.48 +/- 573.91
Episode length: 41.86 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=1695.66 +/- 587.48
Episode length: 41.82 +/- 1.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 84       |
|    time_elapsed    | 762      |
|    total_timesteps | 172032   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=172500, episode_reward=1811.29 +/- 517.47
Episode length: 41.68 +/- 3.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.7        |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 172500      |
| train/                  |             |
|    approx_kl            | 0.012753846 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.021       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.11e+04    |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.000488    |
|    value_loss           | 5.19e+04    |
-----------------------------------------
Eval num_timesteps=173000, episode_reward=1865.81 +/- 437.78
Episode length: 42.12 +/- 1.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=1737.72 +/- 529.07
Episode length: 41.92 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=1674.85 +/- 588.98
Episode length: 41.88 +/- 2.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.8     |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 225      |
|    iterations      | 85       |
|    time_elapsed    | 770      |
|    total_timesteps | 174080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=174500, episode_reward=1836.58 +/- 417.48
Episode length: 42.50 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.5        |
|    mean_reward          | 1.84e+03    |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.004136786 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.0275      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.05e+04    |
|    n_updates            | 121         |
|    policy_gradient_loss | 0.00451     |
|    value_loss           | 5.99e+04    |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=1905.84 +/- 347.19
Episode length: 42.52 +/- 1.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=1699.91 +/- 577.43
Episode length: 41.42 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=1820.40 +/- 477.74
Episode length: 42.32 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.5     |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 226      |
|    iterations      | 86       |
|    time_elapsed    | 778      |
|    total_timesteps | 176128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=176500, episode_reward=1823.66 +/- 464.45
Episode length: 41.94 +/- 2.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.82e+03     |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0054579107 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.261       |
|    explained_variance   | 0.0242       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.02e+04     |
|    n_updates            | 123          |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 3.95e+04     |
------------------------------------------
Eval num_timesteps=177000, episode_reward=1615.78 +/- 623.87
Episode length: 40.92 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.9     |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=1611.93 +/- 647.63
Episode length: 41.14 +/- 3.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.1     |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=1615.68 +/- 629.79
Episode length: 41.46 +/- 3.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.9     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 226      |
|    iterations      | 87       |
|    time_elapsed    | 785      |
|    total_timesteps | 178176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=178500, episode_reward=1697.76 +/- 551.87
Episode length: 41.94 +/- 1.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.7e+03      |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0054916125 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.0253       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.69e+04     |
|    n_updates            | 124          |
|    policy_gradient_loss | 0.00488      |
|    value_loss           | 4.31e+04     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=1818.12 +/- 411.78
Episode length: 42.32 +/- 1.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=1783.41 +/- 462.50
Episode length: 41.94 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=1699.98 +/- 556.89
Episode length: 41.72 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 88       |
|    time_elapsed    | 793      |
|    total_timesteps | 180224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=180500, episode_reward=1871.12 +/- 378.18
Episode length: 42.28 +/- 2.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.3         |
|    mean_reward          | 1.87e+03     |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0053312713 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.278       |
|    explained_variance   | 0.032        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.97e+04     |
|    n_updates            | 125          |
|    policy_gradient_loss | 0.00268      |
|    value_loss           | 4.05e+04     |
------------------------------------------
Eval num_timesteps=181000, episode_reward=1840.72 +/- 468.68
Episode length: 42.22 +/- 2.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=1888.82 +/- 387.17
Episode length: 42.48 +/- 1.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=1841.87 +/- 436.53
Episode length: 42.40 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 227      |
|    iterations      | 89       |
|    time_elapsed    | 800      |
|    total_timesteps | 182272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=182500, episode_reward=1866.24 +/- 446.08
Episode length: 42.16 +/- 1.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.87e+03    |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.007585355 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.0267      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.08e+04    |
|    n_updates            | 126         |
|    policy_gradient_loss | -0.00418    |
|    value_loss           | 4.06e+04    |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=1833.18 +/- 457.96
Episode length: 42.32 +/- 1.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=1841.80 +/- 416.34
Episode length: 42.52 +/- 1.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=1652.16 +/- 605.66
Episode length: 41.84 +/- 2.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44       |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    fps             | 228      |
|    iterations      | 90       |
|    time_elapsed    | 808      |
|    total_timesteps | 184320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=184500, episode_reward=1667.30 +/- 591.55
Episode length: 41.80 +/- 2.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 1.67e+03     |
| time/                   |              |
|    total_timesteps      | 184500       |
| train/                  |              |
|    approx_kl            | 0.0046989154 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0917      |
|    explained_variance   | 0.0429       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.63e+04     |
|    n_updates            | 127          |
|    policy_gradient_loss | 0.00271      |
|    value_loss           | 4.77e+04     |
------------------------------------------
Eval num_timesteps=185000, episode_reward=1791.46 +/- 475.65
Episode length: 42.02 +/- 1.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=1796.85 +/- 494.25
Episode length: 42.08 +/- 1.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=1691.07 +/- 542.71
Episode length: 41.96 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.5     |
|    ep_rew_mean     | 1.61e+03 |
| time/              |          |
|    fps             | 228      |
|    iterations      | 91       |
|    time_elapsed    | 815      |
|    total_timesteps | 186368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=186500, episode_reward=1740.87 +/- 549.79
Episode length: 42.02 +/- 2.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 1.74e+03     |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 0.0071090185 |
|    clip_fraction        | 0.0402       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.176       |
|    explained_variance   | 0.0465       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.29e+04     |
|    n_updates            | 128          |
|    policy_gradient_loss | 0.000673     |
|    value_loss           | 3.75e+04     |
------------------------------------------
Eval num_timesteps=187000, episode_reward=1748.14 +/- 531.74
Episode length: 42.20 +/- 2.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=1856.65 +/- 391.60
Episode length: 42.18 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=1689.92 +/- 528.93
Episode length: 42.14 +/- 1.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    fps             | 228      |
|    iterations      | 92       |
|    time_elapsed    | 823      |
|    total_timesteps | 188416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=188500, episode_reward=1817.83 +/- 510.61
Episode length: 41.98 +/- 2.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.82e+03    |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.005029607 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0867     |
|    explained_variance   | 0.0518      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.77e+04    |
|    n_updates            | 129         |
|    policy_gradient_loss | 0.00537     |
|    value_loss           | 3.31e+04    |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=1726.47 +/- 591.98
Episode length: 41.82 +/- 3.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=1819.79 +/- 453.75
Episode length: 42.18 +/- 1.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=1765.89 +/- 506.96
Episode length: 41.84 +/- 2.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.2     |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 229      |
|    iterations      | 93       |
|    time_elapsed    | 830      |
|    total_timesteps | 190464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=190500, episode_reward=1836.76 +/- 454.17
Episode length: 42.24 +/- 1.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 1.84e+03     |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0076019336 |
|    clip_fraction        | 0.0651       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.188       |
|    explained_variance   | 0.0473       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.98e+04     |
|    n_updates            | 130          |
|    policy_gradient_loss | 0.000631     |
|    value_loss           | 4.18e+04     |
------------------------------------------
Eval num_timesteps=191000, episode_reward=1788.25 +/- 533.28
Episode length: 42.04 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=1825.12 +/- 468.31
Episode length: 41.94 +/- 3.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=1725.27 +/- 519.00
Episode length: 41.88 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=1637.67 +/- 599.09
Episode length: 41.64 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44       |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 229      |
|    iterations      | 94       |
|    time_elapsed    | 839      |
|    total_timesteps | 192512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=193000, episode_reward=1709.61 +/- 591.62
Episode length: 41.86 +/- 2.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.003860084 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.0556      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.64e+04    |
|    n_updates            | 131         |
|    policy_gradient_loss | -0.000261   |
|    value_loss           | 3.29e+04    |
-----------------------------------------
Eval num_timesteps=193500, episode_reward=1866.22 +/- 379.94
Episode length: 42.36 +/- 1.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=1682.70 +/- 610.04
Episode length: 41.82 +/- 3.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=1824.44 +/- 454.87
Episode length: 42.40 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    fps             | 229      |
|    iterations      | 95       |
|    time_elapsed    | 847      |
|    total_timesteps | 194560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=195000, episode_reward=1765.88 +/- 481.20
Episode length: 42.18 +/- 1.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.77e+03    |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.008449726 |
|    clip_fraction        | 0.092       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.0367      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.08e+04    |
|    n_updates            | 132         |
|    policy_gradient_loss | 0.0129      |
|    value_loss           | 4.24e+04    |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=1742.38 +/- 556.79
Episode length: 41.88 +/- 2.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=1893.65 +/- 397.00
Episode length: 42.46 +/- 1.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=1783.62 +/- 457.89
Episode length: 42.02 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    fps             | 230      |
|    iterations      | 96       |
|    time_elapsed    | 854      |
|    total_timesteps | 196608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=197000, episode_reward=-337.71 +/- 701.82
Episode length: 96.02 +/- 28.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | -338        |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.022381898 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | -0.00614    |
|    learning_rate        | 0.0001      |
|    loss                 | 2.91e+04    |
|    n_updates            | 133         |
|    policy_gradient_loss | 0.0224      |
|    value_loss           | 5.39e+04    |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=-489.56 +/- 374.69
Episode length: 95.54 +/- 27.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | -490     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-358.59 +/- 707.67
Episode length: 98.96 +/- 32.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | -359     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-307.12 +/- 714.71
Episode length: 89.40 +/- 23.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.4     |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 59.6     |
|    ep_rew_mean     | 532      |
| time/              |          |
|    fps             | 228      |
|    iterations      | 97       |
|    time_elapsed    | 869      |
|    total_timesteps | 198656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=199000, episode_reward=-569.58 +/- 37.46
Episode length: 99.78 +/- 26.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.8        |
|    mean_reward          | -570        |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.015843121 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.577      |
|    explained_variance   | -0.0319     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.87e+04    |
|    n_updates            | 134         |
|    policy_gradient_loss | 0.00373     |
|    value_loss           | 7.61e+04    |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=-559.38 +/- 35.92
Episode length: 101.80 +/- 23.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -559     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-570.98 +/- 29.41
Episode length: 96.90 +/- 25.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-568.78 +/- 29.53
Episode length: 101.92 +/- 27.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | 192      |
| time/              |          |
|    fps             | 226      |
|    iterations      | 98       |
|    time_elapsed    | 884      |
|    total_timesteps | 200704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=201000, episode_reward=-568.98 +/- 32.45
Episode length: 99.92 +/- 24.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.9        |
|    mean_reward          | -569        |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.010306929 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.49       |
|    explained_variance   | -0.0112     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.65e+04    |
|    n_updates            | 135         |
|    policy_gradient_loss | 0.00497     |
|    value_loss           | 7.48e+04    |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=-566.38 +/- 39.49
Episode length: 101.02 +/- 27.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-565.98 +/- 32.50
Episode length: 99.52 +/- 26.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-560.18 +/- 34.41
Episode length: 93.46 +/- 23.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.5     |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78       |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 225      |
|    iterations      | 99       |
|    time_elapsed    | 899      |
|    total_timesteps | 202752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=203000, episode_reward=-568.78 +/- 34.99
Episode length: 99.28 +/- 26.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.3        |
|    mean_reward          | -569        |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.010585894 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.372      |
|    explained_variance   | 0.00498     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.99e+04    |
|    n_updates            | 136         |
|    policy_gradient_loss | 0.00913     |
|    value_loss           | 7.61e+04    |
-----------------------------------------
Eval num_timesteps=203500, episode_reward=-573.58 +/- 31.47
Episode length: 98.34 +/- 22.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-570.78 +/- 30.28
Episode length: 95.08 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-560.18 +/- 44.14
Episode length: 97.88 +/- 23.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.9     |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.7     |
|    ep_rew_mean     | -380     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 100      |
|    time_elapsed    | 914      |
|    total_timesteps | 204800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=205000, episode_reward=-563.38 +/- 36.54
Episode length: 99.72 +/- 23.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.7         |
|    mean_reward          | -563         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0043998505 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.358       |
|    explained_variance   | 0.00358      |
|    learning_rate        | 0.0001       |
|    loss                 | 3e+04        |
|    n_updates            | 137          |
|    policy_gradient_loss | 0.00493      |
|    value_loss           | 6.56e+04     |
------------------------------------------
Eval num_timesteps=205500, episode_reward=-564.38 +/- 38.80
Episode length: 100.08 +/- 24.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-568.58 +/- 34.46
Episode length: 102.46 +/- 27.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-556.78 +/- 42.56
Episode length: 97.18 +/- 29.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.2     |
|    ep_rew_mean     | -499     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 101      |
|    time_elapsed    | 930      |
|    total_timesteps | 206848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=207000, episode_reward=-569.98 +/- 31.37
Episode length: 101.80 +/- 25.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | -570         |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0070236027 |
|    clip_fraction        | 0.0848       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.46        |
|    explained_variance   | 0.0108       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.61e+04     |
|    n_updates            | 138          |
|    policy_gradient_loss | 0.00282      |
|    value_loss           | 5.43e+04     |
------------------------------------------
Eval num_timesteps=207500, episode_reward=-577.78 +/- 29.17
Episode length: 103.24 +/- 28.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -578     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-568.18 +/- 33.60
Episode length: 97.40 +/- 22.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.4     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-565.98 +/- 28.84
Episode length: 95.78 +/- 26.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97       |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 102      |
|    time_elapsed    | 945      |
|    total_timesteps | 208896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=209000, episode_reward=-564.98 +/- 38.64
Episode length: 99.78 +/- 26.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.8         |
|    mean_reward          | -565         |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0041378443 |
|    clip_fraction        | 0.0693       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.347       |
|    explained_variance   | 0.00163      |
|    learning_rate        | 0.0001       |
|    loss                 | 2.29e+04     |
|    n_updates            | 139          |
|    policy_gradient_loss | 0.00378      |
|    value_loss           | 4.35e+04     |
------------------------------------------
Eval num_timesteps=209500, episode_reward=-573.78 +/- 29.00
Episode length: 106.86 +/- 27.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-568.58 +/- 33.87
Episode length: 100.24 +/- 25.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-569.38 +/- 29.97
Episode length: 96.18 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.7     |
|    ep_rew_mean     | -554     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 103      |
|    time_elapsed    | 961      |
|    total_timesteps | 210944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=211000, episode_reward=-574.38 +/- 25.87
Episode length: 101.26 +/- 28.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | -574         |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0057334155 |
|    clip_fraction        | 0.044        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.254       |
|    explained_variance   | 0.0121       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.42e+04     |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.0048       |
|    value_loss           | 3.05e+04     |
------------------------------------------
Eval num_timesteps=211500, episode_reward=-560.58 +/- 33.30
Episode length: 100.02 +/- 24.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-573.18 +/- 24.82
Episode length: 102.14 +/- 22.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -573     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-571.38 +/- 33.36
Episode length: 97.50 +/- 24.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99       |
|    ep_rew_mean     | -559     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 104      |
|    time_elapsed    | 976      |
|    total_timesteps | 212992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=213000, episode_reward=-568.38 +/- 40.72
Episode length: 101.38 +/- 27.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | -568        |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.006932909 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.438      |
|    explained_variance   | 0.00839     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.24e+04    |
|    n_updates            | 141         |
|    policy_gradient_loss | 0.00191     |
|    value_loss           | 2.6e+04     |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=-576.18 +/- 22.32
Episode length: 99.88 +/- 24.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | -576     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-570.58 +/- 27.00
Episode length: 98.16 +/- 23.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-562.78 +/- 36.79
Episode length: 100.46 +/- 25.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -563     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-563.58 +/- 34.67
Episode length: 101.22 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | -550     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 105      |
|    time_elapsed    | 995      |
|    total_timesteps | 215040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=215500, episode_reward=-462.16 +/- 427.54
Episode length: 97.82 +/- 24.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.8        |
|    mean_reward          | -462        |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.022683593 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.633      |
|    explained_variance   | -0.00388    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.14e+04    |
|    n_updates            | 142         |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 2.26e+04    |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=-495.02 +/- 359.17
Episode length: 95.12 +/- 26.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | -495     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-440.41 +/- 304.82
Episode length: 99.58 +/- 30.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | -440     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-549.69 +/- 69.32
Episode length: 94.68 +/- 27.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.7     |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.2     |
|    ep_rew_mean     | -466     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 106      |
|    time_elapsed    | 1010     |
|    total_timesteps | 217088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=217500, episode_reward=1702.24 +/- 564.50
Episode length: 41.64 +/- 2.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 1.7e+03     |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.028900078 |
|    clip_fraction        | 0.365       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.641      |
|    explained_variance   | -0.0047     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.12e+04    |
|    n_updates            | 143         |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 2.37e+04    |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=1729.96 +/- 573.16
Episode length: 41.44 +/- 2.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=1822.65 +/- 426.30
Episode length: 42.06 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=1906.11 +/- 354.33
Episode length: 42.36 +/- 1.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.2     |
|    ep_rew_mean     | -42      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 107      |
|    time_elapsed    | 1017     |
|    total_timesteps | 219136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=219500, episode_reward=1571.15 +/- 665.87
Episode length: 40.86 +/- 3.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.9        |
|    mean_reward          | 1.57e+03    |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.012156397 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.0174      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.1e+04     |
|    n_updates            | 144         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 2.71e+04    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=1697.05 +/- 550.33
Episode length: 41.96 +/- 2.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=1861.42 +/- 385.81
Episode length: 42.66 +/- 1.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=1831.39 +/- 427.27
Episode length: 42.30 +/- 1.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.6     |
|    ep_rew_mean     | 589      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 108      |
|    time_elapsed    | 1025     |
|    total_timesteps | 221184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=221500, episode_reward=1688.67 +/- 539.81
Episode length: 41.58 +/- 2.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 1.69e+03    |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.013071853 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.3        |
|    explained_variance   | 0.0172      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.67e+04    |
|    n_updates            | 145         |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 3.37e+04    |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=1751.81 +/- 542.09
Episode length: 42.04 +/- 2.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=1730.47 +/- 596.36
Episode length: 41.58 +/- 3.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=1819.00 +/- 437.87
Episode length: 42.48 +/- 1.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.4     |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 109      |
|    time_elapsed    | 1032     |
|    total_timesteps | 223232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=223500, episode_reward=1713.10 +/- 579.49
Episode length: 42.16 +/- 3.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 1.71e+03     |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 0.0031968195 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.196       |
|    explained_variance   | 0.0184       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.99e+04     |
|    n_updates            | 146          |
|    policy_gradient_loss | 0.00478      |
|    value_loss           | 6e+04        |
------------------------------------------
Eval num_timesteps=224000, episode_reward=1884.21 +/- 382.91
Episode length: 42.28 +/- 1.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=1878.35 +/- 443.52
Episode length: 42.22 +/- 1.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=1788.75 +/- 531.57
Episode length: 41.84 +/- 2.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 110      |
|    time_elapsed    | 1040     |
|    total_timesteps | 225280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=225500, episode_reward=1675.82 +/- 578.81
Episode length: 42.06 +/- 1.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.68e+03     |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 0.0062285964 |
|    clip_fraction        | 0.0938       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.356       |
|    explained_variance   | 0.0144       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.25e+04     |
|    n_updates            | 147          |
|    policy_gradient_loss | 0.00163      |
|    value_loss           | 3.81e+04     |
------------------------------------------
Eval num_timesteps=226000, episode_reward=1843.07 +/- 418.20
Episode length: 42.20 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=1831.80 +/- 429.82
Episode length: 42.00 +/- 1.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=1791.40 +/- 495.23
Episode length: 42.14 +/- 1.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.2     |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 111      |
|    time_elapsed    | 1047     |
|    total_timesteps | 227328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=227500, episode_reward=1756.89 +/- 528.19
Episode length: 42.02 +/- 2.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.76e+03    |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.004180968 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.0183      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.03e+04    |
|    n_updates            | 148         |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 5.31e+04    |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=1930.48 +/- 302.94
Episode length: 42.44 +/- 1.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
New best mean reward!
Eval num_timesteps=228500, episode_reward=1757.66 +/- 522.15
Episode length: 41.84 +/- 2.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=1802.56 +/- 492.61
Episode length: 42.46 +/- 2.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.5     |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 112      |
|    time_elapsed    | 1055     |
|    total_timesteps | 229376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=229500, episode_reward=1713.59 +/- 542.81
Episode length: 41.74 +/- 1.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.7        |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.007345422 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.0166      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.92e+04    |
|    n_updates            | 149         |
|    policy_gradient_loss | 0.00124     |
|    value_loss           | 3.98e+04    |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=1760.71 +/- 556.74
Episode length: 42.36 +/- 1.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=1771.33 +/- 535.25
Episode length: 41.78 +/- 2.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=1861.22 +/- 392.73
Episode length: 42.54 +/- 2.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 113      |
|    time_elapsed    | 1062     |
|    total_timesteps | 231424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=231500, episode_reward=1856.79 +/- 390.02
Episode length: 42.08 +/- 1.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.86e+03    |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.009464106 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.209      |
|    explained_variance   | 0.0249      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.78e+04    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 4.28e+04    |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=1877.69 +/- 379.20
Episode length: 42.60 +/- 1.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=1787.02 +/- 497.14
Episode length: 41.98 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=1701.05 +/- 557.29
Episode length: 42.02 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46       |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 114      |
|    time_elapsed    | 1070     |
|    total_timesteps | 233472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=233500, episode_reward=1718.83 +/- 559.84
Episode length: 42.16 +/- 1.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 1.72e+03     |
| time/                   |              |
|    total_timesteps      | 233500       |
| train/                  |              |
|    approx_kl            | 0.0020059121 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.129       |
|    explained_variance   | 0.0315       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.4e+04      |
|    n_updates            | 151          |
|    policy_gradient_loss | -0.00117     |
|    value_loss           | 5.46e+04     |
------------------------------------------
Eval num_timesteps=234000, episode_reward=1833.18 +/- 433.88
Episode length: 42.04 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=1710.00 +/- 579.64
Episode length: 41.70 +/- 2.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=1758.04 +/- 531.00
Episode length: 41.64 +/- 2.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=1772.35 +/- 498.67
Episode length: 41.98 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.6     |
|    ep_rew_mean     | 1.53e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 115      |
|    time_elapsed    | 1079     |
|    total_timesteps | 235520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=236000, episode_reward=1759.29 +/- 540.88
Episode length: 41.86 +/- 2.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.9      |
|    mean_reward          | 1.76e+03  |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0101145 |
|    clip_fraction        | 0.00426   |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.0751   |
|    explained_variance   | 0.0347    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.72e+04  |
|    n_updates            | 152       |
|    policy_gradient_loss | 0.0118    |
|    value_loss           | 5.98e+04  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=1690.99 +/- 573.13
Episode length: 41.50 +/- 3.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=1781.70 +/- 504.09
Episode length: 41.86 +/- 3.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=1749.24 +/- 578.49
Episode length: 41.72 +/- 2.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.1     |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    fps             | 218      |
|    iterations      | 116      |
|    time_elapsed    | 1086     |
|    total_timesteps | 237568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=238000, episode_reward=1674.54 +/- 580.31
Episode length: 41.76 +/- 2.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.67e+03    |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.019944044 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.0385      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.94e+04    |
|    n_updates            | 153         |
|    policy_gradient_loss | 0.0133      |
|    value_loss           | 5.14e+04    |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=1657.59 +/- 575.01
Episode length: 41.22 +/- 2.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=1799.34 +/- 432.36
Episode length: 42.48 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=1816.34 +/- 466.96
Episode length: 42.46 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.2     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 219      |
|    iterations      | 117      |
|    time_elapsed    | 1094     |
|    total_timesteps | 239616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=240000, episode_reward=1381.16 +/- 775.97
Episode length: 57.74 +/- 22.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57.7        |
|    mean_reward          | 1.38e+03    |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.008613891 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 0.0174      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.23e+04    |
|    n_updates            | 154         |
|    policy_gradient_loss | 0.0242      |
|    value_loss           | 4e+04       |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=1514.28 +/- 699.19
Episode length: 55.32 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=1536.32 +/- 782.00
Episode length: 55.98 +/- 22.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=1140.39 +/- 833.00
Episode length: 58.38 +/- 22.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54       |
|    ep_rew_mean     | 698      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 118      |
|    time_elapsed    | 1103     |
|    total_timesteps | 241664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=242000, episode_reward=1764.98 +/- 534.51
Episode length: 42.58 +/- 1.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.6        |
|    mean_reward          | 1.76e+03    |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.013062082 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.569      |
|    explained_variance   | -0.0124     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.9e+04     |
|    n_updates            | 155         |
|    policy_gradient_loss | -0.00766    |
|    value_loss           | 3.48e+04    |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=1804.15 +/- 519.01
Episode length: 42.20 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=1843.00 +/- 423.95
Episode length: 42.20 +/- 1.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=1811.74 +/- 469.03
Episode length: 42.24 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.6     |
|    ep_rew_mean     | 514      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 119      |
|    time_elapsed    | 1110     |
|    total_timesteps | 243712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=244000, episode_reward=1882.25 +/- 382.54
Episode length: 42.66 +/- 2.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.7       |
|    mean_reward          | 1.88e+03   |
| time/                   |            |
|    total_timesteps      | 244000     |
| train/                  |            |
|    approx_kl            | 0.01248885 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.0202     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.15e+04   |
|    n_updates            | 156        |
|    policy_gradient_loss | 0.000377   |
|    value_loss           | 3.24e+04   |
----------------------------------------
Eval num_timesteps=244500, episode_reward=1719.92 +/- 534.81
Episode length: 42.18 +/- 2.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=1786.25 +/- 475.08
Episode length: 42.74 +/- 1.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=1642.50 +/- 622.90
Episode length: 41.70 +/- 2.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 976      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 120      |
|    time_elapsed    | 1118     |
|    total_timesteps | 245760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=246000, episode_reward=1634.92 +/- 625.03
Episode length: 41.88 +/- 2.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.63e+03     |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0073052607 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0.0359       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.31e+04     |
|    n_updates            | 157          |
|    policy_gradient_loss | -0.000444    |
|    value_loss           | 4.49e+04     |
------------------------------------------
Eval num_timesteps=246500, episode_reward=1764.72 +/- 517.88
Episode length: 41.50 +/- 2.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=1771.00 +/- 475.32
Episode length: 41.94 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=1767.10 +/- 523.22
Episode length: 42.22 +/- 2.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.3     |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 121      |
|    time_elapsed    | 1125     |
|    total_timesteps | 247808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=248000, episode_reward=1831.39 +/- 452.94
Episode length: 42.40 +/- 1.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 1.83e+03     |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0038027111 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0.0387       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.21e+04     |
|    n_updates            | 158          |
|    policy_gradient_loss | 0.00246      |
|    value_loss           | 4.23e+04     |
------------------------------------------
Eval num_timesteps=248500, episode_reward=1766.06 +/- 515.38
Episode length: 41.82 +/- 2.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=1718.27 +/- 535.45
Episode length: 41.88 +/- 1.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=1774.38 +/- 531.54
Episode length: 42.22 +/- 2.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.8     |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 122      |
|    time_elapsed    | 1133     |
|    total_timesteps | 249856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=250000, episode_reward=1790.52 +/- 510.18
Episode length: 41.92 +/- 2.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.79e+03     |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0053788996 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.228       |
|    explained_variance   | 0.0385       |
|    learning_rate        | 0.0001       |
|    loss                 | 9.98e+03     |
|    n_updates            | 159          |
|    policy_gradient_loss | 0.00418      |
|    value_loss           | 4.19e+04     |
------------------------------------------
Eval num_timesteps=250500, episode_reward=1816.98 +/- 499.62
Episode length: 41.98 +/- 2.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=1799.42 +/- 510.11
Episode length: 42.26 +/- 2.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=1648.54 +/- 608.25
Episode length: 41.70 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.4     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 220      |
|    iterations      | 123      |
|    time_elapsed    | 1140     |
|    total_timesteps | 251904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=252000, episode_reward=1713.22 +/- 563.23
Episode length: 41.62 +/- 1.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 1.71e+03     |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0052483534 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.38        |
|    explained_variance   | 0.0328       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.6e+04      |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.00259      |
|    value_loss           | 4.64e+04     |
------------------------------------------
Eval num_timesteps=252500, episode_reward=1778.82 +/- 509.81
Episode length: 42.02 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=1696.88 +/- 556.27
Episode length: 41.90 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=1698.90 +/- 573.35
Episode length: 41.74 +/- 2.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.9     |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 124      |
|    time_elapsed    | 1148     |
|    total_timesteps | 253952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=254000, episode_reward=1729.86 +/- 527.79
Episode length: 42.06 +/- 2.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.73e+03     |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0048758863 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.0351       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.05e+04     |
|    n_updates            | 161          |
|    policy_gradient_loss | -0.000955    |
|    value_loss           | 4.2e+04      |
------------------------------------------
Eval num_timesteps=254500, episode_reward=1753.10 +/- 506.21
Episode length: 42.12 +/- 1.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=1801.46 +/- 494.38
Episode length: 42.04 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=1725.73 +/- 582.46
Episode length: 41.56 +/- 3.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=1776.12 +/- 497.86
Episode length: 41.96 +/- 1.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.2     |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 125      |
|    time_elapsed    | 1157     |
|    total_timesteps | 256000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=256500, episode_reward=1687.51 +/- 563.02
Episode length: 42.04 +/- 2.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 1.69e+03     |
| time/                   |              |
|    total_timesteps      | 256500       |
| train/                  |              |
|    approx_kl            | 0.0054188953 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.0347       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.26e+04     |
|    n_updates            | 162          |
|    policy_gradient_loss | -0.000392    |
|    value_loss           | 4e+04        |
------------------------------------------
Eval num_timesteps=257000, episode_reward=1677.82 +/- 558.25
Episode length: 42.20 +/- 2.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=1836.94 +/- 452.31
Episode length: 42.40 +/- 1.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=1790.68 +/- 487.82
Episode length: 41.98 +/- 1.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.7     |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 221      |
|    iterations      | 126      |
|    time_elapsed    | 1165     |
|    total_timesteps | 258048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=258500, episode_reward=1731.68 +/- 570.33
Episode length: 41.72 +/- 2.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.7        |
|    mean_reward          | 1.73e+03    |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.011337431 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.432      |
|    explained_variance   | 0.0364      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.52e+04    |
|    n_updates            | 163         |
|    policy_gradient_loss | 0.0107      |
|    value_loss           | 3.82e+04    |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=1729.82 +/- 550.84
Episode length: 42.16 +/- 1.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=1655.50 +/- 581.30
Episode length: 42.26 +/- 1.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=1871.12 +/- 368.45
Episode length: 43.04 +/- 1.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | 923      |
| time/              |          |
|    fps             | 221      |
|    iterations      | 127      |
|    time_elapsed    | 1172     |
|    total_timesteps | 260096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=260500, episode_reward=-447.22 +/- 435.87
Episode length: 90.80 +/- 22.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90.8        |
|    mean_reward          | -447        |
| time/                   |             |
|    total_timesteps      | 260500      |
| train/                  |             |
|    approx_kl            | 0.009795509 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.0207      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.62e+04    |
|    n_updates            | 164         |
|    policy_gradient_loss | 0.0202      |
|    value_loss           | 5.02e+04    |
-----------------------------------------
Eval num_timesteps=261000, episode_reward=-470.30 +/- 418.01
Episode length: 92.94 +/- 22.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.9     |
|    mean_reward     | -470     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-402.00 +/- 527.05
Episode length: 101.10 +/- 29.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-508.87 +/- 335.96
Episode length: 102.72 +/- 26.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.8     |
|    ep_rew_mean     | 545      |
| time/              |          |
|    fps             | 220      |
|    iterations      | 128      |
|    time_elapsed    | 1188     |
|    total_timesteps | 262144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=262500, episode_reward=-575.98 +/- 24.17
Episode length: 99.78 +/- 25.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.8        |
|    mean_reward          | -576        |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.013023591 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.621      |
|    explained_variance   | -0.00562    |
|    learning_rate        | 0.0001      |
|    loss                 | 2.27e+04    |
|    n_updates            | 165         |
|    policy_gradient_loss | 0.0348      |
|    value_loss           | 4.73e+04    |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=-574.98 +/- 29.88
Episode length: 99.06 +/- 20.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-556.38 +/- 35.61
Episode length: 99.88 +/- 23.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | -556     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-571.78 +/- 31.75
Episode length: 102.36 +/- 24.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | 204      |
| time/              |          |
|    fps             | 219      |
|    iterations      | 129      |
|    time_elapsed    | 1204     |
|    total_timesteps | 264192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=264500, episode_reward=-573.38 +/- 36.54
Episode length: 101.38 +/- 23.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | -573        |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.014929091 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.537      |
|    explained_variance   | -0.0101     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.53e+04    |
|    n_updates            | 166         |
|    policy_gradient_loss | 0.0183      |
|    value_loss           | 5.01e+04    |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=-569.98 +/- 32.56
Episode length: 104.76 +/- 21.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-568.38 +/- 30.04
Episode length: 101.10 +/- 25.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-560.78 +/- 40.56
Episode length: 97.72 +/- 24.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 130      |
|    time_elapsed    | 1221     |
|    total_timesteps | 266240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=266500, episode_reward=-564.38 +/- 39.52
Episode length: 96.08 +/- 28.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.1        |
|    mean_reward          | -564        |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.005060179 |
|    clip_fraction        | 0.0762      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.00685     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.36e+04    |
|    n_updates            | 167         |
|    policy_gradient_loss | 0.0104      |
|    value_loss           | 4.48e+04    |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=-569.58 +/- 28.69
Episode length: 93.14 +/- 27.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-571.78 +/- 29.60
Episode length: 93.98 +/- 27.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-559.18 +/- 35.13
Episode length: 106.84 +/- 27.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | -559     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | -342     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 131      |
|    time_elapsed    | 1236     |
|    total_timesteps | 268288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=268500, episode_reward=-570.98 +/- 33.30
Episode length: 98.72 +/- 27.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.7        |
|    mean_reward          | -571        |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.007727543 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.635      |
|    explained_variance   | -0.018      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.62e+04    |
|    n_updates            | 168         |
|    policy_gradient_loss | -0.00788    |
|    value_loss           | 3.57e+04    |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=-565.58 +/- 31.56
Episode length: 102.04 +/- 26.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-567.58 +/- 36.79
Episode length: 93.56 +/- 22.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-571.38 +/- 28.09
Episode length: 99.18 +/- 27.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.1     |
|    ep_rew_mean     | -414     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 132      |
|    time_elapsed    | 1251     |
|    total_timesteps | 270336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=270500, episode_reward=904.84 +/- 851.38
Episode length: 77.64 +/- 25.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.6         |
|    mean_reward          | 905          |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 0.0064769485 |
|    clip_fraction        | 0.135        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.653       |
|    explained_variance   | -0.0152      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.6e+04      |
|    n_updates            | 169          |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 3.03e+04     |
------------------------------------------
Eval num_timesteps=271000, episode_reward=710.03 +/- 870.02
Episode length: 72.88 +/- 22.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=730.23 +/- 911.37
Episode length: 77.90 +/- 24.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 730      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=921.21 +/- 848.80
Episode length: 77.82 +/- 27.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.4     |
|    ep_rew_mean     | -299     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 133      |
|    time_elapsed    | 1263     |
|    total_timesteps | 272384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=272500, episode_reward=1703.88 +/- 527.22
Episode length: 42.26 +/- 1.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.7e+03     |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.012756082 |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.62       |
|    explained_variance   | 0.00339     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.57e+04    |
|    n_updates            | 170         |
|    policy_gradient_loss | 7.52e-05    |
|    value_loss           | 3.47e+04    |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=1818.98 +/- 510.61
Episode length: 42.02 +/- 1.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=1782.01 +/- 485.60
Episode length: 42.24 +/- 1.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=1737.94 +/- 524.29
Episode length: 42.04 +/- 2.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.5     |
|    ep_rew_mean     | 163      |
| time/              |          |
|    fps             | 215      |
|    iterations      | 134      |
|    time_elapsed    | 1271     |
|    total_timesteps | 274432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=274500, episode_reward=1770.36 +/- 489.07
Episode length: 41.98 +/- 2.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.77e+03    |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.011309113 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.0253      |
|    learning_rate        | 0.0001      |
|    loss                 | 7.1e+03     |
|    n_updates            | 171         |
|    policy_gradient_loss | -0.000982   |
|    value_loss           | 2.71e+04    |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=1702.84 +/- 519.39
Episode length: 42.24 +/- 2.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=1681.66 +/- 552.98
Episode length: 41.88 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=1714.92 +/- 565.88
Episode length: 42.26 +/- 2.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.4     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 216      |
|    iterations      | 135      |
|    time_elapsed    | 1278     |
|    total_timesteps | 276480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=276500, episode_reward=1807.46 +/- 473.08
Episode length: 42.08 +/- 2.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.008791999 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.268      |
|    explained_variance   | 0.0278      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.72e+04    |
|    n_updates            | 172         |
|    policy_gradient_loss | 0.000694    |
|    value_loss           | 5.02e+04    |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=1826.90 +/- 434.31
Episode length: 42.44 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=1707.92 +/- 589.63
Episode length: 41.28 +/- 2.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=1851.19 +/- 388.01
Episode length: 42.66 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=1802.66 +/- 479.00
Episode length: 41.90 +/- 2.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 136      |
|    time_elapsed    | 1287     |
|    total_timesteps | 278528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=279000, episode_reward=1595.21 +/- 575.39
Episode length: 41.62 +/- 2.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 1.6e+03      |
| time/                   |              |
|    total_timesteps      | 279000       |
| train/                  |              |
|    approx_kl            | 0.0036317683 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0.0276       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.24e+04     |
|    n_updates            | 173          |
|    policy_gradient_loss | -0.000115    |
|    value_loss           | 6.41e+04     |
------------------------------------------
Eval num_timesteps=279500, episode_reward=1696.36 +/- 557.57
Episode length: 41.64 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=1826.22 +/- 476.45
Episode length: 42.46 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=1903.10 +/- 390.07
Episode length: 42.48 +/- 1.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | 1.62e+03 |
| time/              |          |
|    fps             | 216      |
|    iterations      | 137      |
|    time_elapsed    | 1295     |
|    total_timesteps | 280576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=281000, episode_reward=1863.98 +/- 381.11
Episode length: 42.50 +/- 1.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.5         |
|    mean_reward          | 1.86e+03     |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0030513597 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.138       |
|    explained_variance   | 0.03         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.14e+04     |
|    n_updates            | 174          |
|    policy_gradient_loss | 0.00249      |
|    value_loss           | 7.59e+04     |
------------------------------------------
Eval num_timesteps=281500, episode_reward=1734.33 +/- 507.77
Episode length: 42.34 +/- 2.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=1830.63 +/- 454.44
Episode length: 42.68 +/- 1.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=1717.02 +/- 551.30
Episode length: 42.08 +/- 2.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.7     |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 216      |
|    iterations      | 138      |
|    time_elapsed    | 1302     |
|    total_timesteps | 282624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=283000, episode_reward=1828.83 +/- 472.85
Episode length: 42.20 +/- 1.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 1.83e+03     |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 0.0075145294 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | 0.019        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.38e+04     |
|    n_updates            | 175          |
|    policy_gradient_loss | 0.00434      |
|    value_loss           | 4.85e+04     |
------------------------------------------
Eval num_timesteps=283500, episode_reward=1760.75 +/- 507.04
Episode length: 42.20 +/- 1.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=1812.18 +/- 430.67
Episode length: 41.90 +/- 2.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=1810.38 +/- 455.55
Episode length: 42.02 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.4     |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 139      |
|    time_elapsed    | 1310     |
|    total_timesteps | 284672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=285000, episode_reward=1726.96 +/- 544.76
Episode length: 41.90 +/- 2.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.73e+03     |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0072879335 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.0329       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.42e+04     |
|    n_updates            | 176          |
|    policy_gradient_loss | 0.00434      |
|    value_loss           | 6.48e+04     |
------------------------------------------
Eval num_timesteps=285500, episode_reward=1765.11 +/- 546.15
Episode length: 41.82 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=1824.74 +/- 435.27
Episode length: 42.08 +/- 2.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=1742.64 +/- 548.55
Episode length: 41.74 +/- 2.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.7     |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 140      |
|    time_elapsed    | 1317     |
|    total_timesteps | 286720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=287000, episode_reward=1779.29 +/- 485.35
Episode length: 42.14 +/- 1.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.78e+03    |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.005571462 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.334      |
|    explained_variance   | 0.0354      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.12e+04    |
|    n_updates            | 177         |
|    policy_gradient_loss | 0.00414     |
|    value_loss           | 3.96e+04    |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=1836.89 +/- 388.00
Episode length: 42.36 +/- 2.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=1784.71 +/- 511.89
Episode length: 41.96 +/- 1.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=1755.14 +/- 515.48
Episode length: 42.28 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 217      |
|    iterations      | 141      |
|    time_elapsed    | 1325     |
|    total_timesteps | 288768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=289000, episode_reward=1781.35 +/- 502.12
Episode length: 42.06 +/- 2.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.78e+03     |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0054669776 |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.242       |
|    explained_variance   | 0.0303       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.68e+04     |
|    n_updates            | 178          |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 4.12e+04     |
------------------------------------------
Eval num_timesteps=289500, episode_reward=1707.43 +/- 595.00
Episode length: 41.46 +/- 3.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=1741.16 +/- 563.40
Episode length: 41.62 +/- 2.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=1644.34 +/- 595.85
Episode length: 41.60 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 142      |
|    time_elapsed    | 1332     |
|    total_timesteps | 290816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=291000, episode_reward=1800.91 +/- 463.48
Episode length: 42.02 +/- 2.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 1.8e+03      |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0052968403 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.353       |
|    explained_variance   | 0.0292       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.74e+04     |
|    n_updates            | 179          |
|    policy_gradient_loss | -0.00347     |
|    value_loss           | 3.88e+04     |
------------------------------------------
Eval num_timesteps=291500, episode_reward=1642.57 +/- 619.79
Episode length: 41.70 +/- 2.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=1719.57 +/- 561.88
Episode length: 41.82 +/- 3.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=1791.48 +/- 494.10
Episode length: 42.26 +/- 1.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.9     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 143      |
|    time_elapsed    | 1340     |
|    total_timesteps | 292864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=293000, episode_reward=1807.61 +/- 448.23
Episode length: 42.10 +/- 1.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.006383708 |
|    clip_fraction        | 0.0443      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.0352      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.2e+04     |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.00454     |
|    value_loss           | 3.68e+04    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=1732.41 +/- 553.62
Episode length: 42.02 +/- 2.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=1771.64 +/- 534.05
Episode length: 41.92 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=1783.16 +/- 503.55
Episode length: 41.92 +/- 2.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.7     |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 218      |
|    iterations      | 144      |
|    time_elapsed    | 1347     |
|    total_timesteps | 294912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=295000, episode_reward=1856.13 +/- 430.76
Episode length: 42.48 +/- 1.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.5        |
|    mean_reward          | 1.86e+03    |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.004872059 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.0481      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.69e+04    |
|    n_updates            | 181         |
|    policy_gradient_loss | 0.00373     |
|    value_loss           | 4.32e+04    |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=1728.20 +/- 558.68
Episode length: 42.64 +/- 2.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=1819.68 +/- 487.11
Episode length: 42.02 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=1849.55 +/- 419.39
Episode length: 42.22 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 219      |
|    iterations      | 145      |
|    time_elapsed    | 1355     |
|    total_timesteps | 296960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=297000, episode_reward=-407.23 +/- 411.72
Episode length: 96.56 +/- 20.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.6        |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.015963908 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.5        |
|    explained_variance   | 0.0203      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.12e+04    |
|    n_updates            | 182         |
|    policy_gradient_loss | 0.0191      |
|    value_loss           | 4.37e+04    |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=-185.92 +/- 768.59
Episode length: 98.98 +/- 31.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-368.54 +/- 519.34
Episode length: 94.46 +/- 26.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-201.76 +/- 719.04
Episode length: 96.26 +/- 28.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.3     |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-375.83 +/- 474.92
Episode length: 92.94 +/- 21.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.9     |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.1     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 217      |
|    iterations      | 146      |
|    time_elapsed    | 1372     |
|    total_timesteps | 299008   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=299500, episode_reward=-570.18 +/- 32.50
Episode length: 103.66 +/- 28.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | -570        |
| time/                   |             |
|    total_timesteps      | 299500      |
| train/                  |             |
|    approx_kl            | 0.015402144 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.627      |
|    explained_variance   | -0.0306     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.76e+04    |
|    n_updates            | 183         |
|    policy_gradient_loss | 0.0144      |
|    value_loss           | 5.03e+04    |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=-572.38 +/- 31.48
Episode length: 99.70 +/- 27.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-566.98 +/- 37.91
Episode length: 100.94 +/- 30.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-571.58 +/- 32.93
Episode length: 105.16 +/- 31.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.8     |
|    ep_rew_mean     | 297      |
| time/              |          |
|    fps             | 216      |
|    iterations      | 147      |
|    time_elapsed    | 1388     |
|    total_timesteps | 301056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=301500, episode_reward=-566.18 +/- 38.29
Episode length: 92.96 +/- 22.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93          |
|    mean_reward          | -566        |
| time/                   |             |
|    total_timesteps      | 301500      |
| train/                  |             |
|    approx_kl            | 0.011478296 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | -0.03       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.08e+04    |
|    n_updates            | 184         |
|    policy_gradient_loss | 0.0161      |
|    value_loss           | 5.92e+04    |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=-572.38 +/- 25.20
Episode length: 98.04 +/- 26.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98       |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=-571.78 +/- 32.19
Episode length: 102.34 +/- 19.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-566.58 +/- 31.27
Episode length: 93.18 +/- 24.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.1     |
|    ep_rew_mean     | -106     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 148      |
|    time_elapsed    | 1403     |
|    total_timesteps | 303104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=303500, episode_reward=-565.98 +/- 38.99
Episode length: 100.36 +/- 25.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 100        |
|    mean_reward          | -566       |
| time/                   |            |
|    total_timesteps      | 303500     |
| train/                  |            |
|    approx_kl            | 0.01633844 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.437     |
|    explained_variance   | -0.00216   |
|    learning_rate        | 0.0001     |
|    loss                 | 2.78e+04   |
|    n_updates            | 185        |
|    policy_gradient_loss | 0.000937   |
|    value_loss           | 5.59e+04   |
----------------------------------------
Eval num_timesteps=304000, episode_reward=-560.78 +/- 34.19
Episode length: 103.32 +/- 27.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-567.58 +/- 28.80
Episode length: 97.66 +/- 25.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-565.18 +/- 40.04
Episode length: 99.12 +/- 26.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.3     |
|    ep_rew_mean     | -358     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 149      |
|    time_elapsed    | 1418     |
|    total_timesteps | 305152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=305500, episode_reward=-556.58 +/- 41.25
Episode length: 99.00 +/- 23.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99          |
|    mean_reward          | -557        |
| time/                   |             |
|    total_timesteps      | 305500      |
| train/                  |             |
|    approx_kl            | 0.009099952 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.357      |
|    explained_variance   | -0.0371     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.43e+04    |
|    n_updates            | 186         |
|    policy_gradient_loss | 0.00356     |
|    value_loss           | 4.94e+04    |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=-578.18 +/- 26.55
Episode length: 97.64 +/- 29.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | -578     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-577.78 +/- 29.64
Episode length: 103.76 +/- 24.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | -578     |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=-567.98 +/- 32.50
Episode length: 108.96 +/- 32.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.9     |
|    ep_rew_mean     | -488     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 150      |
|    time_elapsed    | 1434     |
|    total_timesteps | 307200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=307500, episode_reward=-567.78 +/- 30.05
Episode length: 96.92 +/- 23.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.9       |
|    mean_reward          | -568       |
| time/                   |            |
|    total_timesteps      | 307500     |
| train/                  |            |
|    approx_kl            | 0.00779694 |
|    clip_fraction        | 0.072      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | -0.0666    |
|    learning_rate        | 0.0001     |
|    loss                 | 2.07e+04   |
|    n_updates            | 187        |
|    policy_gradient_loss | 0.00234    |
|    value_loss           | 4.27e+04   |
----------------------------------------
Eval num_timesteps=308000, episode_reward=-570.38 +/- 34.82
Episode length: 97.26 +/- 23.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=-578.78 +/- 21.82
Episode length: 98.40 +/- 26.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | -579     |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-567.38 +/- 38.05
Episode length: 102.82 +/- 22.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | -554     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 151      |
|    time_elapsed    | 1449     |
|    total_timesteps | 309248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=309500, episode_reward=-574.78 +/- 31.79
Episode length: 108.72 +/- 29.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | -575        |
| time/                   |             |
|    total_timesteps      | 309500      |
| train/                  |             |
|    approx_kl            | 0.007614461 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.496      |
|    explained_variance   | -0.0373     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.6e+04     |
|    n_updates            | 188         |
|    policy_gradient_loss | 0.00658     |
|    value_loss           | 2.99e+04    |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=-565.78 +/- 35.47
Episode length: 96.20 +/- 23.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-558.18 +/- 42.01
Episode length: 97.84 +/- 31.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | -558     |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=-559.58 +/- 39.99
Episode length: 100.40 +/- 26.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | -560     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 152      |
|    time_elapsed    | 1464     |
|    total_timesteps | 311296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=311500, episode_reward=-571.58 +/- 24.34
Episode length: 96.46 +/- 21.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.5        |
|    mean_reward          | -572        |
| time/                   |             |
|    total_timesteps      | 311500      |
| train/                  |             |
|    approx_kl            | 0.009722667 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.492      |
|    explained_variance   | -0.00775    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.18e+04    |
|    n_updates            | 189         |
|    policy_gradient_loss | 0.00673     |
|    value_loss           | 2.49e+04    |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=-573.18 +/- 33.77
Episode length: 100.76 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -573     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=-569.98 +/- 31.87
Episode length: 99.62 +/- 25.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-569.78 +/- 32.12
Episode length: 105.14 +/- 24.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.4     |
|    ep_rew_mean     | -567     |
| time/              |          |
|    fps             | 211      |
|    iterations      | 153      |
|    time_elapsed    | 1479     |
|    total_timesteps | 313344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=313500, episode_reward=-568.38 +/- 35.07
Episode length: 94.78 +/- 23.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.8        |
|    mean_reward          | -568        |
| time/                   |             |
|    total_timesteps      | 313500      |
| train/                  |             |
|    approx_kl            | 0.008199938 |
|    clip_fraction        | 0.0379      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | -0.0103     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.06e+04    |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.00689     |
|    value_loss           | 2.14e+04    |
-----------------------------------------
Eval num_timesteps=314000, episode_reward=-570.18 +/- 34.82
Episode length: 101.38 +/- 26.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=-560.38 +/- 39.05
Episode length: 94.40 +/- 26.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.4     |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-571.98 +/- 32.98
Episode length: 100.14 +/- 23.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.9     |
|    ep_rew_mean     | -568     |
| time/              |          |
|    fps             | 211      |
|    iterations      | 154      |
|    time_elapsed    | 1494     |
|    total_timesteps | 315392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=315500, episode_reward=-569.98 +/- 36.11
Episode length: 99.98 +/- 23.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | -570        |
| time/                   |             |
|    total_timesteps      | 315500      |
| train/                  |             |
|    approx_kl            | 0.011532581 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.581      |
|    explained_variance   | -0.00774    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.22e+04    |
|    n_updates            | 191         |
|    policy_gradient_loss | 0.00844     |
|    value_loss           | 2.19e+04    |
-----------------------------------------
Eval num_timesteps=316000, episode_reward=-567.98 +/- 36.00
Episode length: 97.46 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=-570.38 +/- 31.44
Episode length: 97.80 +/- 27.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-573.78 +/- 29.35
Episode length: 102.90 +/- 27.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.1     |
|    ep_rew_mean     | -543     |
| time/              |          |
|    fps             | 210      |
|    iterations      | 155      |
|    time_elapsed    | 1509     |
|    total_timesteps | 317440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=317500, episode_reward=-560.18 +/- 41.43
Episode length: 99.66 +/- 27.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.7       |
|    mean_reward          | -560       |
| time/                   |            |
|    total_timesteps      | 317500     |
| train/                  |            |
|    approx_kl            | 0.00693173 |
|    clip_fraction        | 0.0833     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.695     |
|    explained_variance   | 0.00147    |
|    learning_rate        | 0.0001     |
|    loss                 | 9.24e+03   |
|    n_updates            | 192        |
|    policy_gradient_loss | -0.0156    |
|    value_loss           | 1.89e+04   |
----------------------------------------
Eval num_timesteps=318000, episode_reward=-559.98 +/- 37.89
Episode length: 99.34 +/- 32.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=-564.58 +/- 35.95
Episode length: 100.06 +/- 24.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=-573.98 +/- 32.25
Episode length: 100.10 +/- 26.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.6     |
|    ep_rew_mean     | -495     |
| time/              |          |
|    fps             | 209      |
|    iterations      | 156      |
|    time_elapsed    | 1525     |
|    total_timesteps | 319488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=319500, episode_reward=1115.41 +/- 840.59
Episode length: 72.06 +/- 16.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.1        |
|    mean_reward          | 1.12e+03    |
| time/                   |             |
|    total_timesteps      | 319500      |
| train/                  |             |
|    approx_kl            | 0.012714391 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | -0.0238     |
|    learning_rate        | 0.0001      |
|    loss                 | 9.39e+03    |
|    n_updates            | 193         |
|    policy_gradient_loss | -0.00708    |
|    value_loss           | 1.8e+04     |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=1146.80 +/- 892.75
Episode length: 74.02 +/- 19.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74       |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=1272.21 +/- 803.73
Episode length: 77.38 +/- 22.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=946.29 +/- 870.41
Episode length: 76.80 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=840.73 +/- 767.87
Episode length: 73.58 +/- 21.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | -378     |
| time/              |          |
|    fps             | 208      |
|    iterations      | 157      |
|    time_elapsed    | 1539     |
|    total_timesteps | 321536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=322000, episode_reward=1723.29 +/- 569.58
Episode length: 41.80 +/- 3.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.72e+03    |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 0.014558253 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.749      |
|    explained_variance   | -0.0089     |
|    learning_rate        | 0.0001      |
|    loss                 | 7.77e+03    |
|    n_updates            | 194         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 1.56e+04    |
-----------------------------------------
Eval num_timesteps=322500, episode_reward=1796.81 +/- 489.65
Episode length: 41.84 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=1731.10 +/- 542.65
Episode length: 42.16 +/- 2.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=1626.71 +/- 582.19
Episode length: 41.78 +/- 2.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.4     |
|    ep_rew_mean     | -42.2    |
| time/              |          |
|    fps             | 209      |
|    iterations      | 158      |
|    time_elapsed    | 1546     |
|    total_timesteps | 323584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=324000, episode_reward=1738.10 +/- 551.18
Episode length: 41.78 +/- 2.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.010459847 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | 0.00499     |
|    learning_rate        | 0.0001      |
|    loss                 | 6.44e+03    |
|    n_updates            | 195         |
|    policy_gradient_loss | -0.0018     |
|    value_loss           | 1.39e+04    |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=1937.00 +/- 280.16
Episode length: 42.42 +/- 0.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
New best mean reward!
Eval num_timesteps=325000, episode_reward=1771.82 +/- 534.44
Episode length: 41.74 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=1760.99 +/- 521.39
Episode length: 41.76 +/- 2.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | 426      |
| time/              |          |
|    fps             | 209      |
|    iterations      | 159      |
|    time_elapsed    | 1554     |
|    total_timesteps | 325632   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=326000, episode_reward=1780.10 +/- 533.50
Episode length: 42.22 +/- 2.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.78e+03    |
| time/                   |             |
|    total_timesteps      | 326000      |
| train/                  |             |
|    approx_kl            | 0.007131262 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.437      |
|    explained_variance   | 0.0141      |
|    learning_rate        | 0.0001      |
|    loss                 | 8.51e+03    |
|    n_updates            | 197         |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 3.27e+04    |
-----------------------------------------
Eval num_timesteps=326500, episode_reward=1813.73 +/- 439.01
Episode length: 42.12 +/- 1.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=1870.72 +/- 391.72
Episode length: 42.40 +/- 1.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=1813.16 +/- 461.74
Episode length: 42.36 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.3     |
|    ep_rew_mean     | 411      |
| time/              |          |
|    fps             | 209      |
|    iterations      | 160      |
|    time_elapsed    | 1561     |
|    total_timesteps | 327680   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=328000, episode_reward=1704.16 +/- 564.47
Episode length: 41.88 +/- 2.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.7e+03      |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0056861257 |
|    clip_fraction        | 0.0603       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.645       |
|    explained_variance   | -0.00342     |
|    learning_rate        | 0.0001       |
|    loss                 | 6.02e+03     |
|    n_updates            | 198          |
|    policy_gradient_loss | -0.00417     |
|    value_loss           | 1.51e+04     |
------------------------------------------
Eval num_timesteps=328500, episode_reward=1643.80 +/- 605.71
Episode length: 42.10 +/- 2.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=1749.56 +/- 493.83
Episode length: 41.98 +/- 2.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=1647.52 +/- 585.64
Episode length: 42.34 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 210      |
|    iterations      | 161      |
|    time_elapsed    | 1569     |
|    total_timesteps | 329728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=330000, episode_reward=1742.33 +/- 524.42
Episode length: 42.30 +/- 2.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.009319245 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.00889     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.19e+04    |
|    n_updates            | 199         |
|    policy_gradient_loss | -0.0102     |
|    value_loss           | 4.09e+04    |
-----------------------------------------
Eval num_timesteps=330500, episode_reward=1756.07 +/- 506.94
Episode length: 41.98 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=1811.34 +/- 489.58
Episode length: 41.74 +/- 3.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=1862.75 +/- 449.88
Episode length: 42.52 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.9     |
|    ep_rew_mean     | 798      |
| time/              |          |
|    fps             | 210      |
|    iterations      | 162      |
|    time_elapsed    | 1576     |
|    total_timesteps | 331776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=332000, episode_reward=1700.51 +/- 558.23
Episode length: 42.12 +/- 2.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.7e+03      |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0054858197 |
|    clip_fraction        | 0.0496       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.306       |
|    explained_variance   | 0.0153       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.66e+04     |
|    n_updates            | 200          |
|    policy_gradient_loss | 0.000899     |
|    value_loss           | 5.69e+04     |
------------------------------------------
Eval num_timesteps=332500, episode_reward=1768.12 +/- 519.91
Episode length: 41.84 +/- 2.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=1710.87 +/- 547.59
Episode length: 41.68 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=1644.11 +/- 595.22
Episode length: 41.80 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.64e+03 |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43       |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 163      |
|    time_elapsed    | 1584     |
|    total_timesteps | 333824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=334000, episode_reward=1873.52 +/- 393.25
Episode length: 42.46 +/- 1.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.5         |
|    mean_reward          | 1.87e+03     |
| time/                   |              |
|    total_timesteps      | 334000       |
| train/                  |              |
|    approx_kl            | 0.0018274898 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.212       |
|    explained_variance   | 0.0171       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.65e+04     |
|    n_updates            | 201          |
|    policy_gradient_loss | 0.00163      |
|    value_loss           | 7.56e+04     |
------------------------------------------
Eval num_timesteps=334500, episode_reward=1786.47 +/- 481.59
Episode length: 42.50 +/- 1.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=1747.95 +/- 513.40
Episode length: 41.94 +/- 2.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=1711.15 +/- 532.78
Episode length: 42.10 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 210      |
|    iterations      | 164      |
|    time_elapsed    | 1592     |
|    total_timesteps | 335872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=336000, episode_reward=1656.47 +/- 584.38
Episode length: 41.60 +/- 2.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 1.66e+03     |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0072103143 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.341       |
|    explained_variance   | 0.0148       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.09e+04     |
|    n_updates            | 202          |
|    policy_gradient_loss | 0.0187       |
|    value_loss           | 5.24e+04     |
------------------------------------------
Eval num_timesteps=336500, episode_reward=1672.33 +/- 570.45
Episode length: 41.78 +/- 2.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=1620.67 +/- 607.73
Episode length: 41.48 +/- 2.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=1704.16 +/- 561.91
Episode length: 41.90 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.8     |
|    ep_rew_mean     | 959      |
| time/              |          |
|    fps             | 211      |
|    iterations      | 165      |
|    time_elapsed    | 1599     |
|    total_timesteps | 337920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=338000, episode_reward=1850.00 +/- 406.32
Episode length: 42.40 +/- 1.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.4        |
|    mean_reward          | 1.85e+03    |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.008281502 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.0147      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.73e+04    |
|    n_updates            | 203         |
|    policy_gradient_loss | -0.00322    |
|    value_loss           | 5.28e+04    |
-----------------------------------------
Eval num_timesteps=338500, episode_reward=1770.95 +/- 524.31
Episode length: 41.92 +/- 2.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=1725.34 +/- 564.14
Episode length: 42.12 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=1902.54 +/- 371.98
Episode length: 42.56 +/- 2.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.1     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 166      |
|    time_elapsed    | 1606     |
|    total_timesteps | 339968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=340000, episode_reward=1870.76 +/- 382.03
Episode length: 42.32 +/- 1.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.3         |
|    mean_reward          | 1.87e+03     |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0053284606 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.182       |
|    explained_variance   | 0.022        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.95e+04     |
|    n_updates            | 204          |
|    policy_gradient_loss | -0.000897    |
|    value_loss           | 6.42e+04     |
------------------------------------------
Eval num_timesteps=340500, episode_reward=1802.08 +/- 459.70
Episode length: 42.32 +/- 1.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=1837.13 +/- 468.17
Episode length: 42.40 +/- 1.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=1828.90 +/- 441.50
Episode length: 42.16 +/- 2.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=1711.11 +/- 551.53
Episode length: 41.68 +/- 2.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.9     |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 167      |
|    time_elapsed    | 1615     |
|    total_timesteps | 342016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.30
Eval num_timesteps=342500, episode_reward=1642.56 +/- 591.88
Episode length: 41.76 +/- 2.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.64e+03    |
| time/                   |             |
|    total_timesteps      | 342500      |
| train/                  |             |
|    approx_kl            | 0.022393312 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.0284      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.3e+04     |
|    n_updates            | 205         |
|    policy_gradient_loss | 0.021       |
|    value_loss           | 7e+04       |
-----------------------------------------
Eval num_timesteps=343000, episode_reward=1797.89 +/- 505.19
Episode length: 42.22 +/- 1.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=1665.68 +/- 574.71
Episode length: 42.02 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=1804.75 +/- 477.81
Episode length: 41.90 +/- 2.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 211      |
|    iterations      | 168      |
|    time_elapsed    | 1623     |
|    total_timesteps | 344064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=344500, episode_reward=1839.67 +/- 472.68
Episode length: 42.16 +/- 1.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 1.84e+03     |
| time/                   |              |
|    total_timesteps      | 344500       |
| train/                  |              |
|    approx_kl            | 0.0071174153 |
|    clip_fraction        | 0.058        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.318       |
|    explained_variance   | 0.0103       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17e+04     |
|    n_updates            | 206          |
|    policy_gradient_loss | 0.00639      |
|    value_loss           | 4.03e+04     |
------------------------------------------
Eval num_timesteps=345000, episode_reward=1791.95 +/- 482.69
Episode length: 42.14 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=1761.38 +/- 547.07
Episode length: 41.54 +/- 2.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=1781.39 +/- 498.40
Episode length: 41.64 +/- 2.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41       |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 169      |
|    time_elapsed    | 1631     |
|    total_timesteps | 346112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=346500, episode_reward=1734.72 +/- 539.15
Episode length: 41.96 +/- 2.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 1.73e+03     |
| time/                   |              |
|    total_timesteps      | 346500       |
| train/                  |              |
|    approx_kl            | 0.0071753636 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.0207       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.4e+04      |
|    n_updates            | 207          |
|    policy_gradient_loss | 0.00212      |
|    value_loss           | 6.65e+04     |
------------------------------------------
Eval num_timesteps=347000, episode_reward=1732.47 +/- 600.81
Episode length: 41.56 +/- 2.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=1838.38 +/- 409.98
Episode length: 42.12 +/- 1.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=1775.21 +/- 465.78
Episode length: 42.46 +/- 1.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 170      |
|    time_elapsed    | 1638     |
|    total_timesteps | 348160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=348500, episode_reward=1850.87 +/- 435.93
Episode length: 41.94 +/- 2.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.85e+03    |
| time/                   |             |
|    total_timesteps      | 348500      |
| train/                  |             |
|    approx_kl            | 0.006019408 |
|    clip_fraction        | 0.033       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.234      |
|    explained_variance   | 0.018       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.25e+04    |
|    n_updates            | 208         |
|    policy_gradient_loss | 0.00457     |
|    value_loss           | 4.53e+04    |
-----------------------------------------
Eval num_timesteps=349000, episode_reward=1785.20 +/- 530.32
Episode length: 41.84 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=1847.38 +/- 428.87
Episode length: 42.44 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=1881.40 +/- 328.57
Episode length: 42.36 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 212      |
|    iterations      | 171      |
|    time_elapsed    | 1646     |
|    total_timesteps | 350208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=350500, episode_reward=1757.52 +/- 495.61
Episode length: 42.22 +/- 1.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.76e+03    |
| time/                   |             |
|    total_timesteps      | 350500      |
| train/                  |             |
|    approx_kl            | 0.004068056 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0887     |
|    explained_variance   | 0.0361      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.73e+04    |
|    n_updates            | 209         |
|    policy_gradient_loss | 0.00214     |
|    value_loss           | 5.94e+04    |
-----------------------------------------
Eval num_timesteps=351000, episode_reward=1829.13 +/- 451.88
Episode length: 42.46 +/- 1.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=1796.76 +/- 470.59
Episode length: 42.28 +/- 1.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=1741.64 +/- 541.06
Episode length: 41.80 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.1     |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 172      |
|    time_elapsed    | 1653     |
|    total_timesteps | 352256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=352500, episode_reward=1747.04 +/- 524.13
Episode length: 41.78 +/- 2.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 1.75e+03     |
| time/                   |              |
|    total_timesteps      | 352500       |
| train/                  |              |
|    approx_kl            | 0.0044376315 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.171       |
|    explained_variance   | 0.0156       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.3e+04      |
|    n_updates            | 210          |
|    policy_gradient_loss | 2.24e-05     |
|    value_loss           | 4.81e+04     |
------------------------------------------
Eval num_timesteps=353000, episode_reward=1713.95 +/- 529.28
Episode length: 41.90 +/- 2.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=1739.71 +/- 551.00
Episode length: 42.08 +/- 1.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=1753.87 +/- 507.69
Episode length: 41.78 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41       |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 173      |
|    time_elapsed    | 1661     |
|    total_timesteps | 354304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=354500, episode_reward=1878.86 +/- 442.48
Episode length: 42.44 +/- 2.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 1.88e+03     |
| time/                   |              |
|    total_timesteps      | 354500       |
| train/                  |              |
|    approx_kl            | 0.0043626656 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.101       |
|    explained_variance   | 0.0443       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.77e+04     |
|    n_updates            | 211          |
|    policy_gradient_loss | 0.00078      |
|    value_loss           | 4.5e+04      |
------------------------------------------
Eval num_timesteps=355000, episode_reward=1900.10 +/- 373.40
Episode length: 42.26 +/- 1.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.9e+03  |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=1808.46 +/- 507.17
Episode length: 42.20 +/- 1.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=1871.28 +/- 381.79
Episode length: 42.40 +/- 1.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 213      |
|    iterations      | 174      |
|    time_elapsed    | 1668     |
|    total_timesteps | 356352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=356500, episode_reward=1923.43 +/- 279.97
Episode length: 42.66 +/- 1.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.7        |
|    mean_reward          | 1.92e+03    |
| time/                   |             |
|    total_timesteps      | 356500      |
| train/                  |             |
|    approx_kl            | 0.015448667 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | 0.00245     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.56e+04    |
|    n_updates            | 212         |
|    policy_gradient_loss | 0.027       |
|    value_loss           | 2.77e+04    |
-----------------------------------------
Eval num_timesteps=357000, episode_reward=1783.75 +/- 511.27
Episode length: 42.34 +/- 1.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=1786.08 +/- 485.28
Episode length: 42.14 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=1676.91 +/- 578.21
Episode length: 41.52 +/- 2.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.5     |
|    ep_rew_mean     | 710      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 175      |
|    time_elapsed    | 1676     |
|    total_timesteps | 358400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=358500, episode_reward=1827.70 +/- 481.56
Episode length: 42.70 +/- 1.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.7        |
|    mean_reward          | 1.83e+03    |
| time/                   |             |
|    total_timesteps      | 358500      |
| train/                  |             |
|    approx_kl            | 0.020811535 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.758      |
|    explained_variance   | -0.0745     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.33e+04    |
|    n_updates            | 213         |
|    policy_gradient_loss | 0.0495      |
|    value_loss           | 5.72e+04    |
-----------------------------------------
Eval num_timesteps=359000, episode_reward=1791.73 +/- 507.29
Episode length: 43.94 +/- 10.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=1793.25 +/- 433.51
Episode length: 42.34 +/- 1.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=1807.63 +/- 497.06
Episode length: 42.74 +/- 1.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 176      |
|    time_elapsed    | 1683     |
|    total_timesteps | 360448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=360500, episode_reward=-517.17 +/- 210.38
Episode length: 103.12 +/- 25.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | -517        |
| time/                   |             |
|    total_timesteps      | 360500      |
| train/                  |             |
|    approx_kl            | 0.024176143 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.959      |
|    explained_variance   | -0.122      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.59e+04    |
|    n_updates            | 214         |
|    policy_gradient_loss | 0.04        |
|    value_loss           | 5.32e+04    |
-----------------------------------------
Eval num_timesteps=361000, episode_reward=-563.34 +/- 63.78
Episode length: 99.74 +/- 26.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | -563     |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=-570.72 +/- 34.32
Episode length: 92.40 +/- 23.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=-552.71 +/- 91.61
Episode length: 98.22 +/- 23.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.5     |
|    ep_rew_mean     | -55.7    |
| time/              |          |
|    fps             | 213      |
|    iterations      | 177      |
|    time_elapsed    | 1698     |
|    total_timesteps | 362496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=362500, episode_reward=-557.26 +/- 33.32
Episode length: 100.36 +/- 24.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | -557        |
| time/                   |             |
|    total_timesteps      | 362500      |
| train/                  |             |
|    approx_kl            | 0.014862549 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | -0.311      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.51e+04    |
|    n_updates            | 215         |
|    policy_gradient_loss | 0.0331      |
|    value_loss           | 4.91e+04    |
-----------------------------------------
Eval num_timesteps=363000, episode_reward=-576.54 +/- 25.02
Episode length: 101.62 +/- 23.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-580.94 +/- 27.73
Episode length: 98.70 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.7     |
|    mean_reward     | -581     |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=-571.02 +/- 33.54
Episode length: 96.74 +/- 19.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=-573.88 +/- 28.40
Episode length: 103.74 +/- 21.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -263     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 178      |
|    time_elapsed    | 1717     |
|    total_timesteps | 364544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=365000, episode_reward=-572.40 +/- 34.62
Episode length: 96.56 +/- 20.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.6        |
|    mean_reward          | -572        |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.020507595 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.918      |
|    explained_variance   | -0.259      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.92e+04    |
|    n_updates            | 216         |
|    policy_gradient_loss | 0.0362      |
|    value_loss           | 5.73e+04    |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=-578.94 +/- 32.99
Episode length: 98.78 +/- 24.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | -579     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=-583.30 +/- 34.17
Episode length: 98.62 +/- 21.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | -583     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=-568.78 +/- 41.04
Episode length: 97.54 +/- 24.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.2     |
|    ep_rew_mean     | -406     |
| time/              |          |
|    fps             | 211      |
|    iterations      | 179      |
|    time_elapsed    | 1732     |
|    total_timesteps | 366592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=367000, episode_reward=-592.49 +/- 37.79
Episode length: 103.10 +/- 25.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | -592        |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.010098833 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.846      |
|    explained_variance   | -0.344      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.3e+04     |
|    n_updates            | 217         |
|    policy_gradient_loss | 0.0166      |
|    value_loss           | 4.48e+04    |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=-580.07 +/- 42.63
Episode length: 94.76 +/- 25.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | -580     |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-587.79 +/- 40.53
Episode length: 100.20 +/- 26.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -588     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=-585.97 +/- 36.15
Episode length: 100.30 +/- 24.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 210      |
|    iterations      | 180      |
|    time_elapsed    | 1747     |
|    total_timesteps | 368640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=369000, episode_reward=-568.26 +/- 39.90
Episode length: 98.80 +/- 25.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.8         |
|    mean_reward          | -568         |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 0.0067825136 |
|    clip_fraction        | 0.043        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.806       |
|    explained_variance   | -0.353       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.12e+04     |
|    n_updates            | 218          |
|    policy_gradient_loss | 0.00362      |
|    value_loss           | 4.23e+04     |
------------------------------------------
Eval num_timesteps=369500, episode_reward=-575.42 +/- 33.86
Episode length: 95.42 +/- 23.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-570.96 +/- 32.72
Episode length: 100.60 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=-567.32 +/- 31.99
Episode length: 104.58 +/- 24.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.7     |
|    ep_rew_mean     | -561     |
| time/              |          |
|    fps             | 210      |
|    iterations      | 181      |
|    time_elapsed    | 1762     |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=371000, episode_reward=-575.20 +/- 52.74
Episode length: 94.22 +/- 24.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.2         |
|    mean_reward          | -575         |
| time/                   |              |
|    total_timesteps      | 371000       |
| train/                  |              |
|    approx_kl            | 0.0060013398 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.826       |
|    explained_variance   | -0.225       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.84e+04     |
|    n_updates            | 219          |
|    policy_gradient_loss | 6.41e-05     |
|    value_loss           | 3.17e+04     |
------------------------------------------
Eval num_timesteps=371500, episode_reward=-574.67 +/- 41.83
Episode length: 91.28 +/- 19.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.3     |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-587.41 +/- 47.38
Episode length: 102.06 +/- 25.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -587     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-580.95 +/- 43.42
Episode length: 95.46 +/- 20.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | -581     |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.2     |
|    ep_rew_mean     | -574     |
| time/              |          |
|    fps             | 209      |
|    iterations      | 182      |
|    time_elapsed    | 1777     |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=373000, episode_reward=-587.01 +/- 48.05
Episode length: 95.76 +/- 22.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.8        |
|    mean_reward          | -587        |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.016821133 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.744      |
|    explained_variance   | -0.214      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.43e+04    |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.00664     |
|    value_loss           | 2.97e+04    |
-----------------------------------------
Eval num_timesteps=373500, episode_reward=-586.35 +/- 45.01
Episode length: 99.24 +/- 21.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=-587.71 +/- 44.20
Episode length: 98.86 +/- 24.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | -588     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=-584.87 +/- 54.31
Episode length: 96.82 +/- 21.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.6     |
|    ep_rew_mean     | -577     |
| time/              |          |
|    fps             | 209      |
|    iterations      | 183      |
|    time_elapsed    | 1792     |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=375000, episode_reward=-587.17 +/- 42.90
Episode length: 98.42 +/- 24.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.4        |
|    mean_reward          | -587        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.010818448 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.606      |
|    explained_variance   | -0.231      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.32e+04    |
|    n_updates            | 221         |
|    policy_gradient_loss | -0.0036     |
|    value_loss           | 2.46e+04    |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=-567.92 +/- 52.76
Episode length: 90.38 +/- 21.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.4     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=-586.43 +/- 46.00
Episode length: 97.96 +/- 27.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98       |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=-586.17 +/- 37.15
Episode length: 106.02 +/- 28.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.9     |
|    ep_rew_mean     | -576     |
| time/              |          |
|    fps             | 208      |
|    iterations      | 184      |
|    time_elapsed    | 1807     |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=377000, episode_reward=-592.61 +/- 42.05
Episode length: 103.50 +/- 26.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 104        |
|    mean_reward          | -593       |
| time/                   |            |
|    total_timesteps      | 377000     |
| train/                  |            |
|    approx_kl            | 0.02828772 |
|    clip_fraction        | 0.089      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.544     |
|    explained_variance   | -0.209     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.24e+04   |
|    n_updates            | 223        |
|    policy_gradient_loss | -0.00194   |
|    value_loss           | 2.29e+04   |
----------------------------------------
Eval num_timesteps=377500, episode_reward=-585.51 +/- 44.45
Episode length: 100.26 +/- 26.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-593.87 +/- 44.04
Episode length: 99.82 +/- 27.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | -594     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-579.44 +/- 46.49
Episode length: 97.76 +/- 25.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | -579     |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | -575     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 185      |
|    time_elapsed    | 1823     |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=379000, episode_reward=-572.47 +/- 45.29
Episode length: 95.42 +/- 22.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.4        |
|    mean_reward          | -572        |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.007435029 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.756      |
|    explained_variance   | -0.107      |
|    learning_rate        | 0.0001      |
|    loss                 | 7.47e+03    |
|    n_updates            | 224         |
|    policy_gradient_loss | 0.00886     |
|    value_loss           | 1.47e+04    |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=-584.45 +/- 38.89
Episode length: 93.78 +/- 24.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | -584     |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-566.99 +/- 55.61
Episode length: 94.24 +/- 25.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=-584.97 +/- 43.65
Episode length: 93.64 +/- 23.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.8     |
|    ep_rew_mean     | -579     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 186      |
|    time_elapsed    | 1837     |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=381000, episode_reward=-591.21 +/- 37.32
Episode length: 102.66 +/- 27.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | -591        |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.008648644 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.818      |
|    explained_variance   | -0.103      |
|    learning_rate        | 0.0001      |
|    loss                 | 6.96e+03    |
|    n_updates            | 225         |
|    policy_gradient_loss | 0.000666    |
|    value_loss           | 1.37e+04    |
-----------------------------------------
Eval num_timesteps=381500, episode_reward=-582.48 +/- 34.48
Episode length: 96.68 +/- 27.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | -582     |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=-588.08 +/- 32.82
Episode length: 104.34 +/- 25.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | -588     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-585.19 +/- 38.79
Episode length: 101.78 +/- 28.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | -578     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 187      |
|    time_elapsed    | 1852     |
|    total_timesteps | 382976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=383000, episode_reward=-570.58 +/- 33.54
Episode length: 102.24 +/- 26.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | -571        |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.009926089 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | -0.086      |
|    learning_rate        | 0.0001      |
|    loss                 | 5.81e+03    |
|    n_updates            | 226         |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 1.28e+04    |
-----------------------------------------
Eval num_timesteps=383500, episode_reward=-570.38 +/- 26.01
Episode length: 102.84 +/- 28.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-573.78 +/- 28.86
Episode length: 100.22 +/- 23.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-574.78 +/- 30.70
Episode length: 101.40 +/- 23.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-564.98 +/- 33.00
Episode length: 97.08 +/- 22.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.7     |
|    ep_rew_mean     | -575     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 188      |
|    time_elapsed    | 1871     |
|    total_timesteps | 385024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=385500, episode_reward=-566.58 +/- 37.44
Episode length: 96.88 +/- 26.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.9        |
|    mean_reward          | -567        |
| time/                   |             |
|    total_timesteps      | 385500      |
| train/                  |             |
|    approx_kl            | 0.005923992 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.788      |
|    explained_variance   | -0.0635     |
|    learning_rate        | 0.0001      |
|    loss                 | 6.13e+03    |
|    n_updates            | 227         |
|    policy_gradient_loss | 0.00117     |
|    value_loss           | 1.12e+04    |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=-563.78 +/- 31.96
Episode length: 95.06 +/- 25.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=-564.38 +/- 41.39
Episode length: 104.94 +/- 34.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-569.38 +/- 36.37
Episode length: 99.44 +/- 30.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.8     |
|    ep_rew_mean     | -578     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 189      |
|    time_elapsed    | 1886     |
|    total_timesteps | 387072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=387500, episode_reward=-575.18 +/- 26.67
Episode length: 101.74 +/- 25.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | -575         |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 0.0066102846 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.786       |
|    explained_variance   | -0.0486      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.05e+03     |
|    n_updates            | 228          |
|    policy_gradient_loss | 0.00477      |
|    value_loss           | 1e+04        |
------------------------------------------
Eval num_timesteps=388000, episode_reward=-565.18 +/- 32.55
Episode length: 96.80 +/- 25.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-566.38 +/- 32.68
Episode length: 99.20 +/- 22.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-576.78 +/- 29.04
Episode length: 102.66 +/- 30.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | -582     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 190      |
|    time_elapsed    | 1902     |
|    total_timesteps | 389120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=389500, episode_reward=-564.98 +/- 38.59
Episode length: 91.84 +/- 24.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.8        |
|    mean_reward          | -565        |
| time/                   |             |
|    total_timesteps      | 389500      |
| train/                  |             |
|    approx_kl            | 0.004762444 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.849      |
|    explained_variance   | -0.0211     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.59e+03    |
|    n_updates            | 229         |
|    policy_gradient_loss | -0.0016     |
|    value_loss           | 8.26e+03    |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=-564.58 +/- 35.81
Episode length: 92.06 +/- 25.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.1     |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=-565.66 +/- 40.16
Episode length: 101.76 +/- 23.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-562.46 +/- 30.58
Episode length: 100.30 +/- 25.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -562     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | -583     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 191      |
|    time_elapsed    | 1916     |
|    total_timesteps | 391168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=391500, episode_reward=-582.39 +/- 45.21
Episode length: 100.46 +/- 21.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | -582        |
| time/                   |             |
|    total_timesteps      | 391500      |
| train/                  |             |
|    approx_kl            | 0.007977897 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.897      |
|    explained_variance   | -0.0196     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.53e+03    |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.00222     |
|    value_loss           | 6.73e+03    |
-----------------------------------------
Eval num_timesteps=392000, episode_reward=-577.29 +/- 50.70
Episode length: 94.76 +/- 24.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.8     |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-576.69 +/- 54.26
Episode length: 94.22 +/- 23.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-587.11 +/- 45.15
Episode length: 99.52 +/- 26.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | -587     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | -584     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 192      |
|    time_elapsed    | 1931     |
|    total_timesteps | 393216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=393500, episode_reward=-585.43 +/- 46.90
Episode length: 100.46 +/- 25.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | -585        |
| time/                   |             |
|    total_timesteps      | 393500      |
| train/                  |             |
|    approx_kl            | 0.007498996 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.87       |
|    explained_variance   | -0.0191     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.31e+03    |
|    n_updates            | 231         |
|    policy_gradient_loss | 0.00456     |
|    value_loss           | 6.38e+03    |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=-589.91 +/- 38.24
Episode length: 91.96 +/- 25.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=-594.97 +/- 43.35
Episode length: 101.22 +/- 20.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -595     |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-585.45 +/- 46.38
Episode length: 99.98 +/- 25.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | -583     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 193      |
|    time_elapsed    | 1946     |
|    total_timesteps | 395264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=395500, episode_reward=-585.95 +/- 46.38
Episode length: 98.24 +/- 22.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.2        |
|    mean_reward          | -586        |
| time/                   |             |
|    total_timesteps      | 395500      |
| train/                  |             |
|    approx_kl            | 0.008859828 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.883      |
|    explained_variance   | -0.0149     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.08e+03    |
|    n_updates            | 232         |
|    policy_gradient_loss | 0.00446     |
|    value_loss           | 5.95e+03    |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=-568.49 +/- 53.78
Episode length: 97.76 +/- 28.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-583.11 +/- 47.97
Episode length: 91.94 +/- 18.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | -583     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=-585.49 +/- 45.95
Episode length: 92.80 +/- 28.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98       |
|    ep_rew_mean     | -571     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 194      |
|    time_elapsed    | 1961     |
|    total_timesteps | 397312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=397500, episode_reward=-578.55 +/- 47.49
Episode length: 97.56 +/- 24.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.6         |
|    mean_reward          | -579         |
| time/                   |              |
|    total_timesteps      | 397500       |
| train/                  |              |
|    approx_kl            | 0.0075729005 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.905       |
|    explained_variance   | -0.00582     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.06e+03     |
|    n_updates            | 233          |
|    policy_gradient_loss | 0.00215      |
|    value_loss           | 5.21e+03     |
------------------------------------------
Eval num_timesteps=398000, episode_reward=-574.13 +/- 53.82
Episode length: 93.18 +/- 21.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-583.49 +/- 46.05
Episode length: 99.98 +/- 25.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -583     |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-587.11 +/- 49.24
Episode length: 97.02 +/- 24.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | -587     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.7     |
|    ep_rew_mean     | -560     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 195      |
|    time_elapsed    | 1976     |
|    total_timesteps | 399360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=399500, episode_reward=-594.95 +/- 34.79
Episode length: 94.22 +/- 25.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.2        |
|    mean_reward          | -595        |
| time/                   |             |
|    total_timesteps      | 399500      |
| train/                  |             |
|    approx_kl            | 0.009432852 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | -0.00234    |
|    learning_rate        | 0.0001      |
|    loss                 | 2.23e+03    |
|    n_updates            | 234         |
|    policy_gradient_loss | 0.00878     |
|    value_loss           | 4.12e+03    |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=-579.67 +/- 43.87
Episode length: 93.96 +/- 26.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | -580     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=-587.29 +/- 45.43
Episode length: 99.74 +/- 25.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | -587     |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-571.67 +/- 58.75
Episode length: 94.04 +/- 24.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -548     |
| time/              |          |
|    fps             | 201      |
|    iterations      | 196      |
|    time_elapsed    | 1991     |
|    total_timesteps | 401408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=401500, episode_reward=-590.37 +/- 44.29
Episode length: 97.06 +/- 20.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.1        |
|    mean_reward          | -590        |
| time/                   |             |
|    total_timesteps      | 401500      |
| train/                  |             |
|    approx_kl            | 0.008740591 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.00331     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.35e+03    |
|    n_updates            | 235         |
|    policy_gradient_loss | -0.0017     |
|    value_loss           | 3.27e+03    |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=-574.01 +/- 56.11
Episode length: 102.20 +/- 23.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=-578.15 +/- 47.36
Episode length: 97.10 +/- 27.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | -578     |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=-578.07 +/- 52.69
Episode length: 102.28 +/- 24.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -578     |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.7     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 201      |
|    iterations      | 197      |
|    time_elapsed    | 2006     |
|    total_timesteps | 403456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=403500, episode_reward=-590.91 +/- 38.91
Episode length: 97.06 +/- 25.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.1         |
|    mean_reward          | -591         |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 0.0076345126 |
|    clip_fraction        | 0.145        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | -0.00164     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.04e+03     |
|    n_updates            | 236          |
|    policy_gradient_loss | -0.00531     |
|    value_loss           | 4.14e+03     |
------------------------------------------
Eval num_timesteps=404000, episode_reward=-590.31 +/- 39.67
Episode length: 92.70 +/- 19.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=-587.49 +/- 41.63
Episode length: 101.18 +/- 23.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -587     |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-580.13 +/- 52.40
Episode length: 90.72 +/- 23.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | -580     |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=-590.01 +/- 45.90
Episode length: 92.36 +/- 20.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.3     |
|    ep_rew_mean     | -473     |
| time/              |          |
|    fps             | 200      |
|    iterations      | 198      |
|    time_elapsed    | 2024     |
|    total_timesteps | 405504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=406000, episode_reward=-571.41 +/- 58.02
Episode length: 99.14 +/- 24.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.1        |
|    mean_reward          | -571        |
| time/                   |             |
|    total_timesteps      | 406000      |
| train/                  |             |
|    approx_kl            | 0.009550424 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.003       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.74e+03    |
|    n_updates            | 237         |
|    policy_gradient_loss | -0.00967    |
|    value_loss           | 4.33e+03    |
-----------------------------------------
Eval num_timesteps=406500, episode_reward=-570.21 +/- 59.86
Episode length: 98.26 +/- 25.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=-592.55 +/- 35.26
Episode length: 99.76 +/- 26.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | -593     |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=-591.41 +/- 40.74
Episode length: 95.96 +/- 26.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | -591     |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.7     |
|    ep_rew_mean     | -370     |
| time/              |          |
|    fps             | 199      |
|    iterations      | 199      |
|    time_elapsed    | 2039     |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=408000, episode_reward=-589.97 +/- 46.42
Episode length: 99.76 +/- 23.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.8        |
|    mean_reward          | -590        |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.010577674 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.00171     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.05e+03    |
|    n_updates            | 238         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 3.71e+03    |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=-572.31 +/- 50.05
Episode length: 93.96 +/- 26.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-593.67 +/- 42.56
Episode length: 97.48 +/- 22.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | -594     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=-578.47 +/- 56.89
Episode length: 101.76 +/- 24.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -578     |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.1     |
|    ep_rew_mean     | -216     |
| time/              |          |
|    fps             | 199      |
|    iterations      | 200      |
|    time_elapsed    | 2055     |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=410000, episode_reward=1745.19 +/- 534.06
Episode length: 41.92 +/- 1.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.75e+03    |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.016415903 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.4        |
|    explained_variance   | 0.000989    |
|    learning_rate        | 0.0001      |
|    loss                 | 2.12e+03    |
|    n_updates            | 239         |
|    policy_gradient_loss | 0.000477    |
|    value_loss           | 4.32e+03    |
-----------------------------------------
Eval num_timesteps=410500, episode_reward=1832.84 +/- 397.57
Episode length: 42.60 +/- 1.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=1818.15 +/- 472.13
Episode length: 42.08 +/- 1.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=1858.14 +/- 429.43
Episode length: 42.60 +/- 1.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.2     |
|    ep_rew_mean     | -47      |
| time/              |          |
|    fps             | 199      |
|    iterations      | 201      |
|    time_elapsed    | 2063     |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=412000, episode_reward=1771.34 +/- 513.28
Episode length: 41.94 +/- 2.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 1.77e+03    |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.011551855 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.45       |
|    explained_variance   | 0.00219     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.03e+03    |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00969    |
|    value_loss           | 7.7e+03     |
-----------------------------------------
Eval num_timesteps=412500, episode_reward=1820.87 +/- 469.69
Episode length: 42.32 +/- 1.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=1689.22 +/- 568.34
Episode length: 41.92 +/- 2.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=1806.94 +/- 450.85
Episode length: 42.22 +/- 1.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.3     |
|    ep_rew_mean     | 89.5     |
| time/              |          |
|    fps             | 199      |
|    iterations      | 202      |
|    time_elapsed    | 2070     |
|    total_timesteps | 413696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=414000, episode_reward=1710.16 +/- 571.14
Episode length: 42.42 +/- 1.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 1.71e+03     |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 0.0065170517 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.41        |
|    explained_variance   | 0.00115      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.2e+03      |
|    n_updates            | 241          |
|    policy_gradient_loss | -0.0137      |
|    value_loss           | 7.25e+03     |
------------------------------------------
Eval num_timesteps=414500, episode_reward=1857.72 +/- 429.05
Episode length: 42.68 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=1679.14 +/- 622.70
Episode length: 41.42 +/- 3.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=1703.25 +/- 561.64
Episode length: 41.90 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.2     |
|    ep_rew_mean     | 194      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 203      |
|    time_elapsed    | 2078     |
|    total_timesteps | 415744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=416000, episode_reward=1885.31 +/- 390.08
Episode length: 42.90 +/- 1.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.9         |
|    mean_reward          | 1.89e+03     |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0054458803 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.28        |
|    explained_variance   | 0.000987     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.25e+03     |
|    n_updates            | 242          |
|    policy_gradient_loss | -0.00566     |
|    value_loss           | 8.43e+03     |
------------------------------------------
Eval num_timesteps=416500, episode_reward=1760.46 +/- 505.50
Episode length: 42.24 +/- 1.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=1835.46 +/- 459.96
Episode length: 42.04 +/- 2.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=1763.06 +/- 523.13
Episode length: 41.96 +/- 2.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.4     |
|    ep_rew_mean     | 179      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 204      |
|    time_elapsed    | 2085     |
|    total_timesteps | 417792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=418000, episode_reward=1726.40 +/- 582.03
Episode length: 41.44 +/- 3.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.4         |
|    mean_reward          | 1.73e+03     |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0076333336 |
|    clip_fraction        | 0.0938       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 3.05e-05     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.44e+03     |
|    n_updates            | 243          |
|    policy_gradient_loss | 0.000949     |
|    value_loss           | 7.94e+03     |
------------------------------------------
Eval num_timesteps=418500, episode_reward=1698.00 +/- 561.43
Episode length: 42.00 +/- 2.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=1801.18 +/- 442.40
Episode length: 42.58 +/- 1.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=1752.19 +/- 507.90
Episode length: 42.28 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.7     |
|    ep_rew_mean     | 209      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 205      |
|    time_elapsed    | 2093     |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=420000, episode_reward=1766.13 +/- 514.18
Episode length: 42.10 +/- 2.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.77e+03    |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.008240483 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.00112     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.7e+03     |
|    n_updates            | 244         |
|    policy_gradient_loss | 0.00839     |
|    value_loss           | 1.28e+04    |
-----------------------------------------
Eval num_timesteps=420500, episode_reward=1838.73 +/- 475.72
Episode length: 42.42 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=1735.51 +/- 576.59
Episode length: 41.88 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=1697.20 +/- 577.97
Episode length: 41.90 +/- 2.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 247      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 206      |
|    time_elapsed    | 2100     |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=422000, episode_reward=1874.04 +/- 430.79
Episode length: 42.56 +/- 1.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.6       |
|    mean_reward          | 1.87e+03   |
| time/                   |            |
|    total_timesteps      | 422000     |
| train/                  |            |
|    approx_kl            | 0.00766856 |
|    clip_fraction        | 0.0443     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.799     |
|    explained_variance   | 0.000222   |
|    learning_rate        | 0.0001     |
|    loss                 | 8.9e+03    |
|    n_updates            | 245        |
|    policy_gradient_loss | 0.000401   |
|    value_loss           | 1.02e+04   |
----------------------------------------
Eval num_timesteps=422500, episode_reward=1704.82 +/- 540.99
Episode length: 42.20 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=1842.91 +/- 469.53
Episode length: 42.50 +/- 2.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=1848.59 +/- 408.05
Episode length: 42.04 +/- 2.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.85e+03 |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.8     |
|    ep_rew_mean     | 297      |
| time/              |          |
|    fps             | 201      |
|    iterations      | 207      |
|    time_elapsed    | 2108     |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=424000, episode_reward=1714.28 +/- 570.13
Episode length: 42.38 +/- 2.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 1.71e+03     |
| time/                   |              |
|    total_timesteps      | 424000       |
| train/                  |              |
|    approx_kl            | 0.0034492095 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.695       |
|    explained_variance   | 0.00196      |
|    learning_rate        | 0.0001       |
|    loss                 | 6.48e+03     |
|    n_updates            | 246          |
|    policy_gradient_loss | 0.000992     |
|    value_loss           | 2.22e+04     |
------------------------------------------
Eval num_timesteps=424500, episode_reward=1808.08 +/- 472.42
Episode length: 42.04 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=1818.81 +/- 476.20
Episode length: 42.18 +/- 2.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=1825.74 +/- 469.05
Episode length: 42.26 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.2     |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 201      |
|    iterations      | 208      |
|    time_elapsed    | 2115     |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=426000, episode_reward=1833.45 +/- 438.09
Episode length: 42.08 +/- 2.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.83e+03    |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.008059078 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | -0.000799   |
|    learning_rate        | 0.0001      |
|    loss                 | 4.58e+03    |
|    n_updates            | 247         |
|    policy_gradient_loss | -0.0032     |
|    value_loss           | 1.66e+04    |
-----------------------------------------
Eval num_timesteps=426500, episode_reward=1777.41 +/- 509.74
Episode length: 41.98 +/- 2.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=1766.34 +/- 515.20
Episode length: 42.20 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=1816.58 +/- 455.91
Episode length: 42.32 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=1779.15 +/- 497.99
Episode length: 41.92 +/- 2.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.3     |
|    ep_rew_mean     | 481      |
| time/              |          |
|    fps             | 201      |
|    iterations      | 209      |
|    time_elapsed    | 2124     |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=428500, episode_reward=1654.55 +/- 605.23
Episode length: 41.48 +/- 3.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.5        |
|    mean_reward          | 1.65e+03    |
| time/                   |             |
|    total_timesteps      | 428500      |
| train/                  |             |
|    approx_kl            | 0.006921485 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.426      |
|    explained_variance   | 0.00392     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.21e+04    |
|    n_updates            | 248         |
|    policy_gradient_loss | -0.00393    |
|    value_loss           | 2.87e+04    |
-----------------------------------------
Eval num_timesteps=429000, episode_reward=1716.64 +/- 577.84
Episode length: 41.88 +/- 2.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=1758.48 +/- 511.48
Episode length: 42.16 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=1808.35 +/- 502.40
Episode length: 42.36 +/- 2.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40       |
|    ep_rew_mean     | 718      |
| time/              |          |
|    fps             | 201      |
|    iterations      | 210      |
|    time_elapsed    | 2132     |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=430500, episode_reward=1754.14 +/- 541.54
Episode length: 41.80 +/- 2.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 1.75e+03     |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 0.0051333704 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.312       |
|    explained_variance   | 0.00452      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.09e+04     |
|    n_updates            | 249          |
|    policy_gradient_loss | 0.00152      |
|    value_loss           | 4.87e+04     |
------------------------------------------
Eval num_timesteps=431000, episode_reward=1700.80 +/- 594.25
Episode length: 41.62 +/- 2.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=1757.70 +/- 536.11
Episode length: 41.74 +/- 2.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=1610.02 +/- 632.17
Episode length: 41.46 +/- 3.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.6     |
|    ep_rew_mean     | 963      |
| time/              |          |
|    fps             | 201      |
|    iterations      | 211      |
|    time_elapsed    | 2139     |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=432500, episode_reward=1733.93 +/- 539.27
Episode length: 42.22 +/- 2.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.73e+03    |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.004461837 |
|    clip_fraction        | 0.0367      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | 0.00607     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.73e+04    |
|    n_updates            | 250         |
|    policy_gradient_loss | 0.00217     |
|    value_loss           | 7.42e+04    |
-----------------------------------------
Eval num_timesteps=433000, episode_reward=1834.49 +/- 403.52
Episode length: 42.52 +/- 1.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=1686.85 +/- 544.04
Episode length: 42.08 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=1805.70 +/- 448.84
Episode length: 42.32 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.5     |
|    ep_rew_mean     | 1e+03    |
| time/              |          |
|    fps             | 202      |
|    iterations      | 212      |
|    time_elapsed    | 2147     |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=434500, episode_reward=1533.66 +/- 637.82
Episode length: 41.40 +/- 2.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.4       |
|    mean_reward          | 1.53e+03   |
| time/                   |            |
|    total_timesteps      | 434500     |
| train/                  |            |
|    approx_kl            | 0.00516153 |
|    clip_fraction        | 0.0357     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.00805    |
|    learning_rate        | 0.0001     |
|    loss                 | 5.06e+04   |
|    n_updates            | 251        |
|    policy_gradient_loss | -0.00343   |
|    value_loss           | 5.45e+04   |
----------------------------------------
Eval num_timesteps=435000, episode_reward=1811.74 +/- 458.33
Episode length: 42.42 +/- 1.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=1805.84 +/- 458.91
Episode length: 42.24 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=1705.93 +/- 612.48
Episode length: 41.72 +/- 2.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.9     |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    fps             | 202      |
|    iterations      | 213      |
|    time_elapsed    | 2154     |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=436500, episode_reward=1779.12 +/- 451.17
Episode length: 42.22 +/- 2.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.78e+03    |
| time/                   |             |
|    total_timesteps      | 436500      |
| train/                  |             |
|    approx_kl            | 0.003990328 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.00999     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.91e+04    |
|    n_updates            | 252         |
|    policy_gradient_loss | 0.00217     |
|    value_loss           | 7.51e+04    |
-----------------------------------------
Eval num_timesteps=437000, episode_reward=1779.38 +/- 486.56
Episode length: 42.22 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=1660.37 +/- 633.35
Episode length: 41.50 +/- 2.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=1758.17 +/- 526.02
Episode length: 42.36 +/- 1.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.1     |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    fps             | 202      |
|    iterations      | 214      |
|    time_elapsed    | 2162     |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=438500, episode_reward=1743.52 +/- 525.67
Episode length: 42.06 +/- 2.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 438500      |
| train/                  |             |
|    approx_kl            | 0.004070862 |
|    clip_fraction        | 0.0592      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.315      |
|    explained_variance   | 0.00681     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.88e+04    |
|    n_updates            | 253         |
|    policy_gradient_loss | 0.0033      |
|    value_loss           | 5.17e+04    |
-----------------------------------------
Eval num_timesteps=439000, episode_reward=1628.01 +/- 593.34
Episode length: 41.70 +/- 2.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=1810.34 +/- 502.76
Episode length: 41.70 +/- 2.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=1727.47 +/- 545.26
Episode length: 42.42 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.9     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 202      |
|    iterations      | 215      |
|    time_elapsed    | 2169     |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=440500, episode_reward=1728.04 +/- 578.74
Episode length: 41.64 +/- 3.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 1.73e+03     |
| time/                   |              |
|    total_timesteps      | 440500       |
| train/                  |              |
|    approx_kl            | 0.0071681123 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.462       |
|    explained_variance   | 0.00551      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.9e+04      |
|    n_updates            | 254          |
|    policy_gradient_loss | 0.0159       |
|    value_loss           | 5e+04        |
------------------------------------------
Eval num_timesteps=441000, episode_reward=1802.05 +/- 489.23
Episode length: 42.20 +/- 1.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=1722.89 +/- 580.45
Episode length: 41.74 +/- 2.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=1779.78 +/- 527.11
Episode length: 41.88 +/- 2.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.4     |
|    ep_rew_mean     | 583      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 216      |
|    time_elapsed    | 2177     |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=442500, episode_reward=1813.38 +/- 473.23
Episode length: 42.14 +/- 2.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 442500      |
| train/                  |             |
|    approx_kl            | 0.008089205 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | -0.0138     |
|    learning_rate        | 0.0001      |
|    loss                 | 8.55e+03    |
|    n_updates            | 255         |
|    policy_gradient_loss | -0.000868   |
|    value_loss           | 2.12e+04    |
-----------------------------------------
Eval num_timesteps=443000, episode_reward=1830.70 +/- 438.86
Episode length: 42.16 +/- 1.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=1747.99 +/- 500.71
Episode length: 42.18 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=1777.08 +/- 522.96
Episode length: 42.20 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38       |
|    ep_rew_mean     | 460      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 217      |
|    time_elapsed    | 2184     |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=444500, episode_reward=1738.19 +/- 541.57
Episode length: 42.00 +/- 2.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 444500      |
| train/                  |             |
|    approx_kl            | 0.015498807 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.286      |
|    explained_variance   | 0.000741    |
|    learning_rate        | 0.0001      |
|    loss                 | 2.16e+04    |
|    n_updates            | 256         |
|    policy_gradient_loss | -0.0047     |
|    value_loss           | 3e+04       |
-----------------------------------------
Eval num_timesteps=445000, episode_reward=1614.99 +/- 630.02
Episode length: 41.44 +/- 3.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=1789.65 +/- 506.92
Episode length: 42.36 +/- 2.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=1634.59 +/- 632.64
Episode length: 41.18 +/- 3.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.1     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 218      |
|    time_elapsed    | 2192     |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=446500, episode_reward=1815.36 +/- 484.30
Episode length: 41.90 +/- 2.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.82e+03     |
| time/                   |              |
|    total_timesteps      | 446500       |
| train/                  |              |
|    approx_kl            | 0.0076652425 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.132       |
|    explained_variance   | 0.0117       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.56e+04     |
|    n_updates            | 257          |
|    policy_gradient_loss | -8.79e-05    |
|    value_loss           | 5.73e+04     |
------------------------------------------
Eval num_timesteps=447000, episode_reward=1714.02 +/- 572.01
Episode length: 41.70 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=1718.43 +/- 528.64
Episode length: 42.02 +/- 2.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=1803.51 +/- 473.95
Episode length: 41.96 +/- 2.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=1718.98 +/- 558.62
Episode length: 41.94 +/- 2.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.7     |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 203      |
|    iterations      | 219      |
|    time_elapsed    | 2201     |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=449000, episode_reward=1727.24 +/- 556.63
Episode length: 42.04 +/- 2.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.73e+03    |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.008675139 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0994     |
|    explained_variance   | 0.022       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.35e+04    |
|    n_updates            | 258         |
|    policy_gradient_loss | -0.00114    |
|    value_loss           | 8.63e+04    |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=1801.23 +/- 482.83
Episode length: 42.38 +/- 1.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=1810.89 +/- 457.30
Episode length: 42.30 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=1679.46 +/- 550.11
Episode length: 42.04 +/- 2.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.7     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 203      |
|    iterations      | 220      |
|    time_elapsed    | 2208     |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=451000, episode_reward=1838.40 +/- 462.69
Episode length: 42.34 +/- 1.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.84e+03    |
| time/                   |             |
|    total_timesteps      | 451000      |
| train/                  |             |
|    approx_kl            | 0.023261994 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.0107      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.19e+04    |
|    n_updates            | 259         |
|    policy_gradient_loss | 0.00738     |
|    value_loss           | 3.85e+04    |
-----------------------------------------
Eval num_timesteps=451500, episode_reward=1767.29 +/- 578.36
Episode length: 41.62 +/- 2.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=1817.07 +/- 467.30
Episode length: 42.26 +/- 2.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=1785.28 +/- 501.58
Episode length: 42.28 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.6     |
|    ep_rew_mean     | 667      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 221      |
|    time_elapsed    | 2216     |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=453000, episode_reward=1735.34 +/- 555.34
Episode length: 41.56 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.6        |
|    mean_reward          | 1.74e+03    |
| time/                   |             |
|    total_timesteps      | 453000      |
| train/                  |             |
|    approx_kl            | 0.012338507 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.542      |
|    explained_variance   | -0.0197     |
|    learning_rate        | 0.0001      |
|    loss                 | 6.53e+03    |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.00488     |
|    value_loss           | 1.83e+04    |
-----------------------------------------
Eval num_timesteps=453500, episode_reward=1769.67 +/- 524.08
Episode length: 42.06 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=1788.28 +/- 501.17
Episode length: 42.02 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=1749.13 +/- 541.63
Episode length: 41.88 +/- 1.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.4     |
|    ep_rew_mean     | 333      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 222      |
|    time_elapsed    | 2223     |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=455000, episode_reward=1718.34 +/- 564.39
Episode length: 41.82 +/- 2.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.72e+03    |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.013448869 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | -0.0307     |
|    learning_rate        | 0.0001      |
|    loss                 | 6.98e+03    |
|    n_updates            | 261         |
|    policy_gradient_loss | 0.00345     |
|    value_loss           | 1.77e+04    |
-----------------------------------------
Eval num_timesteps=455500, episode_reward=1761.74 +/- 524.04
Episode length: 41.98 +/- 2.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=1794.93 +/- 495.34
Episode length: 42.04 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=1767.24 +/- 496.40
Episode length: 42.30 +/- 1.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.1     |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 223      |
|    time_elapsed    | 2231     |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=457000, episode_reward=1805.66 +/- 445.24
Episode length: 42.30 +/- 1.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.81e+03    |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.008486045 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | -0.00561    |
|    learning_rate        | 0.0001      |
|    loss                 | 9.15e+03    |
|    n_updates            | 262         |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 2.58e+04    |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=1828.34 +/- 427.01
Episode length: 41.78 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=1712.57 +/- 521.66
Episode length: 41.88 +/- 1.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=1813.31 +/- 483.43
Episode length: 41.88 +/- 2.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.3     |
|    ep_rew_mean     | 696      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 224      |
|    time_elapsed    | 2238     |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=459000, episode_reward=1684.83 +/- 564.45
Episode length: 41.68 +/- 2.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.7         |
|    mean_reward          | 1.68e+03     |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0043341834 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.278       |
|    explained_variance   | 0.00397      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.8e+04      |
|    n_updates            | 263          |
|    policy_gradient_loss | -0.00295     |
|    value_loss           | 3.95e+04     |
------------------------------------------
Eval num_timesteps=459500, episode_reward=1779.93 +/- 523.28
Episode length: 41.88 +/- 2.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=1761.85 +/- 511.04
Episode length: 41.98 +/- 2.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=1775.91 +/- 521.12
Episode length: 42.24 +/- 1.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.7     |
|    ep_rew_mean     | 958      |
| time/              |          |
|    fps             | 205      |
|    iterations      | 225      |
|    time_elapsed    | 2246     |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=461000, episode_reward=1814.28 +/- 464.18
Episode length: 42.12 +/- 2.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.81e+03     |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0021410573 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.19        |
|    explained_variance   | 0.0105       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.67e+04     |
|    n_updates            | 264          |
|    policy_gradient_loss | 4.26e-05     |
|    value_loss           | 5.2e+04      |
------------------------------------------
Eval num_timesteps=461500, episode_reward=1806.46 +/- 459.63
Episode length: 42.18 +/- 1.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=1717.95 +/- 549.96
Episode length: 41.70 +/- 2.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=1573.19 +/- 636.75
Episode length: 41.50 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.1     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 205      |
|    iterations      | 226      |
|    time_elapsed    | 2253     |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=463000, episode_reward=1569.94 +/- 645.85
Episode length: 41.70 +/- 3.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.7         |
|    mean_reward          | 1.57e+03     |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0037559594 |
|    clip_fraction        | 0.0067       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0.0263       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.23e+04     |
|    n_updates            | 265          |
|    policy_gradient_loss | 0.000435     |
|    value_loss           | 6.28e+04     |
------------------------------------------
Eval num_timesteps=463500, episode_reward=1806.59 +/- 507.84
Episode length: 41.90 +/- 1.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=1880.32 +/- 389.50
Episode length: 42.36 +/- 1.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=1672.16 +/- 615.80
Episode length: 41.42 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.6     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 205      |
|    iterations      | 227      |
|    time_elapsed    | 2261     |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=465000, episode_reward=1708.26 +/- 579.01
Episode length: 41.98 +/- 2.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.71e+03    |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.007980117 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.278      |
|    explained_variance   | 0.0104      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.47e+04    |
|    n_updates            | 266         |
|    policy_gradient_loss | 0.0227      |
|    value_loss           | 5.62e+04    |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=1692.77 +/- 584.09
Episode length: 41.72 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=1798.09 +/- 486.45
Episode length: 41.82 +/- 2.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=1694.20 +/- 550.88
Episode length: 42.06 +/- 2.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.69e+03 |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 39.6     |
|    ep_rew_mean     | 895      |
| time/              |          |
|    fps             | 205      |
|    iterations      | 228      |
|    time_elapsed    | 2268     |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=467000, episode_reward=1810.92 +/- 404.22
Episode length: 42.10 +/- 1.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 1.81e+03     |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0072753974 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.309       |
|    explained_variance   | -0.006       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.76e+04     |
|    n_updates            | 267          |
|    policy_gradient_loss | 0.00664      |
|    value_loss           | 3.42e+04     |
------------------------------------------
Eval num_timesteps=467500, episode_reward=1763.84 +/- 523.40
Episode length: 42.22 +/- 2.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=1791.98 +/- 507.78
Episode length: 41.84 +/- 2.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=1790.27 +/- 503.27
Episode length: 42.20 +/- 1.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40       |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 206      |
|    iterations      | 229      |
|    time_elapsed    | 2275     |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=469000, episode_reward=1864.49 +/- 384.79
Episode length: 42.72 +/- 1.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.7       |
|    mean_reward          | 1.86e+03   |
| time/                   |            |
|    total_timesteps      | 469000     |
| train/                  |            |
|    approx_kl            | 0.00576876 |
|    clip_fraction        | 0.0223     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.0115     |
|    learning_rate        | 0.0001     |
|    loss                 | 2.57e+04   |
|    n_updates            | 268        |
|    policy_gradient_loss | -0.00336   |
|    value_loss           | 4.22e+04   |
----------------------------------------
Eval num_timesteps=469500, episode_reward=1862.70 +/- 383.98
Episode length: 42.66 +/- 1.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=1794.05 +/- 461.17
Episode length: 42.24 +/- 1.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=1870.48 +/- 388.47
Episode length: 42.42 +/- 1.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=1869.53 +/- 419.39
Episode length: 42.02 +/- 2.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.2     |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 206      |
|    iterations      | 230      |
|    time_elapsed    | 2284     |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.49
Eval num_timesteps=471500, episode_reward=1751.26 +/- 548.50
Episode length: 41.84 +/- 2.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.75e+03    |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.055753164 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.0326      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.81e+04    |
|    n_updates            | 269         |
|    policy_gradient_loss | 0.0236      |
|    value_loss           | 6.46e+04    |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=1748.27 +/- 544.49
Episode length: 41.58 +/- 2.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=1821.11 +/- 467.60
Episode length: 42.04 +/- 2.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=1822.04 +/- 415.17
Episode length: 42.44 +/- 1.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.82e+03 |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.4     |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    fps             | 206      |
|    iterations      | 231      |
|    time_elapsed    | 2292     |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=473500, episode_reward=354.27 +/- 472.89
Episode length: 42.14 +/- 6.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.1       |
|    mean_reward          | 354        |
| time/                   |            |
|    total_timesteps      | 473500     |
| train/                  |            |
|    approx_kl            | 0.02110244 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.577     |
|    explained_variance   | -0.0368    |
|    learning_rate        | 0.0001     |
|    loss                 | 1.49e+04   |
|    n_updates            | 270        |
|    policy_gradient_loss | 0.0252     |
|    value_loss           | 3.08e+04   |
----------------------------------------
Eval num_timesteps=474000, episode_reward=417.37 +/- 525.23
Episode length: 42.26 +/- 7.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=477.75 +/- 580.75
Episode length: 43.56 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=426.57 +/- 515.32
Episode length: 42.96 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 206      |
|    iterations      | 232      |
|    time_elapsed    | 2300     |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=475500, episode_reward=-589.90 +/- 46.58
Episode length: 104.08 +/- 27.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | -590        |
| time/                   |             |
|    total_timesteps      | 475500      |
| train/                  |             |
|    approx_kl            | 0.037349448 |
|    clip_fraction        | 0.477       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.641      |
|    explained_variance   | -0.107      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.9e+04     |
|    n_updates            | 271         |
|    policy_gradient_loss | 0.0549      |
|    value_loss           | 3.19e+04    |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=-584.97 +/- 49.16
Episode length: 99.42 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=-578.15 +/- 45.50
Episode length: 95.18 +/- 24.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | -578     |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=-592.37 +/- 39.91
Episode length: 100.26 +/- 26.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -592     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | 125      |
| time/              |          |
|    fps             | 206      |
|    iterations      | 233      |
|    time_elapsed    | 2315     |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=477500, episode_reward=-568.53 +/- 57.71
Episode length: 95.42 +/- 23.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.4        |
|    mean_reward          | -569        |
| time/                   |             |
|    total_timesteps      | 477500      |
| train/                  |             |
|    approx_kl            | 0.025833845 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.587      |
|    explained_variance   | -0.104      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.18e+04    |
|    n_updates            | 272         |
|    policy_gradient_loss | 0.0251      |
|    value_loss           | 2.14e+04    |
-----------------------------------------
Eval num_timesteps=478000, episode_reward=-576.99 +/- 47.01
Episode length: 98.24 +/- 26.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=-585.89 +/- 52.42
Episode length: 96.08 +/- 23.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=-588.47 +/- 60.68
Episode length: 102.32 +/- 25.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -588     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.7     |
|    ep_rew_mean     | 4.65     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 234      |
|    time_elapsed    | 2330     |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=479500, episode_reward=-580.59 +/- 51.85
Episode length: 95.02 +/- 22.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95           |
|    mean_reward          | -581         |
| time/                   |              |
|    total_timesteps      | 479500       |
| train/                  |              |
|    approx_kl            | 0.0132857505 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.469       |
|    explained_variance   | -0.137       |
|    learning_rate        | 0.0001       |
|    loss                 | 1.85e+04     |
|    n_updates            | 273          |
|    policy_gradient_loss | 0.00766      |
|    value_loss           | 3.25e+04     |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-583.96 +/- 42.64
Episode length: 92.06 +/- 24.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.1     |
|    mean_reward     | -584     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=-589.89 +/- 37.43
Episode length: 95.24 +/- 22.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=-591.67 +/- 42.79
Episode length: 103.18 +/- 28.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -592     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 62.2     |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 235      |
|    time_elapsed    | 2345     |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=481500, episode_reward=-588.17 +/- 48.26
Episode length: 101.28 +/- 22.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | -588        |
| time/                   |             |
|    total_timesteps      | 481500      |
| train/                  |             |
|    approx_kl            | 0.009740764 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.327      |
|    explained_variance   | -0.18       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.52e+04    |
|    n_updates            | 274         |
|    policy_gradient_loss | 0.000476    |
|    value_loss           | 3.26e+04    |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=-575.93 +/- 45.89
Episode length: 97.50 +/- 24.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.5     |
|    mean_reward     | -576     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=-579.47 +/- 54.64
Episode length: 94.68 +/- 23.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.7     |
|    mean_reward     | -579     |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=-592.29 +/- 41.79
Episode length: 98.86 +/- 27.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | -592     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.8     |
|    ep_rew_mean     | -252     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 236      |
|    time_elapsed    | 2360     |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=483500, episode_reward=-578.21 +/- 43.78
Episode length: 93.54 +/- 25.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93.5       |
|    mean_reward          | -578       |
| time/                   |            |
|    total_timesteps      | 483500     |
| train/                  |            |
|    approx_kl            | 0.00813501 |
|    clip_fraction        | 0.0664     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.21      |
|    explained_variance   | -0.23      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.68e+04   |
|    n_updates            | 275        |
|    policy_gradient_loss | 0.014      |
|    value_loss           | 3.29e+04   |
----------------------------------------
Eval num_timesteps=484000, episode_reward=-577.21 +/- 49.72
Episode length: 96.50 +/- 22.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=-592.65 +/- 40.19
Episode length: 102.30 +/- 25.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -593     |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-578.67 +/- 58.09
Episode length: 103.62 +/- 29.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | -579     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | -398     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 237      |
|    time_elapsed    | 2375     |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=485500, episode_reward=-584.36 +/- 46.62
Episode length: 97.66 +/- 24.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.7        |
|    mean_reward          | -584        |
| time/                   |             |
|    total_timesteps      | 485500      |
| train/                  |             |
|    approx_kl            | 0.004540968 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.226      |
|    explained_variance   | -0.219      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.4e+04     |
|    n_updates            | 276         |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 2.87e+04    |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=-573.23 +/- 50.02
Episode length: 95.74 +/- 24.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.7     |
|    mean_reward     | -573     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=-595.63 +/- 46.01
Episode length: 101.42 +/- 26.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -596     |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=-587.45 +/- 51.75
Episode length: 101.60 +/- 26.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -587     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -433     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 238      |
|    time_elapsed    | 2390     |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=487500, episode_reward=-584.05 +/- 49.33
Episode length: 105.94 +/- 29.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | -584        |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.008377553 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.408      |
|    explained_variance   | -0.17       |
|    learning_rate        | 0.0001      |
|    loss                 | 9.26e+03    |
|    n_updates            | 277         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 2.08e+04    |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=-595.69 +/- 38.02
Episode length: 98.76 +/- 25.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.8     |
|    mean_reward     | -596     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=-580.95 +/- 61.30
Episode length: 101.00 +/- 29.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -581     |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=-571.87 +/- 56.70
Episode length: 100.22 +/- 20.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -572     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.9     |
|    ep_rew_mean     | -374     |
| time/              |          |
|    fps             | 203      |
|    iterations      | 239      |
|    time_elapsed    | 2405     |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=489500, episode_reward=-572.05 +/- 50.53
Episode length: 96.52 +/- 24.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.5       |
|    mean_reward          | -572       |
| time/                   |            |
|    total_timesteps      | 489500     |
| train/                  |            |
|    approx_kl            | 0.00937548 |
|    clip_fraction        | 0.0885     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.551     |
|    explained_variance   | -0.0457    |
|    learning_rate        | 0.0001     |
|    loss                 | 6.57e+03   |
|    n_updates            | 278        |
|    policy_gradient_loss | -0.00715   |
|    value_loss           | 1.18e+04   |
----------------------------------------
Eval num_timesteps=490000, episode_reward=-585.47 +/- 47.59
Episode length: 100.96 +/- 24.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=-579.01 +/- 57.58
Episode length: 99.28 +/- 22.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | -579     |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=-582.23 +/- 47.24
Episode length: 96.02 +/- 22.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96       |
|    mean_reward     | -582     |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=-584.81 +/- 53.51
Episode length: 101.96 +/- 27.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 64.7     |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 202      |
|    iterations      | 240      |
|    time_elapsed    | 2424     |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=492000, episode_reward=-273.70 +/- 244.43
Episode length: 87.48 +/- 22.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.5        |
|    mean_reward          | -274        |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.018308762 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.644      |
|    explained_variance   | -0.04       |
|    learning_rate        | 0.0001      |
|    loss                 | 7.56e+03    |
|    n_updates            | 279         |
|    policy_gradient_loss | 0.0128      |
|    value_loss           | 1.23e+04    |
-----------------------------------------
Eval num_timesteps=492500, episode_reward=-282.25 +/- 179.97
Episode length: 84.30 +/- 22.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.3     |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-303.01 +/- 182.81
Episode length: 89.24 +/- 65.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.2     |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=-261.81 +/- 200.87
Episode length: 87.76 +/- 29.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.8     |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 56.3     |
|    ep_rew_mean     | -50.5    |
| time/              |          |
|    fps             | 202      |
|    iterations      | 241      |
|    time_elapsed    | 2438     |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=494000, episode_reward=884.00 +/- 699.61
Episode length: 40.50 +/- 4.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.5        |
|    mean_reward          | 884         |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.022815764 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.668      |
|    explained_variance   | -0.0416     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.68e+03    |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.00623     |
|    value_loss           | 1.19e+04    |
-----------------------------------------
Eval num_timesteps=494500, episode_reward=967.27 +/- 816.23
Episode length: 40.28 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.3     |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=1208.28 +/- 819.30
Episode length: 41.30 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=1189.81 +/- 757.02
Episode length: 41.88 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | 113      |
| time/              |          |
|    fps             | 202      |
|    iterations      | 242      |
|    time_elapsed    | 2445     |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=496000, episode_reward=1792.44 +/- 486.83
Episode length: 41.98 +/- 2.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.79e+03    |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.010422993 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.607      |
|    explained_variance   | -0.0199     |
|    learning_rate        | 0.0001      |
|    loss                 | 9.86e+03    |
|    n_updates            | 281         |
|    policy_gradient_loss | 0.00262     |
|    value_loss           | 1.46e+04    |
-----------------------------------------
Eval num_timesteps=496500, episode_reward=1634.32 +/- 589.29
Episode length: 41.72 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=1776.17 +/- 525.76
Episode length: 41.72 +/- 2.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.78e+03 |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=1706.27 +/- 521.54
Episode length: 42.22 +/- 1.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.9     |
|    ep_rew_mean     | 260      |
| time/              |          |
|    fps             | 202      |
|    iterations      | 243      |
|    time_elapsed    | 2453     |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=498000, episode_reward=1761.28 +/- 523.80
Episode length: 42.04 +/- 2.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 1.76e+03    |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.010781131 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.543      |
|    explained_variance   | -0.0127     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.27e+03    |
|    n_updates            | 282         |
|    policy_gradient_loss | -0.00911    |
|    value_loss           | 1.44e+04    |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=1714.88 +/- 567.33
Episode length: 41.78 +/- 1.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.71e+03 |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=1732.40 +/- 555.26
Episode length: 42.06 +/- 1.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=1788.63 +/- 488.66
Episode length: 42.40 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40       |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 244      |
|    time_elapsed    | 2460     |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=500000, episode_reward=1859.37 +/- 433.44
Episode length: 42.32 +/- 2.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.86e+03    |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.009063633 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.00101     |
|    learning_rate        | 0.0001      |
|    loss                 | 8.37e+03    |
|    n_updates            | 283         |
|    policy_gradient_loss | -0.00395    |
|    value_loss           | 2.3e+04     |
-----------------------------------------
Eval num_timesteps=500500, episode_reward=1654.70 +/- 607.11
Episode length: 41.72 +/- 2.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=1763.96 +/- 535.91
Episode length: 41.82 +/- 2.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=1673.41 +/- 603.94
Episode length: 42.02 +/- 2.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 40.1     |
|    ep_rew_mean     | 669      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 245      |
|    time_elapsed    | 2468     |
|    total_timesteps | 501760   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-1-last/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Early stopping at step 0 due to reaching max kl: 0.02
