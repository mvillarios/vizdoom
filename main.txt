/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-503.44 +/- 79.69
Episode length: 46.70 +/- 14.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-505.93 +/- 77.13
Episode length: 47.14 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-507.13 +/- 89.05
Episode length: 43.70 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.7     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-519.64 +/- 62.61
Episode length: 51.52 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.6     |
|    ep_rew_mean     | -383     |
| time/              |          |
|    fps             | 248      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=734.11 +/- 660.06
Episode length: 33.96 +/- 7.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34          |
|    mean_reward          | 734         |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.006640106 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 5.89e-05    |
|    learning_rate        | 0.001       |
|    loss                 | 1.44e+03    |
|    n_updates            | 1           |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 3.43e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=803.88 +/- 623.64
Episode length: 35.48 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
New best mean reward!
Eval num_timesteps=3500, episode_reward=858.53 +/- 683.70
Episode length: 35.62 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=761.85 +/- 667.13
Episode length: 34.60 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.2     |
|    ep_rew_mean     | -363     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 2        |
|    time_elapsed    | 14       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=806.80 +/- 685.06
Episode length: 34.82 +/- 6.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | 807         |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.008634382 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.03       |
|    explained_variance   | 5.3e-06     |
|    learning_rate        | 0.001       |
|    loss                 | 1.55e+03    |
|    n_updates            | 2           |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 2.81e+03    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=833.55 +/- 702.01
Episode length: 35.12 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=915.55 +/- 671.16
Episode length: 36.62 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
New best mean reward!
Eval num_timesteps=6000, episode_reward=833.67 +/- 621.81
Episode length: 35.60 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.1     |
|    ep_rew_mean     | -340     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 3        |
|    time_elapsed    | 21       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=760.79 +/- 632.39
Episode length: 34.74 +/- 6.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | 761         |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.011092078 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.95       |
|    explained_variance   | 0.000281    |
|    learning_rate        | 0.001       |
|    loss                 | 2.36e+03    |
|    n_updates            | 3           |
|    policy_gradient_loss | 0.000705    |
|    value_loss           | 5.36e+03    |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=776.71 +/- 668.87
Episode length: 34.18 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=832.79 +/- 706.54
Episode length: 34.86 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=918.34 +/- 686.07
Episode length: 36.58 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.8     |
|    ep_rew_mean     | -290     |
| time/              |          |
|    fps             | 291      |
|    iterations      | 4        |
|    time_elapsed    | 28       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=8500, episode_reward=773.89 +/- 630.33
Episode length: 34.82 +/- 6.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | 774         |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.026644379 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.000556    |
|    learning_rate        | 0.001       |
|    loss                 | 3.39e+03    |
|    n_updates            | 4           |
|    policy_gradient_loss | 0.00867     |
|    value_loss           | 6.64e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=799.89 +/- 686.41
Episode length: 34.96 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=742.62 +/- 670.45
Episode length: 34.34 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=778.77 +/- 724.86
Episode length: 33.70 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | -214     |
| time/              |          |
|    fps             | 295      |
|    iterations      | 5        |
|    time_elapsed    | 34       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.12
Eval num_timesteps=10500, episode_reward=899.32 +/- 723.44
Episode length: 35.72 +/- 6.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.7        |
|    mean_reward          | 899         |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.058474127 |
|    clip_fraction        | 0.492       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.00115     |
|    learning_rate        | 0.001       |
|    loss                 | 1.34e+03    |
|    n_updates            | 5           |
|    policy_gradient_loss | -0.00298    |
|    value_loss           | 7.06e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=726.50 +/- 632.44
Episode length: 34.76 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=775.39 +/- 627.33
Episode length: 34.80 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=757.77 +/- 637.94
Episode length: 35.40 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | -52.4    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 6        |
|    time_elapsed    | 41       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.18
Eval num_timesteps=12500, episode_reward=876.34 +/- 678.53
Episode length: 35.78 +/- 5.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.8       |
|    mean_reward          | 876        |
| time/                   |            |
|    total_timesteps      | 12500      |
| train/                  |            |
|    approx_kl            | 0.08954261 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.807     |
|    explained_variance   | 0.00153    |
|    learning_rate        | 0.001      |
|    loss                 | 4.12e+03   |
|    n_updates            | 6          |
|    policy_gradient_loss | 0.0235     |
|    value_loss           | 8.14e+03   |
----------------------------------------
Eval num_timesteps=13000, episode_reward=986.80 +/- 757.28
Episode length: 36.50 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
New best mean reward!
Eval num_timesteps=13500, episode_reward=775.98 +/- 596.78
Episode length: 35.56 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=1091.44 +/- 780.01
Episode length: 35.98 +/- 8.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 187      |
| time/              |          |
|    fps             | 299      |
|    iterations      | 7        |
|    time_elapsed    | 47       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=14500, episode_reward=875.77 +/- 677.24
Episode length: 35.66 +/- 6.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.7        |
|    mean_reward          | 876         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.024492627 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.366      |
|    explained_variance   | 0.000514    |
|    learning_rate        | 0.001       |
|    loss                 | 9.99e+03    |
|    n_updates            | 7           |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 1.98e+04    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=824.27 +/- 646.68
Episode length: 35.58 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=801.14 +/- 632.17
Episode length: 36.10 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=832.55 +/- 700.73
Episode length: 35.28 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 300      |
|    iterations      | 8        |
|    time_elapsed    | 54       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=16500, episode_reward=800.46 +/- 662.95
Episode length: 34.62 +/- 5.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.6        |
|    mean_reward          | 800         |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.018472357 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 1.33e-05    |
|    learning_rate        | 0.001       |
|    loss                 | 6.19e+03    |
|    n_updates            | 8           |
|    policy_gradient_loss | 0.0051      |
|    value_loss           | 2.7e+04     |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=924.98 +/- 682.60
Episode length: 36.40 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=758.81 +/- 574.76
Episode length: 35.66 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=821.58 +/- 690.30
Episode length: 35.02 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.6     |
|    ep_rew_mean     | 618      |
| time/              |          |
|    fps             | 301      |
|    iterations      | 9        |
|    time_elapsed    | 61       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=898.97 +/- 654.41
Episode length: 36.34 +/- 6.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.3        |
|    mean_reward          | 899         |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.002602592 |
|    clip_fraction        | 0.0074      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0818     |
|    explained_variance   | -0.00138    |
|    learning_rate        | 0.001       |
|    loss                 | 1.67e+04    |
|    n_updates            | 9           |
|    policy_gradient_loss | 0.000685    |
|    value_loss           | 4.15e+04    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=832.99 +/- 715.54
Episode length: 34.76 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=842.26 +/- 713.93
Episode length: 34.96 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=751.20 +/- 596.55
Episode length: 35.66 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 302      |
|    iterations      | 10       |
|    time_elapsed    | 67       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=20500, episode_reward=729.48 +/- 597.14
Episode length: 35.26 +/- 7.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.3        |
|    mean_reward          | 729         |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.004445507 |
|    clip_fraction        | 0.00174     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | -0.00117    |
|    learning_rate        | 0.001       |
|    loss                 | 1.77e+04    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 3.51e+04    |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=793.06 +/- 670.71
Episode length: 35.08 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=837.97 +/- 686.71
Episode length: 35.54 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=830.95 +/- 689.13
Episode length: 35.22 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=961.61 +/- 672.14
Episode length: 37.00 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 708      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 11       |
|    time_elapsed    | 75       |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=782.55 +/- 643.10
Episode length: 35.38 +/- 4.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 783         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.006828003 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | -0.000154   |
|    learning_rate        | 0.001       |
|    loss                 | 1.38e+04    |
|    n_updates            | 11          |
|    policy_gradient_loss | 0.0075      |
|    value_loss           | 2.88e+04    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=797.46 +/- 671.83
Episode length: 34.80 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=852.87 +/- 642.18
Episode length: 35.86 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=865.03 +/- 668.15
Episode length: 35.92 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 686      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 12       |
|    time_elapsed    | 82       |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=25000, episode_reward=943.80 +/- 756.86
Episode length: 35.58 +/- 6.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 944         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011511878 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0718     |
|    explained_variance   | -0.000472   |
|    learning_rate        | 0.001       |
|    loss                 | 9.47e+03    |
|    n_updates            | 12          |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 3.2e+04     |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=883.26 +/- 721.96
Episode length: 35.66 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=713.50 +/- 600.75
Episode length: 34.72 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=695.07 +/- 535.63
Episode length: 35.52 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 912      |
| time/              |          |
|    fps             | 299      |
|    iterations      | 13       |
|    time_elapsed    | 88       |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=27000, episode_reward=731.66 +/- 590.49
Episode length: 35.36 +/- 6.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 732         |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.003907666 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0198     |
|    explained_variance   | -0.000525   |
|    learning_rate        | 0.001       |
|    loss                 | 2.53e+04    |
|    n_updates            | 13          |
|    policy_gradient_loss | -0.000664   |
|    value_loss           | 5.99e+04    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=880.54 +/- 687.31
Episode length: 36.04 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=855.82 +/- 653.65
Episode length: 36.14 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=861.01 +/- 674.14
Episode length: 35.82 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 952      |
| time/              |          |
|    fps             | 299      |
|    iterations      | 14       |
|    time_elapsed    | 95       |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=932.10 +/- 721.24
Episode length: 36.48 +/- 6.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 932          |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0034952597 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0281      |
|    explained_variance   | -0.000431    |
|    learning_rate        | 0.001        |
|    loss                 | 3.22e+04     |
|    n_updates            | 17           |
|    policy_gradient_loss | -9.69e-05    |
|    value_loss           | 4.34e+04     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=768.23 +/- 632.44
Episode length: 35.08 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=761.43 +/- 589.76
Episode length: 35.48 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=940.66 +/- 690.03
Episode length: 36.24 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 467      |
| time/              |          |
|    fps             | 299      |
|    iterations      | 15       |
|    time_elapsed    | 102      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=31000, episode_reward=800.45 +/- 714.55
Episode length: 34.32 +/- 7.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.3       |
|    mean_reward          | 800        |
| time/                   |            |
|    total_timesteps      | 31000      |
| train/                  |            |
|    approx_kl            | 0.01383169 |
|    clip_fraction        | 0.0417     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.000159   |
|    learning_rate        | 0.001      |
|    loss                 | 3.31e+03   |
|    n_updates            | 18         |
|    policy_gradient_loss | 0.0016     |
|    value_loss           | 9.37e+03   |
----------------------------------------
Eval num_timesteps=31500, episode_reward=767.40 +/- 687.20
Episode length: 34.18 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=850.46 +/- 660.56
Episode length: 35.52 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=895.67 +/- 708.43
Episode length: 35.38 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.4     |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 300      |
|    iterations      | 16       |
|    time_elapsed    | 109      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=33000, episode_reward=809.11 +/- 608.27
Episode length: 35.90 +/- 6.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.9       |
|    mean_reward          | 809        |
| time/                   |            |
|    total_timesteps      | 33000      |
| train/                  |            |
|    approx_kl            | 0.01334853 |
|    clip_fraction        | 0.0312     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.279     |
|    explained_variance   | 0.000121   |
|    learning_rate        | 0.001      |
|    loss                 | 5.63e+03   |
|    n_updates            | 19         |
|    policy_gradient_loss | -0.00387   |
|    value_loss           | 1.41e+04   |
----------------------------------------
Eval num_timesteps=33500, episode_reward=722.83 +/- 650.65
Episode length: 34.16 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=757.11 +/- 678.68
Episode length: 34.26 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=882.86 +/- 745.59
Episode length: 34.62 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.1     |
|    ep_rew_mean     | 510      |
| time/              |          |
|    fps             | 300      |
|    iterations      | 17       |
|    time_elapsed    | 115      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=35000, episode_reward=925.87 +/- 726.52
Episode length: 35.36 +/- 6.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 926         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.011268742 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 7.64e-05    |
|    learning_rate        | 0.001       |
|    loss                 | 2.71e+04    |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00428     |
|    value_loss           | 5.14e+04    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=713.47 +/- 629.70
Episode length: 34.32 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=952.97 +/- 741.27
Episode length: 35.86 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 953      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=825.26 +/- 689.53
Episode length: 35.64 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 611      |
| time/              |          |
|    fps             | 301      |
|    iterations      | 18       |
|    time_elapsed    | 122      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=37000, episode_reward=666.96 +/- 585.04
Episode length: 34.10 +/- 6.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.1        |
|    mean_reward          | 667         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.015464106 |
|    clip_fraction        | 0.0104      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.027      |
|    explained_variance   | 0.0001      |
|    learning_rate        | 0.001       |
|    loss                 | 7.39e+03    |
|    n_updates            | 21          |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 2.43e+04    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=903.17 +/- 719.15
Episode length: 35.52 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=904.68 +/- 701.50
Episode length: 35.80 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=745.58 +/- 686.79
Episode length: 33.98 +/- 7.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 301      |
|    iterations      | 19       |
|    time_elapsed    | 128      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.58
Eval num_timesteps=39000, episode_reward=852.08 +/- 712.16
Episode length: 35.00 +/- 6.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 852         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.026184632 |
|    clip_fraction        | 0.00071     |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.000299   |
|    explained_variance   | 0.000357    |
|    learning_rate        | 0.001       |
|    loss                 | 1.9e+04     |
|    n_updates            | 22          |
|    policy_gradient_loss | 0.000367    |
|    value_loss           | 4.79e+04    |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=842.09 +/- 679.59
Episode length: 35.42 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=871.30 +/- 663.70
Episode length: 35.96 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=643.03 +/- 578.30
Episode length: 33.70 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 643      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 302      |
|    iterations      | 20       |
|    time_elapsed    | 135      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=753.61 +/- 672.73
Episode length: 34.44 +/- 7.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 754       |
| time/                   |           |
|    total_timesteps      | 41000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.26e-21 |
|    explained_variance   | 0.00335   |
|    learning_rate        | 0.001     |
|    loss                 | 1.17e+04  |
|    n_updates            | 32        |
|    policy_gradient_loss | 5.82e-11  |
|    value_loss           | 2.72e+04  |
---------------------------------------
Eval num_timesteps=41500, episode_reward=813.38 +/- 705.35
Episode length: 34.58 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=685.80 +/- 592.87
Episode length: 34.00 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 686      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=641.15 +/- 582.60
Episode length: 33.72 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=745.96 +/- 622.85
Episode length: 34.90 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 299      |
|    iterations      | 21       |
|    time_elapsed    | 143      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=807.36 +/- 700.16
Episode length: 34.32 +/- 6.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 807       |
| time/                   |           |
|    total_timesteps      | 43500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.0112    |
|    learning_rate        | 0.001     |
|    loss                 | 6.06e+03  |
|    n_updates            | 42        |
|    policy_gradient_loss | -6.07e-10 |
|    value_loss           | 2.6e+04   |
---------------------------------------
Eval num_timesteps=44000, episode_reward=744.59 +/- 600.17
Episode length: 35.42 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=765.89 +/- 656.26
Episode length: 34.86 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=901.84 +/- 654.22
Episode length: 36.52 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 298      |
|    iterations      | 22       |
|    time_elapsed    | 151      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=809.81 +/- 714.15
Episode length: 34.74 +/- 7.12
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.7     |
|    mean_reward          | 810      |
| time/                   |          |
|    total_timesteps      | 45500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0182  |
|    learning_rate        | 0.001    |
|    loss                 | 7.67e+04 |
|    n_updates            | 52       |
|    policy_gradient_loss | 4.54e-09 |
|    value_loss           | 1.7e+05  |
--------------------------------------
Eval num_timesteps=46000, episode_reward=878.04 +/- 739.43
Episode length: 34.96 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=852.77 +/- 665.45
Episode length: 35.82 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=926.77 +/- 721.82
Episode length: 35.84 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 297      |
|    iterations      | 23       |
|    time_elapsed    | 158      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=859.42 +/- 726.24
Episode length: 35.30 +/- 6.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.3     |
|    mean_reward          | 859      |
| time/                   |          |
|    total_timesteps      | 47500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.0104   |
|    learning_rate        | 0.001    |
|    loss                 | 2.35e+04 |
|    n_updates            | 62       |
|    policy_gradient_loss | 3.9e-10  |
|    value_loss           | 3.06e+04 |
--------------------------------------
Eval num_timesteps=48000, episode_reward=861.52 +/- 756.66
Episode length: 34.50 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=775.13 +/- 637.33
Episode length: 34.66 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=905.10 +/- 700.29
Episode length: 36.00 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 24       |
|    time_elapsed    | 165      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=894.00 +/- 748.96
Episode length: 35.38 +/- 7.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.4     |
|    mean_reward          | 894      |
| time/                   |          |
|    total_timesteps      | 49500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.3e-43 |
|    explained_variance   | -0.0212  |
|    learning_rate        | 0.001    |
|    loss                 | 1.11e+05 |
|    n_updates            | 72       |
|    policy_gradient_loss | 3.38e-09 |
|    value_loss           | 1.89e+05 |
--------------------------------------
Eval num_timesteps=50000, episode_reward=851.48 +/- 660.80
Episode length: 36.46 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=710.60 +/- 656.47
Episode length: 34.04 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=929.45 +/- 721.58
Episode length: 35.82 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 689      |
| time/              |          |
|    fps             | 296      |
|    iterations      | 25       |
|    time_elapsed    | 172      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=807.35 +/- 646.95
Episode length: 35.12 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 807       |
| time/                   |           |
|    total_timesteps      | 51500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.96e-46 |
|    explained_variance   | 0.00761   |
|    learning_rate        | 0.001     |
|    loss                 | 7.83e+03  |
|    n_updates            | 82        |
|    policy_gradient_loss | 7e-10     |
|    value_loss           | 2.27e+04  |
---------------------------------------
Eval num_timesteps=52000, episode_reward=919.67 +/- 707.45
Episode length: 36.32 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=683.14 +/- 579.40
Episode length: 34.52 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=836.75 +/- 649.25
Episode length: 35.56 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 295      |
|    iterations      | 26       |
|    time_elapsed    | 180      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=969.50 +/- 690.27
Episode length: 37.14 +/- 5.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 970       |
| time/                   |           |
|    total_timesteps      | 53500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.46e-39 |
|    explained_variance   | -0.00222  |
|    learning_rate        | 0.001     |
|    loss                 | 8.94e+04  |
|    n_updates            | 92        |
|    policy_gradient_loss | -3.71e-09 |
|    value_loss           | 2.05e+05  |
---------------------------------------
Eval num_timesteps=54000, episode_reward=791.34 +/- 628.98
Episode length: 35.50 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=758.08 +/- 606.94
Episode length: 35.16 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=915.19 +/- 757.54
Episode length: 35.32 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 865      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 27       |
|    time_elapsed    | 187      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=976.66 +/- 691.39
Episode length: 37.18 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.2      |
|    mean_reward          | 977       |
| time/                   |           |
|    total_timesteps      | 55500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-41 |
|    explained_variance   | 0.00776   |
|    learning_rate        | 0.001     |
|    loss                 | 1.01e+04  |
|    n_updates            | 102       |
|    policy_gradient_loss | 1.15e-10  |
|    value_loss           | 2.48e+04  |
---------------------------------------
Eval num_timesteps=56000, episode_reward=885.17 +/- 709.66
Episode length: 34.78 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=940.88 +/- 714.03
Episode length: 35.84 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=821.36 +/- 689.78
Episode length: 34.70 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 28       |
|    time_elapsed    | 194      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=678.85 +/- 557.70
Episode length: 35.08 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 679       |
| time/                   |           |
|    total_timesteps      | 57500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.45e-36 |
|    explained_variance   | -0.0052   |
|    learning_rate        | 0.001     |
|    loss                 | 1.27e+05  |
|    n_updates            | 112       |
|    policy_gradient_loss | -4.68e-09 |
|    value_loss           | 2.47e+05  |
---------------------------------------
Eval num_timesteps=58000, episode_reward=927.97 +/- 724.40
Episode length: 36.14 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=835.68 +/- 672.67
Episode length: 34.96 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=806.82 +/- 685.82
Episode length: 34.98 +/- 7.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 739      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 29       |
|    time_elapsed    | 202      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=738.42 +/- 609.34
Episode length: 35.12 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 738       |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-38 |
|    explained_variance   | 0.00588   |
|    learning_rate        | 0.001     |
|    loss                 | 9.1e+03   |
|    n_updates            | 122       |
|    policy_gradient_loss | -9.97e-10 |
|    value_loss           | 2.31e+04  |
---------------------------------------
Eval num_timesteps=60000, episode_reward=780.42 +/- 742.91
Episode length: 33.84 +/- 8.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=828.40 +/- 770.31
Episode length: 33.88 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=834.83 +/- 678.09
Episode length: 35.62 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 710      |
| time/              |          |
|    fps             | 293      |
|    iterations      | 30       |
|    time_elapsed    | 209      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=668.99 +/- 580.62
Episode length: 34.32 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 669       |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-33 |
|    explained_variance   | -0.031    |
|    learning_rate        | 0.001     |
|    loss                 | 1.41e+05  |
|    n_updates            | 132       |
|    policy_gradient_loss | 5.37e-09  |
|    value_loss           | 2.8e+05   |
---------------------------------------
Eval num_timesteps=62000, episode_reward=791.67 +/- 641.87
Episode length: 35.04 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=748.12 +/- 664.66
Episode length: 33.86 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=896.63 +/- 619.58
Episode length: 36.98 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 716      |
| time/              |          |
|    fps             | 292      |
|    iterations      | 31       |
|    time_elapsed    | 216      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=880.55 +/- 660.69
Episode length: 36.48 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.06e-36 |
|    explained_variance   | 0.00646   |
|    learning_rate        | 0.001     |
|    loss                 | 8.09e+03  |
|    n_updates            | 142       |
|    policy_gradient_loss | -2.26e-10 |
|    value_loss           | 2.39e+04  |
---------------------------------------
Eval num_timesteps=64000, episode_reward=938.23 +/- 765.02
Episode length: 35.40 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=966.91 +/- 725.44
Episode length: 36.20 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=659.38 +/- 540.05
Episode length: 34.26 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=943.82 +/- 732.46
Episode length: 36.48 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 877      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 32       |
|    time_elapsed    | 225      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=799.41 +/- 660.79
Episode length: 35.96 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 799       |
| time/                   |           |
|    total_timesteps      | 66000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-32 |
|    explained_variance   | -0.00618  |
|    learning_rate        | 0.001     |
|    loss                 | 1.27e+05  |
|    n_updates            | 152       |
|    policy_gradient_loss | 1.22e-09  |
|    value_loss           | 2.48e+05  |
---------------------------------------
Eval num_timesteps=66500, episode_reward=925.94 +/- 759.35
Episode length: 35.52 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=783.05 +/- 608.86
Episode length: 35.36 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=695.13 +/- 564.20
Episode length: 34.70 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 290      |
|    iterations      | 33       |
|    time_elapsed    | 232      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=920.29 +/- 694.65
Episode length: 36.54 +/- 5.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 920       |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.67e-35 |
|    explained_variance   | 0.00868   |
|    learning_rate        | 0.001     |
|    loss                 | 8.36e+03  |
|    n_updates            | 162       |
|    policy_gradient_loss | -2.47e-11 |
|    value_loss           | 2.52e+04  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=875.34 +/- 642.79
Episode length: 37.06 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=650.54 +/- 570.75
Episode length: 33.66 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=855.04 +/- 649.61
Episode length: 35.98 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 855      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 34       |
|    time_elapsed    | 240      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=905.02 +/- 674.68
Episode length: 36.40 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 905       |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.79e-32 |
|    explained_variance   | -0.0159   |
|    learning_rate        | 0.001     |
|    loss                 | 9.94e+04  |
|    n_updates            | 172       |
|    policy_gradient_loss | -8.15e-09 |
|    value_loss           | 2.54e+05  |
---------------------------------------
Eval num_timesteps=70500, episode_reward=846.79 +/- 693.61
Episode length: 35.46 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=722.14 +/- 658.62
Episode length: 34.00 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=853.78 +/- 690.13
Episode length: 35.60 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 35       |
|    time_elapsed    | 247      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=573.42 +/- 557.80
Episode length: 32.98 +/- 6.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33        |
|    mean_reward          | 573       |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.44e-34 |
|    explained_variance   | 0.00659   |
|    learning_rate        | 0.001     |
|    loss                 | 1.59e+04  |
|    n_updates            | 182       |
|    policy_gradient_loss | -1.85e-10 |
|    value_loss           | 2.09e+04  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=733.79 +/- 623.20
Episode length: 34.40 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=884.26 +/- 727.28
Episode length: 34.98 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=802.71 +/- 712.51
Episode length: 34.32 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 36       |
|    time_elapsed    | 254      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=881.96 +/- 691.66
Episode length: 35.60 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 882       |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.9e-31  |
|    explained_variance   | -0.014    |
|    learning_rate        | 0.001     |
|    loss                 | 1.02e+05  |
|    n_updates            | 192       |
|    policy_gradient_loss | -4.01e-09 |
|    value_loss           | 2.41e+05  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=884.55 +/- 664.87
Episode length: 36.14 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=854.15 +/- 720.51
Episode length: 35.04 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=811.37 +/- 690.79
Episode length: 35.20 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 664      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 37       |
|    time_elapsed    | 261      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=707.37 +/- 621.47
Episode length: 34.12 +/- 6.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 707       |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-33 |
|    explained_variance   | 0.00846   |
|    learning_rate        | 0.001     |
|    loss                 | 1.59e+04  |
|    n_updates            | 202       |
|    policy_gradient_loss | -3.06e-10 |
|    value_loss           | 2.08e+04  |
---------------------------------------
Eval num_timesteps=76500, episode_reward=845.91 +/- 646.44
Episode length: 35.92 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=824.11 +/- 669.30
Episode length: 35.62 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=898.76 +/- 698.23
Episode length: 35.98 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 38       |
|    time_elapsed    | 268      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=708.11 +/- 625.32
Episode length: 33.94 +/- 5.88
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 33.9     |
|    mean_reward          | 708      |
| time/                   |          |
|    total_timesteps      | 78000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.5e-30 |
|    explained_variance   | -0.0123  |
|    learning_rate        | 0.001    |
|    loss                 | 9.39e+04 |
|    n_updates            | 212      |
|    policy_gradient_loss | 2.63e-10 |
|    value_loss           | 2.07e+05 |
--------------------------------------
Eval num_timesteps=78500, episode_reward=732.81 +/- 643.58
Episode length: 34.44 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=1019.19 +/- 733.73
Episode length: 36.12 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=949.68 +/- 641.13
Episode length: 36.86 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 918      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 39       |
|    time_elapsed    | 276      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=755.43 +/- 634.62
Episode length: 34.74 +/- 5.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.66e-33 |
|    explained_variance   | 0.00956   |
|    learning_rate        | 0.001     |
|    loss                 | 1.54e+04  |
|    n_updates            | 222       |
|    policy_gradient_loss | -1.75e-09 |
|    value_loss           | 2.84e+04  |
---------------------------------------
Eval num_timesteps=80500, episode_reward=960.91 +/- 684.25
Episode length: 36.76 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=751.48 +/- 625.38
Episode length: 35.52 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=988.80 +/- 726.13
Episode length: 36.54 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 903      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 40       |
|    time_elapsed    | 283      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=911.29 +/- 684.45
Episode length: 36.24 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 911       |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.64e-30 |
|    explained_variance   | 6.44e-06  |
|    learning_rate        | 0.001     |
|    loss                 | 8.28e+04  |
|    n_updates            | 232       |
|    policy_gradient_loss | 3.5e-09   |
|    value_loss           | 1.83e+05  |
---------------------------------------
Eval num_timesteps=82500, episode_reward=839.15 +/- 679.88
Episode length: 35.72 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=690.78 +/- 667.21
Episode length: 33.62 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=839.80 +/- 675.98
Episode length: 35.74 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 893      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 41       |
|    time_elapsed    | 290      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=856.72 +/- 706.29
Episode length: 35.30 +/- 7.10
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.3     |
|    mean_reward          | 857      |
| time/                   |          |
|    total_timesteps      | 84000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.7e-32 |
|    explained_variance   | 0.00968  |
|    learning_rate        | 0.001    |
|    loss                 | 9.47e+03 |
|    n_updates            | 242      |
|    policy_gradient_loss | 7.57e-10 |
|    value_loss           | 2.43e+04 |
--------------------------------------
Eval num_timesteps=84500, episode_reward=991.77 +/- 738.10
Episode length: 36.62 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 992      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=844.54 +/- 706.04
Episode length: 34.86 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=758.68 +/- 670.49
Episode length: 34.26 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=926.52 +/- 719.83
Episode length: 36.26 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 815      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 42       |
|    time_elapsed    | 299      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=764.22 +/- 640.06
Episode length: 35.26 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 764       |
| time/                   |           |
|    total_timesteps      | 86500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.75e-26 |
|    explained_variance   | 0.00972   |
|    learning_rate        | 0.001     |
|    loss                 | 9.38e+04  |
|    n_updates            | 252       |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 1.58e+05  |
---------------------------------------
Eval num_timesteps=87000, episode_reward=857.65 +/- 635.08
Episode length: 36.46 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=731.68 +/- 609.80
Episode length: 34.40 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=885.05 +/- 683.79
Episode length: 36.44 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 865      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 43       |
|    time_elapsed    | 306      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=945.49 +/- 697.68
Episode length: 36.16 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 945       |
| time/                   |           |
|    total_timesteps      | 88500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-28 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.001     |
|    loss                 | 2.56e+04  |
|    n_updates            | 262       |
|    policy_gradient_loss | 2.6e-10   |
|    value_loss           | 2.56e+04  |
---------------------------------------
Eval num_timesteps=89000, episode_reward=804.54 +/- 668.71
Episode length: 35.18 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=826.54 +/- 709.28
Episode length: 35.24 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=648.17 +/- 520.91
Episode length: 34.98 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 648      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 920      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 44       |
|    time_elapsed    | 313      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=742.55 +/- 578.38
Episode length: 35.30 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 743       |
| time/                   |           |
|    total_timesteps      | 90500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.57e-25 |
|    explained_variance   | 0.0105    |
|    learning_rate        | 0.001     |
|    loss                 | 6.21e+04  |
|    n_updates            | 272       |
|    policy_gradient_loss | -5.41e-09 |
|    value_loss           | 1.33e+05  |
---------------------------------------
Eval num_timesteps=91000, episode_reward=826.46 +/- 657.19
Episode length: 35.60 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=769.76 +/- 674.09
Episode length: 34.66 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=961.91 +/- 726.11
Episode length: 36.24 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 45       |
|    time_elapsed    | 321      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=686.96 +/- 576.75
Episode length: 34.64 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 687       |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-27 |
|    explained_variance   | 0.007     |
|    learning_rate        | 0.001     |
|    loss                 | 4.65e+03  |
|    n_updates            | 282       |
|    policy_gradient_loss | 3.38e-10  |
|    value_loss           | 2.43e+04  |
---------------------------------------
Eval num_timesteps=93000, episode_reward=834.57 +/- 682.27
Episode length: 35.66 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=814.96 +/- 685.05
Episode length: 34.90 +/- 7.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=893.78 +/- 719.37
Episode length: 35.42 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 46       |
|    time_elapsed    | 328      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=930.71 +/- 707.76
Episode length: 36.32 +/- 6.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 931       |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.33e-24 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.001     |
|    loss                 | 6.56e+04  |
|    n_updates            | 292       |
|    policy_gradient_loss | -1.86e-09 |
|    value_loss           | 1.29e+05  |
---------------------------------------
Eval num_timesteps=95000, episode_reward=713.37 +/- 601.76
Episode length: 34.58 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=1008.46 +/- 735.43
Episode length: 37.12 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=942.16 +/- 710.99
Episode length: 36.26 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 47       |
|    time_elapsed    | 335      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=721.17 +/- 679.65
Episode length: 33.58 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 721       |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-26 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.001     |
|    loss                 | 1.35e+04  |
|    n_updates            | 302       |
|    policy_gradient_loss | -1.34e-10 |
|    value_loss           | 2.24e+04  |
---------------------------------------
Eval num_timesteps=97000, episode_reward=880.33 +/- 666.00
Episode length: 36.04 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=847.51 +/- 731.84
Episode length: 34.54 +/- 7.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=1024.73 +/- 730.01
Episode length: 36.92 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 48       |
|    time_elapsed    | 343      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=691.67 +/- 584.18
Episode length: 33.76 +/- 7.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 692       |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.72e-23 |
|    explained_variance   | 0.009     |
|    learning_rate        | 0.001     |
|    loss                 | 6.19e+04  |
|    n_updates            | 312       |
|    policy_gradient_loss | -3.36e-09 |
|    value_loss           | 1.1e+05   |
---------------------------------------
Eval num_timesteps=99000, episode_reward=868.85 +/- 718.21
Episode length: 34.78 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=638.23 +/- 549.92
Episode length: 34.14 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 638      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=813.56 +/- 644.60
Episode length: 35.70 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 49       |
|    time_elapsed    | 350      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=750.60 +/- 675.66
Episode length: 33.64 +/- 6.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 751       |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.82e-26 |
|    explained_variance   | 0.00938   |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+04  |
|    n_updates            | 322       |
|    policy_gradient_loss | -7.8e-10  |
|    value_loss           | 2.28e+04  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=903.69 +/- 759.66
Episode length: 34.68 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=852.83 +/- 721.78
Episode length: 34.94 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=778.67 +/- 643.81
Episode length: 35.14 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 50       |
|    time_elapsed    | 357      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=757.47 +/- 616.53
Episode length: 35.16 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 757       |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.06e-23 |
|    explained_variance   | 0.00309   |
|    learning_rate        | 0.001     |
|    loss                 | 4.46e+04  |
|    n_updates            | 332       |
|    policy_gradient_loss | 1.49e-09  |
|    value_loss           | 1.09e+05  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=944.07 +/- 715.56
Episode length: 36.68 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=599.37 +/- 485.96
Episode length: 34.14 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 599      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=704.09 +/- 661.73
Episode length: 34.02 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 51       |
|    time_elapsed    | 364      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=694.33 +/- 622.13
Episode length: 34.10 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 694       |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.55e-26 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.001     |
|    loss                 | 1.12e+04  |
|    n_updates            | 342       |
|    policy_gradient_loss | 7.92e-10  |
|    value_loss           | 2.45e+04  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=900.08 +/- 746.17
Episode length: 35.16 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=776.32 +/- 643.38
Episode length: 34.88 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=670.36 +/- 608.29
Episode length: 33.76 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 851      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 52       |
|    time_elapsed    | 371      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=742.19 +/- 611.95
Episode length: 34.76 +/- 6.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.8     |
|    mean_reward          | 742      |
| time/                   |          |
|    total_timesteps      | 106500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.9e-22 |
|    explained_variance   | 0.00413  |
|    learning_rate        | 0.001    |
|    loss                 | 4.69e+04 |
|    n_updates            | 352      |
|    policy_gradient_loss | 1.29e-09 |
|    value_loss           | 9.13e+04 |
--------------------------------------
Eval num_timesteps=107000, episode_reward=842.37 +/- 725.56
Episode length: 35.02 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=663.17 +/- 563.71
Episode length: 34.60 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=684.04 +/- 577.31
Episode length: 34.46 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=851.79 +/- 668.38
Episode length: 35.80 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 798      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 53       |
|    time_elapsed    | 380      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=895.33 +/- 656.97
Episode length: 36.86 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 895       |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-24 |
|    explained_variance   | 0.00933   |
|    learning_rate        | 0.001     |
|    loss                 | 1.41e+04  |
|    n_updates            | 362       |
|    policy_gradient_loss | 1.49e-09  |
|    value_loss           | 2.41e+04  |
---------------------------------------
Eval num_timesteps=109500, episode_reward=902.26 +/- 717.31
Episode length: 35.32 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=918.58 +/- 756.44
Episode length: 35.62 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=889.10 +/- 745.64
Episode length: 35.12 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 874      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 54       |
|    time_elapsed    | 387      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=668.08 +/- 526.82
Episode length: 34.84 +/- 5.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 668       |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.84e-21 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.001     |
|    loss                 | 4.6e+04   |
|    n_updates            | 372       |
|    policy_gradient_loss | 7.69e-09  |
|    value_loss           | 7.81e+04  |
---------------------------------------
Eval num_timesteps=111500, episode_reward=854.12 +/- 713.30
Episode length: 34.78 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=766.42 +/- 669.04
Episode length: 34.50 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=831.49 +/- 712.65
Episode length: 34.74 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 55       |
|    time_elapsed    | 394      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=878.01 +/- 712.68
Episode length: 35.34 +/- 7.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 878       |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.74e-23 |
|    explained_variance   | 0.00867   |
|    learning_rate        | 0.001     |
|    loss                 | 1.39e+04  |
|    n_updates            | 382       |
|    policy_gradient_loss | -7.12e-10 |
|    value_loss           | 2.67e+04  |
---------------------------------------
Eval num_timesteps=113500, episode_reward=754.75 +/- 651.85
Episode length: 34.92 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=592.72 +/- 528.94
Episode length: 33.50 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=817.13 +/- 654.53
Episode length: 35.42 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 56       |
|    time_elapsed    | 401      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=717.59 +/- 637.54
Episode length: 34.26 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 718       |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.89e-19 |
|    explained_variance   | 0.0138    |
|    learning_rate        | 0.001     |
|    loss                 | 3.66e+04  |
|    n_updates            | 392       |
|    policy_gradient_loss | 3.43e-10  |
|    value_loss           | 6.83e+04  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=779.38 +/- 662.43
Episode length: 35.00 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=804.78 +/- 653.34
Episode length: 34.18 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=716.80 +/- 627.61
Episode length: 34.24 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 57       |
|    time_elapsed    | 408      |
|    total_timesteps | 116736   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.12
Eval num_timesteps=117000, episode_reward=859.18 +/- 685.55
Episode length: 35.60 +/- 6.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 859         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.019853909 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0227     |
|    explained_variance   | 0.00636     |
|    learning_rate        | 0.001       |
|    loss                 | 8.34e+03    |
|    n_updates            | 394         |
|    policy_gradient_loss | 0.000879    |
|    value_loss           | 2.21e+04    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=994.99 +/- 711.30
Episode length: 36.68 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=898.13 +/- 706.08
Episode length: 35.56 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=706.74 +/- 524.98
Episode length: 35.70 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 65.2     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 58       |
|    time_elapsed    | 415      |
|    total_timesteps | 118784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.90
Eval num_timesteps=119000, episode_reward=-526.34 +/- 69.32
Episode length: 46.26 +/- 14.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.3      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.4522726 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.62     |
|    explained_variance   | -0.00348  |
|    learning_rate        | 0.001     |
|    loss                 | 4.06e+03  |
|    n_updates            | 395       |
|    policy_gradient_loss | 0.199     |
|    value_loss           | 8.56e+03  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=-510.14 +/- 62.94
Episode length: 49.16 +/- 15.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-499.95 +/- 84.44
Episode length: 47.18 +/- 15.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-498.74 +/- 82.84
Episode length: 44.30 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.3     |
|    ep_rew_mean     | -347     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 59       |
|    time_elapsed    | 423      |
|    total_timesteps | 120832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=121000, episode_reward=-522.15 +/- 70.03
Episode length: 50.74 +/- 19.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | -522        |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.024942648 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.448      |
|    explained_variance   | -0.00316    |
|    learning_rate        | 0.001       |
|    loss                 | 3.35e+03    |
|    n_updates            | 396         |
|    policy_gradient_loss | 0.0162      |
|    value_loss           | 6.49e+03    |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=-538.34 +/- 56.87
Episode length: 48.64 +/- 14.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-497.89 +/- 87.40
Episode length: 47.30 +/- 18.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -498     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-505.34 +/- 82.60
Episode length: 52.36 +/- 19.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -487     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 60       |
|    time_elapsed    | 431      |
|    total_timesteps | 122880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=123000, episode_reward=-526.35 +/- 69.32
Episode length: 52.34 +/- 16.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.3        |
|    mean_reward          | -526        |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.031063296 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.548      |
|    explained_variance   | -0.0016     |
|    learning_rate        | 0.001       |
|    loss                 | 2.5e+03     |
|    n_updates            | 397         |
|    policy_gradient_loss | 0.0287      |
|    value_loss           | 5.76e+03    |
-----------------------------------------
Eval num_timesteps=123500, episode_reward=-532.34 +/- 61.09
Episode length: 48.48 +/- 15.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-511.94 +/- 87.99
Episode length: 51.40 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-540.74 +/- 54.79
Episode length: 48.38 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 283      |
|    iterations      | 61       |
|    time_elapsed    | 440      |
|    total_timesteps | 124928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=125000, episode_reward=-529.96 +/- 61.78
Episode length: 51.84 +/- 14.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.8        |
|    mean_reward          | -530        |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.035887927 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.941      |
|    explained_variance   | -0.000413   |
|    learning_rate        | 0.001       |
|    loss                 | 2.64e+03    |
|    n_updates            | 398         |
|    policy_gradient_loss | 0.0123      |
|    value_loss           | 6.79e+03    |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=-537.14 +/- 48.73
Episode length: 53.60 +/- 18.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-517.35 +/- 72.97
Episode length: 54.60 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-529.95 +/- 68.94
Episode length: 51.30 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.9     |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 62       |
|    time_elapsed    | 449      |
|    total_timesteps | 126976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=127000, episode_reward=-525.74 +/- 69.61
Episode length: 48.62 +/- 18.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.6       |
|    mean_reward          | -526       |
| time/                   |            |
|    total_timesteps      | 127000     |
| train/                  |            |
|    approx_kl            | 0.07077307 |
|    clip_fraction        | 0.414      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.47      |
|    explained_variance   | 4.61e-05   |
|    learning_rate        | 0.001      |
|    loss                 | 2.87e+03   |
|    n_updates            | 399        |
|    policy_gradient_loss | 0.0341     |
|    value_loss           | 4.43e+03   |
----------------------------------------
Eval num_timesteps=127500, episode_reward=-510.15 +/- 75.90
Episode length: 52.34 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-526.35 +/- 61.61
Episode length: 54.36 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-525.14 +/- 61.98
Episode length: 46.48 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-519.15 +/- 67.39
Episode length: 52.04 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.4     |
|    ep_rew_mean     | -483     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 63       |
|    time_elapsed    | 459      |
|    total_timesteps | 129024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=129500, episode_reward=-534.14 +/- 52.46
Episode length: 50.06 +/- 14.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.1        |
|    mean_reward          | -534        |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.018582132 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.81       |
|    explained_variance   | 8.25e-05    |
|    learning_rate        | 0.001       |
|    loss                 | 2.35e+03    |
|    n_updates            | 400         |
|    policy_gradient_loss | 0.00571     |
|    value_loss           | 3.39e+03    |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=-531.14 +/- 57.36
Episode length: 51.90 +/- 19.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-520.94 +/- 78.75
Episode length: 46.66 +/- 12.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-518.55 +/- 77.57
Episode length: 52.64 +/- 13.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | -450     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 64       |
|    time_elapsed    | 467      |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=131500, episode_reward=-521.55 +/- 61.85
Episode length: 50.28 +/- 16.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.3        |
|    mean_reward          | -522        |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.012182733 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.000101    |
|    learning_rate        | 0.001       |
|    loss                 | 1.09e+03    |
|    n_updates            | 401         |
|    policy_gradient_loss | 0.0244      |
|    value_loss           | 2.05e+03    |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=-520.95 +/- 73.54
Episode length: 50.62 +/- 17.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-526.36 +/- 47.41
Episode length: 55.24 +/- 22.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-511.33 +/- 81.77
Episode length: 48.52 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -367     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 65       |
|    time_elapsed    | 476      |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=133500, episode_reward=818.30 +/- 720.01
Episode length: 34.28 +/- 7.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | 818         |
| time/                   |             |
|    total_timesteps      | 133500      |
| train/                  |             |
|    approx_kl            | 0.037081856 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.000171   |
|    learning_rate        | 0.001       |
|    loss                 | 2.5e+03     |
|    n_updates            | 402         |
|    policy_gradient_loss | 0.00692     |
|    value_loss           | 4.72e+03    |
-----------------------------------------
Eval num_timesteps=134000, episode_reward=873.83 +/- 666.80
Episode length: 35.72 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=765.35 +/- 633.53
Episode length: 34.68 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=895.53 +/- 690.77
Episode length: 35.78 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.9     |
|    ep_rew_mean     | -311     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 66       |
|    time_elapsed    | 482      |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.41
Eval num_timesteps=135500, episode_reward=755.59 +/- 665.41
Episode length: 34.66 +/- 6.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.7       |
|    mean_reward          | 756        |
| time/                   |            |
|    total_timesteps      | 135500     |
| train/                  |            |
|    approx_kl            | 0.20348126 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | -3.34e-06  |
|    learning_rate        | 0.001      |
|    loss                 | 1.89e+03   |
|    n_updates            | 403        |
|    policy_gradient_loss | 0.0817     |
|    value_loss           | 4.29e+03   |
----------------------------------------
Eval num_timesteps=136000, episode_reward=879.65 +/- 699.36
Episode length: 35.64 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=765.84 +/- 645.92
Episode length: 35.36 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=850.87 +/- 702.62
Episode length: 35.50 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 38.7     |
|    ep_rew_mean     | -74.9    |
| time/              |          |
|    fps             | 280      |
|    iterations      | 67       |
|    time_elapsed    | 489      |
|    total_timesteps | 137216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.08
Eval num_timesteps=137500, episode_reward=843.44 +/- 664.18
Episode length: 35.92 +/- 6.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | 843         |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.041762907 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.3         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.673      |
|    explained_variance   | 0.002       |
|    learning_rate        | 0.001       |
|    loss                 | 3.52e+03    |
|    n_updates            | 404         |
|    policy_gradient_loss | -0.00226    |
|    value_loss           | 7.04e+03    |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=691.30 +/- 586.74
Episode length: 34.30 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=888.80 +/- 733.98
Episode length: 35.72 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=678.95 +/- 619.84
Episode length: 33.88 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33       |
|    ep_rew_mean     | 206      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 68       |
|    time_elapsed    | 496      |
|    total_timesteps | 139264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.23
Eval num_timesteps=139500, episode_reward=-522.75 +/- 74.01
Episode length: 48.82 +/- 15.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.8       |
|    mean_reward          | -523       |
| time/                   |            |
|    total_timesteps      | 139500     |
| train/                  |            |
|    approx_kl            | 0.41536656 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.63      |
|    explained_variance   | 0.00102    |
|    learning_rate        | 0.001      |
|    loss                 | 6.51e+03   |
|    n_updates            | 405        |
|    policy_gradient_loss | 0.131      |
|    value_loss           | 1.47e+04   |
----------------------------------------
Eval num_timesteps=140000, episode_reward=-530.56 +/- 52.21
Episode length: 51.42 +/- 13.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-526.95 +/- 75.96
Episode length: 49.14 +/- 18.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-514.94 +/- 66.34
Episode length: 52.98 +/- 22.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.3     |
|    ep_rew_mean     | -102     |
| time/              |          |
|    fps             | 280      |
|    iterations      | 69       |
|    time_elapsed    | 504      |
|    total_timesteps | 141312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.45
Eval num_timesteps=141500, episode_reward=-516.02 +/- 89.68
Episode length: 45.32 +/- 15.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.3       |
|    mean_reward          | -516       |
| time/                   |            |
|    total_timesteps      | 141500     |
| train/                  |            |
|    approx_kl            | 0.22634183 |
|    clip_fraction        | 0.0703     |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.846     |
|    explained_variance   | -0.000319  |
|    learning_rate        | 0.001      |
|    loss                 | 3.76e+03   |
|    n_updates            | 406        |
|    policy_gradient_loss | 0.0189     |
|    value_loss           | 5.51e+03   |
----------------------------------------
Eval num_timesteps=142000, episode_reward=-513.15 +/- 73.14
Episode length: 50.52 +/- 19.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-543.74 +/- 60.55
Episode length: 49.40 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -544     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-527.55 +/- 55.34
Episode length: 50.18 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 279      |
|    iterations      | 70       |
|    time_elapsed    | 513      |
|    total_timesteps | 143360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 1.97
Eval num_timesteps=143500, episode_reward=-527.59 +/- 67.89
Episode length: 47.28 +/- 15.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.3       |
|    mean_reward          | -528       |
| time/                   |            |
|    total_timesteps      | 143500     |
| train/                  |            |
|    approx_kl            | 0.66146916 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.451     |
|    explained_variance   | -0.000632  |
|    learning_rate        | 0.001      |
|    loss                 | 5.65e+03   |
|    n_updates            | 407        |
|    policy_gradient_loss | 0.133      |
|    value_loss           | 7.35e+03   |
----------------------------------------
Eval num_timesteps=144000, episode_reward=-536.58 +/- 62.21
Episode length: 50.40 +/- 18.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-530.46 +/- 69.24
Episode length: 53.62 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-508.99 +/- 72.56
Episode length: 51.64 +/- 18.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.1     |
|    ep_rew_mean     | -449     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 71       |
|    time_elapsed    | 521      |
|    total_timesteps | 145408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.86
Eval num_timesteps=145500, episode_reward=-524.52 +/- 79.19
Episode length: 49.36 +/- 16.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 49.4       |
|    mean_reward          | -525       |
| time/                   |            |
|    total_timesteps      | 145500     |
| train/                  |            |
|    approx_kl            | 0.21596333 |
|    clip_fraction        | 0.00781    |
|    clip_range           | 0.3        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0254    |
|    explained_variance   | -0.000432  |
|    learning_rate        | 0.001      |
|    loss                 | 1.11e+03   |
|    n_updates            | 408        |
|    policy_gradient_loss | -0.00129   |
|    value_loss           | 3.05e+03   |
----------------------------------------
Eval num_timesteps=146000, episode_reward=-520.89 +/- 70.45
Episode length: 52.10 +/- 15.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-513.73 +/- 70.40
Episode length: 46.06 +/- 16.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-517.39 +/- 70.72
Episode length: 54.98 +/- 20.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.4     |
|    ep_rew_mean     | -477     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 72       |
|    time_elapsed    | 530      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-513.19 +/- 52.51
Episode length: 49.82 +/- 16.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -513      |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-15 |
|    explained_variance   | -0.00821  |
|    learning_rate        | 0.001     |
|    loss                 | 3.16e+03  |
|    n_updates            | 418       |
|    policy_gradient_loss | 2.11e-09  |
|    value_loss           | 4.68e+03  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=-530.59 +/- 61.68
Episode length: 48.10 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-522.74 +/- 48.07
Episode length: 50.20 +/- 18.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-534.76 +/- 58.80
Episode length: 48.72 +/- 19.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-512.58 +/- 76.43
Episode length: 47.26 +/- 14.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.5     |
|    ep_rew_mean     | -503     |
| time/              |          |
|    fps             | 276      |
|    iterations      | 73       |
|    time_elapsed    | 541      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-526.39 +/- 65.85
Episode length: 54.74 +/- 13.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.7      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.27e-30 |
|    explained_variance   | -0.0188   |
|    learning_rate        | 0.001     |
|    loss                 | 1.34e+04  |
|    n_updates            | 428       |
|    policy_gradient_loss | 1.2e-09   |
|    value_loss           | 2.64e+04  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=-532.96 +/- 67.66
Episode length: 51.98 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-529.35 +/- 47.40
Episode length: 52.02 +/- 19.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-542.59 +/- 54.76
Episode length: 53.52 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 275      |
|    iterations      | 74       |
|    time_elapsed    | 550      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-501.78 +/- 96.44
Episode length: 47.74 +/- 16.57
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47.7     |
|    mean_reward          | -502     |
| time/                   |          |
|    total_timesteps      | 152000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.6e-08 |
|    explained_variance   | -0.00397 |
|    learning_rate        | 0.001    |
|    loss                 | 1.16e+03 |
|    n_updates            | 438      |
|    policy_gradient_loss | 6.17e-10 |
|    value_loss           | 3.13e+03 |
--------------------------------------
Eval num_timesteps=152500, episode_reward=-538.37 +/- 57.19
Episode length: 48.78 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-533.59 +/- 71.22
Episode length: 51.82 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-532.98 +/- 66.89
Episode length: 51.16 +/- 16.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.8     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 75       |
|    time_elapsed    | 559      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-502.38 +/- 74.62
Episode length: 52.96 +/- 19.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53        |
|    mean_reward          | -502      |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.46e-11 |
|    explained_variance   | -0.000949 |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+03  |
|    n_updates            | 448       |
|    policy_gradient_loss | -3.52e-09 |
|    value_loss           | 4.64e+03  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=-511.35 +/- 82.71
Episode length: 52.24 +/- 19.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-532.99 +/- 57.64
Episode length: 50.16 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-527.58 +/- 64.35
Episode length: 49.22 +/- 15.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 76       |
|    time_elapsed    | 569      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-528.79 +/- 74.83
Episode length: 49.82 +/- 16.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -529      |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.88e-08 |
|    explained_variance   | -0.00562  |
|    learning_rate        | 0.001     |
|    loss                 | 1.71e+03  |
|    n_updates            | 458       |
|    policy_gradient_loss | 9.63e-10  |
|    value_loss           | 3.62e+03  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=-508.94 +/- 74.77
Episode length: 49.78 +/- 19.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-532.97 +/- 66.08
Episode length: 52.52 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-520.95 +/- 70.81
Episode length: 50.38 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 77       |
|    time_elapsed    | 578      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-538.95 +/- 61.61
Episode length: 52.22 +/- 15.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -539      |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-10 |
|    explained_variance   | -0.000894 |
|    learning_rate        | 0.001     |
|    loss                 | 4.06e+03  |
|    n_updates            | 468       |
|    policy_gradient_loss | -1.39e-09 |
|    value_loss           | 4.32e+03  |
---------------------------------------
Eval num_timesteps=158500, episode_reward=-532.32 +/- 67.51
Episode length: 54.12 +/- 15.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-538.96 +/- 54.40
Episode length: 53.54 +/- 20.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-518.54 +/- 60.31
Episode length: 49.32 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 271      |
|    iterations      | 78       |
|    time_elapsed    | 588      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-510.78 +/- 66.92
Episode length: 50.90 +/- 15.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.9      |
|    mean_reward          | -511      |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.07e-08 |
|    explained_variance   | -0.00264  |
|    learning_rate        | 0.001     |
|    loss                 | 1.67e+03  |
|    n_updates            | 478       |
|    policy_gradient_loss | 6.2e-10   |
|    value_loss           | 3.5e+03   |
---------------------------------------
Eval num_timesteps=160500, episode_reward=-523.27 +/- 71.04
Episode length: 47.48 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-518.58 +/- 66.58
Episode length: 51.20 +/- 19.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-515.56 +/- 80.43
Episode length: 49.42 +/- 12.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 270      |
|    iterations      | 79       |
|    time_elapsed    | 597      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-524.51 +/- 63.44
Episode length: 54.40 +/- 19.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.4      |
|    mean_reward          | -525      |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.49e-10 |
|    explained_variance   | -0.00121  |
|    learning_rate        | 0.001     |
|    loss                 | 2.49e+03  |
|    n_updates            | 488       |
|    policy_gradient_loss | 1.19e-09  |
|    value_loss           | 4.13e+03  |
---------------------------------------
Eval num_timesteps=162500, episode_reward=-532.95 +/- 63.82
Episode length: 49.28 +/- 15.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-506.55 +/- 85.79
Episode length: 51.04 +/- 20.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-534.71 +/- 51.62
Episode length: 53.54 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 269      |
|    iterations      | 80       |
|    time_elapsed    | 607      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-528.79 +/- 68.03
Episode length: 50.34 +/- 19.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.3      |
|    mean_reward          | -529      |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.63e-08 |
|    explained_variance   | -0.00597  |
|    learning_rate        | 0.001     |
|    loss                 | 1.45e+03  |
|    n_updates            | 498       |
|    policy_gradient_loss | 5.91e-10  |
|    value_loss           | 3.23e+03  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=-535.95 +/- 67.07
Episode length: 46.86 +/- 14.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-512.59 +/- 72.58
Episode length: 47.14 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-523.99 +/- 62.65
Episode length: 50.84 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 269      |
|    iterations      | 81       |
|    time_elapsed    | 616      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-507.76 +/- 78.76
Episode length: 52.76 +/- 18.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -508      |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.07e-10 |
|    explained_variance   | -0.000681 |
|    learning_rate        | 0.001     |
|    loss                 | 1.93e+03  |
|    n_updates            | 508       |
|    policy_gradient_loss | -1.57e-09 |
|    value_loss           | 3.82e+03  |
---------------------------------------
Eval num_timesteps=166500, episode_reward=-515.32 +/- 93.72
Episode length: 50.96 +/- 15.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-529.35 +/- 68.49
Episode length: 46.70 +/- 14.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-510.74 +/- 80.21
Episode length: 48.28 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 268      |
|    iterations      | 82       |
|    time_elapsed    | 625      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-513.17 +/- 84.56
Episode length: 48.48 +/- 18.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -513      |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.49e-08 |
|    explained_variance   | -0.00315  |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+03  |
|    n_updates            | 518       |
|    policy_gradient_loss | -8.45e-10 |
|    value_loss           | 3.25e+03  |
---------------------------------------
Eval num_timesteps=168500, episode_reward=-510.76 +/- 75.38
Episode length: 50.22 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-514.39 +/- 66.82
Episode length: 54.56 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-535.99 +/- 56.93
Episode length: 54.70 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 267      |
|    iterations      | 83       |
|    time_elapsed    | 634      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-530.58 +/- 66.73
Episode length: 50.28 +/- 14.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.3      |
|    mean_reward          | -531      |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-10 |
|    explained_variance   | -0.000855 |
|    learning_rate        | 0.001     |
|    loss                 | 2.19e+03  |
|    n_updates            | 528       |
|    policy_gradient_loss | -1.81e-09 |
|    value_loss           | 3.76e+03  |
---------------------------------------
Eval num_timesteps=170500, episode_reward=-519.19 +/- 69.25
Episode length: 51.92 +/- 18.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-539.55 +/- 58.03
Episode length: 55.12 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | -540     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-514.39 +/- 76.62
Episode length: 47.38 +/- 13.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-514.94 +/- 76.69
Episode length: 46.98 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 266      |
|    iterations      | 84       |
|    time_elapsed    | 646      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-526.34 +/- 67.26
Episode length: 50.02 +/- 15.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.48e-08 |
|    explained_variance   | -0.00431  |
|    learning_rate        | 0.001     |
|    loss                 | 1.1e+03   |
|    n_updates            | 538       |
|    policy_gradient_loss | 1.26e-09  |
|    value_loss           | 3.34e+03  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=-524.58 +/- 64.59
Episode length: 47.08 +/- 12.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-540.14 +/- 53.34
Episode length: 47.34 +/- 15.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -540     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-523.39 +/- 67.38
Episode length: 53.04 +/- 15.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 265      |
|    iterations      | 85       |
|    time_elapsed    | 655      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-518.59 +/- 61.22
Episode length: 49.52 +/- 20.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -519      |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.06e-10 |
|    explained_variance   | -0.000714 |
|    learning_rate        | 0.001     |
|    loss                 | 2.9e+03   |
|    n_updates            | 548       |
|    policy_gradient_loss | 1.49e-09  |
|    value_loss           | 4.9e+03   |
---------------------------------------
Eval num_timesteps=175000, episode_reward=-516.79 +/- 66.48
Episode length: 51.00 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-526.99 +/- 67.69
Episode length: 50.06 +/- 14.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-493.30 +/- 91.79
Episode length: 48.44 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -493     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.4     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 265      |
|    iterations      | 86       |
|    time_elapsed    | 664      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-513.18 +/- 68.30
Episode length: 49.36 +/- 16.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.4      |
|    mean_reward          | -513      |
| time/                   |           |
|    total_timesteps      | 176500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.29e-08 |
|    explained_variance   | -0.00661  |
|    learning_rate        | 0.001     |
|    loss                 | 1.18e+03  |
|    n_updates            | 558       |
|    policy_gradient_loss | 2.04e-11  |
|    value_loss           | 3.54e+03  |
---------------------------------------
Eval num_timesteps=177000, episode_reward=-525.15 +/- 72.74
Episode length: 48.02 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-520.35 +/- 75.12
Episode length: 46.64 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-527.59 +/- 62.94
Episode length: 51.38 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.6     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 264      |
|    iterations      | 87       |
|    time_elapsed    | 673      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-513.19 +/- 69.60
Episode length: 47.08 +/- 16.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.1      |
|    mean_reward          | -513      |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-10 |
|    explained_variance   | 0.000247  |
|    learning_rate        | 0.001     |
|    loss                 | 2.69e+03  |
|    n_updates            | 568       |
|    policy_gradient_loss | 6.26e-11  |
|    value_loss           | 4.08e+03  |
---------------------------------------
Eval num_timesteps=179000, episode_reward=-520.99 +/- 68.74
Episode length: 48.82 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-528.15 +/- 64.34
Episode length: 47.66 +/- 14.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-520.99 +/- 72.32
Episode length: 54.10 +/- 21.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 263      |
|    iterations      | 88       |
|    time_elapsed    | 682      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-540.75 +/- 51.01
Episode length: 49.96 +/- 15.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50        |
|    mean_reward          | -541      |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.18e-08 |
|    explained_variance   | -0.00479  |
|    learning_rate        | 0.001     |
|    loss                 | 1.72e+03  |
|    n_updates            | 578       |
|    policy_gradient_loss | -5.38e-10 |
|    value_loss           | 3.1e+03   |
---------------------------------------
Eval num_timesteps=181000, episode_reward=-522.15 +/- 76.69
Episode length: 50.10 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-528.70 +/- 59.26
Episode length: 51.10 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-549.75 +/- 58.23
Episode length: 50.30 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 263      |
|    iterations      | 89       |
|    time_elapsed    | 692      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-524.52 +/- 64.01
Episode length: 46.62 +/- 17.65
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 46.6     |
|    mean_reward          | -525     |
| time/                   |          |
|    total_timesteps      | 182500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.2e-10 |
|    explained_variance   | 0.000122 |
|    learning_rate        | 0.001    |
|    loss                 | 3.15e+03 |
|    n_updates            | 588      |
|    policy_gradient_loss | 2.75e-09 |
|    value_loss           | 3.97e+03 |
--------------------------------------
Eval num_timesteps=183000, episode_reward=-513.60 +/- 78.55
Episode length: 48.80 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-520.95 +/- 73.54
Episode length: 52.32 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-532.99 +/- 64.69
Episode length: 52.74 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.9     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 262      |
|    iterations      | 90       |
|    time_elapsed    | 701      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-507.78 +/- 71.36
Episode length: 47.86 +/- 19.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.9      |
|    mean_reward          | -508      |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.24e-08 |
|    explained_variance   | -0.00505  |
|    learning_rate        | 0.001     |
|    loss                 | 1.02e+03  |
|    n_updates            | 598       |
|    policy_gradient_loss | -4.77e-10 |
|    value_loss           | 2.93e+03  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=-529.36 +/- 55.42
Episode length: 46.72 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.7     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-520.99 +/- 57.63
Episode length: 52.18 +/- 13.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-532.37 +/- 65.64
Episode length: 47.96 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 262      |
|    iterations      | 91       |
|    time_elapsed    | 710      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-525.17 +/- 63.15
Episode length: 49.26 +/- 15.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.3      |
|    mean_reward          | -525      |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-10 |
|    explained_variance   | -0.000387 |
|    learning_rate        | 0.001     |
|    loss                 | 2.25e+03  |
|    n_updates            | 608       |
|    policy_gradient_loss | -4.6e-10  |
|    value_loss           | 4.14e+03  |
---------------------------------------
Eval num_timesteps=187000, episode_reward=-526.39 +/- 68.52
Episode length: 52.18 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-519.79 +/- 72.80
Episode length: 52.24 +/- 18.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-517.99 +/- 66.83
Episode length: 53.20 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.9     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 261      |
|    iterations      | 92       |
|    time_elapsed    | 720      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-523.39 +/- 65.20
Episode length: 51.66 +/- 20.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.22e-08 |
|    explained_variance   | -0.00505  |
|    learning_rate        | 0.001     |
|    loss                 | 1.32e+03  |
|    n_updates            | 618       |
|    policy_gradient_loss | -1.67e-09 |
|    value_loss           | 3.05e+03  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=-531.11 +/- 69.87
Episode length: 52.28 +/- 17.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-517.24 +/- 67.18
Episode length: 47.04 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-517.92 +/- 74.42
Episode length: 50.32 +/- 18.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 261      |
|    iterations      | 93       |
|    time_elapsed    | 729      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-517.37 +/- 76.81
Episode length: 46.44 +/- 15.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.4      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 190500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.21e-11 |
|    explained_variance   | -0.000547 |
|    learning_rate        | 0.001     |
|    loss                 | 2.82e+03  |
|    n_updates            | 628       |
|    policy_gradient_loss | 3.13e-09  |
|    value_loss           | 3.87e+03  |
---------------------------------------
Eval num_timesteps=191000, episode_reward=-511.36 +/- 54.89
Episode length: 48.32 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-541.35 +/- 59.14
Episode length: 54.72 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-535.99 +/- 62.92
Episode length: 50.90 +/- 13.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-513.76 +/- 80.50
Episode length: 49.48 +/- 16.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 259      |
|    iterations      | 94       |
|    time_elapsed    | 740      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-528.19 +/- 65.68
Episode length: 54.16 +/- 19.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.2      |
|    mean_reward          | -528      |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.46e-08 |
|    explained_variance   | -0.00208  |
|    learning_rate        | 0.001     |
|    loss                 | 2.34e+03  |
|    n_updates            | 638       |
|    policy_gradient_loss | -1.16e-10 |
|    value_loss           | 3.01e+03  |
---------------------------------------
Eval num_timesteps=193500, episode_reward=-525.18 +/- 66.48
Episode length: 48.94 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-531.72 +/- 58.51
Episode length: 46.50 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-504.19 +/- 79.02
Episode length: 51.60 +/- 16.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.8     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 259      |
|    iterations      | 95       |
|    time_elapsed    | 749      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-528.15 +/- 61.70
Episode length: 51.46 +/- 17.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -528      |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.37e-10 |
|    explained_variance   | 2.29e-05  |
|    learning_rate        | 0.001     |
|    loss                 | 694       |
|    n_updates            | 648       |
|    policy_gradient_loss | 9.04e-10  |
|    value_loss           | 3.38e+03  |
---------------------------------------
Eval num_timesteps=195500, episode_reward=-505.95 +/- 61.44
Episode length: 47.92 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-519.17 +/- 65.84
Episode length: 53.12 +/- 20.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-511.95 +/- 64.93
Episode length: 50.20 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 258      |
|    iterations      | 96       |
|    time_elapsed    | 759      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-522.19 +/- 71.81
Episode length: 53.72 +/- 15.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.7      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.58e-08 |
|    explained_variance   | -0.00365  |
|    learning_rate        | 0.001     |
|    loss                 | 1.99e+03  |
|    n_updates            | 658       |
|    policy_gradient_loss | -3.41e-10 |
|    value_loss           | 3.13e+03  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=-514.91 +/- 81.93
Episode length: 48.84 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-524.55 +/- 72.99
Episode length: 50.16 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-516.71 +/- 71.28
Episode length: 49.08 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 258      |
|    iterations      | 97       |
|    time_elapsed    | 768      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-530.50 +/- 68.61
Episode length: 46.98 +/- 17.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47        |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.17e-11 |
|    explained_variance   | -8.49e-05 |
|    learning_rate        | 0.001     |
|    loss                 | 1.74e+03  |
|    n_updates            | 668       |
|    policy_gradient_loss | 2.76e-10  |
|    value_loss           | 3.74e+03  |
---------------------------------------
Eval num_timesteps=199500, episode_reward=-517.98 +/- 73.97
Episode length: 51.24 +/- 17.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-525.10 +/- 64.83
Episode length: 51.62 +/- 17.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-519.70 +/- 77.87
Episode length: 51.20 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.3     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 258      |
|    iterations      | 98       |
|    time_elapsed    | 777      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-525.75 +/- 64.55
Episode length: 53.52 +/- 16.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.5      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.92e-08 |
|    explained_variance   | -0.00271  |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+03  |
|    n_updates            | 678       |
|    policy_gradient_loss | 7.51e-10  |
|    value_loss           | 3.1e+03   |
---------------------------------------
Eval num_timesteps=201500, episode_reward=-514.39 +/- 66.28
Episode length: 52.22 +/- 15.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-507.19 +/- 77.29
Episode length: 53.34 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-525.72 +/- 57.55
Episode length: 47.12 +/- 13.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.9     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 257      |
|    iterations      | 99       |
|    time_elapsed    | 787      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-533.55 +/- 74.57
Episode length: 48.86 +/- 15.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.9      |
|    mean_reward          | -534      |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.59e-10 |
|    explained_variance   | -0.000305 |
|    learning_rate        | 0.001     |
|    loss                 | 539       |
|    n_updates            | 688       |
|    policy_gradient_loss | -1.89e-09 |
|    value_loss           | 3.94e+03  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=-516.77 +/- 63.17
Episode length: 53.28 +/- 20.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-514.27 +/- 73.79
Episode length: 52.34 +/- 18.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-528.79 +/- 70.63
Episode length: 50.64 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 256      |
|    iterations      | 100      |
|    time_elapsed    | 796      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-529.35 +/- 70.43
Episode length: 49.62 +/- 15.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.6      |
|    mean_reward          | -529      |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.93e-09 |
|    explained_variance   | -0.00414  |
|    learning_rate        | 0.001     |
|    loss                 | 1.33e+03  |
|    n_updates            | 698       |
|    policy_gradient_loss | -5.68e-10 |
|    value_loss           | 3.29e+03  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=-518.59 +/- 74.72
Episode length: 50.08 +/- 18.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-537.79 +/- 56.41
Episode length: 53.26 +/- 18.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-536.59 +/- 69.85
Episode length: 54.24 +/- 20.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 256      |
|    iterations      | 101      |
|    time_elapsed    | 806      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-518.53 +/- 66.28
Episode length: 47.00 +/- 16.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 47       |
|    mean_reward          | -519     |
| time/                   |          |
|    total_timesteps      | 207000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.6e-11 |
|    explained_variance   | 9.57e-05 |
|    learning_rate        | 0.001    |
|    loss                 | 2.86e+03 |
|    n_updates            | 708      |
|    policy_gradient_loss | 6.77e-10 |
|    value_loss           | 3.62e+03 |
--------------------------------------
Eval num_timesteps=207500, episode_reward=-501.08 +/- 88.65
Episode length: 48.58 +/- 16.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-528.19 +/- 64.01
Episode length: 51.32 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-520.34 +/- 63.88
Episode length: 51.08 +/- 18.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -506     |
| time/              |          |
|    fps             | 256      |
|    iterations      | 102      |
|    time_elapsed    | 815      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-540.16 +/- 62.62
Episode length: 53.24 +/- 15.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -540      |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.68e-08 |
|    explained_variance   | -0.00276  |
|    learning_rate        | 0.001     |
|    loss                 | 1.42e+03  |
|    n_updates            | 718       |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 2.88e+03  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=-519.75 +/- 62.20
Episode length: 51.34 +/- 19.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-519.75 +/- 71.17
Episode length: 50.98 +/- 18.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-515.48 +/- 74.45
Episode length: 47.90 +/- 16.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 255      |
|    iterations      | 103      |
|    time_elapsed    | 824      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-526.20 +/- 69.08
Episode length: 49.06 +/- 16.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-10 |
|    explained_variance   | 0.000405  |
|    learning_rate        | 0.001     |
|    loss                 | 1.34e+03  |
|    n_updates            | 728       |
|    policy_gradient_loss | 5.53e-10  |
|    value_loss           | 3.46e+03  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=-534.78 +/- 74.70
Episode length: 49.00 +/- 17.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-532.34 +/- 53.20
Episode length: 53.70 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-511.38 +/- 79.98
Episode length: 48.38 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 255      |
|    iterations      | 104      |
|    time_elapsed    | 834      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-535.33 +/- 60.24
Episode length: 52.60 +/- 17.24
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 52.6     |
|    mean_reward          | -535     |
| time/                   |          |
|    total_timesteps      | 213000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.4e-08 |
|    explained_variance   | 5.41e-05 |
|    learning_rate        | 0.001    |
|    loss                 | 2.06e+03 |
|    n_updates            | 738      |
|    policy_gradient_loss | 1.78e-10 |
|    value_loss           | 3.27e+03 |
--------------------------------------
Eval num_timesteps=213500, episode_reward=-507.66 +/- 92.44
Episode length: 48.40 +/- 18.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-532.36 +/- 57.46
Episode length: 53.40 +/- 21.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-525.18 +/- 64.27
Episode length: 51.68 +/- 20.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-532.38 +/- 60.19
Episode length: 53.08 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 254      |
|    iterations      | 105      |
|    time_elapsed    | 845      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-503.54 +/- 68.43
Episode length: 52.16 +/- 16.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.2      |
|    mean_reward          | -504      |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-10 |
|    explained_variance   | 0.000315  |
|    learning_rate        | 0.001     |
|    loss                 | 2.25e+03  |
|    n_updates            | 748       |
|    policy_gradient_loss | -1.36e-09 |
|    value_loss           | 4.04e+03  |
---------------------------------------
Eval num_timesteps=216000, episode_reward=-525.19 +/- 79.32
Episode length: 51.44 +/- 19.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-543.18 +/- 61.94
Episode length: 53.18 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-523.34 +/- 64.10
Episode length: 46.04 +/- 14.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 253      |
|    iterations      | 106      |
|    time_elapsed    | 855      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-535.36 +/- 69.59
Episode length: 48.32 +/- 14.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -535      |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.6e-08  |
|    explained_variance   | -0.000787 |
|    learning_rate        | 0.001     |
|    loss                 | 1.11e+03  |
|    n_updates            | 758       |
|    policy_gradient_loss | -5.24e-11 |
|    value_loss           | 2.87e+03  |
---------------------------------------
Eval num_timesteps=218000, episode_reward=-538.39 +/- 57.18
Episode length: 52.96 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-511.95 +/- 72.73
Episode length: 48.76 +/- 18.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-513.15 +/- 67.49
Episode length: 47.70 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 253      |
|    iterations      | 107      |
|    time_elapsed    | 864      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-525.16 +/- 69.77
Episode length: 49.96 +/- 14.27
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 50       |
|    mean_reward          | -525     |
| time/                   |          |
|    total_timesteps      | 219500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -9.7e-11 |
|    explained_variance   | 0.000466 |
|    learning_rate        | 0.001    |
|    loss                 | 2.16e+03 |
|    n_updates            | 768      |
|    policy_gradient_loss | 1.83e-09 |
|    value_loss           | 4.15e+03 |
--------------------------------------
Eval num_timesteps=220000, episode_reward=-484.84 +/- 110.08
Episode length: 48.70 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -485     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-531.19 +/- 69.29
Episode length: 53.54 +/- 22.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-510.18 +/- 74.21
Episode length: 51.64 +/- 14.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.7     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 253      |
|    iterations      | 108      |
|    time_elapsed    | 873      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-529.35 +/- 61.07
Episode length: 49.28 +/- 16.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.3      |
|    mean_reward          | -529      |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.21e-25 |
|    explained_variance   | -0.000511 |
|    learning_rate        | 0.001     |
|    loss                 | 1.05e+03  |
|    n_updates            | 778       |
|    policy_gradient_loss | 6e-10     |
|    value_loss           | 3.43e+03  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=-520.99 +/- 66.34
Episode length: 49.56 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-516.19 +/- 69.09
Episode length: 54.32 +/- 23.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-517.99 +/- 70.49
Episode length: 52.78 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 252      |
|    iterations      | 109      |
|    time_elapsed    | 883      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-523.38 +/- 62.95
Episode length: 51.62 +/- 16.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.6      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.03e-11 |
|    explained_variance   | 0.00262   |
|    learning_rate        | 0.001     |
|    loss                 | 5.24e+03  |
|    n_updates            | 788       |
|    policy_gradient_loss | -2.18e-10 |
|    value_loss           | 1.1e+04   |
---------------------------------------
Eval num_timesteps=224000, episode_reward=-530.57 +/- 48.98
Episode length: 52.58 +/- 17.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-511.38 +/- 61.69
Episode length: 48.26 +/- 14.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-523.38 +/- 63.24
Episode length: 50.88 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 252      |
|    iterations      | 110      |
|    time_elapsed    | 892      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-493.15 +/- 85.96
Episode length: 48.64 +/- 16.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.6      |
|    mean_reward          | -493      |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.33e-13 |
|    explained_variance   | 0.00607   |
|    learning_rate        | 0.001     |
|    loss                 | 1.14e+03  |
|    n_updates            | 798       |
|    policy_gradient_loss | 2.12e-09  |
|    value_loss           | 3.76e+03  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=-531.19 +/- 65.55
Episode length: 50.60 +/- 18.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-501.70 +/- 74.27
Episode length: 49.48 +/- 13.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-512.59 +/- 75.49
Episode length: 48.80 +/- 15.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -536     |
| time/              |          |
|    fps             | 252      |
|    iterations      | 111      |
|    time_elapsed    | 901      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-521.58 +/- 69.52
Episode length: 51.52 +/- 17.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.5      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.97e-20 |
|    explained_variance   | -0.00179  |
|    learning_rate        | 0.001     |
|    loss                 | 5.69e+03  |
|    n_updates            | 808       |
|    policy_gradient_loss | -1.8e-09  |
|    value_loss           | 1.04e+04  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=-522.79 +/- 64.11
Episode length: 49.60 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-527.59 +/- 62.07
Episode length: 47.66 +/- 13.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-531.15 +/- 52.08
Episode length: 47.72 +/- 13.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 251      |
|    iterations      | 112      |
|    time_elapsed    | 910      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-522.78 +/- 75.24
Episode length: 52.16 +/- 20.86
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 52.2     |
|    mean_reward          | -523     |
| time/                   |          |
|    total_timesteps      | 229500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1e-22   |
|    explained_variance   | 0.00156  |
|    learning_rate        | 0.001    |
|    loss                 | 2.02e+03 |
|    n_updates            | 818      |
|    policy_gradient_loss | 5.41e-10 |
|    value_loss           | 3.5e+03  |
--------------------------------------
Eval num_timesteps=230000, episode_reward=-522.11 +/- 58.58
Episode length: 49.28 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-519.77 +/- 75.46
Episode length: 46.02 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-502.92 +/- 63.58
Episode length: 46.62 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 251      |
|    iterations      | 113      |
|    time_elapsed    | 920      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-530.59 +/- 62.26
Episode length: 53.46 +/- 16.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.5      |
|    mean_reward          | -531      |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-09 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.001     |
|    loss                 | 2.81e+03  |
|    n_updates            | 828       |
|    policy_gradient_loss | 3.98e-09  |
|    value_loss           | 5.84e+03  |
---------------------------------------
Eval num_timesteps=232000, episode_reward=-520.98 +/- 63.58
Episode length: 47.82 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-522.15 +/- 82.63
Episode length: 52.38 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-525.78 +/- 57.74
Episode length: 54.20 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.8     |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 251      |
|    iterations      | 114      |
|    time_elapsed    | 929      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-535.98 +/- 65.17
Episode length: 49.04 +/- 12.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49        |
|    mean_reward          | -536      |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.18e-12 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.001     |
|    loss                 | 1.82e+03  |
|    n_updates            | 838       |
|    policy_gradient_loss | -1.88e-09 |
|    value_loss           | 3.72e+03  |
---------------------------------------
Eval num_timesteps=234000, episode_reward=-514.35 +/- 78.50
Episode length: 52.52 +/- 17.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-515.59 +/- 58.00
Episode length: 46.08 +/- 13.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-524.53 +/- 71.97
Episode length: 49.74 +/- 16.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-527.59 +/- 63.22
Episode length: 50.80 +/- 13.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 250      |
|    iterations      | 115      |
|    time_elapsed    | 940      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-522.76 +/- 68.22
Episode length: 53.52 +/- 24.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.5      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.54e-15 |
|    explained_variance   | 0.00998   |
|    learning_rate        | 0.001     |
|    loss                 | 3.44e+03  |
|    n_updates            | 848       |
|    policy_gradient_loss | 3.38e-10  |
|    value_loss           | 5.96e+03  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=-519.18 +/- 64.95
Episode length: 50.08 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-523.38 +/- 67.65
Episode length: 53.74 +/- 16.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-507.74 +/- 76.28
Episode length: 44.96 +/- 14.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 250      |
|    iterations      | 116      |
|    time_elapsed    | 949      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-538.39 +/- 54.29
Episode length: 53.28 +/- 17.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.3      |
|    mean_reward          | -538      |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.93e-17 |
|    explained_variance   | 0.00712   |
|    learning_rate        | 0.001     |
|    loss                 | 1.3e+03   |
|    n_updates            | 858       |
|    policy_gradient_loss | 6.11e-11  |
|    value_loss           | 3.49e+03  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=-525.19 +/- 60.83
Episode length: 48.16 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-535.38 +/- 56.44
Episode length: 49.78 +/- 16.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-517.94 +/- 92.35
Episode length: 51.02 +/- 20.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 249      |
|    iterations      | 117      |
|    time_elapsed    | 959      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-529.39 +/- 70.82
Episode length: 51.96 +/- 18.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52        |
|    mean_reward          | -529      |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6e-15    |
|    explained_variance   | 0.0116    |
|    learning_rate        | 0.001     |
|    loss                 | 2.9e+03   |
|    n_updates            | 868       |
|    policy_gradient_loss | -3.09e-09 |
|    value_loss           | 5.7e+03   |
---------------------------------------
Eval num_timesteps=240500, episode_reward=-514.38 +/- 76.15
Episode length: 44.22 +/- 15.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-519.17 +/- 73.04
Episode length: 49.90 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-506.58 +/- 78.35
Episode length: 46.88 +/- 15.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 249      |
|    iterations      | 118      |
|    time_elapsed    | 968      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-519.19 +/- 74.49
Episode length: 54.88 +/- 20.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.9      |
|    mean_reward          | -519      |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.02e-17 |
|    explained_variance   | 0.00844   |
|    learning_rate        | 0.001     |
|    loss                 | 2.14e+03  |
|    n_updates            | 878       |
|    policy_gradient_loss | -8.44e-11 |
|    value_loss           | 3.23e+03  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=-526.39 +/- 62.48
Episode length: 51.52 +/- 15.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-518.48 +/- 88.10
Episode length: 52.74 +/- 21.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-516.74 +/- 75.34
Episode length: 51.06 +/- 14.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 249      |
|    iterations      | 119      |
|    time_elapsed    | 978      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-520.94 +/- 65.08
Episode length: 47.44 +/- 14.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.4      |
|    mean_reward          | -521      |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-14 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.001     |
|    loss                 | 3.12e+03  |
|    n_updates            | 888       |
|    policy_gradient_loss | 1.75e-09  |
|    value_loss           | 5.8e+03   |
---------------------------------------
Eval num_timesteps=244500, episode_reward=-523.38 +/- 66.02
Episode length: 53.16 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-526.99 +/- 70.03
Episode length: 52.88 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-524.51 +/- 71.15
Episode length: 55.56 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 248      |
|    iterations      | 120      |
|    time_elapsed    | 987      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-510.74 +/- 74.44
Episode length: 46.74 +/- 17.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.7      |
|    mean_reward          | -511      |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.35e-17 |
|    explained_variance   | 0.00639   |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+03  |
|    n_updates            | 898       |
|    policy_gradient_loss | -2.1e-10  |
|    value_loss           | 3.38e+03  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=-544.38 +/- 41.15
Episode length: 49.72 +/- 15.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -544     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-538.92 +/- 54.71
Episode length: 53.12 +/- 18.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-522.79 +/- 75.22
Episode length: 51.54 +/- 18.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.1     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 248      |
|    iterations      | 121      |
|    time_elapsed    | 997      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-517.95 +/- 77.19
Episode length: 51.10 +/- 18.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.1      |
|    mean_reward          | -518      |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.03e-14 |
|    explained_variance   | 0.0104    |
|    learning_rate        | 0.001     |
|    loss                 | 2.28e+03  |
|    n_updates            | 908       |
|    policy_gradient_loss | -2.08e-09 |
|    value_loss           | 5.01e+03  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=-516.19 +/- 77.21
Episode length: 48.36 +/- 13.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-534.79 +/- 64.89
Episode length: 53.02 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-528.74 +/- 65.01
Episode length: 47.26 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 248      |
|    iterations      | 122      |
|    time_elapsed    | 1006     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-523.95 +/- 71.72
Episode length: 48.54 +/- 13.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -524      |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-16 |
|    explained_variance   | 0.00622   |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+03  |
|    n_updates            | 918       |
|    policy_gradient_loss | -6.29e-10 |
|    value_loss           | 2.91e+03  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=-548.55 +/- 50.62
Episode length: 53.40 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-510.77 +/- 61.28
Episode length: 47.68 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-520.39 +/- 67.41
Episode length: 49.72 +/- 19.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 123      |
|    time_elapsed    | 1016     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-520.39 +/- 78.98
Episode length: 46.10 +/- 16.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.1      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.51e-14 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.001     |
|    loss                 | 2.55e+03  |
|    n_updates            | 928       |
|    policy_gradient_loss | 1.45e-09  |
|    value_loss           | 4.94e+03  |
---------------------------------------
Eval num_timesteps=252500, episode_reward=-511.33 +/- 88.16
Episode length: 47.36 +/- 14.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-529.99 +/- 57.56
Episode length: 48.80 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-546.78 +/- 58.41
Episode length: 50.48 +/- 18.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.4     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 124      |
|    time_elapsed    | 1024     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-516.78 +/- 73.42
Episode length: 55.54 +/- 21.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 55.5      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.13e-16 |
|    explained_variance   | 0.00738   |
|    learning_rate        | 0.001     |
|    loss                 | 1.36e+03  |
|    n_updates            | 938       |
|    policy_gradient_loss | -3.84e-10 |
|    value_loss           | 3.46e+03  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=-517.91 +/- 82.44
Episode length: 46.90 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-520.99 +/- 76.66
Episode length: 48.66 +/- 18.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-528.78 +/- 69.08
Episode length: 50.52 +/- 17.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-525.79 +/- 65.35
Episode length: 54.36 +/- 17.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 125      |
|    time_elapsed    | 1036     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-510.12 +/- 75.54
Episode length: 49.70 +/- 18.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.7      |
|    mean_reward          | -510      |
| time/                   |           |
|    total_timesteps      | 256500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.03e-13 |
|    explained_variance   | 0.00821   |
|    learning_rate        | 0.001     |
|    loss                 | 2.28e+03  |
|    n_updates            | 948       |
|    policy_gradient_loss | -4.12e-10 |
|    value_loss           | 4.64e+03  |
---------------------------------------
Eval num_timesteps=257000, episode_reward=-507.77 +/- 78.56
Episode length: 44.86 +/- 12.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.9     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-526.39 +/- 60.73
Episode length: 49.04 +/- 15.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-526.38 +/- 64.18
Episode length: 50.58 +/- 18.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.2     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 126      |
|    time_elapsed    | 1045     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-522.79 +/- 72.04
Episode length: 48.84 +/- 17.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.8      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.99e-16 |
|    explained_variance   | 0.00766   |
|    learning_rate        | 0.001     |
|    loss                 | 2.27e+03  |
|    n_updates            | 958       |
|    policy_gradient_loss | 2.12e-10  |
|    value_loss           | 3.64e+03  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=-529.39 +/- 80.56
Episode length: 52.04 +/- 19.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-524.59 +/- 70.20
Episode length: 54.78 +/- 19.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-528.19 +/- 64.85
Episode length: 47.32 +/- 15.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.3     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 127      |
|    time_elapsed    | 1054     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=-526.37 +/- 74.10
Episode length: 49.54 +/- 17.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.04e-13 |
|    explained_variance   | 0.00916   |
|    learning_rate        | 0.001     |
|    loss                 | 2.11e+03  |
|    n_updates            | 968       |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 4.6e+03   |
---------------------------------------
Eval num_timesteps=261000, episode_reward=-525.13 +/- 57.74
Episode length: 55.34 +/- 15.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-504.12 +/- 97.59
Episode length: 45.40 +/- 15.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-532.39 +/- 53.88
Episode length: 56.54 +/- 19.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.5     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.4     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 128      |
|    time_elapsed    | 1063     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-507.19 +/- 81.16
Episode length: 50.44 +/- 16.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -507      |
| time/                   |           |
|    total_timesteps      | 262500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.29e-16 |
|    explained_variance   | 0.00732   |
|    learning_rate        | 0.001     |
|    loss                 | 1.37e+03  |
|    n_updates            | 978       |
|    policy_gradient_loss | 1.09e-09  |
|    value_loss           | 3.13e+03  |
---------------------------------------
Eval num_timesteps=263000, episode_reward=-515.59 +/- 62.48
Episode length: 47.20 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-534.19 +/- 72.35
Episode length: 52.44 +/- 18.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-534.15 +/- 63.67
Episode length: 53.14 +/- 18.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.8     |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 129      |
|    time_elapsed    | 1073     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=-514.88 +/- 72.94
Episode length: 48.50 +/- 16.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -515      |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.12e-13 |
|    explained_variance   | 0.00496   |
|    learning_rate        | 0.001     |
|    loss                 | 1.97e+03  |
|    n_updates            | 988       |
|    policy_gradient_loss | -3.92e-09 |
|    value_loss           | 4.62e+03  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=-518.57 +/- 62.68
Episode length: 46.38 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-552.71 +/- 40.77
Episode length: 51.40 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-530.48 +/- 69.85
Episode length: 50.76 +/- 16.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 130      |
|    time_elapsed    | 1082     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-499.77 +/- 95.68
Episode length: 47.02 +/- 17.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47        |
|    mean_reward          | -500      |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.93e-16 |
|    explained_variance   | 0.00202   |
|    learning_rate        | 0.001     |
|    loss                 | 1.37e+03  |
|    n_updates            | 998       |
|    policy_gradient_loss | -4.74e-10 |
|    value_loss           | 2.31e+03  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=-531.18 +/- 57.03
Episode length: 53.90 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-510.75 +/- 73.10
Episode length: 53.14 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-523.33 +/- 69.01
Episode length: 48.02 +/- 17.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 131      |
|    time_elapsed    | 1091     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=-514.36 +/- 73.50
Episode length: 48.94 +/- 15.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.9      |
|    mean_reward          | -514      |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.07e-13 |
|    explained_variance   | 0.0063    |
|    learning_rate        | 0.001     |
|    loss                 | 2.16e+03  |
|    n_updates            | 1008      |
|    policy_gradient_loss | -1.46e-09 |
|    value_loss           | 4.51e+03  |
---------------------------------------
Eval num_timesteps=269000, episode_reward=-527.58 +/- 64.35
Episode length: 47.62 +/- 15.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-502.96 +/- 78.75
Episode length: 52.04 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-522.17 +/- 60.66
Episode length: 51.42 +/- 17.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 132      |
|    time_elapsed    | 1100     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-516.74 +/- 76.82
Episode length: 51.60 +/- 18.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.6      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.17e-15 |
|    explained_variance   | 0.00481   |
|    learning_rate        | 0.001     |
|    loss                 | 2.02e+03  |
|    n_updates            | 1018      |
|    policy_gradient_loss | 1.55e-09  |
|    value_loss           | 2.88e+03  |
---------------------------------------
Eval num_timesteps=271000, episode_reward=-511.99 +/- 65.17
Episode length: 47.32 +/- 15.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-537.79 +/- 65.29
Episode length: 54.40 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-531.79 +/- 56.29
Episode length: 53.42 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 133      |
|    time_elapsed    | 1110     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=-537.18 +/- 61.76
Episode length: 48.20 +/- 17.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.2      |
|    mean_reward          | -537      |
| time/                   |           |
|    total_timesteps      | 272500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.73e-12 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.001     |
|    loss                 | 1.92e+03  |
|    n_updates            | 1028      |
|    policy_gradient_loss | -2.48e-09 |
|    value_loss           | 4.43e+03  |
---------------------------------------
Eval num_timesteps=273000, episode_reward=-528.13 +/- 64.31
Episode length: 52.02 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-513.68 +/- 75.60
Episode length: 49.38 +/- 15.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-517.98 +/- 68.93
Episode length: 49.94 +/- 16.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 134      |
|    time_elapsed    | 1119     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=-508.97 +/- 76.91
Episode length: 47.70 +/- 17.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.7      |
|    mean_reward          | -509      |
| time/                   |           |
|    total_timesteps      | 274500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.73e-14 |
|    explained_variance   | 0.00722   |
|    learning_rate        | 0.001     |
|    loss                 | 266       |
|    n_updates            | 1038      |
|    policy_gradient_loss | -1.17e-09 |
|    value_loss           | 3.28e+03  |
---------------------------------------
Eval num_timesteps=275000, episode_reward=-534.79 +/- 68.13
Episode length: 56.50 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.5     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-527.55 +/- 70.82
Episode length: 53.12 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-522.79 +/- 76.41
Episode length: 44.40 +/- 12.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 135      |
|    time_elapsed    | 1128     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=-529.37 +/- 62.17
Episode length: 52.82 +/- 16.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.8      |
|    mean_reward          | -529      |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.43e-12 |
|    explained_variance   | 0.00563   |
|    learning_rate        | 0.001     |
|    loss                 | 1.75e+03  |
|    n_updates            | 1048      |
|    policy_gradient_loss | -5.95e-09 |
|    value_loss           | 4.29e+03  |
---------------------------------------
Eval num_timesteps=277000, episode_reward=-520.99 +/- 75.48
Episode length: 45.96 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-526.33 +/- 66.61
Episode length: 52.80 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-515.59 +/- 73.60
Episode length: 48.86 +/- 14.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-516.14 +/- 78.62
Episode length: 50.80 +/- 21.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 136      |
|    time_elapsed    | 1139     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=-512.56 +/- 80.80
Episode length: 51.00 +/- 15.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51        |
|    mean_reward          | -513      |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.31e-14 |
|    explained_variance   | 0.00326   |
|    learning_rate        | 0.001     |
|    loss                 | 1.6e+03   |
|    n_updates            | 1058      |
|    policy_gradient_loss | -5.3e-10  |
|    value_loss           | 3.08e+03  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=-526.95 +/- 64.17
Episode length: 48.92 +/- 12.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-523.37 +/- 71.53
Episode length: 51.02 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-536.59 +/- 58.33
Episode length: 53.04 +/- 17.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 137      |
|    time_elapsed    | 1148     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=-501.11 +/- 74.80
Episode length: 50.48 +/- 19.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -501      |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.08e-13 |
|    explained_variance   | 0.0023    |
|    learning_rate        | 0.001     |
|    loss                 | 1.96e+03  |
|    n_updates            | 1068      |
|    policy_gradient_loss | 1.32e-09  |
|    value_loss           | 4.7e+03   |
---------------------------------------
Eval num_timesteps=281500, episode_reward=-538.31 +/- 62.33
Episode length: 49.54 +/- 16.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-533.58 +/- 58.13
Episode length: 50.72 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-507.78 +/- 78.56
Episode length: 44.50 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.7     |
|    ep_rew_mean     | -505     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 138      |
|    time_elapsed    | 1157     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-531.18 +/- 61.59
Episode length: 48.28 +/- 16.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.3      |
|    mean_reward          | -531      |
| time/                   |           |
|    total_timesteps      | 283000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-15 |
|    explained_variance   | 0.00366   |
|    learning_rate        | 0.001     |
|    loss                 | 817       |
|    n_updates            | 1078      |
|    policy_gradient_loss | -1.63e-09 |
|    value_loss           | 3.21e+03  |
---------------------------------------
Eval num_timesteps=283500, episode_reward=-518.59 +/- 81.19
Episode length: 52.66 +/- 15.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-502.99 +/- 77.60
Episode length: 47.12 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-513.75 +/- 60.86
Episode length: 47.20 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.7     |
|    ep_rew_mean     | -505     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 139      |
|    time_elapsed    | 1167     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-512.59 +/- 86.59
Episode length: 44.52 +/- 16.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44.5      |
|    mean_reward          | -513      |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.25e-13 |
|    explained_variance   | 0.00688   |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+03  |
|    n_updates            | 1088      |
|    policy_gradient_loss | -8.75e-10 |
|    value_loss           | 4.32e+03  |
---------------------------------------
Eval num_timesteps=285500, episode_reward=-522.79 +/- 79.18
Episode length: 51.44 +/- 13.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-529.38 +/- 56.69
Episode length: 51.88 +/- 21.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-518.54 +/- 72.24
Episode length: 54.52 +/- 19.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -506     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 140      |
|    time_elapsed    | 1176     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-520.31 +/- 85.17
Episode length: 54.56 +/- 19.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.6      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.83e-15 |
|    explained_variance   | 0.00544   |
|    learning_rate        | 0.001     |
|    loss                 | 860       |
|    n_updates            | 1098      |
|    policy_gradient_loss | 4.96e-10  |
|    value_loss           | 2.69e+03  |
---------------------------------------
Eval num_timesteps=287500, episode_reward=-526.32 +/- 76.17
Episode length: 47.70 +/- 15.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-536.57 +/- 69.61
Episode length: 51.10 +/- 15.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-525.18 +/- 59.63
Episode length: 47.64 +/- 15.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 141      |
|    time_elapsed    | 1185     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-511.39 +/- 80.66
Episode length: 49.42 +/- 16.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.4      |
|    mean_reward          | -511      |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.08e-12 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.001     |
|    loss                 | 1.8e+03   |
|    n_updates            | 1108      |
|    policy_gradient_loss | 1.55e-09  |
|    value_loss           | 4.33e+03  |
---------------------------------------
Eval num_timesteps=289500, episode_reward=-523.39 +/- 61.51
Episode length: 53.58 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-514.38 +/- 76.14
Episode length: 49.68 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-504.15 +/- 77.17
Episode length: 49.42 +/- 16.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 142      |
|    time_elapsed    | 1194     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-515.55 +/- 83.74
Episode length: 50.10 +/- 16.79
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 50.1     |
|    mean_reward          | -516     |
| time/                   |          |
|    total_timesteps      | 291000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.1e-14 |
|    explained_variance   | 0.00889  |
|    learning_rate        | 0.001    |
|    loss                 | 1.33e+03 |
|    n_updates            | 1118     |
|    policy_gradient_loss | -2.2e-10 |
|    value_loss           | 2.9e+03  |
--------------------------------------
Eval num_timesteps=291500, episode_reward=-514.18 +/- 81.37
Episode length: 51.34 +/- 14.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-523.39 +/- 52.34
Episode length: 50.62 +/- 20.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-535.99 +/- 56.28
Episode length: 47.58 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53       |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 143      |
|    time_elapsed    | 1203     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-522.79 +/- 64.11
Episode length: 54.70 +/- 19.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 54.7      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.91e-12 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.001     |
|    loss                 | 2.32e+03  |
|    n_updates            | 1128      |
|    policy_gradient_loss | 4.48e-10  |
|    value_loss           | 4.24e+03  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=-523.38 +/- 63.23
Episode length: 49.34 +/- 13.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-523.37 +/- 55.68
Episode length: 50.38 +/- 20.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-524.48 +/- 76.20
Episode length: 51.56 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 144      |
|    time_elapsed    | 1213     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-506.48 +/- 90.50
Episode length: 49.30 +/- 16.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.3      |
|    mean_reward          | -506      |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-14 |
|    explained_variance   | 0.0118    |
|    learning_rate        | 0.001     |
|    loss                 | 1.28e+03  |
|    n_updates            | 1138      |
|    policy_gradient_loss | 2.05e-09  |
|    value_loss           | 4e+03     |
---------------------------------------
Eval num_timesteps=295500, episode_reward=-519.15 +/- 73.25
Episode length: 54.86 +/- 17.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-517.95 +/- 62.61
Episode length: 52.82 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-531.18 +/- 62.75
Episode length: 52.84 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.1     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 145      |
|    time_elapsed    | 1222     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=-530.42 +/- 67.89
Episode length: 53.16 +/- 18.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.93e-11 |
|    explained_variance   | 0.0123    |
|    learning_rate        | 0.001     |
|    loss                 | 1.98e+03  |
|    n_updates            | 1148      |
|    policy_gradient_loss | 7.95e-10  |
|    value_loss           | 3.97e+03  |
---------------------------------------
Eval num_timesteps=297500, episode_reward=-524.54 +/- 58.56
Episode length: 52.40 +/- 19.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-511.39 +/- 74.63
Episode length: 55.10 +/- 20.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-544.39 +/- 51.28
Episode length: 51.50 +/- 14.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -544     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-523.38 +/- 69.49
Episode length: 48.96 +/- 18.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 146      |
|    time_elapsed    | 1234     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=-523.28 +/- 88.35
Episode length: 49.18 +/- 18.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.2      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.37e-13 |
|    explained_variance   | 0.01      |
|    learning_rate        | 0.001     |
|    loss                 | 2.06e+03  |
|    n_updates            | 1158      |
|    policy_gradient_loss | 1.38e-10  |
|    value_loss           | 3.54e+03  |
---------------------------------------
Eval num_timesteps=300000, episode_reward=-526.97 +/- 53.73
Episode length: 50.94 +/- 15.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-528.15 +/- 69.39
Episode length: 54.60 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-525.77 +/- 66.97
Episode length: 50.00 +/- 21.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 147      |
|    time_elapsed    | 1243     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=-522.63 +/- 83.09
Episode length: 50.14 +/- 15.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 301500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.8e-13  |
|    explained_variance   | 0.0137    |
|    learning_rate        | 0.001     |
|    loss                 | 2.08e+03  |
|    n_updates            | 1168      |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 4.55e+03  |
---------------------------------------
Eval num_timesteps=302000, episode_reward=-516.78 +/- 68.08
Episode length: 53.92 +/- 14.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=-523.39 +/- 54.03
Episode length: 48.30 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-536.59 +/- 61.04
Episode length: 46.38 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.5     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 241      |
|    iterations      | 148      |
|    time_elapsed    | 1252     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=-535.31 +/- 73.41
Episode length: 50.54 +/- 20.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -535      |
| time/                   |           |
|    total_timesteps      | 303500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.59e-15 |
|    explained_variance   | 0.0106    |
|    learning_rate        | 0.001     |
|    loss                 | 548       |
|    n_updates            | 1178      |
|    policy_gradient_loss | 7.86e-11  |
|    value_loss           | 2.47e+03  |
---------------------------------------
Eval num_timesteps=304000, episode_reward=-531.78 +/- 56.61
Episode length: 53.16 +/- 17.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-525.73 +/- 60.23
Episode length: 47.96 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-524.58 +/- 63.18
Episode length: 50.26 +/- 18.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.5     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 241      |
|    iterations      | 149      |
|    time_elapsed    | 1262     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=-514.87 +/- 87.29
Episode length: 47.96 +/- 19.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48        |
|    mean_reward          | -515      |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.13e-13 |
|    explained_variance   | 0.0115    |
|    learning_rate        | 0.001     |
|    loss                 | 2.5e+03   |
|    n_updates            | 1188      |
|    policy_gradient_loss | 1.32e-09  |
|    value_loss           | 4.47e+03  |
---------------------------------------
Eval num_timesteps=306000, episode_reward=-515.59 +/- 84.95
Episode length: 49.22 +/- 16.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-529.39 +/- 63.87
Episode length: 51.28 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=-519.73 +/- 72.80
Episode length: 52.44 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.2     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 241      |
|    iterations      | 150      |
|    time_elapsed    | 1271     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=-514.95 +/- 75.43
Episode length: 47.86 +/- 14.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 47.9      |
|    mean_reward          | -515      |
| time/                   |           |
|    total_timesteps      | 307500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.19e-15 |
|    explained_variance   | 0.00969   |
|    learning_rate        | 0.001     |
|    loss                 | 885       |
|    n_updates            | 1198      |
|    policy_gradient_loss | 2.91e-11  |
|    value_loss           | 3.02e+03  |
---------------------------------------
Eval num_timesteps=308000, episode_reward=-535.39 +/- 57.40
Episode length: 52.24 +/- 20.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=-532.95 +/- 65.05
Episode length: 53.34 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-512.87 +/- 96.15
Episode length: 47.62 +/- 15.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -503     |
| time/              |          |
|    fps             | 241      |
|    iterations      | 151      |
|    time_elapsed    | 1281     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=-510.78 +/- 73.57
Episode length: 53.30 +/- 20.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.3      |
|    mean_reward          | -511      |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.56e-13 |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.001     |
|    loss                 | 2.29e+03  |
|    n_updates            | 1208      |
|    policy_gradient_loss | -1.64e-09 |
|    value_loss           | 4.15e+03  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=-531.18 +/- 58.59
Episode length: 49.36 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-509.57 +/- 70.14
Episode length: 46.06 +/- 15.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=-513.19 +/- 73.87
Episode length: 49.46 +/- 13.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49       |
|    ep_rew_mean     | -503     |
| time/              |          |
|    fps             | 241      |
|    iterations      | 152      |
|    time_elapsed    | 1290     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=-520.94 +/- 68.73
Episode length: 49.56 +/- 18.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.6      |
|    mean_reward          | -521      |
| time/                   |           |
|    total_timesteps      | 311500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.92e-15 |
|    explained_variance   | 0.00921   |
|    learning_rate        | 0.001     |
|    loss                 | 801       |
|    n_updates            | 1218      |
|    policy_gradient_loss | -1.07e-09 |
|    value_loss           | 3.11e+03  |
---------------------------------------
Eval num_timesteps=312000, episode_reward=-534.75 +/- 57.25
Episode length: 51.90 +/- 19.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=-512.57 +/- 58.59
Episode length: 50.20 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-514.98 +/- 65.81
Episode length: 50.64 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 241      |
|    iterations      | 153      |
|    time_elapsed    | 1299     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=-525.79 +/- 66.44
Episode length: 52.10 +/- 16.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.1      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-12 |
|    explained_variance   | 0.0211    |
|    learning_rate        | 0.001     |
|    loss                 | 2.69e+03  |
|    n_updates            | 1228      |
|    policy_gradient_loss | -4.96e-09 |
|    value_loss           | 4.42e+03  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=-527.59 +/- 66.01
Episode length: 47.28 +/- 15.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=-523.99 +/- 60.89
Episode length: 47.38 +/- 12.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-546.15 +/- 53.19
Episode length: 50.76 +/- 15.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -546     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 154      |
|    time_elapsed    | 1309     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=-510.76 +/- 69.56
Episode length: 48.08 +/- 16.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.1      |
|    mean_reward          | -511      |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.86e-15 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.001     |
|    loss                 | 2.6e+03   |
|    n_updates            | 1238      |
|    policy_gradient_loss | -3.49e-11 |
|    value_loss           | 3.4e+03   |
---------------------------------------
Eval num_timesteps=316000, episode_reward=-531.18 +/- 65.82
Episode length: 49.88 +/- 18.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=-519.16 +/- 61.81
Episode length: 51.68 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-531.79 +/- 80.05
Episode length: 54.70 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 155      |
|    time_elapsed    | 1318     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=-505.39 +/- 73.36
Episode length: 53.18 +/- 15.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.2      |
|    mean_reward          | -505      |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.41e-12 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.001     |
|    loss                 | 2.42e+03  |
|    n_updates            | 1248      |
|    policy_gradient_loss | -1.01e-09 |
|    value_loss           | 4.39e+03  |
---------------------------------------
Eval num_timesteps=318000, episode_reward=-532.95 +/- 69.31
Episode length: 53.56 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=-531.19 +/- 64.44
Episode length: 49.74 +/- 19.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=-528.18 +/- 52.24
Episode length: 49.96 +/- 17.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 156      |
|    time_elapsed    | 1327     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=-520.35 +/- 83.30
Episode length: 49.82 +/- 14.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.44e-15 |
|    explained_variance   | 0.0121    |
|    learning_rate        | 0.001     |
|    loss                 | 939       |
|    n_updates            | 1258      |
|    policy_gradient_loss | -1.46e-10 |
|    value_loss           | 2.82e+03  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=-523.98 +/- 75.18
Episode length: 49.64 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=-514.38 +/- 72.26
Episode length: 49.78 +/- 16.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=-510.68 +/- 89.06
Episode length: 49.82 +/- 14.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=-529.98 +/- 61.48
Episode length: 51.82 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.6     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 157      |
|    time_elapsed    | 1338     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=-535.38 +/- 56.44
Episode length: 52.02 +/- 17.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52        |
|    mean_reward          | -535      |
| time/                   |           |
|    total_timesteps      | 322000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.93e-11 |
|    explained_variance   | 0.0172    |
|    learning_rate        | 0.001     |
|    loss                 | 2.27e+03  |
|    n_updates            | 1268      |
|    policy_gradient_loss | -1.78e-09 |
|    value_loss           | 4.3e+03   |
---------------------------------------
Eval num_timesteps=322500, episode_reward=-502.98 +/- 77.13
Episode length: 48.66 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=-508.91 +/- 85.13
Episode length: 53.06 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=-529.39 +/- 62.73
Episode length: 50.74 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 158      |
|    time_elapsed    | 1348     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=-526.99 +/- 66.88
Episode length: 51.38 +/- 18.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -527      |
| time/                   |           |
|    total_timesteps      | 324000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.49e-13 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.001     |
|    loss                 | 2e+03     |
|    n_updates            | 1278      |
|    policy_gradient_loss | -3.41e-10 |
|    value_loss           | 2.95e+03  |
---------------------------------------
Eval num_timesteps=324500, episode_reward=-517.39 +/- 67.59
Episode length: 49.90 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-530.56 +/- 79.82
Episode length: 51.08 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=-516.74 +/- 75.42
Episode length: 51.70 +/- 17.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 159      |
|    time_elapsed    | 1357     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=-525.75 +/- 55.56
Episode length: 53.10 +/- 17.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.1      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-12 |
|    explained_variance   | 0.0107    |
|    learning_rate        | 0.001     |
|    loss                 | 1.87e+03  |
|    n_updates            | 1288      |
|    policy_gradient_loss | -1.99e-09 |
|    value_loss           | 4.56e+03  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=-513.18 +/- 78.60
Episode length: 48.38 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=-538.37 +/- 64.30
Episode length: 51.98 +/- 14.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=-509.55 +/- 77.23
Episode length: 47.38 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.7     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 160      |
|    time_elapsed    | 1366     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=-519.67 +/- 75.27
Episode length: 50.54 +/- 19.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.5      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 328000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.71e-15 |
|    explained_variance   | 0.00697   |
|    learning_rate        | 0.001     |
|    loss                 | 779       |
|    n_updates            | 1298      |
|    policy_gradient_loss | 4.38e-10  |
|    value_loss           | 2.56e+03  |
---------------------------------------
Eval num_timesteps=328500, episode_reward=-522.75 +/- 73.99
Episode length: 46.30 +/- 13.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=-527.52 +/- 71.83
Episode length: 51.16 +/- 15.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=-528.76 +/- 64.08
Episode length: 51.56 +/- 20.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.6     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 161      |
|    time_elapsed    | 1376     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-506.53 +/- 83.17
Episode length: 49.84 +/- 17.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.8      |
|    mean_reward          | -507      |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.59e-12 |
|    explained_variance   | 0.018     |
|    learning_rate        | 0.001     |
|    loss                 | 2.08e+03  |
|    n_updates            | 1308      |
|    policy_gradient_loss | 1.61e-09  |
|    value_loss           | 4.27e+03  |
---------------------------------------
Eval num_timesteps=330500, episode_reward=-520.39 +/- 66.34
Episode length: 50.34 +/- 16.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=-511.38 +/- 66.73
Episode length: 44.48 +/- 14.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=-520.95 +/- 74.06
Episode length: 50.16 +/- 16.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 162      |
|    time_elapsed    | 1385     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=-492.11 +/- 97.22
Episode length: 43.28 +/- 12.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 43.3      |
|    mean_reward          | -492      |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.04e-14 |
|    explained_variance   | 0.0131    |
|    learning_rate        | 0.001     |
|    loss                 | 1.81e+03  |
|    n_updates            | 1318      |
|    policy_gradient_loss | 9.9e-11   |
|    value_loss           | 2.68e+03  |
---------------------------------------
Eval num_timesteps=332500, episode_reward=-517.98 +/- 78.46
Episode length: 50.82 +/- 19.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=-531.78 +/- 57.88
Episode length: 48.66 +/- 15.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=-528.70 +/- 67.49
Episode length: 50.08 +/- 18.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.3     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 163      |
|    time_elapsed    | 1394     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=-505.84 +/- 81.59
Episode length: 49.06 +/- 19.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -506      |
| time/                   |           |
|    total_timesteps      | 334000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.91e-12 |
|    explained_variance   | 0.0173    |
|    learning_rate        | 0.001     |
|    loss                 | 2.1e+03   |
|    n_updates            | 1328      |
|    policy_gradient_loss | 9.02e-11  |
|    value_loss           | 4.2e+03   |
---------------------------------------
Eval num_timesteps=334500, episode_reward=-516.04 +/- 86.13
Episode length: 46.26 +/- 16.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=-516.19 +/- 58.37
Episode length: 51.90 +/- 16.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=-511.38 +/- 69.39
Episode length: 52.06 +/- 19.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 164      |
|    time_elapsed    | 1403     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=-534.15 +/- 74.68
Episode length: 48.42 +/- 17.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.4      |
|    mean_reward          | -534      |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.41e-15 |
|    explained_variance   | 0.0109    |
|    learning_rate        | 0.001     |
|    loss                 | 1.89e+03  |
|    n_updates            | 1338      |
|    policy_gradient_loss | -3.65e-10 |
|    value_loss           | 2.75e+03  |
---------------------------------------
Eval num_timesteps=336500, episode_reward=-507.18 +/- 90.98
Episode length: 51.10 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=-529.37 +/- 57.65
Episode length: 50.44 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=-521.59 +/- 74.52
Episode length: 49.10 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.3     |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 165      |
|    time_elapsed    | 1412     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=-522.18 +/- 59.76
Episode length: 45.22 +/- 17.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.2      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 338000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.38e-10 |
|    explained_variance   | 0.0233    |
|    learning_rate        | 0.001     |
|    loss                 | 2.12e+03  |
|    n_updates            | 1348      |
|    policy_gradient_loss | 6.29e-10  |
|    value_loss           | 4.17e+03  |
---------------------------------------
Eval num_timesteps=338500, episode_reward=-524.55 +/- 76.77
Episode length: 49.98 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=-522.18 +/- 65.24
Episode length: 50.42 +/- 17.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=-510.76 +/- 76.21
Episode length: 49.36 +/- 16.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 166      |
|    time_elapsed    | 1421     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-516.16 +/- 81.31
Episode length: 54.28 +/- 17.63
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 54.3     |
|    mean_reward          | -516     |
| time/                   |          |
|    total_timesteps      | 340000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -5.3e-13 |
|    explained_variance   | 0.0213   |
|    learning_rate        | 0.001    |
|    loss                 | 1.16e+03 |
|    n_updates            | 1358     |
|    policy_gradient_loss | 1.23e-09 |
|    value_loss           | 2.87e+03 |
--------------------------------------
Eval num_timesteps=340500, episode_reward=-532.39 +/- 59.90
Episode length: 51.08 +/- 20.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-508.92 +/- 69.73
Episode length: 47.70 +/- 15.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=-516.79 +/- 66.75
Episode length: 51.48 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-517.98 +/- 70.74
Episode length: 49.30 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 167      |
|    time_elapsed    | 1432     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=-526.31 +/- 79.25
Episode length: 51.40 +/- 16.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -526      |
| time/                   |           |
|    total_timesteps      | 342500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.38e-12 |
|    explained_variance   | 0.0223    |
|    learning_rate        | 0.001     |
|    loss                 | 1.94e+03  |
|    n_updates            | 1368      |
|    policy_gradient_loss | -5.01e-10 |
|    value_loss           | 4.65e+03  |
---------------------------------------
Eval num_timesteps=343000, episode_reward=-511.37 +/- 81.35
Episode length: 51.84 +/- 21.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=-521.59 +/- 74.28
Episode length: 51.54 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-528.74 +/- 60.20
Episode length: 48.98 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.4     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 168      |
|    time_elapsed    | 1441     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=-525.11 +/- 71.96
Episode length: 51.36 +/- 21.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.4      |
|    mean_reward          | -525      |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.32e-14 |
|    explained_variance   | 0.0147    |
|    learning_rate        | 0.001     |
|    loss                 | 746       |
|    n_updates            | 1378      |
|    policy_gradient_loss | -6.98e-11 |
|    value_loss           | 3.25e+03  |
---------------------------------------
Eval num_timesteps=345000, episode_reward=-516.76 +/- 65.37
Episode length: 52.70 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=-523.38 +/- 61.79
Episode length: 50.82 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=-519.18 +/- 71.80
Episode length: 48.98 +/- 15.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.2     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 169      |
|    time_elapsed    | 1451     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=-518.52 +/- 56.04
Episode length: 50.72 +/- 19.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.7      |
|    mean_reward          | -519      |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.08e-12 |
|    explained_variance   | 0.0212    |
|    learning_rate        | 0.001     |
|    loss                 | 3.17e+03  |
|    n_updates            | 1388      |
|    policy_gradient_loss | -2.63e-09 |
|    value_loss           | 4.47e+03  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=-523.99 +/- 72.00
Episode length: 55.92 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=-538.38 +/- 65.68
Episode length: 53.70 +/- 15.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=-524.59 +/- 62.03
Episode length: 51.82 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.4     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 170      |
|    time_elapsed    | 1460     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=-521.55 +/- 76.16
Episode length: 51.06 +/- 16.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.1      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 348500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.89e-14 |
|    explained_variance   | 0.0182    |
|    learning_rate        | 0.001     |
|    loss                 | 1.23e+03  |
|    n_updates            | 1398      |
|    policy_gradient_loss | -4.13e-10 |
|    value_loss           | 3.18e+03  |
---------------------------------------
Eval num_timesteps=349000, episode_reward=-528.78 +/- 63.37
Episode length: 49.08 +/- 17.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=-517.40 +/- 66.51
Episode length: 52.48 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-520.91 +/- 69.73
Episode length: 50.60 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.6     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 171      |
|    time_elapsed    | 1470     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=-529.99 +/- 63.21
Episode length: 53.14 +/- 19.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.1      |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 350500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.72e-12 |
|    explained_variance   | 0.026     |
|    learning_rate        | 0.001     |
|    loss                 | 2.05e+03  |
|    n_updates            | 1408      |
|    policy_gradient_loss | -1.42e-09 |
|    value_loss           | 4.52e+03  |
---------------------------------------
Eval num_timesteps=351000, episode_reward=-525.19 +/- 65.39
Episode length: 54.56 +/- 18.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-512.58 +/- 86.81
Episode length: 52.86 +/- 20.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-522.78 +/- 75.70
Episode length: 50.94 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 172      |
|    time_elapsed    | 1479     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=-520.99 +/- 71.56
Episode length: 48.22 +/- 14.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.2      |
|    mean_reward          | -521      |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.55e-14 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.001     |
|    loss                 | 1.47e+03  |
|    n_updates            | 1418      |
|    policy_gradient_loss | 1.08e-09  |
|    value_loss           | 2.61e+03  |
---------------------------------------
Eval num_timesteps=353000, episode_reward=-532.39 +/- 53.88
Episode length: 52.54 +/- 16.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=-517.98 +/- 70.74
Episode length: 50.38 +/- 15.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-517.99 +/- 78.69
Episode length: 51.42 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.5     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 173      |
|    time_elapsed    | 1489     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=-504.17 +/- 71.91
Episode length: 50.40 +/- 18.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -504      |
| time/                   |           |
|    total_timesteps      | 354500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.18e-11 |
|    explained_variance   | 0.0303    |
|    learning_rate        | 0.001     |
|    loss                 | 2.31e+03  |
|    n_updates            | 1428      |
|    policy_gradient_loss | -2.87e-09 |
|    value_loss           | 4.47e+03  |
---------------------------------------
Eval num_timesteps=355000, episode_reward=-527.56 +/- 81.84
Episode length: 45.06 +/- 15.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=-528.74 +/- 64.86
Episode length: 47.24 +/- 16.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=-540.79 +/- 72.09
Episode length: 48.86 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 174      |
|    time_elapsed    | 1498     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=-498.79 +/- 84.56
Episode length: 49.50 +/- 18.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.5      |
|    mean_reward          | -499      |
| time/                   |           |
|    total_timesteps      | 356500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.14e-14 |
|    explained_variance   | 0.0216    |
|    learning_rate        | 0.001     |
|    loss                 | 1.17e+03  |
|    n_updates            | 1438      |
|    policy_gradient_loss | -1.76e-09 |
|    value_loss           | 3.08e+03  |
---------------------------------------
Eval num_timesteps=357000, episode_reward=-519.75 +/- 79.31
Episode length: 49.56 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=-516.74 +/- 79.53
Episode length: 48.02 +/- 14.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=-525.71 +/- 74.94
Episode length: 48.06 +/- 17.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.9     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 175      |
|    time_elapsed    | 1507     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=-536.55 +/- 57.66
Episode length: 50.44 +/- 13.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.4      |
|    mean_reward          | -537      |
| time/                   |           |
|    total_timesteps      | 358500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.76e-11 |
|    explained_variance   | 0.0273    |
|    learning_rate        | 0.001     |
|    loss                 | 1.99e+03  |
|    n_updates            | 1448      |
|    policy_gradient_loss | -2.22e-09 |
|    value_loss           | 4.16e+03  |
---------------------------------------
Eval num_timesteps=359000, episode_reward=-534.79 +/- 63.49
Episode length: 52.58 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=-524.59 +/- 75.63
Episode length: 51.26 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-518.44 +/- 81.64
Episode length: 48.62 +/- 14.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 176      |
|    time_elapsed    | 1516     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=-517.92 +/- 74.20
Episode length: 49.06 +/- 18.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -518      |
| time/                   |           |
|    total_timesteps      | 360500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.51e-14 |
|    explained_variance   | 0.0256    |
|    learning_rate        | 0.001     |
|    loss                 | 1.67e+03  |
|    n_updates            | 1458      |
|    policy_gradient_loss | -7.17e-10 |
|    value_loss           | 3.1e+03   |
---------------------------------------
Eval num_timesteps=361000, episode_reward=-526.39 +/- 55.46
Episode length: 51.68 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=-534.66 +/- 54.54
Episode length: 50.56 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=-511.38 +/- 66.73
Episode length: 50.84 +/- 17.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.7     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 177      |
|    time_elapsed    | 1525     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=-516.79 +/- 64.56
Episode length: 46.54 +/- 12.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.5      |
|    mean_reward          | -517      |
| time/                   |           |
|    total_timesteps      | 362500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.33e-11 |
|    explained_variance   | 0.0432    |
|    learning_rate        | 0.001     |
|    loss                 | 2.04e+03  |
|    n_updates            | 1468      |
|    policy_gradient_loss | 1.89e-09  |
|    value_loss           | 4.28e+03  |
---------------------------------------
Eval num_timesteps=363000, episode_reward=-529.95 +/- 65.05
Episode length: 51.62 +/- 19.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-532.39 +/- 55.86
Episode length: 51.16 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=-536.59 +/- 68.28
Episode length: 53.64 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=-522.19 +/- 74.51
Episode length: 49.62 +/- 13.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 178      |
|    time_elapsed    | 1537     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=-520.99 +/- 61.85
Episode length: 49.08 +/- 17.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.1      |
|    mean_reward          | -521      |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.05e-14 |
|    explained_variance   | 0.0246    |
|    learning_rate        | 0.001     |
|    loss                 | 1.78e+03  |
|    n_updates            | 1478      |
|    policy_gradient_loss | 8.59e-11  |
|    value_loss           | 3.32e+03  |
---------------------------------------
Eval num_timesteps=365500, episode_reward=-527.55 +/- 63.37
Episode length: 50.44 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=-520.99 +/- 67.42
Episode length: 53.40 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=-547.34 +/- 46.42
Episode length: 50.58 +/- 15.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 179      |
|    time_elapsed    | 1546     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=-520.20 +/- 75.69
Episode length: 50.60 +/- 15.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.6      |
|    mean_reward          | -520      |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.09e-11 |
|    explained_variance   | 0.0236    |
|    learning_rate        | 0.001     |
|    loss                 | 1.94e+03  |
|    n_updates            | 1488      |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 4.43e+03  |
---------------------------------------
Eval num_timesteps=367500, episode_reward=-519.15 +/- 55.08
Episode length: 50.50 +/- 13.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-532.97 +/- 65.09
Episode length: 53.88 +/- 17.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=-526.39 +/- 73.10
Episode length: 54.70 +/- 17.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 180      |
|    time_elapsed    | 1556     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=-529.99 +/- 67.88
Episode length: 51.08 +/- 14.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.1      |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 369000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-13 |
|    explained_variance   | 0.0229    |
|    learning_rate        | 0.001     |
|    loss                 | 1.46e+03  |
|    n_updates            | 1498      |
|    policy_gradient_loss | -4.77e-10 |
|    value_loss           | 2.92e+03  |
---------------------------------------
Eval num_timesteps=369500, episode_reward=-550.35 +/- 45.84
Episode length: 58.44 +/- 21.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-525.75 +/- 72.68
Episode length: 51.12 +/- 15.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=-533.56 +/- 57.81
Episode length: 53.58 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.7     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 181      |
|    time_elapsed    | 1566     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=-515.59 +/- 61.32
Episode length: 51.26 +/- 15.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.3         |
|    mean_reward          | -516         |
| time/                   |              |
|    total_timesteps      | 371000       |
| train/                  |              |
|    approx_kl            | 8.731149e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -2.58e-05    |
|    explained_variance   | 0.0701       |
|    learning_rate        | 0.001        |
|    loss                 | 1.45e+03     |
|    n_updates            | 1508         |
|    policy_gradient_loss | 6.92e-07     |
|    value_loss           | 2.82e+03     |
------------------------------------------
Eval num_timesteps=371500, episode_reward=-514.95 +/- 69.51
Episode length: 50.32 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-520.33 +/- 69.80
Episode length: 51.50 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-524.58 +/- 68.12
Episode length: 50.92 +/- 18.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 182      |
|    time_elapsed    | 1576     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=-528.76 +/- 61.63
Episode length: 46.84 +/- 15.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 46.8      |
|    mean_reward          | -529      |
| time/                   |           |
|    total_timesteps      | 373000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.2e-08  |
|    explained_variance   | 0.026     |
|    learning_rate        | 0.001     |
|    loss                 | 2.72e+03  |
|    n_updates            | 1518      |
|    policy_gradient_loss | -5.41e-08 |
|    value_loss           | 5.02e+03  |
---------------------------------------
Eval num_timesteps=373500, episode_reward=-507.79 +/- 66.66
Episode length: 54.48 +/- 17.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=-510.09 +/- 87.89
Episode length: 47.30 +/- 16.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=-531.78 +/- 48.38
Episode length: 48.74 +/- 12.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47       |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 183      |
|    time_elapsed    | 1585     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=-521.58 +/- 64.97
Episode length: 50.10 +/- 14.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.1      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 375000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.65e-27 |
|    explained_variance   | 0.0491    |
|    learning_rate        | 0.001     |
|    loss                 | 1.16e+03  |
|    n_updates            | 1528      |
|    policy_gradient_loss | -1.56e-09 |
|    value_loss           | 3.06e+03  |
---------------------------------------
Eval num_timesteps=375500, episode_reward=-519.14 +/- 63.26
Episode length: 49.92 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=-523.32 +/- 64.74
Episode length: 48.56 +/- 16.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=-536.58 +/- 48.93
Episode length: 51.06 +/- 13.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.1     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 184      |
|    time_elapsed    | 1594     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=-530.59 +/- 56.18
Episode length: 52.56 +/- 15.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 52.6      |
|    mean_reward          | -531      |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.66e-30 |
|    explained_variance   | 0.0254    |
|    learning_rate        | 0.001     |
|    loss                 | 1.79e+03  |
|    n_updates            | 1538      |
|    policy_gradient_loss | -7.13e-10 |
|    value_loss           | 3.09e+03  |
---------------------------------------
Eval num_timesteps=377500, episode_reward=-522.71 +/- 66.54
Episode length: 45.94 +/- 13.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-516.78 +/- 77.48
Episode length: 51.56 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-525.19 +/- 62.58
Episode length: 50.62 +/- 17.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.2     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 185      |
|    time_elapsed    | 1603     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=-521.43 +/- 80.02
Episode length: 48.58 +/- 16.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.6      |
|    mean_reward          | -521      |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.64e-29 |
|    explained_variance   | 0.0421    |
|    learning_rate        | 0.001     |
|    loss                 | 1.79e+03  |
|    n_updates            | 1548      |
|    policy_gradient_loss | -4.39e-10 |
|    value_loss           | 2.64e+03  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=-516.10 +/- 86.61
Episode length: 45.02 +/- 16.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-524.55 +/- 62.33
Episode length: 55.46 +/- 14.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.5     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=-530.58 +/- 59.91
Episode length: 51.42 +/- 16.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.6     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 186      |
|    time_elapsed    | 1613     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=-499.34 +/- 74.59
Episode length: 50.26 +/- 20.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.3      |
|    mean_reward          | -499      |
| time/                   |           |
|    total_timesteps      | 381000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.3e-31  |
|    explained_variance   | 0.0269    |
|    learning_rate        | 0.001     |
|    loss                 | 882       |
|    n_updates            | 1558      |
|    policy_gradient_loss | -5.82e-10 |
|    value_loss           | 3.48e+03  |
---------------------------------------
Eval num_timesteps=381500, episode_reward=-533.58 +/- 55.91
Episode length: 48.62 +/- 13.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=-513.77 +/- 75.41
Episode length: 48.16 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-514.20 +/- 68.26
Episode length: 47.58 +/- 14.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 187      |
|    time_elapsed    | 1622     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=-529.99 +/- 64.63
Episode length: 49.36 +/- 13.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 49.4      |
|    mean_reward          | -530      |
| time/                   |           |
|    total_timesteps      | 383000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.65e-28 |
|    explained_variance   | 0.0472    |
|    learning_rate        | 0.001     |
|    loss                 | 1.26e+03  |
|    n_updates            | 1568      |
|    policy_gradient_loss | 6.3e-10   |
|    value_loss           | 2.84e+03  |
---------------------------------------
Eval num_timesteps=383500, episode_reward=-507.12 +/- 71.78
Episode length: 53.32 +/- 19.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-508.99 +/- 67.69
Episode length: 55.20 +/- 20.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-517.34 +/- 85.04
Episode length: 50.66 +/- 16.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-523.99 +/- 69.97
Episode length: 52.88 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51       |
|    ep_rew_mean     | -506     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 188      |
|    time_elapsed    | 1633     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=-522.74 +/- 80.86
Episode length: 53.66 +/- 19.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 53.7      |
|    mean_reward          | -523      |
| time/                   |           |
|    total_timesteps      | 385500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.87e-31 |
|    explained_variance   | 0.0375    |
|    learning_rate        | 0.001     |
|    loss                 | 1.48e+03  |
|    n_updates            | 1578      |
|    policy_gradient_loss | 3.06e-09  |
|    value_loss           | 3.25e+03  |
---------------------------------------
Eval num_timesteps=386000, episode_reward=-526.99 +/- 69.00
Episode length: 50.54 +/- 16.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=-520.99 +/- 76.42
Episode length: 52.24 +/- 16.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-529.96 +/- 61.76
Episode length: 48.70 +/- 18.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.1     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 189      |
|    time_elapsed    | 1642     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=-511.39 +/- 69.90
Episode length: 51.80 +/- 17.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.8      |
|    mean_reward          | -511      |
| time/                   |           |
|    total_timesteps      | 387500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.58e-28 |
|    explained_variance   | 0.0524    |
|    learning_rate        | 0.001     |
|    loss                 | 1.98e+03  |
|    n_updates            | 1588      |
|    policy_gradient_loss | -7.8e-10  |
|    value_loss           | 2.75e+03  |
---------------------------------------
Eval num_timesteps=388000, episode_reward=-528.72 +/- 61.07
Episode length: 48.96 +/- 17.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-528.17 +/- 63.18
Episode length: 47.34 +/- 16.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-525.79 +/- 65.34
Episode length: 53.04 +/- 17.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 190      |
|    time_elapsed    | 1652     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=-532.36 +/- 62.86
Episode length: 51.68 +/- 14.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.7      |
|    mean_reward          | -532      |
| time/                   |           |
|    total_timesteps      | 389500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.14e-31 |
|    explained_variance   | 0.0278    |
|    learning_rate        | 0.001     |
|    loss                 | 1.63e+03  |
|    n_updates            | 1598      |
|    policy_gradient_loss | -1.77e-09 |
|    value_loss           | 3.68e+03  |
---------------------------------------
Eval num_timesteps=390000, episode_reward=-511.39 +/- 73.41
Episode length: 49.70 +/- 19.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=-524.55 +/- 70.74
Episode length: 47.16 +/- 13.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-528.78 +/- 58.04
Episode length: 45.98 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 191      |
|    time_elapsed    | 1661     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=-523.93 +/- 79.59
Episode length: 45.66 +/- 16.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 45.7      |
|    mean_reward          | -524      |
| time/                   |           |
|    total_timesteps      | 391500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.33e-24 |
|    explained_variance   | 0.049     |
|    learning_rate        | 0.001     |
|    loss                 | 1.33e+03  |
|    n_updates            | 1608      |
|    policy_gradient_loss | -4.05e-10 |
|    value_loss           | 2.8e+03   |
---------------------------------------
Eval num_timesteps=392000, episode_reward=-536.55 +/- 65.05
Episode length: 56.84 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.8     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-523.97 +/- 64.33
Episode length: 48.78 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-529.37 +/- 67.46
Episode length: 51.92 +/- 18.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.2     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 192      |
|    time_elapsed    | 1670     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=-515.58 +/- 74.32
Episode length: 50.26 +/- 16.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 50.3      |
|    mean_reward          | -516      |
| time/                   |           |
|    total_timesteps      | 393500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.11e-27 |
|    explained_variance   | 0.0326    |
|    learning_rate        | 0.001     |
|    loss                 | 2.2e+03   |
|    n_updates            | 1618      |
|    policy_gradient_loss | 6.52e-10  |
|    value_loss           | 3.48e+03  |
---------------------------------------
Eval num_timesteps=394000, episode_reward=-510.74 +/- 81.90
Episode length: 48.96 +/- 18.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=-515.59 +/- 75.29
Episode length: 55.76 +/- 17.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-526.98 +/- 58.87
Episode length: 54.16 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.8     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 193      |
|    time_elapsed    | 1680     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=-506.54 +/- 76.24
Episode length: 50.36 +/- 17.42
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 50.4     |
|    mean_reward          | -507     |
| time/                   |          |
|    total_timesteps      | 395500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.3      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4e-27   |
|    explained_variance   | 0.0571   |
|    learning_rate        | 0.001    |
|    loss                 | 2.34e+03 |
|    n_updates            | 1628     |
|    policy_gradient_loss | 9.81e-10 |
|    value_loss           | 3.07e+03 |
--------------------------------------
Eval num_timesteps=396000, episode_reward=-525.15 +/- 57.78
Episode length: 48.58 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-517.39 +/- 66.79
Episode length: 48.76 +/- 13.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=-507.07 +/- 79.45
Episode length: 50.10 +/- 16.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 47.5     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 194      |
|    time_elapsed    | 1689     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=-522.15 +/- 66.93
Episode length: 48.48 +/- 18.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 48.5      |
|    mean_reward          | -522      |
| time/                   |           |
|    total_timesteps      | 397500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-29 |
|    explained_variance   | 0.0328    |
|    learning_rate        | 0.001     |
|    loss                 | 2.17e+03  |
|    n_updates            | 1638      |
|    policy_gradient_loss | -9.75e-10 |
|    value_loss           | 4.02e+03  |
---------------------------------------
Eval num_timesteps=398000, episode_reward=-513.19 +/- 76.51
Episode length: 50.22 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-516.15 +/- 71.89
Episode length: 49.88 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-522.15 +/- 72.38
Episode length: 49.36 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 195      |
|    time_elapsed    | 1698     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=-532.38 +/- 65.62
Episode length: 51.06 +/- 20.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 51.1      |
|    mean_reward          | -532      |
| time/                   |           |
|    total_timesteps      | 399500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.94e-24 |
|    explained_variance   | 0.0554    |
|    learning_rate        | 0.001     |
|    loss                 | 1.68e+03  |
|    n_updates            | 1648      |
|    policy_gradient_loss | -1.84e-09 |
|    value_loss           | 2.63e+03  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=-507.15 +/- 62.41
Episode length: 53.20 +/- 19.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-ppo-stop-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Deteniendo el entrenamiento en el 400000 timesteps (40.0% del total).
