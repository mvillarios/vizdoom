/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 428      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 188      |
|    time_elapsed     | 2        |
|    total_timesteps  | 528      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 8        |
|    fps              | 321      |
|    time_elapsed     | 2        |
|    total_timesteps  | 936      |
----------------------------------
Eval num_timesteps=1000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 12       |
|    fps              | 238      |
|    time_elapsed     | 5        |
|    total_timesteps  | 1408     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.11     |
|    n_updates        | 101      |
----------------------------------
Eval num_timesteps=1500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.85     |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 16       |
|    fps              | 210      |
|    time_elapsed     | 8        |
|    total_timesteps  | 1856     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.14     |
|    n_updates        | 213      |
----------------------------------
Eval num_timesteps=2000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.509    |
|    n_updates        | 249      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 356      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 20       |
|    fps              | 194      |
|    time_elapsed     | 11       |
|    total_timesteps  | 2280     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.316    |
|    n_updates        | 319      |
----------------------------------
Eval num_timesteps=2500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.28     |
|    n_updates        | 374      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 24       |
|    fps              | 186      |
|    time_elapsed     | 14       |
|    total_timesteps  | 2720     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.287    |
|    n_updates        | 429      |
----------------------------------
Eval num_timesteps=3000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.116    |
|    n_updates        | 499      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 351      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 28       |
|    fps              | 180      |
|    time_elapsed     | 17       |
|    total_timesteps  | 3160     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.105    |
|    n_updates        | 539      |
----------------------------------
Eval num_timesteps=3500, episode_reward=287.84 +/- 26.88
Episode length: 96.96 +/- 6.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.59     |
|    n_updates        | 624      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 355      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 32       |
|    fps              | 177      |
|    time_elapsed     | 20       |
|    total_timesteps  | 3640     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.14     |
|    n_updates        | 659      |
----------------------------------
Eval num_timesteps=4000, episode_reward=528.48 +/- 263.08
Episode length: 157.12 +/- 65.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 157      |
|    mean_reward      | 528      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0229   |
|    n_updates        | 749      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 360      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 36       |
|    fps              | 165      |
|    time_elapsed     | 25       |
|    total_timesteps  | 4144     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0223   |
|    n_updates        | 785      |
----------------------------------
Eval num_timesteps=4500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.57     |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 40       |
|    fps              | 166      |
|    time_elapsed     | 27       |
|    total_timesteps  | 4656     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0102   |
|    n_updates        | 913      |
----------------------------------
Eval num_timesteps=5000, episode_reward=490.72 +/- 248.80
Episode length: 147.68 +/- 62.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 491      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00881  |
|    n_updates        | 999      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 44       |
|    fps              | 160      |
|    time_elapsed     | 32       |
|    total_timesteps  | 5184     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.57     |
|    n_updates        | 1045     |
----------------------------------
Eval num_timesteps=5500, episode_reward=316.64 +/- 38.66
Episode length: 104.16 +/- 9.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 48       |
|    fps              | 158      |
|    time_elapsed     | 35       |
|    total_timesteps  | 5584     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1145     |
----------------------------------
Eval num_timesteps=6000, episode_reward=365.92 +/- 175.64
Episode length: 116.48 +/- 43.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 366      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00557  |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 52       |
|    fps              | 156      |
|    time_elapsed     | 38       |
|    total_timesteps  | 6048     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00621  |
|    n_updates        | 1261     |
----------------------------------
Eval num_timesteps=6500, episode_reward=306.40 +/- 48.85
Episode length: 101.60 +/- 12.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 56       |
|    fps              | 155      |
|    time_elapsed     | 41       |
|    total_timesteps  | 6504     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1375     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 361      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 60       |
|    fps              | 164      |
|    time_elapsed     | 42       |
|    total_timesteps  | 6920     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.11     |
|    n_updates        | 1479     |
----------------------------------
Eval num_timesteps=7000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.11     |
|    n_updates        | 1499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 360      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 64       |
|    fps              | 163      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7368     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00537  |
|    n_updates        | 1591     |
----------------------------------
Eval num_timesteps=7500, episode_reward=301.28 +/- 42.55
Episode length: 100.32 +/- 10.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 301      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00368  |
|    n_updates        | 1624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 68       |
|    fps              | 162      |
|    time_elapsed     | 47       |
|    total_timesteps  | 7800     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00368  |
|    n_updates        | 1699     |
----------------------------------
Eval num_timesteps=8000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00503  |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 72       |
|    fps              | 164      |
|    time_elapsed     | 50       |
|    total_timesteps  | 8392     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1847     |
----------------------------------
Eval num_timesteps=8500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 76       |
|    fps              | 164      |
|    time_elapsed     | 53       |
|    total_timesteps  | 8848     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00293  |
|    n_updates        | 1961     |
----------------------------------
Eval num_timesteps=9000, episode_reward=288.48 +/- 19.21
Episode length: 97.12 +/- 4.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.1     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00202  |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 80       |
|    fps              | 164      |
|    time_elapsed     | 56       |
|    total_timesteps  | 9344     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00251  |
|    n_updates        | 2085     |
----------------------------------
Eval num_timesteps=9500, episode_reward=287.84 +/- 16.49
Episode length: 96.96 +/- 4.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00163  |
|    n_updates        | 2124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 84       |
|    fps              | 164      |
|    time_elapsed     | 59       |
|    total_timesteps  | 9848     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0028   |
|    n_updates        | 2211     |
----------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00253  |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 88       |
|    fps              | 165      |
|    time_elapsed     | 62       |
|    total_timesteps  | 10392    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 2347     |
----------------------------------
Eval num_timesteps=10500, episode_reward=301.92 +/- 40.05
Episode length: 100.48 +/- 10.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 302      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 92       |
|    fps              | 165      |
|    time_elapsed     | 65       |
|    total_timesteps  | 10872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00984  |
|    n_updates        | 2467     |
----------------------------------
Eval num_timesteps=11000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 96       |
|    fps              | 164      |
|    time_elapsed     | 68       |
|    total_timesteps  | 11304    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 2575     |
----------------------------------
Eval num_timesteps=11500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00607  |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 100      |
|    fps              | 165      |
|    time_elapsed     | 71       |
|    total_timesteps  | 11848    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00656  |
|    n_updates        | 2711     |
----------------------------------
Eval num_timesteps=12000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.24     |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 104      |
|    fps              | 166      |
|    time_elapsed     | 74       |
|    total_timesteps  | 12456    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 2863     |
----------------------------------
Eval num_timesteps=12500, episode_reward=410.72 +/- 192.53
Episode length: 127.68 +/- 48.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 411      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00402  |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 108      |
|    fps              | 164      |
|    time_elapsed     | 78       |
|    total_timesteps  | 12920    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00554  |
|    n_updates        | 2979     |
----------------------------------
Eval num_timesteps=13000, episode_reward=591.04 +/- 373.47
Episode length: 172.26 +/- 91.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 591      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0047   |
|    n_updates        | 2999     |
----------------------------------
New best mean reward!
Eval num_timesteps=13500, episode_reward=289.12 +/- 22.50
Episode length: 97.28 +/- 5.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 289      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00417  |
|    n_updates        | 3124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 112      |
|    fps              | 157      |
|    time_elapsed     | 86       |
|    total_timesteps  | 13552    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00575  |
|    n_updates        | 3137     |
----------------------------------
Eval num_timesteps=14000, episode_reward=293.60 +/- 33.41
Episode length: 98.40 +/- 8.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0041   |
|    n_updates        | 3249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 116      |
|    fps              | 157      |
|    time_elapsed     | 89       |
|    total_timesteps  | 14016    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.23     |
|    n_updates        | 3253     |
----------------------------------
Eval num_timesteps=14500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00373  |
|    n_updates        | 3374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 120      |
|    fps              | 157      |
|    time_elapsed     | 92       |
|    total_timesteps  | 14552    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3387     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 124      |
|    fps              | 161      |
|    time_elapsed     | 92       |
|    total_timesteps  | 14952    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3487     |
----------------------------------
Eval num_timesteps=15000, episode_reward=312.16 +/- 37.12
Episode length: 103.04 +/- 9.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00322  |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 128      |
|    fps              | 161      |
|    time_elapsed     | 95       |
|    total_timesteps  | 15360    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0041   |
|    n_updates        | 3589     |
----------------------------------
Eval num_timesteps=15500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 132      |
|    fps              | 160      |
|    time_elapsed     | 98       |
|    total_timesteps  | 15776    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3693     |
----------------------------------
Eval num_timesteps=16000, episode_reward=297.44 +/- 37.37
Episode length: 99.36 +/- 9.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00309  |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 136      |
|    fps              | 160      |
|    time_elapsed     | 101      |
|    total_timesteps  | 16248    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.23     |
|    n_updates        | 3811     |
----------------------------------
Eval num_timesteps=16500, episode_reward=339.68 +/- 83.15
Episode length: 109.92 +/- 20.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 340      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 140      |
|    fps              | 159      |
|    time_elapsed     | 104      |
|    total_timesteps  | 16704    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00286  |
|    n_updates        | 3925     |
----------------------------------
Eval num_timesteps=17000, episode_reward=297.44 +/- 45.30
Episode length: 99.36 +/- 11.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00507  |
|    n_updates        | 3999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 144      |
|    fps              | 159      |
|    time_elapsed     | 107      |
|    total_timesteps  | 17144    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4035     |
----------------------------------
Eval num_timesteps=17500, episode_reward=318.56 +/- 37.78
Episode length: 104.64 +/- 9.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00226  |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 148      |
|    fps              | 159      |
|    time_elapsed     | 110      |
|    total_timesteps  | 17576    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00287  |
|    n_updates        | 4143     |
----------------------------------
Eval num_timesteps=18000, episode_reward=287.20 +/- 22.40
Episode length: 96.80 +/- 5.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.8     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 152      |
|    fps              | 159      |
|    time_elapsed     | 113      |
|    total_timesteps  | 18048    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00335  |
|    n_updates        | 4261     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 156      |
|    fps              | 162      |
|    time_elapsed     | 113      |
|    total_timesteps  | 18464    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00187  |
|    n_updates        | 4365     |
----------------------------------
Eval num_timesteps=18500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 160      |
|    fps              | 162      |
|    time_elapsed     | 116      |
|    total_timesteps  | 18904    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00299  |
|    n_updates        | 4475     |
----------------------------------
Eval num_timesteps=19000, episode_reward=292.96 +/- 53.94
Episode length: 98.24 +/- 13.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.2     |
|    mean_reward      | 293      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 164      |
|    fps              | 161      |
|    time_elapsed     | 119      |
|    total_timesteps  | 19360    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00395  |
|    n_updates        | 4589     |
----------------------------------
Eval num_timesteps=19500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00347  |
|    n_updates        | 4624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 168      |
|    fps              | 161      |
|    time_elapsed     | 122      |
|    total_timesteps  | 19808    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4701     |
----------------------------------
Eval num_timesteps=20000, episode_reward=429.28 +/- 219.87
Episode length: 132.32 +/- 54.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 429      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0026   |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 172      |
|    fps              | 160      |
|    time_elapsed     | 126      |
|    total_timesteps  | 20248    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 4811     |
----------------------------------
Eval num_timesteps=20500, episode_reward=351.20 +/- 65.66
Episode length: 112.80 +/- 16.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 351      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00716  |
|    n_updates        | 4874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 176      |
|    fps              | 159      |
|    time_elapsed     | 129      |
|    total_timesteps  | 20680    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00852  |
|    n_updates        | 4919     |
----------------------------------
Eval num_timesteps=21000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00507  |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 180      |
|    fps              | 160      |
|    time_elapsed     | 132      |
|    total_timesteps  | 21256    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0045   |
|    n_updates        | 5063     |
----------------------------------
Eval num_timesteps=21500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00484  |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 184      |
|    fps              | 160      |
|    time_elapsed     | 135      |
|    total_timesteps  | 21728    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00534  |
|    n_updates        | 5181     |
----------------------------------
Eval num_timesteps=22000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00653  |
|    n_updates        | 5249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 188      |
|    fps              | 160      |
|    time_elapsed     | 138      |
|    total_timesteps  | 22224    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5305     |
----------------------------------
Eval num_timesteps=22500, episode_reward=311.52 +/- 34.47
Episode length: 102.88 +/- 8.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00498  |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 192      |
|    fps              | 160      |
|    time_elapsed     | 141      |
|    total_timesteps  | 22680    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00359  |
|    n_updates        | 5419     |
----------------------------------
Eval num_timesteps=23000, episode_reward=314.72 +/- 32.61
Episode length: 103.68 +/- 8.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 196      |
|    fps              | 160      |
|    time_elapsed     | 144      |
|    total_timesteps  | 23184    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00443  |
|    n_updates        | 5545     |
----------------------------------
Eval num_timesteps=23500, episode_reward=286.56 +/- 14.08
Episode length: 96.64 +/- 3.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 200      |
|    fps              | 160      |
|    time_elapsed     | 147      |
|    total_timesteps  | 23760    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00414  |
|    n_updates        | 5689     |
----------------------------------
Eval num_timesteps=24000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00499  |
|    n_updates        | 5749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 204      |
|    fps              | 160      |
|    time_elapsed     | 150      |
|    total_timesteps  | 24240    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.67     |
|    n_updates        | 5809     |
----------------------------------
Eval num_timesteps=24500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00346  |
|    n_updates        | 5874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 208      |
|    fps              | 161      |
|    time_elapsed     | 153      |
|    total_timesteps  | 24768    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00292  |
|    n_updates        | 5941     |
----------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00507  |
|    n_updates        | 5999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 212      |
|    fps              | 161      |
|    time_elapsed     | 156      |
|    total_timesteps  | 25280    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.005    |
|    n_updates        | 6069     |
----------------------------------
Eval num_timesteps=25500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00333  |
|    n_updates        | 6124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 216      |
|    fps              | 161      |
|    time_elapsed     | 159      |
|    total_timesteps  | 25752    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6187     |
----------------------------------
Eval num_timesteps=26000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 220      |
|    fps              | 161      |
|    time_elapsed     | 162      |
|    total_timesteps  | 26272    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6317     |
----------------------------------
Eval num_timesteps=26500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 224      |
|    fps              | 161      |
|    time_elapsed     | 165      |
|    total_timesteps  | 26720    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.67     |
|    n_updates        | 6429     |
----------------------------------
Eval num_timesteps=27000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 228      |
|    fps              | 161      |
|    time_elapsed     | 168      |
|    total_timesteps  | 27152    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00314  |
|    n_updates        | 6537     |
----------------------------------
Eval num_timesteps=27500, episode_reward=301.28 +/- 36.32
Episode length: 100.32 +/- 9.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 301      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 232      |
|    fps              | 161      |
|    time_elapsed     | 171      |
|    total_timesteps  | 27656    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00295  |
|    n_updates        | 6663     |
----------------------------------
Eval num_timesteps=28000, episode_reward=502.24 +/- 234.43
Episode length: 150.56 +/- 58.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 502      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 236      |
|    fps              | 160      |
|    time_elapsed     | 175      |
|    total_timesteps  | 28176    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0033   |
|    n_updates        | 6793     |
----------------------------------
Eval num_timesteps=28500, episode_reward=310.24 +/- 43.26
Episode length: 102.56 +/- 10.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00251  |
|    n_updates        | 6874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 240      |
|    fps              | 160      |
|    time_elapsed     | 178      |
|    total_timesteps  | 28632    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00382  |
|    n_updates        | 6907     |
----------------------------------
Eval num_timesteps=29000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00395  |
|    n_updates        | 6999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 244      |
|    fps              | 160      |
|    time_elapsed     | 181      |
|    total_timesteps  | 29120    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0028   |
|    n_updates        | 7029     |
----------------------------------
Eval num_timesteps=29500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 248      |
|    fps              | 160      |
|    time_elapsed     | 184      |
|    total_timesteps  | 29624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 7155     |
----------------------------------
Eval num_timesteps=30000, episode_reward=312.80 +/- 38.00
Episode length: 103.20 +/- 9.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 252      |
|    fps              | 160      |
|    time_elapsed     | 187      |
|    total_timesteps  | 30088    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.04     |
|    n_updates        | 7271     |
----------------------------------
Eval num_timesteps=30500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00701  |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 256      |
|    fps              | 160      |
|    time_elapsed     | 190      |
|    total_timesteps  | 30520    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00635  |
|    n_updates        | 7379     |
----------------------------------
Eval num_timesteps=31000, episode_reward=316.64 +/- 45.02
Episode length: 104.16 +/- 11.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 260      |
|    fps              | 160      |
|    time_elapsed     | 193      |
|    total_timesteps  | 31000    |
----------------------------------
Eval num_timesteps=31500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 264      |
|    fps              | 160      |
|    time_elapsed     | 196      |
|    total_timesteps  | 31584    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00495  |
|    n_updates        | 7645     |
----------------------------------
Eval num_timesteps=32000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00453  |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 268      |
|    fps              | 160      |
|    time_elapsed     | 199      |
|    total_timesteps  | 32016    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00486  |
|    n_updates        | 7753     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 272      |
|    fps              | 162      |
|    time_elapsed     | 199      |
|    total_timesteps  | 32488    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00339  |
|    n_updates        | 7871     |
----------------------------------
Eval num_timesteps=32500, episode_reward=1591.52 +/- 665.37
Episode length: 408.88 +/- 155.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 409      |
|    mean_reward      | 1.59e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00391  |
|    n_updates        | 7874     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 276      |
|    fps              | 155      |
|    time_elapsed     | 211      |
|    total_timesteps  | 32968    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 7991     |
----------------------------------
Eval num_timesteps=33000, episode_reward=393.44 +/- 127.85
Episode length: 123.36 +/- 31.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 393      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00415  |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 280      |
|    fps              | 155      |
|    time_elapsed     | 215      |
|    total_timesteps  | 33400    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 8099     |
----------------------------------
Eval num_timesteps=33500, episode_reward=362.72 +/- 92.79
Episode length: 115.68 +/- 23.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 363      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00298  |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 284      |
|    fps              | 155      |
|    time_elapsed     | 218      |
|    total_timesteps  | 33880    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 8219     |
----------------------------------
Eval num_timesteps=34000, episode_reward=1283.36 +/- 777.41
Episode length: 334.84 +/- 182.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 335      |
|    mean_reward      | 1.28e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 8249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 288      |
|    fps              | 150      |
|    time_elapsed     | 227      |
|    total_timesteps  | 34400    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 8349     |
----------------------------------
Eval num_timesteps=34500, episode_reward=665.44 +/- 327.70
Episode length: 191.36 +/- 81.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 665      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00971  |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 292      |
|    fps              | 149      |
|    time_elapsed     | 233      |
|    total_timesteps  | 34944    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00454  |
|    n_updates        | 8485     |
----------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 8499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 296      |
|    fps              | 149      |
|    time_elapsed     | 236      |
|    total_timesteps  | 35440    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 8609     |
----------------------------------
Eval num_timesteps=35500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00446  |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 300      |
|    fps              | 149      |
|    time_elapsed     | 239      |
|    total_timesteps  | 35880    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00466  |
|    n_updates        | 8719     |
----------------------------------
Eval num_timesteps=36000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00461  |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 304      |
|    fps              | 150      |
|    time_elapsed     | 242      |
|    total_timesteps  | 36424    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00376  |
|    n_updates        | 8855     |
----------------------------------
Eval num_timesteps=36500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00411  |
|    n_updates        | 8874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 308      |
|    fps              | 150      |
|    time_elapsed     | 245      |
|    total_timesteps  | 36856    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.46     |
|    n_updates        | 8963     |
----------------------------------
Eval num_timesteps=37000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00503  |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 312      |
|    fps              | 150      |
|    time_elapsed     | 248      |
|    total_timesteps  | 37328    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00627  |
|    n_updates        | 9081     |
----------------------------------
Eval num_timesteps=37500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 316      |
|    fps              | 150      |
|    time_elapsed     | 251      |
|    total_timesteps  | 37808    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00415  |
|    n_updates        | 9201     |
----------------------------------
Eval num_timesteps=38000, episode_reward=554.56 +/- 344.84
Episode length: 163.14 +/- 84.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 163      |
|    mean_reward      | 555      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00391  |
|    n_updates        | 9249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 320      |
|    fps              | 149      |
|    time_elapsed     | 256      |
|    total_timesteps  | 38240    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00463  |
|    n_updates        | 9309     |
----------------------------------
Eval num_timesteps=38500, episode_reward=287.84 +/- 26.88
Episode length: 96.96 +/- 6.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00705  |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 324      |
|    fps              | 149      |
|    time_elapsed     | 258      |
|    total_timesteps  | 38680    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 9419     |
----------------------------------
Eval num_timesteps=39000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00521  |
|    n_updates        | 9499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 328      |
|    fps              | 149      |
|    time_elapsed     | 261      |
|    total_timesteps  | 39168    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0041   |
|    n_updates        | 9541     |
----------------------------------
Eval num_timesteps=39500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 332      |
|    fps              | 149      |
|    time_elapsed     | 264      |
|    total_timesteps  | 39672    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00607  |
|    n_updates        | 9667     |
----------------------------------
Eval num_timesteps=40000, episode_reward=337.76 +/- 67.26
Episode length: 109.44 +/- 16.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 338      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00503  |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 336      |
|    fps              | 150      |
|    time_elapsed     | 268      |
|    total_timesteps  | 40296    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.58     |
|    n_updates        | 9823     |
----------------------------------
Eval num_timesteps=40500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00719  |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 340      |
|    fps              | 150      |
|    time_elapsed     | 271      |
|    total_timesteps  | 40776    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00731  |
|    n_updates        | 9943     |
----------------------------------
Eval num_timesteps=41000, episode_reward=285.28 +/- 8.96
Episode length: 96.32 +/- 2.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00723  |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 344      |
|    fps              | 150      |
|    time_elapsed     | 274      |
|    total_timesteps  | 41200    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00679  |
|    n_updates        | 10049    |
----------------------------------
Eval num_timesteps=41500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.8      |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 348      |
|    fps              | 150      |
|    time_elapsed     | 276      |
|    total_timesteps  | 41664    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00731  |
|    n_updates        | 10165    |
----------------------------------
Eval num_timesteps=42000, episode_reward=305.76 +/- 42.66
Episode length: 101.44 +/- 10.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00504  |
|    n_updates        | 10249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 352      |
|    fps              | 150      |
|    time_elapsed     | 280      |
|    total_timesteps  | 42104    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00469  |
|    n_updates        | 10275    |
----------------------------------
Eval num_timesteps=42500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 356      |
|    fps              | 150      |
|    time_elapsed     | 283      |
|    total_timesteps  | 42664    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10415    |
----------------------------------
Eval num_timesteps=43000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00849  |
|    n_updates        | 10499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 360      |
|    fps              | 150      |
|    time_elapsed     | 285      |
|    total_timesteps  | 43176    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00531  |
|    n_updates        | 10543    |
----------------------------------
Eval num_timesteps=43500, episode_reward=314.72 +/- 37.30
Episode length: 103.68 +/- 9.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 364      |
|    fps              | 150      |
|    time_elapsed     | 288      |
|    total_timesteps  | 43560    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00641  |
|    n_updates        | 10639    |
----------------------------------
Eval num_timesteps=44000, episode_reward=286.56 +/- 14.08
Episode length: 96.64 +/- 3.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.57     |
|    n_updates        | 10749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 368      |
|    fps              | 151      |
|    time_elapsed     | 291      |
|    total_timesteps  | 44088    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10771    |
----------------------------------
Eval num_timesteps=44500, episode_reward=353.12 +/- 67.54
Episode length: 113.28 +/- 16.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 353      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 372      |
|    fps              | 151      |
|    time_elapsed     | 295      |
|    total_timesteps  | 44624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00685  |
|    n_updates        | 10905    |
----------------------------------
Eval num_timesteps=45000, episode_reward=310.88 +/- 34.68
Episode length: 102.72 +/- 8.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 376      |
|    fps              | 151      |
|    time_elapsed     | 298      |
|    total_timesteps  | 45080    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00683  |
|    n_updates        | 11019    |
----------------------------------
Eval num_timesteps=45500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 380      |
|    fps              | 151      |
|    time_elapsed     | 301      |
|    total_timesteps  | 45560    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00687  |
|    n_updates        | 11139    |
----------------------------------
Eval num_timesteps=46000, episode_reward=495.04 +/- 262.75
Episode length: 148.26 +/- 62.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 495      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00449  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 384      |
|    fps              | 150      |
|    time_elapsed     | 305      |
|    total_timesteps  | 46040    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11259    |
----------------------------------
Eval num_timesteps=46500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 11374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 388      |
|    fps              | 151      |
|    time_elapsed     | 308      |
|    total_timesteps  | 46592    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11397    |
----------------------------------
Eval num_timesteps=47000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 392      |
|    fps              | 151      |
|    time_elapsed     | 311      |
|    total_timesteps  | 47040    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11509    |
----------------------------------
Eval num_timesteps=47500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 396      |
|    fps              | 151      |
|    time_elapsed     | 314      |
|    total_timesteps  | 47544    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11635    |
----------------------------------
Eval num_timesteps=48000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00499  |
|    n_updates        | 11749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 400      |
|    fps              | 151      |
|    time_elapsed     | 317      |
|    total_timesteps  | 48064    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00754  |
|    n_updates        | 11765    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 404      |
|    fps              | 152      |
|    time_elapsed     | 317      |
|    total_timesteps  | 48448    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00653  |
|    n_updates        | 11861    |
----------------------------------
Eval num_timesteps=48500, episode_reward=308.96 +/- 41.62
Episode length: 102.24 +/- 10.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00493  |
|    n_updates        | 11874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 408      |
|    fps              | 152      |
|    time_elapsed     | 320      |
|    total_timesteps  | 48928    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.57     |
|    n_updates        | 11981    |
----------------------------------
Eval num_timesteps=49000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11999    |
----------------------------------
Eval num_timesteps=49500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00564  |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 412      |
|    fps              | 151      |
|    time_elapsed     | 326      |
|    total_timesteps  | 49512    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00527  |
|    n_updates        | 12127    |
----------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 12249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 416      |
|    fps              | 151      |
|    time_elapsed     | 329      |
|    total_timesteps  | 50016    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.36     |
|    n_updates        | 12253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 420      |
|    fps              | 153      |
|    time_elapsed     | 329      |
|    total_timesteps  | 50448    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 12361    |
----------------------------------
Eval num_timesteps=50500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00615  |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 424      |
|    fps              | 153      |
|    time_elapsed     | 332      |
|    total_timesteps  | 50904    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 12475    |
----------------------------------
Eval num_timesteps=51000, episode_reward=285.28 +/- 6.27
Episode length: 96.32 +/- 1.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 428      |
|    fps              | 153      |
|    time_elapsed     | 335      |
|    total_timesteps  | 51432    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00544  |
|    n_updates        | 12607    |
----------------------------------
Eval num_timesteps=51500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 432      |
|    fps              | 153      |
|    time_elapsed     | 338      |
|    total_timesteps  | 51840    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00859  |
|    n_updates        | 12709    |
----------------------------------
Eval num_timesteps=52000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0101   |
|    n_updates        | 12749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 436      |
|    fps              | 153      |
|    time_elapsed     | 341      |
|    total_timesteps  | 52344    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 12835    |
----------------------------------
Eval num_timesteps=52500, episode_reward=285.28 +/- 8.96
Episode length: 96.32 +/- 2.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00611  |
|    n_updates        | 12874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 440      |
|    fps              | 153      |
|    time_elapsed     | 344      |
|    total_timesteps  | 52808    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 12951    |
----------------------------------
Eval num_timesteps=53000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 444      |
|    fps              | 153      |
|    time_elapsed     | 347      |
|    total_timesteps  | 53440    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00589  |
|    n_updates        | 13109    |
----------------------------------
Eval num_timesteps=53500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.85     |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 448      |
|    fps              | 154      |
|    time_elapsed     | 350      |
|    total_timesteps  | 53912    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 13227    |
----------------------------------
Eval num_timesteps=54000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00604  |
|    n_updates        | 13249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 452      |
|    fps              | 154      |
|    time_elapsed     | 352      |
|    total_timesteps  | 54416    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00736  |
|    n_updates        | 13353    |
----------------------------------
Eval num_timesteps=54500, episode_reward=497.76 +/- 224.22
Episode length: 149.44 +/- 56.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 498      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00815  |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 456      |
|    fps              | 153      |
|    time_elapsed     | 357      |
|    total_timesteps  | 54856    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0054   |
|    n_updates        | 13463    |
----------------------------------
Eval num_timesteps=55000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00836  |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 460      |
|    fps              | 153      |
|    time_elapsed     | 360      |
|    total_timesteps  | 55432    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00921  |
|    n_updates        | 13607    |
----------------------------------
Eval num_timesteps=55500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00545  |
|    n_updates        | 13624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 464      |
|    fps              | 153      |
|    time_elapsed     | 363      |
|    total_timesteps  | 55920    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.68     |
|    n_updates        | 13729    |
----------------------------------
Eval num_timesteps=56000, episode_reward=348.00 +/- 95.57
Episode length: 112.00 +/- 23.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 348      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0102   |
|    n_updates        | 13749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 468      |
|    fps              | 153      |
|    time_elapsed     | 366      |
|    total_timesteps  | 56360    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0056   |
|    n_updates        | 13839    |
----------------------------------
Eval num_timesteps=56500, episode_reward=439.52 +/- 202.69
Episode length: 134.88 +/- 50.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 440      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00862  |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 472      |
|    fps              | 153      |
|    time_elapsed     | 370      |
|    total_timesteps  | 56880    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00583  |
|    n_updates        | 13969    |
----------------------------------
Eval num_timesteps=57000, episode_reward=365.28 +/- 84.23
Episode length: 116.32 +/- 21.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 365      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 13999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 476      |
|    fps              | 153      |
|    time_elapsed     | 373      |
|    total_timesteps  | 57352    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 14087    |
----------------------------------
Eval num_timesteps=57500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 480      |
|    fps              | 153      |
|    time_elapsed     | 376      |
|    total_timesteps  | 57808    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00585  |
|    n_updates        | 14201    |
----------------------------------
Eval num_timesteps=58000, episode_reward=298.08 +/- 38.49
Episode length: 99.52 +/- 9.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.5     |
|    mean_reward      | 298      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00437  |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 484      |
|    fps              | 153      |
|    time_elapsed     | 379      |
|    total_timesteps  | 58288    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 14321    |
----------------------------------
Eval num_timesteps=58500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00645  |
|    n_updates        | 14374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 488      |
|    fps              | 153      |
|    time_elapsed     | 382      |
|    total_timesteps  | 58720    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00586  |
|    n_updates        | 14429    |
----------------------------------
Eval num_timesteps=59000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00542  |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 492      |
|    fps              | 153      |
|    time_elapsed     | 385      |
|    total_timesteps  | 59192    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00569  |
|    n_updates        | 14547    |
----------------------------------
Eval num_timesteps=59500, episode_reward=307.68 +/- 32.51
Episode length: 101.92 +/- 8.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 496      |
|    fps              | 153      |
|    time_elapsed     | 388      |
|    total_timesteps  | 59648    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00895  |
|    n_updates        | 14661    |
----------------------------------
Eval num_timesteps=60000, episode_reward=296.80 +/- 30.02
Episode length: 99.20 +/- 7.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.2     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00844  |
|    n_updates        | 14749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 500      |
|    fps              | 153      |
|    time_elapsed     | 391      |
|    total_timesteps  | 60144    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.76     |
|    n_updates        | 14785    |
----------------------------------
Eval num_timesteps=60500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 504      |
|    fps              | 153      |
|    time_elapsed     | 394      |
|    total_timesteps  | 60640    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00657  |
|    n_updates        | 14909    |
----------------------------------
Eval num_timesteps=61000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00733  |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 508      |
|    fps              | 153      |
|    time_elapsed     | 397      |
|    total_timesteps  | 61128    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0123   |
|    n_updates        | 15031    |
----------------------------------
Eval num_timesteps=61500, episode_reward=753.44 +/- 468.83
Episode length: 212.36 +/- 114.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 753      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 15124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 512      |
|    fps              | 152      |
|    time_elapsed     | 403      |
|    total_timesteps  | 61584    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 15145    |
----------------------------------
Eval num_timesteps=62000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.79     |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 516      |
|    fps              | 152      |
|    time_elapsed     | 406      |
|    total_timesteps  | 62080    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00747  |
|    n_updates        | 15269    |
----------------------------------
Eval num_timesteps=62500, episode_reward=525.28 +/- 258.72
Episode length: 156.32 +/- 64.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 525      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00891  |
|    n_updates        | 15374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 520      |
|    fps              | 152      |
|    time_elapsed     | 410      |
|    total_timesteps  | 62640    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0114   |
|    n_updates        | 15409    |
----------------------------------
Eval num_timesteps=63000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.79     |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 524      |
|    fps              | 152      |
|    time_elapsed     | 413      |
|    total_timesteps  | 63024    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00892  |
|    n_updates        | 15505    |
----------------------------------
Eval num_timesteps=63500, episode_reward=294.88 +/- 35.41
Episode length: 98.72 +/- 8.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 295      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00951  |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 528      |
|    fps              | 152      |
|    time_elapsed     | 416      |
|    total_timesteps  | 63512    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 15627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 532      |
|    fps              | 153      |
|    time_elapsed     | 417      |
|    total_timesteps  | 63968    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 15741    |
----------------------------------
Eval num_timesteps=64000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00796  |
|    n_updates        | 15749    |
----------------------------------
Eval num_timesteps=64500, episode_reward=321.76 +/- 37.15
Episode length: 105.44 +/- 9.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.78     |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 536      |
|    fps              | 152      |
|    time_elapsed     | 422      |
|    total_timesteps  | 64512    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 15877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 540      |
|    fps              | 153      |
|    time_elapsed     | 423      |
|    total_timesteps  | 64920    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00994  |
|    n_updates        | 15979    |
----------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00882  |
|    n_updates        | 15999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 544      |
|    fps              | 153      |
|    time_elapsed     | 426      |
|    total_timesteps  | 65456    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00839  |
|    n_updates        | 16113    |
----------------------------------
Eval num_timesteps=65500, episode_reward=310.24 +/- 57.13
Episode length: 102.56 +/- 14.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 548      |
|    fps              | 153      |
|    time_elapsed     | 429      |
|    total_timesteps  | 65888    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 16221    |
----------------------------------
Eval num_timesteps=66000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 552      |
|    fps              | 153      |
|    time_elapsed     | 432      |
|    total_timesteps  | 66408    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00663  |
|    n_updates        | 16351    |
----------------------------------
Eval num_timesteps=66500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 16374    |
----------------------------------
Eval num_timesteps=67000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 556      |
|    fps              | 153      |
|    time_elapsed     | 437      |
|    total_timesteps  | 67016    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 16503    |
----------------------------------
Eval num_timesteps=67500, episode_reward=327.52 +/- 38.80
Episode length: 106.88 +/- 9.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00872  |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 560      |
|    fps              | 153      |
|    time_elapsed     | 441      |
|    total_timesteps  | 67592    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 16647    |
----------------------------------
Eval num_timesteps=68000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 68000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 16749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 564      |
|    fps              | 153      |
|    time_elapsed     | 444      |
|    total_timesteps  | 68040    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0064   |
|    n_updates        | 16759    |
----------------------------------
Eval num_timesteps=68500, episode_reward=293.60 +/- 43.99
Episode length: 98.40 +/- 11.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 568      |
|    fps              | 153      |
|    time_elapsed     | 447      |
|    total_timesteps  | 68592    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 16897    |
----------------------------------
Eval num_timesteps=69000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 16999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 572      |
|    fps              | 153      |
|    time_elapsed     | 450      |
|    total_timesteps  | 69112    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00583  |
|    n_updates        | 17027    |
----------------------------------
Eval num_timesteps=69500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00516  |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 576      |
|    fps              | 153      |
|    time_elapsed     | 452      |
|    total_timesteps  | 69624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00604  |
|    n_updates        | 17155    |
----------------------------------
Eval num_timesteps=70000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.78     |
|    n_updates        | 17249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 580      |
|    fps              | 153      |
|    time_elapsed     | 455      |
|    total_timesteps  | 70056    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.256    |
|    n_updates        | 17263    |
----------------------------------
Eval num_timesteps=70500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 584      |
|    fps              | 153      |
|    time_elapsed     | 458      |
|    total_timesteps  | 70592    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00521  |
|    n_updates        | 17397    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 588      |
|    fps              | 154      |
|    time_elapsed     | 459      |
|    total_timesteps  | 70976    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.022    |
|    n_updates        | 17493    |
----------------------------------
Eval num_timesteps=71000, episode_reward=452.96 +/- 182.49
Episode length: 138.24 +/- 45.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 453      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.82     |
|    n_updates        | 17499    |
----------------------------------
Eval num_timesteps=71500, episode_reward=321.76 +/- 44.66
Episode length: 105.44 +/- 11.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0086   |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 592      |
|    fps              | 153      |
|    time_elapsed     | 465      |
|    total_timesteps  | 71504    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.88     |
|    n_updates        | 17625    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 596      |
|    fps              | 154      |
|    time_elapsed     | 466      |
|    total_timesteps  | 71984    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00779  |
|    n_updates        | 17745    |
----------------------------------
Eval num_timesteps=72000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 17749    |
----------------------------------
Eval num_timesteps=72500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 17874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 600      |
|    fps              | 153      |
|    time_elapsed     | 471      |
|    total_timesteps  | 72560    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.81     |
|    n_updates        | 17889    |
----------------------------------
Eval num_timesteps=73000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00909  |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 604      |
|    fps              | 153      |
|    time_elapsed     | 474      |
|    total_timesteps  | 73096    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00831  |
|    n_updates        | 18023    |
----------------------------------
Eval num_timesteps=73500, episode_reward=748.64 +/- 444.52
Episode length: 212.16 +/- 111.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 749      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00804  |
|    n_updates        | 18124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 608      |
|    fps              | 153      |
|    time_elapsed     | 480      |
|    total_timesteps  | 73616    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00739  |
|    n_updates        | 18153    |
----------------------------------
Eval num_timesteps=74000, episode_reward=298.72 +/- 31.49
Episode length: 99.68 +/- 7.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.7     |
|    mean_reward      | 299      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00972  |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 612      |
|    fps              | 153      |
|    time_elapsed     | 483      |
|    total_timesteps  | 74072    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 18267    |
----------------------------------
Eval num_timesteps=74500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0149   |
|    n_updates        | 18374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 616      |
|    fps              | 153      |
|    time_elapsed     | 486      |
|    total_timesteps  | 74632    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.88     |
|    n_updates        | 18407    |
----------------------------------
Eval num_timesteps=75000, episode_reward=1041.28 +/- 640.28
Episode length: 279.82 +/- 151.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 280      |
|    mean_reward      | 1.04e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00992  |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 620      |
|    fps              | 151      |
|    time_elapsed     | 494      |
|    total_timesteps  | 75184    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0081   |
|    n_updates        | 18545    |
----------------------------------
Eval num_timesteps=75500, episode_reward=301.92 +/- 51.26
Episode length: 100.48 +/- 12.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 302      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00743  |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 624      |
|    fps              | 152      |
|    time_elapsed     | 498      |
|    total_timesteps  | 75712    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00873  |
|    n_updates        | 18677    |
----------------------------------
Eval num_timesteps=76000, episode_reward=339.04 +/- 61.07
Episode length: 109.76 +/- 15.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 339      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 628      |
|    fps              | 152      |
|    time_elapsed     | 501      |
|    total_timesteps  | 76192    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00614  |
|    n_updates        | 18797    |
----------------------------------
Eval num_timesteps=76500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 18874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 632      |
|    fps              | 152      |
|    time_elapsed     | 504      |
|    total_timesteps  | 76720    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 18929    |
----------------------------------
Eval num_timesteps=77000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00497  |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 636      |
|    fps              | 152      |
|    time_elapsed     | 507      |
|    total_timesteps  | 77168    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.89     |
|    n_updates        | 19041    |
----------------------------------
Eval num_timesteps=77500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.81     |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 640      |
|    fps              | 152      |
|    time_elapsed     | 510      |
|    total_timesteps  | 77624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00918  |
|    n_updates        | 19155    |
----------------------------------
Eval num_timesteps=78000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 78000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 19249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 644      |
|    fps              | 152      |
|    time_elapsed     | 512      |
|    total_timesteps  | 78112    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00639  |
|    n_updates        | 19277    |
----------------------------------
Eval num_timesteps=78500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.88     |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 648      |
|    fps              | 152      |
|    time_elapsed     | 515      |
|    total_timesteps  | 78704    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00877  |
|    n_updates        | 19425    |
----------------------------------
Eval num_timesteps=79000, episode_reward=314.72 +/- 37.84
Episode length: 103.68 +/- 9.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00917  |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 652      |
|    fps              | 152      |
|    time_elapsed     | 518      |
|    total_timesteps  | 79160    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 19539    |
----------------------------------
Eval num_timesteps=79500, episode_reward=289.76 +/- 22.80
Episode length: 97.44 +/- 5.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.4     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0093   |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 656      |
|    fps              | 152      |
|    time_elapsed     | 521      |
|    total_timesteps  | 79680    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 19669    |
----------------------------------
Eval num_timesteps=80000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00739  |
|    n_updates        | 19749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 660      |
|    fps              | 152      |
|    time_elapsed     | 524      |
|    total_timesteps  | 80112    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.192    |
|    n_updates        | 19777    |
----------------------------------
Eval num_timesteps=80500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00959  |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 664      |
|    fps              | 152      |
|    time_elapsed     | 527      |
|    total_timesteps  | 80648    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.97     |
|    n_updates        | 19911    |
----------------------------------
Eval num_timesteps=81000, episode_reward=355.04 +/- 69.90
Episode length: 113.76 +/- 17.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 355      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 19999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 668      |
|    fps              | 152      |
|    time_elapsed     | 531      |
|    total_timesteps  | 81232    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00864  |
|    n_updates        | 20057    |
----------------------------------
Eval num_timesteps=81500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0116   |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 672      |
|    fps              | 153      |
|    time_elapsed     | 534      |
|    total_timesteps  | 81816    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 20203    |
----------------------------------
Eval num_timesteps=82000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 20249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 676      |
|    fps              | 153      |
|    time_elapsed     | 537      |
|    total_timesteps  | 82288    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0153   |
|    n_updates        | 20321    |
----------------------------------
Eval num_timesteps=82500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2        |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 680      |
|    fps              | 153      |
|    time_elapsed     | 539      |
|    total_timesteps  | 82720    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.93     |
|    n_updates        | 20429    |
----------------------------------
Eval num_timesteps=83000, episode_reward=703.52 +/- 465.34
Episode length: 199.88 +/- 113.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 704      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 20499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 684      |
|    fps              | 152      |
|    time_elapsed     | 545      |
|    total_timesteps  | 83184    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00881  |
|    n_updates        | 20545    |
----------------------------------
Eval num_timesteps=83500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0128   |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 688      |
|    fps              | 152      |
|    time_elapsed     | 548      |
|    total_timesteps  | 83696    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 20673    |
----------------------------------
Eval num_timesteps=84000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 20749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 692      |
|    fps              | 152      |
|    time_elapsed     | 551      |
|    total_timesteps  | 84160    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 20789    |
----------------------------------
Eval num_timesteps=84500, episode_reward=444.64 +/- 222.49
Episode length: 136.16 +/- 55.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 445      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 20874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 696      |
|    fps              | 152      |
|    time_elapsed     | 555      |
|    total_timesteps  | 84656    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00988  |
|    n_updates        | 20913    |
----------------------------------
Eval num_timesteps=85000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.97     |
|    n_updates        | 20999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 700      |
|    fps              | 152      |
|    time_elapsed     | 558      |
|    total_timesteps  | 85112    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 21027    |
----------------------------------
Eval num_timesteps=85500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2        |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 704      |
|    fps              | 152      |
|    time_elapsed     | 561      |
|    total_timesteps  | 85536    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 21133    |
----------------------------------
Eval num_timesteps=86000, episode_reward=399.20 +/- 145.80
Episode length: 124.80 +/- 36.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 399      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 708      |
|    fps              | 152      |
|    time_elapsed     | 565      |
|    total_timesteps  | 86040    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00843  |
|    n_updates        | 21259    |
----------------------------------
Eval num_timesteps=86500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2        |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 712      |
|    fps              | 152      |
|    time_elapsed     | 568      |
|    total_timesteps  | 86528    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00907  |
|    n_updates        | 21381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 716      |
|    fps              | 153      |
|    time_elapsed     | 568      |
|    total_timesteps  | 86984    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.98     |
|    n_updates        | 21495    |
----------------------------------
Eval num_timesteps=87000, episode_reward=302.56 +/- 69.25
Episode length: 100.64 +/- 17.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0114   |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 720      |
|    fps              | 152      |
|    time_elapsed     | 571      |
|    total_timesteps  | 87392    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0106   |
|    n_updates        | 21597    |
----------------------------------
Eval num_timesteps=87500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.97     |
|    n_updates        | 21624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 724      |
|    fps              | 152      |
|    time_elapsed     | 574      |
|    total_timesteps  | 87848    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.97     |
|    n_updates        | 21711    |
----------------------------------
Eval num_timesteps=88000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 21749    |
----------------------------------
Eval num_timesteps=88500, episode_reward=285.28 +/- 6.27
Episode length: 96.32 +/- 1.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00847  |
|    n_updates        | 21874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 728      |
|    fps              | 152      |
|    time_elapsed     | 580      |
|    total_timesteps  | 88568    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 21891    |
----------------------------------
Eval num_timesteps=89000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00702  |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 732      |
|    fps              | 152      |
|    time_elapsed     | 582      |
|    total_timesteps  | 89048    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00951  |
|    n_updates        | 22011    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 736      |
|    fps              | 153      |
|    time_elapsed     | 583      |
|    total_timesteps  | 89488    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 22121    |
----------------------------------
Eval num_timesteps=89500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 22124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 740      |
|    fps              | 153      |
|    time_elapsed     | 586      |
|    total_timesteps  | 89920    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 22229    |
----------------------------------
Eval num_timesteps=90000, episode_reward=390.88 +/- 126.00
Episode length: 122.72 +/- 31.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 391      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00799  |
|    n_updates        | 22249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 744      |
|    fps              | 153      |
|    time_elapsed     | 589      |
|    total_timesteps  | 90400    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 22349    |
----------------------------------
Eval num_timesteps=90500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0181   |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 748      |
|    fps              | 153      |
|    time_elapsed     | 592      |
|    total_timesteps  | 90872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.07     |
|    n_updates        | 22467    |
----------------------------------
Eval num_timesteps=91000, episode_reward=310.24 +/- 31.80
Episode length: 102.56 +/- 7.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 752      |
|    fps              | 153      |
|    time_elapsed     | 595      |
|    total_timesteps  | 91440    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 22609    |
----------------------------------
Eval num_timesteps=91500, episode_reward=287.20 +/- 14.66
Episode length: 96.80 +/- 3.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.8     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.014    |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 756      |
|    fps              | 153      |
|    time_elapsed     | 598      |
|    total_timesteps  | 91872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 22717    |
----------------------------------
Eval num_timesteps=92000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00968  |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 760      |
|    fps              | 153      |
|    time_elapsed     | 601      |
|    total_timesteps  | 92304    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00784  |
|    n_updates        | 22825    |
----------------------------------
Eval num_timesteps=92500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0101   |
|    n_updates        | 22874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 764      |
|    fps              | 153      |
|    time_elapsed     | 604      |
|    total_timesteps  | 92816    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 22953    |
----------------------------------
Eval num_timesteps=93000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 22999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 768      |
|    fps              | 153      |
|    time_elapsed     | 607      |
|    total_timesteps  | 93296    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 23073    |
----------------------------------
Eval num_timesteps=93500, episode_reward=308.32 +/- 32.36
Episode length: 102.08 +/- 8.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00561  |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 772      |
|    fps              | 153      |
|    time_elapsed     | 610      |
|    total_timesteps  | 93880    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0142   |
|    n_updates        | 23219    |
----------------------------------
Eval num_timesteps=94000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0102   |
|    n_updates        | 23249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 776      |
|    fps              | 153      |
|    time_elapsed     | 613      |
|    total_timesteps  | 94320    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0258   |
|    n_updates        | 23329    |
----------------------------------
Eval num_timesteps=94500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 94500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 780      |
|    fps              | 153      |
|    time_elapsed     | 616      |
|    total_timesteps  | 94824    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00678  |
|    n_updates        | 23455    |
----------------------------------
Eval num_timesteps=95000, episode_reward=334.56 +/- 118.55
Episode length: 108.64 +/- 29.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 335      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 95000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 784      |
|    fps              | 153      |
|    time_elapsed     | 619      |
|    total_timesteps  | 95344    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0138   |
|    n_updates        | 23585    |
----------------------------------
Eval num_timesteps=95500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 23624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 788      |
|    fps              | 153      |
|    time_elapsed     | 622      |
|    total_timesteps  | 95776    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0234   |
|    n_updates        | 23693    |
----------------------------------
Eval num_timesteps=96000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 792      |
|    fps              | 153      |
|    time_elapsed     | 625      |
|    total_timesteps  | 96296    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0088   |
|    n_updates        | 23823    |
----------------------------------
Eval num_timesteps=96500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0235   |
|    n_updates        | 23874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 796      |
|    fps              | 154      |
|    time_elapsed     | 628      |
|    total_timesteps  | 96824    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 23955    |
----------------------------------
Eval num_timesteps=97000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 23999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 800      |
|    fps              | 154      |
|    time_elapsed     | 631      |
|    total_timesteps  | 97248    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 24061    |
----------------------------------
Eval num_timesteps=97500, episode_reward=310.88 +/- 35.84
Episode length: 102.72 +/- 8.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 804      |
|    fps              | 154      |
|    time_elapsed     | 634      |
|    total_timesteps  | 97736    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 24183    |
----------------------------------
Eval num_timesteps=98000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00986  |
|    n_updates        | 24249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 808      |
|    fps              | 154      |
|    time_elapsed     | 637      |
|    total_timesteps  | 98320    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 24329    |
----------------------------------
Eval num_timesteps=98500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 812      |
|    fps              | 154      |
|    time_elapsed     | 640      |
|    total_timesteps  | 98832    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0101   |
|    n_updates        | 24457    |
----------------------------------
Eval num_timesteps=99000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.05     |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 816      |
|    fps              | 154      |
|    time_elapsed     | 643      |
|    total_timesteps  | 99336    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.016    |
|    n_updates        | 24583    |
----------------------------------
Eval num_timesteps=99500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 820      |
|    fps              | 154      |
|    time_elapsed     | 646      |
|    total_timesteps  | 99768    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.07     |
|    n_updates        | 24691    |
----------------------------------
Eval num_timesteps=100000, episode_reward=360.80 +/- 79.94
Episode length: 115.20 +/- 19.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 361      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0103   |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 824      |
|    fps              | 154      |
|    time_elapsed     | 649      |
|    total_timesteps  | 100192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0224   |
|    n_updates        | 24797    |
----------------------------------
Eval num_timesteps=100500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.15     |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 828      |
|    fps              | 154      |
|    time_elapsed     | 652      |
|    total_timesteps  | 100592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0091   |
|    n_updates        | 24897    |
----------------------------------
Eval num_timesteps=101000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 832      |
|    fps              | 154      |
|    time_elapsed     | 655      |
|    total_timesteps  | 101056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 25013    |
----------------------------------
Eval num_timesteps=101500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0167   |
|    n_updates        | 25124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 836      |
|    fps              | 154      |
|    time_elapsed     | 658      |
|    total_timesteps  | 101560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 25139    |
----------------------------------
Eval num_timesteps=102000, episode_reward=362.08 +/- 90.77
Episode length: 115.52 +/- 22.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 362      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 25249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 840      |
|    fps              | 154      |
|    time_elapsed     | 661      |
|    total_timesteps  | 102072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00981  |
|    n_updates        | 25267    |
----------------------------------
Eval num_timesteps=102500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 844      |
|    fps              | 154      |
|    time_elapsed     | 664      |
|    total_timesteps  | 102536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0125   |
|    n_updates        | 25383    |
----------------------------------
Eval num_timesteps=103000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 848      |
|    fps              | 154      |
|    time_elapsed     | 667      |
|    total_timesteps  | 103024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 25505    |
----------------------------------
Eval num_timesteps=103500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 852      |
|    fps              | 154      |
|    time_elapsed     | 670      |
|    total_timesteps  | 103568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 25641    |
----------------------------------
Eval num_timesteps=104000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 856      |
|    fps              | 154      |
|    time_elapsed     | 673      |
|    total_timesteps  | 104008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 25751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 860      |
|    fps              | 155      |
|    time_elapsed     | 673      |
|    total_timesteps  | 104464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0178   |
|    n_updates        | 25865    |
----------------------------------
Eval num_timesteps=104500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 25874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 864      |
|    fps              | 155      |
|    time_elapsed     | 676      |
|    total_timesteps  | 104920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 25979    |
----------------------------------
Eval num_timesteps=105000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 868      |
|    fps              | 154      |
|    time_elapsed     | 679      |
|    total_timesteps  | 105336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 26083    |
----------------------------------
Eval num_timesteps=105500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 872      |
|    fps              | 155      |
|    time_elapsed     | 682      |
|    total_timesteps  | 105872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00975  |
|    n_updates        | 26217    |
----------------------------------
Eval num_timesteps=106000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00884  |
|    n_updates        | 26249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 876      |
|    fps              | 155      |
|    time_elapsed     | 685      |
|    total_timesteps  | 106424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 26355    |
----------------------------------
Eval num_timesteps=106500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 26374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 880      |
|    fps              | 155      |
|    time_elapsed     | 688      |
|    total_timesteps  | 106928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 26481    |
----------------------------------
Eval num_timesteps=107000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 884      |
|    fps              | 155      |
|    time_elapsed     | 691      |
|    total_timesteps  | 107360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 26589    |
----------------------------------
Eval num_timesteps=107500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 888      |
|    fps              | 155      |
|    time_elapsed     | 694      |
|    total_timesteps  | 107848   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 26711    |
----------------------------------
Eval num_timesteps=108000, episode_reward=596.16 +/- 328.76
Episode length: 173.54 +/- 79.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 596      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 26749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 892      |
|    fps              | 155      |
|    time_elapsed     | 699      |
|    total_timesteps  | 108480   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 26869    |
----------------------------------
Eval num_timesteps=108500, episode_reward=307.04 +/- 33.89
Episode length: 101.76 +/- 8.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0129   |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 896      |
|    fps              | 155      |
|    time_elapsed     | 702      |
|    total_timesteps  | 108912   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 26977    |
----------------------------------
Eval num_timesteps=109000, episode_reward=314.08 +/- 34.71
Episode length: 103.52 +/- 8.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 26999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 900      |
|    fps              | 155      |
|    time_elapsed     | 705      |
|    total_timesteps  | 109344   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00983  |
|    n_updates        | 27085    |
----------------------------------
Eval num_timesteps=109500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.15     |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 904      |
|    fps              | 155      |
|    time_elapsed     | 708      |
|    total_timesteps  | 109800   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 27199    |
----------------------------------
Eval num_timesteps=110000, episode_reward=296.16 +/- 40.42
Episode length: 99.04 +/- 10.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99       |
|    mean_reward      | 296      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 908      |
|    fps              | 155      |
|    time_elapsed     | 711      |
|    total_timesteps  | 110288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0144   |
|    n_updates        | 27321    |
----------------------------------
Eval num_timesteps=110500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.12     |
|    n_updates        | 27374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 912      |
|    fps              | 155      |
|    time_elapsed     | 714      |
|    total_timesteps  | 110744   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 27435    |
----------------------------------
Eval num_timesteps=111000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 27499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 916      |
|    fps              | 155      |
|    time_elapsed     | 717      |
|    total_timesteps  | 111232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 27557    |
----------------------------------
Eval num_timesteps=111500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 920      |
|    fps              | 155      |
|    time_elapsed     | 720      |
|    total_timesteps  | 111632   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 27657    |
----------------------------------
Eval num_timesteps=112000, episode_reward=310.88 +/- 35.26
Episode length: 102.72 +/- 8.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.14     |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 924      |
|    fps              | 155      |
|    time_elapsed     | 723      |
|    total_timesteps  | 112112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0119   |
|    n_updates        | 27777    |
----------------------------------
Eval num_timesteps=112500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0101   |
|    n_updates        | 27874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 928      |
|    fps              | 155      |
|    time_elapsed     | 725      |
|    total_timesteps  | 112544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 27885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 932      |
|    fps              | 155      |
|    time_elapsed     | 726      |
|    total_timesteps  | 112976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.24     |
|    n_updates        | 27993    |
----------------------------------
Eval num_timesteps=113000, episode_reward=311.52 +/- 32.64
Episode length: 102.88 +/- 8.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 27999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 936      |
|    fps              | 155      |
|    time_elapsed     | 729      |
|    total_timesteps  | 113360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 28089    |
----------------------------------
Eval num_timesteps=113500, episode_reward=316.00 +/- 35.63
Episode length: 104.00 +/- 8.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 28124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 940      |
|    fps              | 155      |
|    time_elapsed     | 732      |
|    total_timesteps  | 113880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 28219    |
----------------------------------
Eval num_timesteps=114000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.15     |
|    n_updates        | 28249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 944      |
|    fps              | 155      |
|    time_elapsed     | 735      |
|    total_timesteps  | 114424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.014    |
|    n_updates        | 28355    |
----------------------------------
Eval num_timesteps=114500, episode_reward=737.60 +/- 417.97
Episode length: 208.90 +/- 102.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 738      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00704  |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 948      |
|    fps              | 154      |
|    time_elapsed     | 741      |
|    total_timesteps  | 114856   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00979  |
|    n_updates        | 28463    |
----------------------------------
Eval num_timesteps=115000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 952      |
|    fps              | 154      |
|    time_elapsed     | 744      |
|    total_timesteps  | 115264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 28565    |
----------------------------------
Eval num_timesteps=115500, episode_reward=303.20 +/- 40.98
Episode length: 100.80 +/- 10.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.12     |
|    n_updates        | 28624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 956      |
|    fps              | 154      |
|    time_elapsed     | 747      |
|    total_timesteps  | 115728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 28681    |
----------------------------------
Eval num_timesteps=116000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 28749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 960      |
|    fps              | 154      |
|    time_elapsed     | 750      |
|    total_timesteps  | 116160   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0142   |
|    n_updates        | 28789    |
----------------------------------
Eval num_timesteps=116500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 964      |
|    fps              | 154      |
|    time_elapsed     | 752      |
|    total_timesteps  | 116616   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0219   |
|    n_updates        | 28903    |
----------------------------------
Eval num_timesteps=117000, episode_reward=287.84 +/- 18.81
Episode length: 96.96 +/- 4.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 968      |
|    fps              | 154      |
|    time_elapsed     | 755      |
|    total_timesteps  | 117064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0167   |
|    n_updates        | 29015    |
----------------------------------
Eval num_timesteps=117500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 29124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 972      |
|    fps              | 154      |
|    time_elapsed     | 758      |
|    total_timesteps  | 117544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 29135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 360      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 976      |
|    fps              | 155      |
|    time_elapsed     | 759      |
|    total_timesteps  | 117936   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0123   |
|    n_updates        | 29233    |
----------------------------------
Eval num_timesteps=118000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 360      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 980      |
|    fps              | 155      |
|    time_elapsed     | 762      |
|    total_timesteps  | 118440   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 29359    |
----------------------------------
Eval num_timesteps=118500, episode_reward=312.16 +/- 38.74
Episode length: 103.04 +/- 9.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 29374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 360      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 984      |
|    fps              | 155      |
|    time_elapsed     | 765      |
|    total_timesteps  | 118872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.25     |
|    n_updates        | 29467    |
----------------------------------
Eval num_timesteps=119000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 29499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 988      |
|    fps              | 155      |
|    time_elapsed     | 767      |
|    total_timesteps  | 119304   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.25     |
|    n_updates        | 29575    |
----------------------------------
Eval num_timesteps=119500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 357      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 992      |
|    fps              | 155      |
|    time_elapsed     | 771      |
|    total_timesteps  | 119912   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0166   |
|    n_updates        | 29727    |
----------------------------------
Eval num_timesteps=120000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 29749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 360      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 996      |
|    fps              | 155      |
|    time_elapsed     | 773      |
|    total_timesteps  | 120408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 29851    |
----------------------------------
Eval num_timesteps=120500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 29874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 155      |
|    time_elapsed     | 776      |
|    total_timesteps  | 120824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 29955    |
----------------------------------
Eval num_timesteps=121000, episode_reward=842.24 +/- 601.64
Episode length: 232.06 +/- 143.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 842      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 29999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 154      |
|    time_elapsed     | 783      |
|    total_timesteps  | 121256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 30063    |
----------------------------------
Eval num_timesteps=121500, episode_reward=496.48 +/- 217.67
Episode length: 149.12 +/- 54.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 496      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.48     |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 357      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 154      |
|    time_elapsed     | 787      |
|    total_timesteps  | 121704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 30175    |
----------------------------------
Eval num_timesteps=122000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 30249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 356      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 154      |
|    time_elapsed     | 790      |
|    total_timesteps  | 122136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 30283    |
----------------------------------
Eval num_timesteps=122500, episode_reward=403.04 +/- 152.27
Episode length: 125.76 +/- 38.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 403      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.19     |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 355      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 154      |
|    time_elapsed     | 794      |
|    total_timesteps  | 122600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 30399    |
----------------------------------
Eval num_timesteps=123000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 356      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 154      |
|    time_elapsed     | 797      |
|    total_timesteps  | 123032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 30507    |
----------------------------------
Eval num_timesteps=123500, episode_reward=305.12 +/- 40.78
Episode length: 101.28 +/- 10.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.2      |
|    n_updates        | 30624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 356      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 154      |
|    time_elapsed     | 800      |
|    total_timesteps  | 123520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0352   |
|    n_updates        | 30629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 154      |
|    time_elapsed     | 800      |
|    total_timesteps  | 123984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 30745    |
----------------------------------
Eval num_timesteps=124000, episode_reward=309.60 +/- 35.05
Episode length: 102.40 +/- 8.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 357      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 154      |
|    time_elapsed     | 803      |
|    total_timesteps  | 124392   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 30847    |
----------------------------------
Eval num_timesteps=124500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.33     |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 360      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 154      |
|    time_elapsed     | 806      |
|    total_timesteps  | 124872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 30967    |
----------------------------------
Eval num_timesteps=125000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 30999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 356      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 154      |
|    time_elapsed     | 809      |
|    total_timesteps  | 125280   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 31069    |
----------------------------------
Eval num_timesteps=125500, episode_reward=391.52 +/- 147.03
Episode length: 122.88 +/- 36.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 392      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 154      |
|    time_elapsed     | 812      |
|    total_timesteps  | 125680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.028    |
|    n_updates        | 31169    |
----------------------------------
Eval num_timesteps=126000, episode_reward=317.92 +/- 41.18
Episode length: 104.48 +/- 10.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 351      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 154      |
|    time_elapsed     | 815      |
|    total_timesteps  | 126128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.32     |
|    n_updates        | 31281    |
----------------------------------
Eval num_timesteps=126500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 154      |
|    time_elapsed     | 818      |
|    total_timesteps  | 126584   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0209   |
|    n_updates        | 31395    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 351      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 155      |
|    time_elapsed     | 819      |
|    total_timesteps  | 126992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 31497    |
----------------------------------
Eval num_timesteps=127000, episode_reward=433.12 +/- 202.95
Episode length: 133.28 +/- 50.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 433      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0111   |
|    n_updates        | 31499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 154      |
|    time_elapsed     | 822      |
|    total_timesteps  | 127400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 31599    |
----------------------------------
Eval num_timesteps=127500, episode_reward=477.92 +/- 239.93
Episode length: 144.48 +/- 59.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 478      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 349      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 154      |
|    time_elapsed     | 827      |
|    total_timesteps  | 127832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 31707    |
----------------------------------
Eval num_timesteps=128000, episode_reward=330.72 +/- 45.35
Episode length: 107.68 +/- 11.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 331      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0137   |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 154      |
|    time_elapsed     | 830      |
|    total_timesteps  | 128368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 31841    |
----------------------------------
Eval num_timesteps=128500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 31874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 351      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 154      |
|    time_elapsed     | 833      |
|    total_timesteps  | 128824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0111   |
|    n_updates        | 31955    |
----------------------------------
Eval num_timesteps=129000, episode_reward=305.76 +/- 35.31
Episode length: 101.44 +/- 8.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0188   |
|    n_updates        | 31999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 357      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 154      |
|    time_elapsed     | 836      |
|    total_timesteps  | 129352   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.32     |
|    n_updates        | 32087    |
----------------------------------
Eval num_timesteps=129500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 354      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 154      |
|    time_elapsed     | 839      |
|    total_timesteps  | 129784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00904  |
|    n_updates        | 32195    |
----------------------------------
Eval num_timesteps=130000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 154      |
|    time_elapsed     | 841      |
|    total_timesteps  | 130208   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.016    |
|    n_updates        | 32301    |
----------------------------------
Eval num_timesteps=130500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 32374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 154      |
|    time_elapsed     | 844      |
|    total_timesteps  | 130640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00996  |
|    n_updates        | 32409    |
----------------------------------
Eval num_timesteps=131000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0182   |
|    n_updates        | 32499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 346      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 154      |
|    time_elapsed     | 847      |
|    total_timesteps  | 131064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 32515    |
----------------------------------
Eval num_timesteps=131500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 344      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 154      |
|    time_elapsed     | 850      |
|    total_timesteps  | 131512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 32627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 347      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 155      |
|    time_elapsed     | 851      |
|    total_timesteps  | 131992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0209   |
|    n_updates        | 32747    |
----------------------------------
Eval num_timesteps=132000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.024    |
|    n_updates        | 32749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 349      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 155      |
|    time_elapsed     | 854      |
|    total_timesteps  | 132488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 32871    |
----------------------------------
Eval num_timesteps=132500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 348      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 155      |
|    time_elapsed     | 856      |
|    total_timesteps  | 132912   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.23     |
|    n_updates        | 32977    |
----------------------------------
Eval num_timesteps=133000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0279   |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 354      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 155      |
|    time_elapsed     | 860      |
|    total_timesteps  | 133496   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.42     |
|    n_updates        | 33123    |
----------------------------------
Eval num_timesteps=133500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 355      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 155      |
|    time_elapsed     | 862      |
|    total_timesteps  | 133976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 33243    |
----------------------------------
Eval num_timesteps=134000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 33249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 155      |
|    time_elapsed     | 865      |
|    total_timesteps  | 134480   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0233   |
|    n_updates        | 33369    |
----------------------------------
Eval num_timesteps=134500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00784  |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 354      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 155      |
|    time_elapsed     | 868      |
|    total_timesteps  | 134880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 33469    |
----------------------------------
Eval num_timesteps=135000, episode_reward=319.20 +/- 43.05
Episode length: 104.80 +/- 10.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0134   |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 155      |
|    time_elapsed     | 871      |
|    total_timesteps  | 135424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.41     |
|    n_updates        | 33605    |
----------------------------------
Eval num_timesteps=135500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 33624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 155      |
|    time_elapsed     | 874      |
|    total_timesteps  | 135848   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 33711    |
----------------------------------
Eval num_timesteps=136000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 155      |
|    time_elapsed     | 877      |
|    total_timesteps  | 136328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.01     |
|    n_updates        | 33831    |
----------------------------------
Eval num_timesteps=136500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 33874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 155      |
|    time_elapsed     | 880      |
|    total_timesteps  | 136928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0116   |
|    n_updates        | 33981    |
----------------------------------
Eval num_timesteps=137000, episode_reward=306.40 +/- 28.80
Episode length: 101.60 +/- 7.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 155      |
|    time_elapsed     | 883      |
|    total_timesteps  | 137400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 34099    |
----------------------------------
Eval num_timesteps=137500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 155      |
|    time_elapsed     | 886      |
|    total_timesteps  | 137816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.41     |
|    n_updates        | 34203    |
----------------------------------
Eval num_timesteps=138000, episode_reward=312.16 +/- 36.00
Episode length: 103.04 +/- 9.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 34249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 155      |
|    time_elapsed     | 889      |
|    total_timesteps  | 138272   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 34317    |
----------------------------------
Eval num_timesteps=138500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 34374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 155      |
|    time_elapsed     | 892      |
|    total_timesteps  | 138776   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 34443    |
----------------------------------
Eval num_timesteps=139000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 155      |
|    time_elapsed     | 895      |
|    total_timesteps  | 139248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 34561    |
----------------------------------
Eval num_timesteps=139500, episode_reward=319.20 +/- 37.46
Episode length: 104.80 +/- 9.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 34624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 155      |
|    time_elapsed     | 898      |
|    total_timesteps  | 139736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0299   |
|    n_updates        | 34683    |
----------------------------------
Eval num_timesteps=140000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 155      |
|    time_elapsed     | 901      |
|    total_timesteps  | 140216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.25     |
|    n_updates        | 34803    |
----------------------------------
Eval num_timesteps=140500, episode_reward=437.60 +/- 151.86
Episode length: 134.40 +/- 37.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 438      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00924  |
|    n_updates        | 34874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 155      |
|    time_elapsed     | 905      |
|    total_timesteps  | 140680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 34919    |
----------------------------------
Eval num_timesteps=141000, episode_reward=381.92 +/- 113.66
Episode length: 120.48 +/- 28.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 382      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0144   |
|    n_updates        | 34999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 155      |
|    time_elapsed     | 909      |
|    total_timesteps  | 141112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 35027    |
----------------------------------
Eval num_timesteps=141500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 155      |
|    time_elapsed     | 911      |
|    total_timesteps  | 141584   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.25     |
|    n_updates        | 35145    |
----------------------------------
Eval num_timesteps=142000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0549   |
|    n_updates        | 35249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 155      |
|    time_elapsed     | 914      |
|    total_timesteps  | 142080   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0134   |
|    n_updates        | 35269    |
----------------------------------
Eval num_timesteps=142500, episode_reward=312.16 +/- 36.56
Episode length: 103.04 +/- 9.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 155      |
|    time_elapsed     | 917      |
|    total_timesteps  | 142608   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0194   |
|    n_updates        | 35401    |
----------------------------------
Eval num_timesteps=143000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 35499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 155      |
|    time_elapsed     | 920      |
|    total_timesteps  | 143152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0245   |
|    n_updates        | 35537    |
----------------------------------
Eval num_timesteps=143500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.019    |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 155      |
|    time_elapsed     | 923      |
|    total_timesteps  | 143664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0189   |
|    n_updates        | 35665    |
----------------------------------
Eval num_timesteps=144000, episode_reward=310.24 +/- 35.46
Episode length: 102.56 +/- 8.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 155      |
|    time_elapsed     | 926      |
|    total_timesteps  | 144160   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 35789    |
----------------------------------
Eval num_timesteps=144500, episode_reward=354.40 +/- 79.42
Episode length: 113.60 +/- 19.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 354      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0166   |
|    n_updates        | 35874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 155      |
|    time_elapsed     | 930      |
|    total_timesteps  | 144632   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 35907    |
----------------------------------
Eval num_timesteps=145000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.25     |
|    n_updates        | 35999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 155      |
|    time_elapsed     | 933      |
|    total_timesteps  | 145088   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0223   |
|    n_updates        | 36021    |
----------------------------------
Eval num_timesteps=145500, episode_reward=447.84 +/- 169.89
Episode length: 136.96 +/- 42.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 448      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0178   |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 155      |
|    time_elapsed     | 937      |
|    total_timesteps  | 145568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.52     |
|    n_updates        | 36141    |
----------------------------------
Eval num_timesteps=146000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0245   |
|    n_updates        | 36249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 155      |
|    time_elapsed     | 940      |
|    total_timesteps  | 146096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 36273    |
----------------------------------
Eval num_timesteps=146500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 155      |
|    time_elapsed     | 943      |
|    total_timesteps  | 146552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 36387    |
----------------------------------
Eval num_timesteps=147000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.029    |
|    n_updates        | 36499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 155      |
|    time_elapsed     | 946      |
|    total_timesteps  | 147056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.25     |
|    n_updates        | 36513    |
----------------------------------
Eval num_timesteps=147500, episode_reward=314.72 +/- 37.30
Episode length: 103.68 +/- 9.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0126   |
|    n_updates        | 36624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 155      |
|    time_elapsed     | 949      |
|    total_timesteps  | 147544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 36635    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 155      |
|    time_elapsed     | 949      |
|    total_timesteps  | 147976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 36743    |
----------------------------------
Eval num_timesteps=148000, episode_reward=456.80 +/- 195.80
Episode length: 139.20 +/- 48.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 457      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0215   |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 155      |
|    time_elapsed     | 953      |
|    total_timesteps  | 148432   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.014    |
|    n_updates        | 36857    |
----------------------------------
Eval num_timesteps=148500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 36874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 155      |
|    time_elapsed     | 956      |
|    total_timesteps  | 148896   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00777  |
|    n_updates        | 36973    |
----------------------------------
Eval num_timesteps=149000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 36999    |
----------------------------------
Eval num_timesteps=149500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0141   |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 155      |
|    time_elapsed     | 961      |
|    total_timesteps  | 149528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0316   |
|    n_updates        | 37131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 155      |
|    time_elapsed     | 962      |
|    total_timesteps  | 149960   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0101   |
|    n_updates        | 37239    |
----------------------------------
Eval num_timesteps=150000, episode_reward=319.84 +/- 42.28
Episode length: 104.96 +/- 10.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.27     |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 155      |
|    time_elapsed     | 965      |
|    total_timesteps  | 150488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.32     |
|    n_updates        | 37371    |
----------------------------------
Eval num_timesteps=150500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0325   |
|    n_updates        | 37374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 155      |
|    time_elapsed     | 968      |
|    total_timesteps  | 150968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.29     |
|    n_updates        | 37491    |
----------------------------------
Eval num_timesteps=151000, episode_reward=311.52 +/- 39.46
Episode length: 102.88 +/- 9.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 155      |
|    time_elapsed     | 971      |
|    total_timesteps  | 151384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0198   |
|    n_updates        | 37595    |
----------------------------------
Eval num_timesteps=151500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0339   |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 155      |
|    time_elapsed     | 974      |
|    total_timesteps  | 151912   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.28     |
|    n_updates        | 37727    |
----------------------------------
Eval num_timesteps=152000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 37749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 155      |
|    time_elapsed     | 977      |
|    total_timesteps  | 152344   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 37835    |
----------------------------------
Eval num_timesteps=152500, episode_reward=321.12 +/- 53.68
Episode length: 105.28 +/- 13.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0257   |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 155      |
|    time_elapsed     | 980      |
|    total_timesteps  | 152800   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 37949    |
----------------------------------
Eval num_timesteps=153000, episode_reward=312.16 +/- 35.43
Episode length: 103.04 +/- 8.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.28     |
|    n_updates        | 37999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 155      |
|    time_elapsed     | 983      |
|    total_timesteps  | 153320   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.57     |
|    n_updates        | 38079    |
----------------------------------
Eval num_timesteps=153500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0193   |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 155      |
|    time_elapsed     | 986      |
|    total_timesteps  | 153752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0285   |
|    n_updates        | 38187    |
----------------------------------
Eval num_timesteps=154000, episode_reward=322.40 +/- 36.77
Episode length: 105.60 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 38249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 155      |
|    time_elapsed     | 989      |
|    total_timesteps  | 154248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 38311    |
----------------------------------
Eval num_timesteps=154500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.59     |
|    n_updates        | 38374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 155      |
|    time_elapsed     | 992      |
|    total_timesteps  | 154728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 38431    |
----------------------------------
Eval num_timesteps=155000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 38499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 155      |
|    time_elapsed     | 995      |
|    total_timesteps  | 155160   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 38539    |
----------------------------------
Eval num_timesteps=155500, episode_reward=310.88 +/- 42.63
Episode length: 102.72 +/- 10.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 38624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 155      |
|    time_elapsed     | 998      |
|    total_timesteps  | 155616   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.56     |
|    n_updates        | 38653    |
----------------------------------
Eval num_timesteps=156000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 155      |
|    time_elapsed     | 1000     |
|    total_timesteps  | 156024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 38755    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 156      |
|    time_elapsed     | 1001     |
|    total_timesteps  | 156432   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0254   |
|    n_updates        | 38857    |
----------------------------------
Eval num_timesteps=156500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.31     |
|    n_updates        | 38874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 156      |
|    time_elapsed     | 1004     |
|    total_timesteps  | 156848   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.29     |
|    n_updates        | 38961    |
----------------------------------
Eval num_timesteps=157000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.29     |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 156      |
|    time_elapsed     | 1007     |
|    total_timesteps  | 157408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0298   |
|    n_updates        | 39101    |
----------------------------------
Eval num_timesteps=157500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0209   |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 156      |
|    time_elapsed     | 1010     |
|    total_timesteps  | 157864   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.025    |
|    n_updates        | 39215    |
----------------------------------
Eval num_timesteps=158000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 39249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 156      |
|    time_elapsed     | 1012     |
|    total_timesteps  | 158320   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.29     |
|    n_updates        | 39329    |
----------------------------------
Eval num_timesteps=158500, episode_reward=424.16 +/- 137.10
Episode length: 131.04 +/- 34.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 424      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0358   |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 156      |
|    time_elapsed     | 1016     |
|    total_timesteps  | 158768   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 39441    |
----------------------------------
Eval num_timesteps=159000, episode_reward=466.40 +/- 206.12
Episode length: 141.60 +/- 51.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 466      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 39499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 155      |
|    time_elapsed     | 1020     |
|    total_timesteps  | 159232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0202   |
|    n_updates        | 39557    |
----------------------------------
Eval num_timesteps=159500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 39624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 156      |
|    time_elapsed     | 1023     |
|    total_timesteps  | 159720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.28     |
|    n_updates        | 39679    |
----------------------------------
Eval num_timesteps=160000, episode_reward=404.96 +/- 126.11
Episode length: 126.24 +/- 31.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 405      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.29     |
|    n_updates        | 39749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 155      |
|    time_elapsed     | 1027     |
|    total_timesteps  | 160176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.64     |
|    n_updates        | 39793    |
----------------------------------
Eval num_timesteps=160500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.32     |
|    n_updates        | 39874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 155      |
|    time_elapsed     | 1030     |
|    total_timesteps  | 160632   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0167   |
|    n_updates        | 39907    |
----------------------------------
Eval num_timesteps=161000, episode_reward=317.92 +/- 38.62
Episode length: 104.48 +/- 9.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 155      |
|    time_elapsed     | 1033     |
|    total_timesteps  | 161160   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.32     |
|    n_updates        | 40039    |
----------------------------------
Eval num_timesteps=161500, episode_reward=381.28 +/- 100.17
Episode length: 120.32 +/- 25.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 381      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.32     |
|    n_updates        | 40124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 155      |
|    time_elapsed     | 1037     |
|    total_timesteps  | 161648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 40161    |
----------------------------------
Eval num_timesteps=162000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 155      |
|    time_elapsed     | 1039     |
|    total_timesteps  | 162192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0138   |
|    n_updates        | 40297    |
----------------------------------
Eval num_timesteps=162500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.34     |
|    n_updates        | 40374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 155      |
|    time_elapsed     | 1042     |
|    total_timesteps  | 162664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0272   |
|    n_updates        | 40415    |
----------------------------------
Eval num_timesteps=163000, episode_reward=286.56 +/- 17.92
Episode length: 96.64 +/- 4.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.32     |
|    n_updates        | 40499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 155      |
|    time_elapsed     | 1045     |
|    total_timesteps  | 163144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.64     |
|    n_updates        | 40535    |
----------------------------------
Eval num_timesteps=163500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 40624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 155      |
|    time_elapsed     | 1048     |
|    total_timesteps  | 163600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 40649    |
----------------------------------
Eval num_timesteps=164000, episode_reward=323.04 +/- 43.07
Episode length: 105.76 +/- 10.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 323      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0186   |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 156      |
|    time_elapsed     | 1051     |
|    total_timesteps  | 164128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0208   |
|    n_updates        | 40781    |
----------------------------------
Eval num_timesteps=164500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.34     |
|    n_updates        | 40874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 156      |
|    time_elapsed     | 1054     |
|    total_timesteps  | 164616   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 40903    |
----------------------------------
Eval num_timesteps=165000, episode_reward=314.08 +/- 38.08
Episode length: 103.52 +/- 9.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.32     |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 156      |
|    time_elapsed     | 1057     |
|    total_timesteps  | 165128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.021    |
|    n_updates        | 41031    |
----------------------------------
Eval num_timesteps=165500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 41124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 156      |
|    time_elapsed     | 1060     |
|    total_timesteps  | 165544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0223   |
|    n_updates        | 41135    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 156      |
|    time_elapsed     | 1061     |
|    total_timesteps  | 165976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.64     |
|    n_updates        | 41243    |
----------------------------------
Eval num_timesteps=166000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 41249    |
----------------------------------
Eval num_timesteps=166500, episode_reward=353.76 +/- 95.29
Episode length: 113.44 +/- 23.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 354      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0235   |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 156      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 166512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 41377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 156      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 166968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 41491    |
----------------------------------
Eval num_timesteps=167000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 156      |
|    time_elapsed     | 1070     |
|    total_timesteps  | 167464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 41615    |
----------------------------------
Eval num_timesteps=167500, episode_reward=444.64 +/- 236.41
Episode length: 136.16 +/- 59.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 445      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 41624    |
----------------------------------
Eval num_timesteps=168000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 156      |
|    time_elapsed     | 1076     |
|    total_timesteps  | 168000   |
----------------------------------
Eval num_timesteps=168500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0223   |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 156      |
|    time_elapsed     | 1079     |
|    total_timesteps  | 168504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 41875    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 156      |
|    time_elapsed     | 1080     |
|    total_timesteps  | 168968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.34     |
|    n_updates        | 41991    |
----------------------------------
Eval num_timesteps=169000, episode_reward=293.60 +/- 38.53
Episode length: 98.40 +/- 9.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.34     |
|    n_updates        | 41999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 156      |
|    time_elapsed     | 1083     |
|    total_timesteps  | 169400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0144   |
|    n_updates        | 42099    |
----------------------------------
Eval num_timesteps=169500, episode_reward=354.40 +/- 83.20
Episode length: 113.60 +/- 20.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 354      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 156      |
|    time_elapsed     | 1086     |
|    total_timesteps  | 169832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 42207    |
----------------------------------
Eval num_timesteps=170000, episode_reward=312.16 +/- 40.29
Episode length: 103.04 +/- 10.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.64     |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 156      |
|    time_elapsed     | 1089     |
|    total_timesteps  | 170288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 42321    |
----------------------------------
Eval num_timesteps=170500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0375   |
|    n_updates        | 42374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 156      |
|    time_elapsed     | 1092     |
|    total_timesteps  | 170784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 42445    |
----------------------------------
Eval num_timesteps=171000, episode_reward=294.88 +/- 29.75
Episode length: 98.72 +/- 7.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 295      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.36     |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 156      |
|    time_elapsed     | 1095     |
|    total_timesteps  | 171240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.71     |
|    n_updates        | 42559    |
----------------------------------
Eval num_timesteps=171500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.37     |
|    n_updates        | 42624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 156      |
|    time_elapsed     | 1098     |
|    total_timesteps  | 171648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0177   |
|    n_updates        | 42661    |
----------------------------------
Eval num_timesteps=172000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.35     |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 156      |
|    time_elapsed     | 1101     |
|    total_timesteps  | 172168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 42791    |
----------------------------------
Eval num_timesteps=172500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0141   |
|    n_updates        | 42874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 156      |
|    time_elapsed     | 1104     |
|    total_timesteps  | 172688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0209   |
|    n_updates        | 42921    |
----------------------------------
Eval num_timesteps=173000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0144   |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 156      |
|    time_elapsed     | 1107     |
|    total_timesteps  | 173184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.71     |
|    n_updates        | 43045    |
----------------------------------
Eval num_timesteps=173500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.36     |
|    n_updates        | 43124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 156      |
|    time_elapsed     | 1109     |
|    total_timesteps  | 173664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.38     |
|    n_updates        | 43165    |
----------------------------------
Eval num_timesteps=174000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0367   |
|    n_updates        | 43249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 156      |
|    time_elapsed     | 1112     |
|    total_timesteps  | 174144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00972  |
|    n_updates        | 43285    |
----------------------------------
Eval num_timesteps=174500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.38     |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 156      |
|    time_elapsed     | 1115     |
|    total_timesteps  | 174600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0385   |
|    n_updates        | 43399    |
----------------------------------
Eval num_timesteps=175000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0155   |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 156      |
|    time_elapsed     | 1118     |
|    total_timesteps  | 175120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 43529    |
----------------------------------
Eval num_timesteps=175500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0704   |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 156      |
|    time_elapsed     | 1121     |
|    total_timesteps  | 175592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0182   |
|    n_updates        | 43647    |
----------------------------------
Eval num_timesteps=176000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.37     |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 156      |
|    time_elapsed     | 1124     |
|    total_timesteps  | 176096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 43773    |
----------------------------------
Eval num_timesteps=176500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.36     |
|    n_updates        | 43874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 156      |
|    time_elapsed     | 1127     |
|    total_timesteps  | 176528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 43881    |
----------------------------------
Eval num_timesteps=177000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0235   |
|    n_updates        | 43999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 156      |
|    time_elapsed     | 1130     |
|    total_timesteps  | 177152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0191   |
|    n_updates        | 44037    |
----------------------------------
Eval num_timesteps=177500, episode_reward=556.00 +/- 201.60
Episode length: 164.00 +/- 50.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 556      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 44124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 156      |
|    time_elapsed     | 1135     |
|    total_timesteps  | 177608   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0286   |
|    n_updates        | 44151    |
----------------------------------
Eval num_timesteps=178000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0234   |
|    n_updates        | 44249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 156      |
|    time_elapsed     | 1138     |
|    total_timesteps  | 178208   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.71     |
|    n_updates        | 44301    |
----------------------------------
Eval num_timesteps=178500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.37     |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 156      |
|    time_elapsed     | 1141     |
|    total_timesteps  | 178736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.71     |
|    n_updates        | 44433    |
----------------------------------
Eval num_timesteps=179000, episode_reward=326.24 +/- 69.06
Episode length: 106.56 +/- 17.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 44499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 156      |
|    time_elapsed     | 1144     |
|    total_timesteps  | 179248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.72     |
|    n_updates        | 44561    |
----------------------------------
Eval num_timesteps=179500, episode_reward=1304.00 +/- 600.14
Episode length: 344.50 +/- 141.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 344      |
|    mean_reward      | 1.3e+03  |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.37     |
|    n_updates        | 44624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 155      |
|    time_elapsed     | 1154     |
|    total_timesteps  | 179656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.73     |
|    n_updates        | 44663    |
----------------------------------
Eval num_timesteps=180000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.72     |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 155      |
|    time_elapsed     | 1157     |
|    total_timesteps  | 180184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.79     |
|    n_updates        | 44795    |
----------------------------------
Eval num_timesteps=180500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0167   |
|    n_updates        | 44874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 155      |
|    time_elapsed     | 1159     |
|    total_timesteps  | 180664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.47     |
|    n_updates        | 44915    |
----------------------------------
Eval num_timesteps=181000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 44999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 155      |
|    time_elapsed     | 1162     |
|    total_timesteps  | 181088   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0445   |
|    n_updates        | 45021    |
----------------------------------
Eval num_timesteps=181500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.16     |
|    n_updates        | 45124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 155      |
|    time_elapsed     | 1165     |
|    total_timesteps  | 181528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.011    |
|    n_updates        | 45131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 156      |
|    time_elapsed     | 1166     |
|    total_timesteps  | 181992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0438   |
|    n_updates        | 45247    |
----------------------------------
Eval num_timesteps=182000, episode_reward=322.40 +/- 39.97
Episode length: 105.60 +/- 9.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 156      |
|    time_elapsed     | 1169     |
|    total_timesteps  | 182496   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.39     |
|    n_updates        | 45373    |
----------------------------------
Eval num_timesteps=182500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 45374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 156      |
|    time_elapsed     | 1172     |
|    total_timesteps  | 182976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.42     |
|    n_updates        | 45493    |
----------------------------------
Eval num_timesteps=183000, episode_reward=403.68 +/- 143.22
Episode length: 125.92 +/- 35.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 404      |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.38     |
|    n_updates        | 45499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 156      |
|    time_elapsed     | 1175     |
|    total_timesteps  | 183448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0266   |
|    n_updates        | 45611    |
----------------------------------
Eval num_timesteps=183500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0189   |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 156      |
|    time_elapsed     | 1178     |
|    total_timesteps  | 183928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0327   |
|    n_updates        | 45731    |
----------------------------------
Eval num_timesteps=184000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0172   |
|    n_updates        | 45749    |
----------------------------------
Eval num_timesteps=184500, episode_reward=371.04 +/- 89.61
Episode length: 117.76 +/- 22.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 371      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 45874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 155      |
|    time_elapsed     | 1185     |
|    total_timesteps  | 184664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 45915    |
----------------------------------
Eval num_timesteps=185000, episode_reward=388.32 +/- 95.96
Episode length: 122.08 +/- 23.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 388      |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.77     |
|    n_updates        | 45999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 155      |
|    time_elapsed     | 1188     |
|    total_timesteps  | 185120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 46029    |
----------------------------------
Eval num_timesteps=185500, episode_reward=700.00 +/- 297.72
Episode length: 200.00 +/- 74.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 700      |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.42     |
|    n_updates        | 46124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 155      |
|    time_elapsed     | 1194     |
|    total_timesteps  | 185528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0182   |
|    n_updates        | 46131    |
----------------------------------
Eval num_timesteps=186000, episode_reward=396.00 +/- 191.06
Episode length: 124.00 +/- 47.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 396      |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0583   |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 155      |
|    time_elapsed     | 1197     |
|    total_timesteps  | 186024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 46255    |
----------------------------------
Eval num_timesteps=186500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 155      |
|    time_elapsed     | 1200     |
|    total_timesteps  | 186528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0198   |
|    n_updates        | 46381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 155      |
|    time_elapsed     | 1201     |
|    total_timesteps  | 186936   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 46483    |
----------------------------------
Eval num_timesteps=187000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 46499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 155      |
|    time_elapsed     | 1204     |
|    total_timesteps  | 187416   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 46603    |
----------------------------------
Eval num_timesteps=187500, episode_reward=310.88 +/- 52.53
Episode length: 102.72 +/- 13.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.79     |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 155      |
|    time_elapsed     | 1207     |
|    total_timesteps  | 187832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.41     |
|    n_updates        | 46707    |
----------------------------------
Eval num_timesteps=188000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0311   |
|    n_updates        | 46749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 155      |
|    time_elapsed     | 1210     |
|    total_timesteps  | 188288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.024    |
|    n_updates        | 46821    |
----------------------------------
Eval num_timesteps=188500, episode_reward=317.28 +/- 36.18
Episode length: 104.32 +/- 9.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 155      |
|    time_elapsed     | 1213     |
|    total_timesteps  | 188720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.77     |
|    n_updates        | 46929    |
----------------------------------
Eval num_timesteps=189000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0166   |
|    n_updates        | 46999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 155      |
|    time_elapsed     | 1216     |
|    total_timesteps  | 189224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.78     |
|    n_updates        | 47055    |
----------------------------------
Eval num_timesteps=189500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0198   |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 155      |
|    time_elapsed     | 1218     |
|    total_timesteps  | 189776   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0233   |
|    n_updates        | 47193    |
----------------------------------
Eval num_timesteps=190000, episode_reward=340.96 +/- 45.83
Episode length: 110.24 +/- 11.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 341      |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.77     |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 155      |
|    time_elapsed     | 1222     |
|    total_timesteps  | 190336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0177   |
|    n_updates        | 47333    |
----------------------------------
Eval num_timesteps=190500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.84     |
|    n_updates        | 47374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 155      |
|    time_elapsed     | 1225     |
|    total_timesteps  | 190784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 47445    |
----------------------------------
Eval num_timesteps=191000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0158   |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 155      |
|    time_elapsed     | 1228     |
|    total_timesteps  | 191256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0137   |
|    n_updates        | 47563    |
----------------------------------
Eval num_timesteps=191500, episode_reward=339.68 +/- 111.37
Episode length: 109.92 +/- 27.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 340      |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0304   |
|    n_updates        | 47624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 155      |
|    time_elapsed     | 1231     |
|    total_timesteps  | 191760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0243   |
|    n_updates        | 47689    |
----------------------------------
Eval num_timesteps=192000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0188   |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 155      |
|    time_elapsed     | 1234     |
|    total_timesteps  | 192280   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0289   |
|    n_updates        | 47819    |
----------------------------------
Eval num_timesteps=192500, episode_reward=312.80 +/- 34.61
Episode length: 103.20 +/- 8.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.44     |
|    n_updates        | 47874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 155      |
|    time_elapsed     | 1237     |
|    total_timesteps  | 192696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.018    |
|    n_updates        | 47923    |
----------------------------------
Eval num_timesteps=193000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0299   |
|    n_updates        | 47999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 155      |
|    time_elapsed     | 1240     |
|    total_timesteps  | 193240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 48059    |
----------------------------------
Eval num_timesteps=193500, episode_reward=317.92 +/- 38.08
Episode length: 104.48 +/- 9.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0256   |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 155      |
|    time_elapsed     | 1243     |
|    total_timesteps  | 193840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 48209    |
----------------------------------
Eval num_timesteps=194000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.98     |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 48249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 155      |
|    time_elapsed     | 1246     |
|    total_timesteps  | 194280   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.44     |
|    n_updates        | 48319    |
----------------------------------
Eval num_timesteps=194500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0318   |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 155      |
|    time_elapsed     | 1249     |
|    total_timesteps  | 194688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0245   |
|    n_updates        | 48421    |
----------------------------------
Eval num_timesteps=195000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 48499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 155      |
|    time_elapsed     | 1252     |
|    total_timesteps  | 195096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.84     |
|    n_updates        | 48523    |
----------------------------------
Eval num_timesteps=195500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.977    |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.25     |
|    n_updates        | 48624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 155      |
|    time_elapsed     | 1255     |
|    total_timesteps  | 195544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 48635    |
----------------------------------
Eval num_timesteps=196000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 48749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 155      |
|    time_elapsed     | 1258     |
|    total_timesteps  | 196080   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 48769    |
----------------------------------
Eval num_timesteps=196500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.44     |
|    n_updates        | 48874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 155      |
|    time_elapsed     | 1261     |
|    total_timesteps  | 196536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.85     |
|    n_updates        | 48883    |
----------------------------------
Eval num_timesteps=197000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.974    |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.44     |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 155      |
|    time_elapsed     | 1264     |
|    total_timesteps  | 197072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 49017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 156      |
|    time_elapsed     | 1264     |
|    total_timesteps  | 197480   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 49119    |
----------------------------------
Eval num_timesteps=197500, episode_reward=321.12 +/- 38.59
Episode length: 105.28 +/- 9.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 49124    |
----------------------------------
Eval num_timesteps=198000, episode_reward=312.80 +/- 35.78
Episode length: 103.20 +/- 8.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.972    |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0116   |
|    n_updates        | 49249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 155      |
|    time_elapsed     | 1270     |
|    total_timesteps  | 198048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 49261    |
----------------------------------
Eval num_timesteps=198500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.971    |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0779   |
|    n_updates        | 49374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 155      |
|    time_elapsed     | 1273     |
|    total_timesteps  | 198640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0435   |
|    n_updates        | 49409    |
----------------------------------
Eval num_timesteps=199000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.44     |
|    n_updates        | 49499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 156      |
|    time_elapsed     | 1276     |
|    total_timesteps  | 199192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.42     |
|    n_updates        | 49547    |
----------------------------------
Eval num_timesteps=199500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.968    |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0219   |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 156      |
|    time_elapsed     | 1279     |
|    total_timesteps  | 199648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.42     |
|    n_updates        | 49661    |
----------------------------------
Eval num_timesteps=200000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 156      |
|    time_elapsed     | 1282     |
|    total_timesteps  | 200120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0413   |
|    n_updates        | 49779    |
----------------------------------
Eval num_timesteps=200500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.966    |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0195   |
|    n_updates        | 49874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 156      |
|    time_elapsed     | 1285     |
|    total_timesteps  | 200592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0169   |
|    n_updates        | 49897    |
----------------------------------
Eval num_timesteps=201000, episode_reward=310.24 +/- 36.59
Episode length: 102.56 +/- 9.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.965    |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.91     |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 156      |
|    time_elapsed     | 1288     |
|    total_timesteps  | 201072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 50017    |
----------------------------------
Eval num_timesteps=201500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0259   |
|    n_updates        | 50124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 156      |
|    time_elapsed     | 1291     |
|    total_timesteps  | 201560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0199   |
|    n_updates        | 50139    |
----------------------------------
Eval num_timesteps=202000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 156      |
|    time_elapsed     | 1294     |
|    total_timesteps  | 202040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.9      |
|    n_updates        | 50259    |
----------------------------------
Eval num_timesteps=202500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0244   |
|    n_updates        | 50374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 156      |
|    time_elapsed     | 1297     |
|    total_timesteps  | 202592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 50397    |
----------------------------------
Eval num_timesteps=203000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.96     |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 156      |
|    time_elapsed     | 1300     |
|    total_timesteps  | 203032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.021    |
|    n_updates        | 50507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 156      |
|    time_elapsed     | 1300     |
|    total_timesteps  | 203456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 50613    |
----------------------------------
Eval num_timesteps=203500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.958    |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0265   |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 156      |
|    time_elapsed     | 1303     |
|    total_timesteps  | 203920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0215   |
|    n_updates        | 50729    |
----------------------------------
Eval num_timesteps=204000, episode_reward=342.24 +/- 59.24
Episode length: 110.56 +/- 14.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 342      |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.47     |
|    n_updates        | 50749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 156      |
|    time_elapsed     | 1306     |
|    total_timesteps  | 204376   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.47     |
|    n_updates        | 50843    |
----------------------------------
Eval num_timesteps=204500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.956    |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.9      |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 156      |
|    time_elapsed     | 1309     |
|    total_timesteps  | 204792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0259   |
|    n_updates        | 50947    |
----------------------------------
Eval num_timesteps=205000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.955    |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0239   |
|    n_updates        | 50999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 156      |
|    time_elapsed     | 1312     |
|    total_timesteps  | 205248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.47     |
|    n_updates        | 51061    |
----------------------------------
Eval num_timesteps=205500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.953    |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 51124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 156      |
|    time_elapsed     | 1315     |
|    total_timesteps  | 205752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 51187    |
----------------------------------
Eval num_timesteps=206000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.952    |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.47     |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 156      |
|    time_elapsed     | 1318     |
|    total_timesteps  | 206240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 51309    |
----------------------------------
Eval num_timesteps=206500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.951    |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0191   |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 156      |
|    time_elapsed     | 1321     |
|    total_timesteps  | 206680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.89     |
|    n_updates        | 51419    |
----------------------------------
Eval num_timesteps=207000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.949    |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 51499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 156      |
|    time_elapsed     | 1324     |
|    total_timesteps  | 207224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0261   |
|    n_updates        | 51555    |
----------------------------------
Eval num_timesteps=207500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.948    |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0196   |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 156      |
|    time_elapsed     | 1327     |
|    total_timesteps  | 207664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0186   |
|    n_updates        | 51665    |
----------------------------------
Eval num_timesteps=208000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.946    |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.92     |
|    n_updates        | 51749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 156      |
|    time_elapsed     | 1330     |
|    total_timesteps  | 208152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 51787    |
----------------------------------
Eval num_timesteps=208500, episode_reward=322.40 +/- 38.93
Episode length: 105.60 +/- 9.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 0.945    |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.48     |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 156      |
|    time_elapsed     | 1333     |
|    total_timesteps  | 208608   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 51901    |
----------------------------------
Eval num_timesteps=209000, episode_reward=330.72 +/- 46.24
Episode length: 107.68 +/- 11.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 331      |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.47     |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 156      |
|    time_elapsed     | 1336     |
|    total_timesteps  | 209064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0333   |
|    n_updates        | 52015    |
----------------------------------
Eval num_timesteps=209500, episode_reward=529.12 +/- 235.20
Episode length: 157.28 +/- 58.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 157      |
|    mean_reward      | 529      |
| rollout/            |          |
|    exploration_rate | 0.942    |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0169   |
|    n_updates        | 52124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 156      |
|    time_elapsed     | 1341     |
|    total_timesteps  | 209688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 52171    |
----------------------------------
Eval num_timesteps=210000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 52249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 156      |
|    time_elapsed     | 1343     |
|    total_timesteps  | 210144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0229   |
|    n_updates        | 52285    |
----------------------------------
Eval num_timesteps=210500, episode_reward=289.12 +/- 19.58
Episode length: 97.28 +/- 4.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 289      |
| rollout/            |          |
|    exploration_rate | 0.939    |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 156      |
|    time_elapsed     | 1346     |
|    total_timesteps  | 210696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 52423    |
----------------------------------
Eval num_timesteps=211000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.938    |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0203   |
|    n_updates        | 52499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 156      |
|    time_elapsed     | 1349     |
|    total_timesteps  | 211176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0188   |
|    n_updates        | 52543    |
----------------------------------
Eval num_timesteps=211500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.936    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 156      |
|    time_elapsed     | 1352     |
|    total_timesteps  | 211584   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0226   |
|    n_updates        | 52645    |
----------------------------------
Eval num_timesteps=212000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 52749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 156      |
|    time_elapsed     | 1355     |
|    total_timesteps  | 212088   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0137   |
|    n_updates        | 52771    |
----------------------------------
Eval num_timesteps=212500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0182   |
|    n_updates        | 52874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 156      |
|    time_elapsed     | 1358     |
|    total_timesteps  | 212672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0263   |
|    n_updates        | 52917    |
----------------------------------
Eval num_timesteps=213000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.932    |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 156      |
|    time_elapsed     | 1361     |
|    total_timesteps  | 213216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0229   |
|    n_updates        | 53053    |
----------------------------------
Eval num_timesteps=213500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.93     |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.039    |
|    n_updates        | 53124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 156      |
|    time_elapsed     | 1364     |
|    total_timesteps  | 213688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0375   |
|    n_updates        | 53171    |
----------------------------------
Eval num_timesteps=214000, episode_reward=294.88 +/- 29.75
Episode length: 98.72 +/- 7.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 295      |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0138   |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 156      |
|    time_elapsed     | 1367     |
|    total_timesteps  | 214224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.5      |
|    n_updates        | 53305    |
----------------------------------
Eval num_timesteps=214500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.927    |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.5      |
|    n_updates        | 53374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 156      |
|    time_elapsed     | 1370     |
|    total_timesteps  | 214680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.96     |
|    n_updates        | 53419    |
----------------------------------
Eval num_timesteps=215000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.926    |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.96     |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 156      |
|    time_elapsed     | 1373     |
|    total_timesteps  | 215168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 53541    |
----------------------------------
Eval num_timesteps=215500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.924    |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0167   |
|    n_updates        | 53624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 156      |
|    time_elapsed     | 1376     |
|    total_timesteps  | 215680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0216   |
|    n_updates        | 53669    |
----------------------------------
Eval num_timesteps=216000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.923    |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 53749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 156      |
|    time_elapsed     | 1379     |
|    total_timesteps  | 216136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.51     |
|    n_updates        | 53783    |
----------------------------------
Eval num_timesteps=216500, episode_reward=321.12 +/- 48.89
Episode length: 105.28 +/- 12.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.03     |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 156      |
|    time_elapsed     | 1382     |
|    total_timesteps  | 216672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 53917    |
----------------------------------
Eval num_timesteps=217000, episode_reward=307.04 +/- 34.49
Episode length: 101.76 +/- 8.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.92     |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 53999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 156      |
|    time_elapsed     | 1385     |
|    total_timesteps  | 217128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 54031    |
----------------------------------
Eval num_timesteps=217500, episode_reward=317.28 +/- 42.91
Episode length: 104.32 +/- 10.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.918    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 54124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 156      |
|    time_elapsed     | 1388     |
|    total_timesteps  | 217576   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 54143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 156      |
|    time_elapsed     | 1389     |
|    total_timesteps  | 217976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0316   |
|    n_updates        | 54243    |
----------------------------------
Eval num_timesteps=218000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.916    |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0261   |
|    n_updates        | 54249    |
----------------------------------
Eval num_timesteps=218500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.915    |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0143   |
|    n_updates        | 54374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 156      |
|    time_elapsed     | 1394     |
|    total_timesteps  | 218520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.96     |
|    n_updates        | 54379    |
----------------------------------
Eval num_timesteps=219000, episode_reward=357.60 +/- 81.02
Episode length: 114.40 +/- 20.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 358      |
| rollout/            |          |
|    exploration_rate | 0.913    |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.913    |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 156      |
|    time_elapsed     | 1398     |
|    total_timesteps  | 219056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 54513    |
----------------------------------
Eval num_timesteps=219500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.911    |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.51     |
|    n_updates        | 54624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 156      |
|    time_elapsed     | 1401     |
|    total_timesteps  | 219536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0172   |
|    n_updates        | 54633    |
----------------------------------
Eval num_timesteps=220000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.91     |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 156      |
|    time_elapsed     | 1404     |
|    total_timesteps  | 220008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.3      |
|    n_updates        | 54751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 156      |
|    time_elapsed     | 1404     |
|    total_timesteps  | 220448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 54861    |
----------------------------------
Eval num_timesteps=220500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.908    |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.01     |
|    n_updates        | 54874    |
----------------------------------
Eval num_timesteps=221000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.907    |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 156      |
|    time_elapsed     | 1409     |
|    total_timesteps  | 221000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 156      |
|    time_elapsed     | 1410     |
|    total_timesteps  | 221384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 55095    |
----------------------------------
Eval num_timesteps=221500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.905    |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0158   |
|    n_updates        | 55124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 156      |
|    time_elapsed     | 1413     |
|    total_timesteps  | 221792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 55197    |
----------------------------------
Eval num_timesteps=222000, episode_reward=317.92 +/- 38.62
Episode length: 104.48 +/- 9.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.903    |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0166   |
|    n_updates        | 55249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 157      |
|    time_elapsed     | 1416     |
|    total_timesteps  | 222360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 55339    |
----------------------------------
Eval num_timesteps=222500, episode_reward=291.68 +/- 26.04
Episode length: 97.92 +/- 6.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.9     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.901    |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0404   |
|    n_updates        | 55374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 156      |
|    time_elapsed     | 1419     |
|    total_timesteps  | 222816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 55453    |
----------------------------------
Eval num_timesteps=223000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.53     |
|    n_updates        | 55499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 157      |
|    time_elapsed     | 1422     |
|    total_timesteps  | 223344   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 10       |
|    n_updates        | 55585    |
----------------------------------
Eval num_timesteps=223500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0285   |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.897    |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 157      |
|    time_elapsed     | 1425     |
|    total_timesteps  | 223800   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.02     |
|    n_updates        | 55699    |
----------------------------------
Eval num_timesteps=224000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.03     |
|    n_updates        | 55749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 157      |
|    time_elapsed     | 1428     |
|    total_timesteps  | 224296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.05     |
|    n_updates        | 55823    |
----------------------------------
Eval num_timesteps=224500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.895    |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.56     |
|    n_updates        | 55874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 157      |
|    time_elapsed     | 1431     |
|    total_timesteps  | 224736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 55933    |
----------------------------------
Eval num_timesteps=225000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0214   |
|    n_updates        | 55999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 157      |
|    time_elapsed     | 1434     |
|    total_timesteps  | 225216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0357   |
|    n_updates        | 56053    |
----------------------------------
Eval num_timesteps=225500, episode_reward=315.36 +/- 35.91
Episode length: 103.84 +/- 8.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0307   |
|    n_updates        | 56124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 157      |
|    time_elapsed     | 1437     |
|    total_timesteps  | 225640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0571   |
|    n_updates        | 56159    |
----------------------------------
Eval num_timesteps=226000, episode_reward=327.52 +/- 50.70
Episode length: 106.88 +/- 12.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 0.889    |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0224   |
|    n_updates        | 56249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.889    |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 156      |
|    time_elapsed     | 1440     |
|    total_timesteps  | 226080   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 56269    |
----------------------------------
Eval num_timesteps=226500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.887    |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 56374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 156      |
|    time_elapsed     | 1443     |
|    total_timesteps  | 226560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0269   |
|    n_updates        | 56389    |
----------------------------------
Eval num_timesteps=227000, episode_reward=310.88 +/- 34.68
Episode length: 102.72 +/- 8.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 0.886    |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0303   |
|    n_updates        | 56499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 157      |
|    time_elapsed     | 1446     |
|    total_timesteps  | 227128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 56531    |
----------------------------------
Eval num_timesteps=227500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0274   |
|    n_updates        | 56624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 157      |
|    time_elapsed     | 1449     |
|    total_timesteps  | 227656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 56663    |
----------------------------------
Eval num_timesteps=228000, episode_reward=288.48 +/- 15.69
Episode length: 97.12 +/- 3.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.1     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.882    |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.881    |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 157      |
|    time_elapsed     | 1452     |
|    total_timesteps  | 228184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.047    |
|    n_updates        | 56795    |
----------------------------------
Eval num_timesteps=228500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.88     |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.03     |
|    n_updates        | 56874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 157      |
|    time_elapsed     | 1455     |
|    total_timesteps  | 228640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0228   |
|    n_updates        | 56909    |
----------------------------------
Eval num_timesteps=229000, episode_reward=310.88 +/- 31.59
Episode length: 102.72 +/- 7.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 0.878    |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 56999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 157      |
|    time_elapsed     | 1458     |
|    total_timesteps  | 229216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0478   |
|    n_updates        | 57053    |
----------------------------------
Eval num_timesteps=229500, episode_reward=417.12 +/- 148.36
Episode length: 129.28 +/- 37.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 417      |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0233   |
|    n_updates        | 57124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 157      |
|    time_elapsed     | 1462     |
|    total_timesteps  | 229672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 57167    |
----------------------------------
Eval num_timesteps=230000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.875    |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.51     |
|    n_updates        | 57249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 157      |
|    time_elapsed     | 1465     |
|    total_timesteps  | 230152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.095    |
|    n_updates        | 57287    |
----------------------------------
Eval num_timesteps=230500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.873    |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 157      |
|    time_elapsed     | 1468     |
|    total_timesteps  | 230744   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0218   |
|    n_updates        | 57435    |
----------------------------------
Eval num_timesteps=231000, episode_reward=333.92 +/- 61.11
Episode length: 108.48 +/- 15.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 334      |
| rollout/            |          |
|    exploration_rate | 0.871    |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.07     |
|    n_updates        | 57499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 157      |
|    time_elapsed     | 1471     |
|    total_timesteps  | 231224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 57555    |
----------------------------------
Eval num_timesteps=231500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.869    |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0149   |
|    n_updates        | 57624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.868    |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 157      |
|    time_elapsed     | 1474     |
|    total_timesteps  | 231720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.07     |
|    n_updates        | 57679    |
----------------------------------
Eval num_timesteps=232000, episode_reward=291.68 +/- 23.57
Episode length: 97.92 +/- 5.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.9     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.867    |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.07     |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 157      |
|    time_elapsed     | 1477     |
|    total_timesteps  | 232312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 57827    |
----------------------------------
Eval num_timesteps=232500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.865    |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0263   |
|    n_updates        | 57874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 157      |
|    time_elapsed     | 1480     |
|    total_timesteps  | 232864   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 57965    |
----------------------------------
Eval num_timesteps=233000, episode_reward=820.80 +/- 587.02
Episode length: 227.70 +/- 141.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 821      |
| rollout/            |          |
|    exploration_rate | 0.864    |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0194   |
|    n_updates        | 57999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 156      |
|    time_elapsed     | 1486     |
|    total_timesteps  | 233368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0471   |
|    n_updates        | 58091    |
----------------------------------
Eval num_timesteps=233500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.862    |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 58124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.86     |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 156      |
|    time_elapsed     | 1489     |
|    total_timesteps  | 233888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.56     |
|    n_updates        | 58221    |
----------------------------------
Eval num_timesteps=234000, episode_reward=345.44 +/- 57.90
Episode length: 111.36 +/- 14.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 345      |
| rollout/            |          |
|    exploration_rate | 0.86     |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0153   |
|    n_updates        | 58249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 156      |
|    time_elapsed     | 1493     |
|    total_timesteps  | 234344   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 58335    |
----------------------------------
Eval num_timesteps=234500, episode_reward=308.32 +/- 32.36
Episode length: 102.08 +/- 8.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.858    |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 58374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 156      |
|    time_elapsed     | 1496     |
|    total_timesteps  | 234784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.011    |
|    n_updates        | 58445    |
----------------------------------
Eval num_timesteps=235000, episode_reward=428.64 +/- 147.23
Episode length: 132.16 +/- 36.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 429      |
| rollout/            |          |
|    exploration_rate | 0.856    |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00974  |
|    n_updates        | 58499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 156      |
|    time_elapsed     | 1500     |
|    total_timesteps  | 235328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.54     |
|    n_updates        | 58581    |
----------------------------------
Eval num_timesteps=235500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.854    |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0192   |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.853    |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 156      |
|    time_elapsed     | 1502     |
|    total_timesteps  | 235752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 58687    |
----------------------------------
Eval num_timesteps=236000, episode_reward=320.48 +/- 39.97
Episode length: 105.12 +/- 9.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.852    |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0186   |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 156      |
|    time_elapsed     | 1506     |
|    total_timesteps  | 236136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 58783    |
----------------------------------
Eval num_timesteps=236500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.85     |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0715   |
|    n_updates        | 58874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 156      |
|    time_elapsed     | 1509     |
|    total_timesteps  | 236600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0392   |
|    n_updates        | 58899    |
----------------------------------
Eval num_timesteps=237000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.848    |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.54     |
|    n_updates        | 58999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.848    |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 156      |
|    time_elapsed     | 1512     |
|    total_timesteps  | 237120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 59029    |
----------------------------------
Eval num_timesteps=237500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 59124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 156      |
|    time_elapsed     | 1515     |
|    total_timesteps  | 237672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0219   |
|    n_updates        | 59167    |
----------------------------------
Eval num_timesteps=238000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.844    |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0193   |
|    n_updates        | 59249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 156      |
|    time_elapsed     | 1517     |
|    total_timesteps  | 238192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0201   |
|    n_updates        | 59297    |
----------------------------------
Eval num_timesteps=238500, episode_reward=324.32 +/- 41.87
Episode length: 106.08 +/- 10.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.842    |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 59374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.841    |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 156      |
|    time_elapsed     | 1521     |
|    total_timesteps  | 238680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 59419    |
----------------------------------
Eval num_timesteps=239000, episode_reward=343.52 +/- 65.27
Episode length: 110.88 +/- 16.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 0.84     |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0297   |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 156      |
|    time_elapsed     | 1524     |
|    total_timesteps  | 239112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.54     |
|    n_updates        | 59527    |
----------------------------------
Eval num_timesteps=239500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.838    |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0206   |
|    n_updates        | 59624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 156      |
|    time_elapsed     | 1527     |
|    total_timesteps  | 239584   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.54     |
|    n_updates        | 59645    |
----------------------------------
Eval num_timesteps=240000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.08     |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.836    |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 156      |
|    time_elapsed     | 1530     |
|    total_timesteps  | 240040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.374    |
|    n_updates        | 59759    |
----------------------------------
Eval num_timesteps=240500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.834    |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 59874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 156      |
|    time_elapsed     | 1533     |
|    total_timesteps  | 240520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0822   |
|    n_updates        | 59879    |
----------------------------------
Eval num_timesteps=241000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.832    |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 59999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 156      |
|    time_elapsed     | 1536     |
|    total_timesteps  | 241024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.12     |
|    n_updates        | 60005    |
----------------------------------
Eval num_timesteps=241500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.83     |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 60124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 156      |
|    time_elapsed     | 1539     |
|    total_timesteps  | 241528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.15     |
|    n_updates        | 60131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 157      |
|    time_elapsed     | 1539     |
|    total_timesteps  | 241984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0203   |
|    n_updates        | 60245    |
----------------------------------
Eval num_timesteps=242000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 60249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 157      |
|    time_elapsed     | 1542     |
|    total_timesteps  | 242424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 60355    |
----------------------------------
Eval num_timesteps=242500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.826    |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.13     |
|    n_updates        | 60374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 157      |
|    time_elapsed     | 1545     |
|    total_timesteps  | 242992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 60497    |
----------------------------------
Eval num_timesteps=243000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.824    |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.13     |
|    n_updates        | 60499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.822    |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 157      |
|    time_elapsed     | 1548     |
|    total_timesteps  | 243424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 60605    |
----------------------------------
Eval num_timesteps=243500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.822    |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00954  |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 157      |
|    time_elapsed     | 1551     |
|    total_timesteps  | 243880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0146   |
|    n_updates        | 60719    |
----------------------------------
Eval num_timesteps=244000, episode_reward=308.32 +/- 32.98
Episode length: 102.08 +/- 8.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.82     |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 60749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.818    |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 157      |
|    time_elapsed     | 1554     |
|    total_timesteps  | 244424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 60855    |
----------------------------------
Eval num_timesteps=244500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.818    |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.59     |
|    n_updates        | 60874    |
----------------------------------
Eval num_timesteps=245000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 60999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 157      |
|    time_elapsed     | 1560     |
|    total_timesteps  | 245000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 157      |
|    time_elapsed     | 1560     |
|    total_timesteps  | 245432   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0292   |
|    n_updates        | 61107    |
----------------------------------
Eval num_timesteps=245500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.813    |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0251   |
|    n_updates        | 61124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 157      |
|    time_elapsed     | 1563     |
|    total_timesteps  | 245856   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0191   |
|    n_updates        | 61213    |
----------------------------------
Eval num_timesteps=246000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.811    |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0408   |
|    n_updates        | 61249    |
----------------------------------
Eval num_timesteps=246500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.809    |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 61374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.809    |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 157      |
|    time_elapsed     | 1569     |
|    total_timesteps  | 246512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.59     |
|    n_updates        | 61377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 157      |
|    time_elapsed     | 1569     |
|    total_timesteps  | 246976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0074   |
|    n_updates        | 61493    |
----------------------------------
Eval num_timesteps=247000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.807    |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0146   |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.805    |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 157      |
|    time_elapsed     | 1572     |
|    total_timesteps  | 247440   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0306   |
|    n_updates        | 61609    |
----------------------------------
Eval num_timesteps=247500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.805    |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 61624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 157      |
|    time_elapsed     | 1575     |
|    total_timesteps  | 247896   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 61723    |
----------------------------------
Eval num_timesteps=248000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.803    |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.68     |
|    n_updates        | 61749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.801    |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 157      |
|    time_elapsed     | 1578     |
|    total_timesteps  | 248328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 61831    |
----------------------------------
Eval num_timesteps=248500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.801    |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 61874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 157      |
|    time_elapsed     | 1581     |
|    total_timesteps  | 248864   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 61965    |
----------------------------------
Eval num_timesteps=249000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0231   |
|    n_updates        | 61999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.797    |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 157      |
|    time_elapsed     | 1584     |
|    total_timesteps  | 249248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.12     |
|    n_updates        | 62061    |
----------------------------------
Eval num_timesteps=249500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.796    |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 62124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.795    |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 157      |
|    time_elapsed     | 1586     |
|    total_timesteps  | 249736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 62183    |
----------------------------------
Eval num_timesteps=250000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.794    |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.7      |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.793    |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 157      |
|    time_elapsed     | 1590     |
|    total_timesteps  | 250256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 62313    |
----------------------------------
Eval num_timesteps=250500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.792    |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0341   |
|    n_updates        | 62374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.79     |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 157      |
|    time_elapsed     | 1593     |
|    total_timesteps  | 250824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.61     |
|    n_updates        | 62455    |
----------------------------------
Eval num_timesteps=251000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0336   |
|    n_updates        | 62499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.788    |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 157      |
|    time_elapsed     | 1596     |
|    total_timesteps  | 251336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 62583    |
----------------------------------
Eval num_timesteps=251500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.788    |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0213   |
|    n_updates        | 62624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 157      |
|    time_elapsed     | 1599     |
|    total_timesteps  | 251816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.025    |
|    n_updates        | 62703    |
----------------------------------
Eval num_timesteps=252000, episode_reward=328.16 +/- 42.88
Episode length: 107.04 +/- 10.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 0.785    |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0181   |
|    n_updates        | 62749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.784    |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 157      |
|    time_elapsed     | 1602     |
|    total_timesteps  | 252224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.62     |
|    n_updates        | 62805    |
----------------------------------
Eval num_timesteps=252500, episode_reward=335.84 +/- 55.75
Episode length: 108.96 +/- 13.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.783    |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0203   |
|    n_updates        | 62874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.782    |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 157      |
|    time_elapsed     | 1605     |
|    total_timesteps  | 252712   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0258   |
|    n_updates        | 62927    |
----------------------------------
Eval num_timesteps=253000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.781    |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0215   |
|    n_updates        | 62999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.78     |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 157      |
|    time_elapsed     | 1608     |
|    total_timesteps  | 253256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0169   |
|    n_updates        | 63063    |
----------------------------------
Eval num_timesteps=253500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 63124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 157      |
|    time_elapsed     | 1611     |
|    total_timesteps  | 253688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.62     |
|    n_updates        | 63171    |
----------------------------------
Eval num_timesteps=254000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.776    |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0662   |
|    n_updates        | 63249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.776    |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 157      |
|    time_elapsed     | 1614     |
|    total_timesteps  | 254168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.62     |
|    n_updates        | 63291    |
----------------------------------
Eval num_timesteps=254500, episode_reward=306.40 +/- 48.42
Episode length: 101.60 +/- 12.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 0.774    |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.773    |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 157      |
|    time_elapsed     | 1617     |
|    total_timesteps  | 254664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 63415    |
----------------------------------
Eval num_timesteps=255000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.772    |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0224   |
|    n_updates        | 63499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.771    |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 157      |
|    time_elapsed     | 1620     |
|    total_timesteps  | 255160   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0196   |
|    n_updates        | 63539    |
----------------------------------
Eval num_timesteps=255500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.77     |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0178   |
|    n_updates        | 63624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.768    |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 157      |
|    time_elapsed     | 1623     |
|    total_timesteps  | 255768   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 63691    |
----------------------------------
Eval num_timesteps=256000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.767    |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.61     |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.766    |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 157      |
|    time_elapsed     | 1626     |
|    total_timesteps  | 256200   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 63799    |
----------------------------------
Eval num_timesteps=256500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.765    |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.64     |
|    n_updates        | 63874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.764    |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 157      |
|    time_elapsed     | 1629     |
|    total_timesteps  | 256736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0374   |
|    n_updates        | 63933    |
----------------------------------
Eval num_timesteps=257000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.763    |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0422   |
|    n_updates        | 63999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.762    |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 157      |
|    time_elapsed     | 1632     |
|    total_timesteps  | 257240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0157   |
|    n_updates        | 64059    |
----------------------------------
Eval num_timesteps=257500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.761    |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 64124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.76     |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 157      |
|    time_elapsed     | 1635     |
|    total_timesteps  | 257696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 64173    |
----------------------------------
Eval num_timesteps=258000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.758    |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.757    |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 157      |
|    time_elapsed     | 1638     |
|    total_timesteps  | 258240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0233   |
|    n_updates        | 64309    |
----------------------------------
Eval num_timesteps=258500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.756    |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.62     |
|    n_updates        | 64374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.755    |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 157      |
|    time_elapsed     | 1641     |
|    total_timesteps  | 258768   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0243   |
|    n_updates        | 64441    |
----------------------------------
Eval num_timesteps=259000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.754    |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.61     |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.752    |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 157      |
|    time_elapsed     | 1644     |
|    total_timesteps  | 259416   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 64603    |
----------------------------------
Eval num_timesteps=259500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.751    |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0328   |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.749    |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 157      |
|    time_elapsed     | 1647     |
|    total_timesteps  | 259896   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.61     |
|    n_updates        | 64723    |
----------------------------------
Eval num_timesteps=260000, episode_reward=303.20 +/- 44.80
Episode length: 100.80 +/- 11.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.749    |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0363   |
|    n_updates        | 64749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.747    |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 157      |
|    time_elapsed     | 1650     |
|    total_timesteps  | 260328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0324   |
|    n_updates        | 64831    |
----------------------------------
Eval num_timesteps=260500, episode_reward=303.20 +/- 44.80
Episode length: 100.80 +/- 11.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.747    |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0442   |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 157      |
|    time_elapsed     | 1653     |
|    total_timesteps  | 260816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0183   |
|    n_updates        | 64953    |
----------------------------------
Eval num_timesteps=261000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.744    |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0307   |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.742    |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 157      |
|    time_elapsed     | 1656     |
|    total_timesteps  | 261392   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0262   |
|    n_updates        | 65097    |
----------------------------------
Eval num_timesteps=261500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.742    |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0192   |
|    n_updates        | 65124    |
----------------------------------
Eval num_timesteps=262000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.74     |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 157      |
|    time_elapsed     | 1662     |
|    total_timesteps  | 262008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0221   |
|    n_updates        | 65251    |
----------------------------------
Eval num_timesteps=262500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.737    |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 65374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.737    |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 157      |
|    time_elapsed     | 1665     |
|    total_timesteps  | 262536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.019    |
|    n_updates        | 65383    |
----------------------------------
Eval num_timesteps=263000, episode_reward=317.92 +/- 41.68
Episode length: 104.48 +/- 10.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.735    |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0217   |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 157      |
|    time_elapsed     | 1668     |
|    total_timesteps  | 263040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.64     |
|    n_updates        | 65509    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.733    |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 157      |
|    time_elapsed     | 1668     |
|    total_timesteps  | 263448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.64     |
|    n_updates        | 65611    |
----------------------------------
Eval num_timesteps=263500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.733    |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.24     |
|    n_updates        | 65624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.731    |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 157      |
|    time_elapsed     | 1671     |
|    total_timesteps  | 263848   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0378   |
|    n_updates        | 65711    |
----------------------------------
Eval num_timesteps=264000, episode_reward=326.88 +/- 54.54
Episode length: 106.72 +/- 13.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 327      |
| rollout/            |          |
|    exploration_rate | 0.73     |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.031    |
|    n_updates        | 65749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.728    |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 157      |
|    time_elapsed     | 1674     |
|    total_timesteps  | 264344   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 65835    |
----------------------------------
Eval num_timesteps=264500, episode_reward=304.48 +/- 31.85
Episode length: 101.12 +/- 7.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.728    |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0618   |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.726    |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 157      |
|    time_elapsed     | 1678     |
|    total_timesteps  | 264936   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 65983    |
----------------------------------
Eval num_timesteps=265000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.725    |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.86     |
|    n_updates        | 65999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.723    |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 157      |
|    time_elapsed     | 1681     |
|    total_timesteps  | 265440   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.87     |
|    n_updates        | 66109    |
----------------------------------
Eval num_timesteps=265500, episode_reward=489.44 +/- 241.01
Episode length: 147.36 +/- 60.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 489      |
| rollout/            |          |
|    exploration_rate | 0.723    |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0319   |
|    n_updates        | 66124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.721    |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 157      |
|    time_elapsed     | 1685     |
|    total_timesteps  | 265872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 66217    |
----------------------------------
Eval num_timesteps=266000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.721    |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.719    |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 157      |
|    time_elapsed     | 1688     |
|    total_timesteps  | 266336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0244   |
|    n_updates        | 66333    |
----------------------------------
Eval num_timesteps=266500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.718    |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 66374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.716    |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 157      |
|    time_elapsed     | 1691     |
|    total_timesteps  | 266840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0225   |
|    n_updates        | 66459    |
----------------------------------
Eval num_timesteps=267000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.716    |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0413   |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.714    |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 157      |
|    time_elapsed     | 1694     |
|    total_timesteps  | 267344   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 66585    |
----------------------------------
Eval num_timesteps=267500, episode_reward=303.84 +/- 31.28
Episode length: 100.96 +/- 7.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.713    |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 66624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.711    |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 157      |
|    time_elapsed     | 1697     |
|    total_timesteps  | 267952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 66737    |
----------------------------------
Eval num_timesteps=268000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.711    |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 66749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 157      |
|    time_elapsed     | 1700     |
|    total_timesteps  | 268360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.24     |
|    n_updates        | 66839    |
----------------------------------
Eval num_timesteps=268500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.708    |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 66874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.707    |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 157      |
|    time_elapsed     | 1703     |
|    total_timesteps  | 268824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 66955    |
----------------------------------
Eval num_timesteps=269000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.706    |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.704    |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 157      |
|    time_elapsed     | 1706     |
|    total_timesteps  | 269360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0116   |
|    n_updates        | 67089    |
----------------------------------
Eval num_timesteps=269500, episode_reward=316.00 +/- 36.20
Episode length: 104.00 +/- 9.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 0.703    |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0356   |
|    n_updates        | 67124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 157      |
|    time_elapsed     | 1709     |
|    total_timesteps  | 269792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.022    |
|    n_updates        | 67197    |
----------------------------------
Eval num_timesteps=270000, episode_reward=409.44 +/- 174.90
Episode length: 127.36 +/- 43.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 409      |
| rollout/            |          |
|    exploration_rate | 0.701    |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0453   |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.7      |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 157      |
|    time_elapsed     | 1713     |
|    total_timesteps  | 270264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 67315    |
----------------------------------
Eval num_timesteps=270500, episode_reward=422.24 +/- 219.24
Episode length: 130.56 +/- 54.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 422      |
| rollout/            |          |
|    exploration_rate | 0.699    |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.67     |
|    n_updates        | 67374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.697    |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 157      |
|    time_elapsed     | 1717     |
|    total_timesteps  | 270776   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 67443    |
----------------------------------
Eval num_timesteps=271000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.696    |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.028    |
|    n_updates        | 67499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 157      |
|    time_elapsed     | 1720     |
|    total_timesteps  | 271256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.028    |
|    n_updates        | 67563    |
----------------------------------
Eval num_timesteps=271500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.694    |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.69     |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 157      |
|    time_elapsed     | 1723     |
|    total_timesteps  | 271760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.31     |
|    n_updates        | 67689    |
----------------------------------
Eval num_timesteps=272000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.691    |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 67749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.69     |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 157      |
|    time_elapsed     | 1726     |
|    total_timesteps  | 272264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.06     |
|    n_updates        | 67815    |
----------------------------------
Eval num_timesteps=272500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.689    |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0294   |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.687    |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 157      |
|    time_elapsed     | 1729     |
|    total_timesteps  | 272840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 67959    |
----------------------------------
Eval num_timesteps=273000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0155   |
|    n_updates        | 67999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.684    |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 157      |
|    time_elapsed     | 1732     |
|    total_timesteps  | 273376   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68093    |
----------------------------------
Eval num_timesteps=273500, episode_reward=420.32 +/- 128.61
Episode length: 130.08 +/- 32.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 420      |
| rollout/            |          |
|    exploration_rate | 0.684    |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0296   |
|    n_updates        | 68124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 157      |
|    time_elapsed     | 1736     |
|    total_timesteps  | 273928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68231    |
----------------------------------
Eval num_timesteps=274000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.681    |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.679    |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 157      |
|    time_elapsed     | 1739     |
|    total_timesteps  | 274376   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.67     |
|    n_updates        | 68343    |
----------------------------------
Eval num_timesteps=274500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.679    |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.31     |
|    n_updates        | 68374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.677    |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 157      |
|    time_elapsed     | 1742     |
|    total_timesteps  | 274872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0197   |
|    n_updates        | 68467    |
----------------------------------
Eval num_timesteps=275000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.676    |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0132   |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 157      |
|    time_elapsed     | 1745     |
|    total_timesteps  | 275392   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 68597    |
----------------------------------
Eval num_timesteps=275500, episode_reward=312.80 +/- 43.05
Episode length: 103.20 +/- 10.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.673    |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0402   |
|    n_updates        | 68624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.672    |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 157      |
|    time_elapsed     | 1748     |
|    total_timesteps  | 275808   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.67     |
|    n_updates        | 68701    |
----------------------------------
Eval num_timesteps=276000, episode_reward=364.64 +/- 89.88
Episode length: 116.16 +/- 22.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 365      |
| rollout/            |          |
|    exploration_rate | 0.671    |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0319   |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 157      |
|    time_elapsed     | 1751     |
|    total_timesteps  | 276288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 68821    |
----------------------------------
Eval num_timesteps=276500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.668    |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.666    |
| time/               |          |
|    episodes         | 2288     |
|    fps              | 157      |
|    time_elapsed     | 1754     |
|    total_timesteps  | 276896   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68973    |
----------------------------------
Eval num_timesteps=277000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.666    |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0295   |
|    n_updates        | 68999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.664    |
| time/               |          |
|    episodes         | 2292     |
|    fps              | 157      |
|    time_elapsed     | 1757     |
|    total_timesteps  | 277352   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.31     |
|    n_updates        | 69087    |
----------------------------------
Eval num_timesteps=277500, episode_reward=412.00 +/- 154.80
Episode length: 128.00 +/- 38.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 412      |
| rollout/            |          |
|    exploration_rate | 0.663    |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.3      |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.661    |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 157      |
|    time_elapsed     | 1761     |
|    total_timesteps  | 277856   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0268   |
|    n_updates        | 69213    |
----------------------------------
Eval num_timesteps=278000, episode_reward=307.04 +/- 30.72
Episode length: 101.76 +/- 7.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.661    |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 69249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.659    |
| time/               |          |
|    episodes         | 2300     |
|    fps              | 157      |
|    time_elapsed     | 1764     |
|    total_timesteps  | 278328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0309   |
|    n_updates        | 69331    |
----------------------------------
Eval num_timesteps=278500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.658    |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0217   |
|    n_updates        | 69374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.657    |
| time/               |          |
|    episodes         | 2304     |
|    fps              | 157      |
|    time_elapsed     | 1767     |
|    total_timesteps  | 278736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0237   |
|    n_updates        | 69433    |
----------------------------------
Eval num_timesteps=279000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.656    |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.654    |
| time/               |          |
|    episodes         | 2308     |
|    fps              | 157      |
|    time_elapsed     | 1770     |
|    total_timesteps  | 279280   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0286   |
|    n_updates        | 69569    |
----------------------------------
Eval num_timesteps=279500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.653    |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 69624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.652    |
| time/               |          |
|    episodes         | 2312     |
|    fps              | 157      |
|    time_elapsed     | 1773     |
|    total_timesteps  | 279736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0197   |
|    n_updates        | 69683    |
----------------------------------
Eval num_timesteps=280000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.65     |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 69749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.65     |
| time/               |          |
|    episodes         | 2316     |
|    fps              | 157      |
|    time_elapsed     | 1776     |
|    total_timesteps  | 280120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 69779    |
----------------------------------
Eval num_timesteps=280500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.648    |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.69     |
|    n_updates        | 69874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 2320     |
|    fps              | 157      |
|    time_elapsed     | 1779     |
|    total_timesteps  | 280672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0279   |
|    n_updates        | 69917    |
----------------------------------
Eval num_timesteps=281000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.645    |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.644    |
| time/               |          |
|    episodes         | 2324     |
|    fps              | 157      |
|    time_elapsed     | 1782     |
|    total_timesteps  | 281168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 70041    |
----------------------------------
Eval num_timesteps=281500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.642    |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0489   |
|    n_updates        | 70124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.642    |
| time/               |          |
|    episodes         | 2328     |
|    fps              | 157      |
|    time_elapsed     | 1785     |
|    total_timesteps  | 281664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 70165    |
----------------------------------
Eval num_timesteps=282000, episode_reward=324.32 +/- 46.07
Episode length: 106.08 +/- 11.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.64     |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 70249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 2332     |
|    fps              | 157      |
|    time_elapsed     | 1788     |
|    total_timesteps  | 282264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0249   |
|    n_updates        | 70315    |
----------------------------------
Eval num_timesteps=282500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.637    |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0189   |
|    n_updates        | 70374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.636    |
| time/               |          |
|    episodes         | 2336     |
|    fps              | 157      |
|    time_elapsed     | 1791     |
|    total_timesteps  | 282752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0241   |
|    n_updates        | 70437    |
----------------------------------
Eval num_timesteps=283000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.635    |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 70499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.633    |
| time/               |          |
|    episodes         | 2340     |
|    fps              | 157      |
|    time_elapsed     | 1794     |
|    total_timesteps  | 283304   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0114   |
|    n_updates        | 70575    |
----------------------------------
Eval num_timesteps=283500, episode_reward=767.52 +/- 619.82
Episode length: 213.88 +/- 148.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 768      |
| rollout/            |          |
|    exploration_rate | 0.632    |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.63     |
| time/               |          |
|    episodes         | 2344     |
|    fps              | 157      |
|    time_elapsed     | 1800     |
|    total_timesteps  | 283904   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.36     |
|    n_updates        | 70725    |
----------------------------------
Eval num_timesteps=284000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.629    |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0181   |
|    n_updates        | 70749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.627    |
| time/               |          |
|    episodes         | 2348     |
|    fps              | 157      |
|    time_elapsed     | 1803     |
|    total_timesteps  | 284360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0393   |
|    n_updates        | 70839    |
----------------------------------
Eval num_timesteps=284500, episode_reward=308.32 +/- 36.52
Episode length: 102.08 +/- 9.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.627    |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0231   |
|    n_updates        | 70874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.625    |
| time/               |          |
|    episodes         | 2352     |
|    fps              | 157      |
|    time_elapsed     | 1806     |
|    total_timesteps  | 284792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 70947    |
----------------------------------
Eval num_timesteps=285000, episode_reward=323.04 +/- 39.60
Episode length: 105.76 +/- 9.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 323      |
| rollout/            |          |
|    exploration_rate | 0.624    |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 2356     |
|    fps              | 157      |
|    time_elapsed     | 1809     |
|    total_timesteps  | 285336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.72     |
|    n_updates        | 71083    |
----------------------------------
Eval num_timesteps=285500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.621    |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0263   |
|    n_updates        | 71124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.62     |
| time/               |          |
|    episodes         | 2360     |
|    fps              | 157      |
|    time_elapsed     | 1812     |
|    total_timesteps  | 285768   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 71191    |
----------------------------------
Eval num_timesteps=286000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.619    |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.617    |
| time/               |          |
|    episodes         | 2364     |
|    fps              | 157      |
|    time_elapsed     | 1815     |
|    total_timesteps  | 286248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0231   |
|    n_updates        | 71311    |
----------------------------------
Eval num_timesteps=286500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.616    |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0328   |
|    n_updates        | 71374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 2368     |
|    fps              | 157      |
|    time_elapsed     | 1818     |
|    total_timesteps  | 286784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0323   |
|    n_updates        | 71445    |
----------------------------------
Eval num_timesteps=287000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.613    |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.69     |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.611    |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 157      |
|    time_elapsed     | 1821     |
|    total_timesteps  | 287368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0149   |
|    n_updates        | 71591    |
----------------------------------
Eval num_timesteps=287500, episode_reward=323.68 +/- 40.25
Episode length: 105.92 +/- 10.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.611    |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 71624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.608    |
| time/               |          |
|    episodes         | 2376     |
|    fps              | 157      |
|    time_elapsed     | 1825     |
|    total_timesteps  | 287920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 71729    |
----------------------------------
Eval num_timesteps=288000, episode_reward=314.72 +/- 35.03
Episode length: 103.68 +/- 8.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.608    |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0366   |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.605    |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 157      |
|    time_elapsed     | 1828     |
|    total_timesteps  | 288456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 71863    |
----------------------------------
Eval num_timesteps=288500, episode_reward=314.72 +/- 37.84
Episode length: 103.68 +/- 9.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.605    |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0198   |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.603    |
| time/               |          |
|    episodes         | 2384     |
|    fps              | 157      |
|    time_elapsed     | 1831     |
|    total_timesteps  | 288936   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 71983    |
----------------------------------
Eval num_timesteps=289000, episode_reward=294.24 +/- 34.13
Episode length: 98.56 +/- 8.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 0.602    |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 71999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.6      |
| time/               |          |
|    episodes         | 2388     |
|    fps              | 157      |
|    time_elapsed     | 1834     |
|    total_timesteps  | 289384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.72     |
|    n_updates        | 72095    |
----------------------------------
Eval num_timesteps=289500, episode_reward=314.72 +/- 36.74
Episode length: 103.68 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.6      |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0373   |
|    n_updates        | 72124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.598    |
| time/               |          |
|    episodes         | 2392     |
|    fps              | 157      |
|    time_elapsed     | 1837     |
|    total_timesteps  | 289872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0185   |
|    n_updates        | 72217    |
----------------------------------
Eval num_timesteps=290000, episode_reward=318.56 +/- 42.38
Episode length: 104.64 +/- 10.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.597    |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 72249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 2396     |
|    fps              | 157      |
|    time_elapsed     | 1840     |
|    total_timesteps  | 290312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.4      |
|    n_updates        | 72327    |
----------------------------------
Eval num_timesteps=290500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.594    |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 72374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.593    |
| time/               |          |
|    episodes         | 2400     |
|    fps              | 157      |
|    time_elapsed     | 1843     |
|    total_timesteps  | 290736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0469   |
|    n_updates        | 72433    |
----------------------------------
Eval num_timesteps=291000, episode_reward=307.04 +/- 63.05
Episode length: 101.76 +/- 15.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.592    |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.4      |
|    n_updates        | 72499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.59     |
| time/               |          |
|    episodes         | 2404     |
|    fps              | 157      |
|    time_elapsed     | 1846     |
|    total_timesteps  | 291320   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 72579    |
----------------------------------
Eval num_timesteps=291500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.589    |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 72624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.587    |
| time/               |          |
|    episodes         | 2408     |
|    fps              | 157      |
|    time_elapsed     | 1849     |
|    total_timesteps  | 291848   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 72711    |
----------------------------------
Eval num_timesteps=292000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.586    |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 72749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.584    |
| time/               |          |
|    episodes         | 2412     |
|    fps              | 157      |
|    time_elapsed     | 1852     |
|    total_timesteps  | 292304   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.39     |
|    n_updates        | 72825    |
----------------------------------
Eval num_timesteps=292500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.583    |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.72     |
|    n_updates        | 72874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.582    |
| time/               |          |
|    episodes         | 2416     |
|    fps              | 157      |
|    time_elapsed     | 1855     |
|    total_timesteps  | 292736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.72     |
|    n_updates        | 72933    |
----------------------------------
Eval num_timesteps=293000, episode_reward=313.44 +/- 34.37
Episode length: 103.36 +/- 8.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.581    |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0181   |
|    n_updates        | 72999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.579    |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 157      |
|    time_elapsed     | 1859     |
|    total_timesteps  | 293208   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.048    |
|    n_updates        | 73051    |
----------------------------------
Eval num_timesteps=293500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.578    |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0309   |
|    n_updates        | 73124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.577    |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 157      |
|    time_elapsed     | 1861     |
|    total_timesteps  | 293688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 73171    |
----------------------------------
Eval num_timesteps=294000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.575    |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 73249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.574    |
| time/               |          |
|    episodes         | 2428     |
|    fps              | 157      |
|    time_elapsed     | 1864     |
|    total_timesteps  | 294144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.4      |
|    n_updates        | 73285    |
----------------------------------
Eval num_timesteps=294500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.572    |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0141   |
|    n_updates        | 73374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 2432     |
|    fps              | 157      |
|    time_elapsed     | 1867     |
|    total_timesteps  | 294624   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0351   |
|    n_updates        | 73405    |
----------------------------------
Eval num_timesteps=295000, episode_reward=499.52 +/- 297.12
Episode length: 149.38 +/- 71.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 500      |
| rollout/            |          |
|    exploration_rate | 0.569    |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0388   |
|    n_updates        | 73499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.568    |
| time/               |          |
|    episodes         | 2436     |
|    fps              | 157      |
|    time_elapsed     | 1872     |
|    total_timesteps  | 295176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 73543    |
----------------------------------
Eval num_timesteps=295500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.567    |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.018    |
|    n_updates        | 73624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.566    |
| time/               |          |
|    episodes         | 2440     |
|    fps              | 157      |
|    time_elapsed     | 1875     |
|    total_timesteps  | 295656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.72     |
|    n_updates        | 73663    |
----------------------------------
Eval num_timesteps=296000, episode_reward=348.64 +/- 59.52
Episode length: 112.16 +/- 14.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 349      |
| rollout/            |          |
|    exploration_rate | 0.564    |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.77     |
|    n_updates        | 73749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.563    |
| time/               |          |
|    episodes         | 2444     |
|    fps              | 157      |
|    time_elapsed     | 1878     |
|    total_timesteps  | 296232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0284   |
|    n_updates        | 73807    |
----------------------------------
Eval num_timesteps=296500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.561    |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0258   |
|    n_updates        | 73874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 2448     |
|    fps              | 157      |
|    time_elapsed     | 1881     |
|    total_timesteps  | 296720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0379   |
|    n_updates        | 73929    |
----------------------------------
Eval num_timesteps=297000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.558    |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.4      |
|    n_updates        | 73999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.557    |
| time/               |          |
|    episodes         | 2452     |
|    fps              | 157      |
|    time_elapsed     | 1884     |
|    total_timesteps  | 297272   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0182   |
|    n_updates        | 74067    |
----------------------------------
Eval num_timesteps=297500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.555    |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 74124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.554    |
| time/               |          |
|    episodes         | 2456     |
|    fps              | 157      |
|    time_elapsed     | 1887     |
|    total_timesteps  | 297784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.72     |
|    n_updates        | 74195    |
----------------------------------
Eval num_timesteps=298000, episode_reward=316.64 +/- 37.59
Episode length: 104.16 +/- 9.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.553    |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0172   |
|    n_updates        | 74249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.551    |
| time/               |          |
|    episodes         | 2460     |
|    fps              | 157      |
|    time_elapsed     | 1891     |
|    total_timesteps  | 298320   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0155   |
|    n_updates        | 74329    |
----------------------------------
Eval num_timesteps=298500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.55     |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 74374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.548    |
| time/               |          |
|    episodes         | 2464     |
|    fps              | 157      |
|    time_elapsed     | 1894     |
|    total_timesteps  | 298784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0526   |
|    n_updates        | 74445    |
----------------------------------
Eval num_timesteps=299000, episode_reward=287.20 +/- 14.66
Episode length: 96.80 +/- 3.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.8     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.547    |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.033    |
|    n_updates        | 74499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 2468     |
|    fps              | 157      |
|    time_elapsed     | 1897     |
|    total_timesteps  | 299200   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0354   |
|    n_updates        | 74549    |
----------------------------------
Eval num_timesteps=299500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.544    |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0299   |
|    n_updates        | 74624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.543    |
| time/               |          |
|    episodes         | 2472     |
|    fps              | 157      |
|    time_elapsed     | 1900     |
|    total_timesteps  | 299704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 74675    |
----------------------------------
Eval num_timesteps=300000, episode_reward=287.84 +/- 18.81
Episode length: 96.96 +/- 4.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.541    |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.73     |
|    n_updates        | 74749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.54     |
| time/               |          |
|    episodes         | 2476     |
|    fps              | 157      |
|    time_elapsed     | 1903     |
|    total_timesteps  | 300152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 74787    |
----------------------------------
Eval num_timesteps=300500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.538    |
| time/               |          |
|    total_timesteps  | 300500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.17     |
|    n_updates        | 74874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.538    |
| time/               |          |
|    episodes         | 2480     |
|    fps              | 157      |
|    time_elapsed     | 1906     |
|    total_timesteps  | 300656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.74     |
|    n_updates        | 74913    |
----------------------------------
Eval num_timesteps=301000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 301000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0252   |
|    n_updates        | 74999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.534    |
| time/               |          |
|    episodes         | 2484     |
|    fps              | 157      |
|    time_elapsed     | 1909     |
|    total_timesteps  | 301264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 75065    |
----------------------------------
Eval num_timesteps=301500, episode_reward=317.28 +/- 50.78
Episode length: 104.32 +/- 12.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.533    |
| time/               |          |
|    total_timesteps  | 301500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 75124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 2488     |
|    fps              | 157      |
|    time_elapsed     | 1912     |
|    total_timesteps  | 301872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0379   |
|    n_updates        | 75217    |
----------------------------------
Eval num_timesteps=302000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.53     |
| time/               |          |
|    total_timesteps  | 302000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0276   |
|    n_updates        | 75249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.528    |
| time/               |          |
|    episodes         | 2492     |
|    fps              | 157      |
|    time_elapsed     | 1915     |
|    total_timesteps  | 302304   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0316   |
|    n_updates        | 75325    |
----------------------------------
Eval num_timesteps=302500, episode_reward=309.60 +/- 41.48
Episode length: 102.40 +/- 10.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.527    |
| time/               |          |
|    total_timesteps  | 302500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.74     |
|    n_updates        | 75374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.526    |
| time/               |          |
|    episodes         | 2496     |
|    fps              | 157      |
|    time_elapsed     | 1918     |
|    total_timesteps  | 302712   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.73     |
|    n_updates        | 75427    |
----------------------------------
Eval num_timesteps=303000, episode_reward=312.80 +/- 32.79
Episode length: 103.20 +/- 8.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.524    |
| time/               |          |
|    total_timesteps  | 303000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0197   |
|    n_updates        | 75499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 2500     |
|    fps              | 157      |
|    time_elapsed     | 1921     |
|    total_timesteps  | 303272   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.73     |
|    n_updates        | 75567    |
----------------------------------
Eval num_timesteps=303500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.521    |
| time/               |          |
|    total_timesteps  | 303500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 75624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 2504     |
|    fps              | 157      |
|    time_elapsed     | 1924     |
|    total_timesteps  | 303776   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.018    |
|    n_updates        | 75693    |
----------------------------------
Eval num_timesteps=304000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.518    |
| time/               |          |
|    total_timesteps  | 304000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0265   |
|    n_updates        | 75749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.516    |
| time/               |          |
|    episodes         | 2508     |
|    fps              | 157      |
|    time_elapsed     | 1927     |
|    total_timesteps  | 304328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.45     |
|    n_updates        | 75831    |
----------------------------------
Eval num_timesteps=304500, episode_reward=316.64 +/- 34.16
Episode length: 104.16 +/- 8.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.515    |
| time/               |          |
|    total_timesteps  | 304500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0205   |
|    n_updates        | 75874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.514    |
| time/               |          |
|    episodes         | 2512     |
|    fps              | 157      |
|    time_elapsed     | 1931     |
|    total_timesteps  | 304816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 75953    |
----------------------------------
Eval num_timesteps=305000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.513    |
| time/               |          |
|    total_timesteps  | 305000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0189   |
|    n_updates        | 75999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 2516     |
|    fps              | 157      |
|    time_elapsed     | 1934     |
|    total_timesteps  | 305280   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0314   |
|    n_updates        | 76069    |
----------------------------------
Eval num_timesteps=305500, episode_reward=309.60 +/- 36.77
Episode length: 102.40 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.51     |
| time/               |          |
|    total_timesteps  | 305500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.75     |
|    n_updates        | 76124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.508    |
| time/               |          |
|    episodes         | 2520     |
|    fps              | 157      |
|    time_elapsed     | 1937     |
|    total_timesteps  | 305824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.76     |
|    n_updates        | 76205    |
----------------------------------
Eval num_timesteps=306000, episode_reward=325.60 +/- 43.99
Episode length: 106.40 +/- 11.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.507    |
| time/               |          |
|    total_timesteps  | 306000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0203   |
|    n_updates        | 76249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 2524     |
|    fps              | 157      |
|    time_elapsed     | 1940     |
|    total_timesteps  | 306336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0246   |
|    n_updates        | 76333    |
----------------------------------
Eval num_timesteps=306500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.504    |
| time/               |          |
|    total_timesteps  | 306500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 76374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.502    |
| time/               |          |
|    episodes         | 2528     |
|    fps              | 157      |
|    time_elapsed     | 1943     |
|    total_timesteps  | 306744   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 76435    |
----------------------------------
Eval num_timesteps=307000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.501    |
| time/               |          |
|    total_timesteps  | 307000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 76499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.5      |
| time/               |          |
|    episodes         | 2532     |
|    fps              | 157      |
|    time_elapsed     | 1946     |
|    total_timesteps  | 307152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.45     |
|    n_updates        | 76537    |
----------------------------------
Eval num_timesteps=307500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.498    |
| time/               |          |
|    total_timesteps  | 307500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0368   |
|    n_updates        | 76624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.498    |
| time/               |          |
|    episodes         | 2536     |
|    fps              | 157      |
|    time_elapsed     | 1949     |
|    total_timesteps  | 307568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.45     |
|    n_updates        | 76641    |
----------------------------------
Eval num_timesteps=308000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.495    |
| time/               |          |
|    total_timesteps  | 308000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.73     |
|    n_updates        | 76749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.495    |
| time/               |          |
|    episodes         | 2540     |
|    fps              | 157      |
|    time_elapsed     | 1952     |
|    total_timesteps  | 308072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.73     |
|    n_updates        | 76767    |
----------------------------------
Eval num_timesteps=308500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.492    |
| time/               |          |
|    total_timesteps  | 308500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0286   |
|    n_updates        | 76874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.492    |
| time/               |          |
|    episodes         | 2544     |
|    fps              | 157      |
|    time_elapsed     | 1955     |
|    total_timesteps  | 308552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.45     |
|    n_updates        | 76887    |
----------------------------------
Eval num_timesteps=309000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.489    |
| time/               |          |
|    total_timesteps  | 309000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 76999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.489    |
| time/               |          |
|    episodes         | 2548     |
|    fps              | 157      |
|    time_elapsed     | 1958     |
|    total_timesteps  | 309040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 77009    |
----------------------------------
Eval num_timesteps=309500, episode_reward=313.44 +/- 34.96
Episode length: 103.36 +/- 8.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.486    |
| time/               |          |
|    total_timesteps  | 309500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 77124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.486    |
| time/               |          |
|    episodes         | 2552     |
|    fps              | 157      |
|    time_elapsed     | 1961     |
|    total_timesteps  | 309592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0214   |
|    n_updates        | 77147    |
----------------------------------
Eval num_timesteps=310000, episode_reward=357.60 +/- 98.16
Episode length: 114.40 +/- 24.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 358      |
| rollout/            |          |
|    exploration_rate | 0.483    |
| time/               |          |
|    total_timesteps  | 310000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.032    |
|    n_updates        | 77249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.482    |
| time/               |          |
|    episodes         | 2556     |
|    fps              | 157      |
|    time_elapsed     | 1965     |
|    total_timesteps  | 310128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0188   |
|    n_updates        | 77281    |
----------------------------------
Eval num_timesteps=310500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.48     |
| time/               |          |
|    total_timesteps  | 310500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 77374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.48     |
| time/               |          |
|    episodes         | 2560     |
|    fps              | 157      |
|    time_elapsed     | 1968     |
|    total_timesteps  | 310536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.77     |
|    n_updates        | 77383    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.477    |
| time/               |          |
|    episodes         | 2564     |
|    fps              | 157      |
|    time_elapsed     | 1968     |
|    total_timesteps  | 310968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0267   |
|    n_updates        | 77491    |
----------------------------------
Eval num_timesteps=311000, episode_reward=308.96 +/- 34.64
Episode length: 102.24 +/- 8.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 0.477    |
| time/               |          |
|    total_timesteps  | 311000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.75     |
|    n_updates        | 77499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.475    |
| time/               |          |
|    episodes         | 2568     |
|    fps              | 157      |
|    time_elapsed     | 1971     |
|    total_timesteps  | 311376   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 77593    |
----------------------------------
Eval num_timesteps=311500, episode_reward=285.92 +/- 7.60
Episode length: 96.48 +/- 1.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.474    |
| time/               |          |
|    total_timesteps  | 311500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0613   |
|    n_updates        | 77624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.472    |
| time/               |          |
|    episodes         | 2572     |
|    fps              | 157      |
|    time_elapsed     | 1974     |
|    total_timesteps  | 311928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0265   |
|    n_updates        | 77731    |
----------------------------------
Eval num_timesteps=312000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.471    |
| time/               |          |
|    total_timesteps  | 312000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.76     |
|    n_updates        | 77749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.469    |
| time/               |          |
|    episodes         | 2576     |
|    fps              | 157      |
|    time_elapsed     | 1977     |
|    total_timesteps  | 312400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0221   |
|    n_updates        | 77849    |
----------------------------------
Eval num_timesteps=312500, episode_reward=506.72 +/- 263.88
Episode length: 151.68 +/- 65.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 507      |
| rollout/            |          |
|    exploration_rate | 0.468    |
| time/               |          |
|    total_timesteps  | 312500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.76     |
|    n_updates        | 77874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.466    |
| time/               |          |
|    episodes         | 2580     |
|    fps              | 157      |
|    time_elapsed     | 1982     |
|    total_timesteps  | 312880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0355   |
|    n_updates        | 77969    |
----------------------------------
Eval num_timesteps=313000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.465    |
| time/               |          |
|    total_timesteps  | 313000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 77999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.463    |
| time/               |          |
|    episodes         | 2584     |
|    fps              | 157      |
|    time_elapsed     | 1985     |
|    total_timesteps  | 313352   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0368   |
|    n_updates        | 78087    |
----------------------------------
Eval num_timesteps=313500, episode_reward=315.36 +/- 40.72
Episode length: 103.84 +/- 10.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.462    |
| time/               |          |
|    total_timesteps  | 313500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0955   |
|    n_updates        | 78124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.46     |
| time/               |          |
|    episodes         | 2588     |
|    fps              | 157      |
|    time_elapsed     | 1988     |
|    total_timesteps  | 313840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 78209    |
----------------------------------
Eval num_timesteps=314000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.459    |
| time/               |          |
|    total_timesteps  | 314000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.014    |
|    n_updates        | 78249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.458    |
| time/               |          |
|    episodes         | 2592     |
|    fps              | 157      |
|    time_elapsed     | 1991     |
|    total_timesteps  | 314248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.78     |
|    n_updates        | 78311    |
----------------------------------
Eval num_timesteps=314500, episode_reward=339.04 +/- 58.32
Episode length: 109.76 +/- 14.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 339      |
| rollout/            |          |
|    exploration_rate | 0.456    |
| time/               |          |
|    total_timesteps  | 314500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0189   |
|    n_updates        | 78374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.455    |
| time/               |          |
|    episodes         | 2596     |
|    fps              | 157      |
|    time_elapsed     | 1994     |
|    total_timesteps  | 314720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.77     |
|    n_updates        | 78429    |
----------------------------------
Eval num_timesteps=315000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.453    |
| time/               |          |
|    total_timesteps  | 315000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0404   |
|    n_updates        | 78499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 2600     |
|    fps              | 157      |
|    time_elapsed     | 1997     |
|    total_timesteps  | 315200   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0169   |
|    n_updates        | 78549    |
----------------------------------
Eval num_timesteps=315500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.45     |
| time/               |          |
|    total_timesteps  | 315500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.76     |
|    n_updates        | 78624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.449    |
| time/               |          |
|    episodes         | 2604     |
|    fps              | 157      |
|    time_elapsed     | 2000     |
|    total_timesteps  | 315688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 78671    |
----------------------------------
Eval num_timesteps=316000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.447    |
| time/               |          |
|    total_timesteps  | 316000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0216   |
|    n_updates        | 78749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.446    |
| time/               |          |
|    episodes         | 2608     |
|    fps              | 157      |
|    time_elapsed     | 2003     |
|    total_timesteps  | 316152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.76     |
|    n_updates        | 78787    |
----------------------------------
Eval num_timesteps=316500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.444    |
| time/               |          |
|    total_timesteps  | 316500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.044    |
|    n_updates        | 78874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.443    |
| time/               |          |
|    episodes         | 2612     |
|    fps              | 157      |
|    time_elapsed     | 2006     |
|    total_timesteps  | 316728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.052    |
|    n_updates        | 78931    |
----------------------------------
Eval num_timesteps=317000, episode_reward=1926.56 +/- 435.39
Episode length: 485.64 +/- 100.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 486      |
|    mean_reward      | 1.93e+03 |
| rollout/            |          |
|    exploration_rate | 0.441    |
| time/               |          |
|    total_timesteps  | 317000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0435   |
|    n_updates        | 78999    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.44     |
| time/               |          |
|    episodes         | 2616     |
|    fps              | 157      |
|    time_elapsed     | 2020     |
|    total_timesteps  | 317208   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.49     |
|    n_updates        | 79051    |
----------------------------------
Eval num_timesteps=317500, episode_reward=321.76 +/- 37.69
Episode length: 105.44 +/- 9.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 0.438    |
| time/               |          |
|    total_timesteps  | 317500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 79124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 2620     |
|    fps              | 157      |
|    time_elapsed     | 2023     |
|    total_timesteps  | 317744   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0203   |
|    n_updates        | 79185    |
----------------------------------
Eval num_timesteps=318000, episode_reward=335.84 +/- 63.96
Episode length: 108.96 +/- 15.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.435    |
| time/               |          |
|    total_timesteps  | 318000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.49     |
|    n_updates        | 79249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.434    |
| time/               |          |
|    episodes         | 2624     |
|    fps              | 156      |
|    time_elapsed     | 2026     |
|    total_timesteps  | 318224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.76     |
|    n_updates        | 79305    |
----------------------------------
Eval num_timesteps=318500, episode_reward=301.92 +/- 46.66
Episode length: 100.48 +/- 11.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 302      |
| rollout/            |          |
|    exploration_rate | 0.432    |
| time/               |          |
|    total_timesteps  | 318500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0169   |
|    n_updates        | 79374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 2628     |
|    fps              | 156      |
|    time_elapsed     | 2030     |
|    total_timesteps  | 318664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0191   |
|    n_updates        | 79415    |
----------------------------------
Eval num_timesteps=319000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.429    |
| time/               |          |
|    total_timesteps  | 319000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0197   |
|    n_updates        | 79499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.428    |
| time/               |          |
|    episodes         | 2632     |
|    fps              | 156      |
|    time_elapsed     | 2033     |
|    total_timesteps  | 319144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 79535    |
----------------------------------
Eval num_timesteps=319500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.426    |
| time/               |          |
|    total_timesteps  | 319500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0166   |
|    n_updates        | 79624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.426    |
| time/               |          |
|    episodes         | 2636     |
|    fps              | 156      |
|    time_elapsed     | 2036     |
|    total_timesteps  | 319552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.75     |
|    n_updates        | 79637    |
----------------------------------
Eval num_timesteps=320000, episode_reward=285.28 +/- 8.96
Episode length: 96.32 +/- 2.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.423    |
| time/               |          |
|    total_timesteps  | 320000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.75     |
|    n_updates        | 79749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.423    |
| time/               |          |
|    episodes         | 2640     |
|    fps              | 156      |
|    time_elapsed     | 2039     |
|    total_timesteps  | 320000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.42     |
| time/               |          |
|    episodes         | 2644     |
|    fps              | 157      |
|    time_elapsed     | 2039     |
|    total_timesteps  | 320464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 79865    |
----------------------------------
Eval num_timesteps=320500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.42     |
| time/               |          |
|    total_timesteps  | 320500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.78     |
|    n_updates        | 79874    |
----------------------------------
Eval num_timesteps=321000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.417    |
| time/               |          |
|    total_timesteps  | 321000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0313   |
|    n_updates        | 79999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.416    |
| time/               |          |
|    episodes         | 2648     |
|    fps              | 156      |
|    time_elapsed     | 2045     |
|    total_timesteps  | 321048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0246   |
|    n_updates        | 80011    |
----------------------------------
Eval num_timesteps=321500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.414    |
| time/               |          |
|    total_timesteps  | 321500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 80124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.413    |
| time/               |          |
|    episodes         | 2652     |
|    fps              | 156      |
|    time_elapsed     | 2048     |
|    total_timesteps  | 321560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0478   |
|    n_updates        | 80139    |
----------------------------------
Eval num_timesteps=322000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.411    |
| time/               |          |
|    total_timesteps  | 322000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.3      |
|    n_updates        | 80249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 2656     |
|    fps              | 156      |
|    time_elapsed     | 2051     |
|    total_timesteps  | 322040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.78     |
|    n_updates        | 80259    |
----------------------------------
Eval num_timesteps=322500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.407    |
| time/               |          |
|    total_timesteps  | 322500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 80374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.407    |
| time/               |          |
|    episodes         | 2660     |
|    fps              | 156      |
|    time_elapsed     | 2054     |
|    total_timesteps  | 322528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0129   |
|    n_updates        | 80381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 2664     |
|    fps              | 157      |
|    time_elapsed     | 2054     |
|    total_timesteps  | 322944   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0745   |
|    n_updates        | 80485    |
----------------------------------
Eval num_timesteps=323000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.404    |
| time/               |          |
|    total_timesteps  | 323000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0872   |
|    n_updates        | 80499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.401    |
| time/               |          |
|    episodes         | 2668     |
|    fps              | 157      |
|    time_elapsed     | 2057     |
|    total_timesteps  | 323488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.77     |
|    n_updates        | 80621    |
----------------------------------
Eval num_timesteps=323500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.401    |
| time/               |          |
|    total_timesteps  | 323500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.79     |
|    n_updates        | 80624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 2672     |
|    fps              | 157      |
|    time_elapsed     | 2060     |
|    total_timesteps  | 323992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.77     |
|    n_updates        | 80747    |
----------------------------------
Eval num_timesteps=324000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.398    |
| time/               |          |
|    total_timesteps  | 324000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.78     |
|    n_updates        | 80749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.396    |
| time/               |          |
|    episodes         | 2676     |
|    fps              | 157      |
|    time_elapsed     | 2063     |
|    total_timesteps  | 324400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0303   |
|    n_updates        | 80849    |
----------------------------------
Eval num_timesteps=324500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.395    |
| time/               |          |
|    total_timesteps  | 324500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.78     |
|    n_updates        | 80874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.393    |
| time/               |          |
|    episodes         | 2680     |
|    fps              | 157      |
|    time_elapsed     | 2066     |
|    total_timesteps  | 324832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.021    |
|    n_updates        | 80957    |
----------------------------------
Eval num_timesteps=325000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.392    |
| time/               |          |
|    total_timesteps  | 325000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0186   |
|    n_updates        | 80999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.39     |
| time/               |          |
|    episodes         | 2684     |
|    fps              | 157      |
|    time_elapsed     | 2069     |
|    total_timesteps  | 325336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.54     |
|    n_updates        | 81083    |
----------------------------------
Eval num_timesteps=325500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.389    |
| time/               |          |
|    total_timesteps  | 325500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0195   |
|    n_updates        | 81124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.387    |
| time/               |          |
|    episodes         | 2688     |
|    fps              | 157      |
|    time_elapsed     | 2072     |
|    total_timesteps  | 325792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0234   |
|    n_updates        | 81197    |
----------------------------------
Eval num_timesteps=326000, episode_reward=598.88 +/- 258.02
Episode length: 174.72 +/- 64.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 599      |
| rollout/            |          |
|    exploration_rate | 0.386    |
| time/               |          |
|    total_timesteps  | 326000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 81249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.384    |
| time/               |          |
|    episodes         | 2692     |
|    fps              | 157      |
|    time_elapsed     | 2078     |
|    total_timesteps  | 326272   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.3      |
|    n_updates        | 81317    |
----------------------------------
Eval num_timesteps=326500, episode_reward=346.08 +/- 67.55
Episode length: 111.52 +/- 16.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 346      |
| rollout/            |          |
|    exploration_rate | 0.382    |
| time/               |          |
|    total_timesteps  | 326500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0295   |
|    n_updates        | 81374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.381    |
| time/               |          |
|    episodes         | 2696     |
|    fps              | 156      |
|    time_elapsed     | 2081     |
|    total_timesteps  | 326760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0125   |
|    n_updates        | 81439    |
----------------------------------
Eval num_timesteps=327000, episode_reward=324.32 +/- 38.30
Episode length: 106.08 +/- 9.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.379    |
| time/               |          |
|    total_timesteps  | 327000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0353   |
|    n_updates        | 81499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.378    |
| time/               |          |
|    episodes         | 2700     |
|    fps              | 156      |
|    time_elapsed     | 2084     |
|    total_timesteps  | 327232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 81557    |
----------------------------------
Eval num_timesteps=327500, episode_reward=287.84 +/- 26.88
Episode length: 96.96 +/- 6.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.376    |
| time/               |          |
|    total_timesteps  | 327500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0231   |
|    n_updates        | 81624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.375    |
| time/               |          |
|    episodes         | 2704     |
|    fps              | 156      |
|    time_elapsed     | 2087     |
|    total_timesteps  | 327696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.025    |
|    n_updates        | 81673    |
----------------------------------
Eval num_timesteps=328000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.373    |
| time/               |          |
|    total_timesteps  | 328000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 81749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.372    |
| time/               |          |
|    episodes         | 2708     |
|    fps              | 156      |
|    time_elapsed     | 2090     |
|    total_timesteps  | 328184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 81795    |
----------------------------------
Eval num_timesteps=328500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.37     |
| time/               |          |
|    total_timesteps  | 328500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.82     |
|    n_updates        | 81874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.369    |
| time/               |          |
|    episodes         | 2712     |
|    fps              | 156      |
|    time_elapsed     | 2093     |
|    total_timesteps  | 328640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.59     |
|    n_updates        | 81909    |
----------------------------------
Eval num_timesteps=329000, episode_reward=345.44 +/- 65.53
Episode length: 111.36 +/- 16.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 345      |
| rollout/            |          |
|    exploration_rate | 0.367    |
| time/               |          |
|    total_timesteps  | 329000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0392   |
|    n_updates        | 81999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.366    |
| time/               |          |
|    episodes         | 2716     |
|    fps              | 156      |
|    time_elapsed     | 2096     |
|    total_timesteps  | 329088   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.78     |
|    n_updates        | 82021    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.364    |
| time/               |          |
|    episodes         | 2720     |
|    fps              | 157      |
|    time_elapsed     | 2097     |
|    total_timesteps  | 329472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.79     |
|    n_updates        | 82117    |
----------------------------------
Eval num_timesteps=329500, episode_reward=319.84 +/- 80.86
Episode length: 104.96 +/- 20.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.363    |
| time/               |          |
|    total_timesteps  | 329500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.78     |
|    n_updates        | 82124    |
----------------------------------
Eval num_timesteps=330000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.36     |
| time/               |          |
|    total_timesteps  | 330000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0258   |
|    n_updates        | 82249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.36     |
| time/               |          |
|    episodes         | 2724     |
|    fps              | 156      |
|    time_elapsed     | 2103     |
|    total_timesteps  | 330016   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.15     |
|    n_updates        | 82253    |
----------------------------------
Eval num_timesteps=330500, episode_reward=359.52 +/- 75.66
Episode length: 114.88 +/- 18.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 360      |
| rollout/            |          |
|    exploration_rate | 0.357    |
| time/               |          |
|    total_timesteps  | 330500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.36     |
|    n_updates        | 82374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.356    |
| time/               |          |
|    episodes         | 2728     |
|    fps              | 156      |
|    time_elapsed     | 2106     |
|    total_timesteps  | 330672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.61     |
|    n_updates        | 82417    |
----------------------------------
Eval num_timesteps=331000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.354    |
| time/               |          |
|    total_timesteps  | 331000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.81     |
|    n_updates        | 82499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.353    |
| time/               |          |
|    episodes         | 2732     |
|    fps              | 156      |
|    time_elapsed     | 2110     |
|    total_timesteps  | 331104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.37     |
|    n_updates        | 82525    |
----------------------------------
Eval num_timesteps=331500, episode_reward=320.48 +/- 41.48
Episode length: 105.12 +/- 10.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.351    |
| time/               |          |
|    total_timesteps  | 331500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.57     |
|    n_updates        | 82624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.351    |
| time/               |          |
|    episodes         | 2736     |
|    fps              | 156      |
|    time_elapsed     | 2113     |
|    total_timesteps  | 331536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 82633    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.348    |
| time/               |          |
|    episodes         | 2740     |
|    fps              | 157      |
|    time_elapsed     | 2113     |
|    total_timesteps  | 331968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.83     |
|    n_updates        | 82741    |
----------------------------------
Eval num_timesteps=332000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.348    |
| time/               |          |
|    total_timesteps  | 332000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.59     |
|    n_updates        | 82749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.345    |
| time/               |          |
|    episodes         | 2744     |
|    fps              | 157      |
|    time_elapsed     | 2116     |
|    total_timesteps  | 332424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.043    |
|    n_updates        | 82855    |
----------------------------------
Eval num_timesteps=332500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.344    |
| time/               |          |
|    total_timesteps  | 332500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0203   |
|    n_updates        | 82874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.342    |
| time/               |          |
|    episodes         | 2748     |
|    fps              | 157      |
|    time_elapsed     | 2119     |
|    total_timesteps  | 332872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 82967    |
----------------------------------
Eval num_timesteps=333000, episode_reward=601.28 +/- 367.43
Episode length: 174.82 +/- 89.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 601      |
| rollout/            |          |
|    exploration_rate | 0.341    |
| time/               |          |
|    total_timesteps  | 333000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 82999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.338    |
| time/               |          |
|    episodes         | 2752     |
|    fps              | 156      |
|    time_elapsed     | 2125     |
|    total_timesteps  | 333448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0345   |
|    n_updates        | 83111    |
----------------------------------
Eval num_timesteps=333500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.338    |
| time/               |          |
|    total_timesteps  | 333500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 83124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.336    |
| time/               |          |
|    episodes         | 2756     |
|    fps              | 156      |
|    time_elapsed     | 2128     |
|    total_timesteps  | 333872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 83217    |
----------------------------------
Eval num_timesteps=334000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.335    |
| time/               |          |
|    total_timesteps  | 334000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 83249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.332    |
| time/               |          |
|    episodes         | 2760     |
|    fps              | 156      |
|    time_elapsed     | 2131     |
|    total_timesteps  | 334440   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0143   |
|    n_updates        | 83359    |
----------------------------------
Eval num_timesteps=334500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.331    |
| time/               |          |
|    total_timesteps  | 334500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 83374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.329    |
| time/               |          |
|    episodes         | 2764     |
|    fps              | 156      |
|    time_elapsed     | 2134     |
|    total_timesteps  | 334872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 83467    |
----------------------------------
Eval num_timesteps=335000, episode_reward=308.32 +/- 32.98
Episode length: 102.08 +/- 8.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.328    |
| time/               |          |
|    total_timesteps  | 335000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.58     |
|    n_updates        | 83499    |
----------------------------------
Eval num_timesteps=335500, episode_reward=312.80 +/- 32.79
Episode length: 103.20 +/- 8.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.325    |
| time/               |          |
|    total_timesteps  | 335500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.35     |
|    n_updates        | 83624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.325    |
| time/               |          |
|    episodes         | 2768     |
|    fps              | 156      |
|    time_elapsed     | 2140     |
|    total_timesteps  | 335512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.81     |
|    n_updates        | 83627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.322    |
| time/               |          |
|    episodes         | 2772     |
|    fps              | 156      |
|    time_elapsed     | 2140     |
|    total_timesteps  | 335952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.79     |
|    n_updates        | 83737    |
----------------------------------
Eval num_timesteps=336000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.322    |
| time/               |          |
|    total_timesteps  | 336000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.79     |
|    n_updates        | 83749    |
----------------------------------
Eval num_timesteps=336500, episode_reward=439.52 +/- 168.48
Episode length: 134.88 +/- 42.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 440      |
| rollout/            |          |
|    exploration_rate | 0.319    |
| time/               |          |
|    total_timesteps  | 336500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.59     |
|    n_updates        | 83874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.318    |
| time/               |          |
|    episodes         | 2776     |
|    fps              | 156      |
|    time_elapsed     | 2147     |
|    total_timesteps  | 336544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0209   |
|    n_updates        | 83885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.315    |
| time/               |          |
|    episodes         | 2780     |
|    fps              | 156      |
|    time_elapsed     | 2147     |
|    total_timesteps  | 336968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0528   |
|    n_updates        | 83991    |
----------------------------------
Eval num_timesteps=337000, episode_reward=303.20 +/- 35.63
Episode length: 100.80 +/- 8.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.315    |
| time/               |          |
|    total_timesteps  | 337000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 83999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.313    |
| time/               |          |
|    episodes         | 2784     |
|    fps              | 156      |
|    time_elapsed     | 2150     |
|    total_timesteps  | 337400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0194   |
|    n_updates        | 84099    |
----------------------------------
Eval num_timesteps=337500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.312    |
| time/               |          |
|    total_timesteps  | 337500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 84124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.309    |
| time/               |          |
|    episodes         | 2788     |
|    fps              | 156      |
|    time_elapsed     | 2153     |
|    total_timesteps  | 337952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 84237    |
----------------------------------
Eval num_timesteps=338000, episode_reward=324.32 +/- 37.21
Episode length: 106.08 +/- 9.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.309    |
| time/               |          |
|    total_timesteps  | 338000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 84249    |
----------------------------------
Eval num_timesteps=338500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.306    |
| time/               |          |
|    total_timesteps  | 338500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0579   |
|    n_updates        | 84374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.305    |
| time/               |          |
|    episodes         | 2792     |
|    fps              | 156      |
|    time_elapsed     | 2159     |
|    total_timesteps  | 338504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0461   |
|    n_updates        | 84375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.302    |
| time/               |          |
|    episodes         | 2796     |
|    fps              | 156      |
|    time_elapsed     | 2160     |
|    total_timesteps  | 338984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 84495    |
----------------------------------
Eval num_timesteps=339000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.302    |
| time/               |          |
|    total_timesteps  | 339000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 84499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.299    |
| time/               |          |
|    episodes         | 2800     |
|    fps              | 156      |
|    time_elapsed     | 2163     |
|    total_timesteps  | 339464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0277   |
|    n_updates        | 84615    |
----------------------------------
Eval num_timesteps=339500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.299    |
| time/               |          |
|    total_timesteps  | 339500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0146   |
|    n_updates        | 84624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 2804     |
|    fps              | 156      |
|    time_elapsed     | 2166     |
|    total_timesteps  | 339904   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.83     |
|    n_updates        | 84725    |
----------------------------------
Eval num_timesteps=340000, episode_reward=312.80 +/- 36.90
Episode length: 103.20 +/- 9.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.296    |
| time/               |          |
|    total_timesteps  | 340000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 84749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.293    |
| time/               |          |
|    episodes         | 2808     |
|    fps              | 156      |
|    time_elapsed     | 2169     |
|    total_timesteps  | 340360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.63     |
|    n_updates        | 84839    |
----------------------------------
Eval num_timesteps=340500, episode_reward=358.88 +/- 94.63
Episode length: 114.72 +/- 23.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 359      |
| rollout/            |          |
|    exploration_rate | 0.292    |
| time/               |          |
|    total_timesteps  | 340500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.024    |
|    n_updates        | 84874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.29     |
| time/               |          |
|    episodes         | 2812     |
|    fps              | 156      |
|    time_elapsed     | 2172     |
|    total_timesteps  | 340816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0354   |
|    n_updates        | 84953    |
----------------------------------
Eval num_timesteps=341000, episode_reward=318.56 +/- 39.89
Episode length: 104.64 +/- 9.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.289    |
| time/               |          |
|    total_timesteps  | 341000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.81     |
|    n_updates        | 84999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.287    |
| time/               |          |
|    episodes         | 2816     |
|    fps              | 156      |
|    time_elapsed     | 2176     |
|    total_timesteps  | 341296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0202   |
|    n_updates        | 85073    |
----------------------------------
Eval num_timesteps=341500, episode_reward=326.24 +/- 85.01
Episode length: 106.56 +/- 21.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.286    |
| time/               |          |
|    total_timesteps  | 341500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 85124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.284    |
| time/               |          |
|    episodes         | 2820     |
|    fps              | 156      |
|    time_elapsed     | 2179     |
|    total_timesteps  | 341752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 85187    |
----------------------------------
Eval num_timesteps=342000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.282    |
| time/               |          |
|    total_timesteps  | 342000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.42     |
|    n_updates        | 85249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.281    |
| time/               |          |
|    episodes         | 2824     |
|    fps              | 156      |
|    time_elapsed     | 2182     |
|    total_timesteps  | 342248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.82     |
|    n_updates        | 85311    |
----------------------------------
Eval num_timesteps=342500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.279    |
| time/               |          |
|    total_timesteps  | 342500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0167   |
|    n_updates        | 85374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.278    |
| time/               |          |
|    episodes         | 2828     |
|    fps              | 156      |
|    time_elapsed     | 2185     |
|    total_timesteps  | 342656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 85413    |
----------------------------------
Eval num_timesteps=343000, episode_reward=377.44 +/- 145.92
Episode length: 119.36 +/- 36.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 377      |
| rollout/            |          |
|    exploration_rate | 0.276    |
| time/               |          |
|    total_timesteps  | 343000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 85499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.275    |
| time/               |          |
|    episodes         | 2832     |
|    fps              | 156      |
|    time_elapsed     | 2188     |
|    total_timesteps  | 343160   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 85539    |
----------------------------------
Eval num_timesteps=343500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.273    |
| time/               |          |
|    total_timesteps  | 343500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.62     |
|    n_updates        | 85624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.272    |
| time/               |          |
|    episodes         | 2836     |
|    fps              | 156      |
|    time_elapsed     | 2191     |
|    total_timesteps  | 343616   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.82     |
|    n_updates        | 85653    |
----------------------------------
Eval num_timesteps=344000, episode_reward=1735.04 +/- 621.50
Episode length: 441.26 +/- 145.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 441      |
|    mean_reward      | 1.74e+03 |
| rollout/            |          |
|    exploration_rate | 0.269    |
| time/               |          |
|    total_timesteps  | 344000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.82     |
|    n_updates        | 85749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.268    |
| time/               |          |
|    episodes         | 2840     |
|    fps              | 156      |
|    time_elapsed     | 2204     |
|    total_timesteps  | 344120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.62     |
|    n_updates        | 85779    |
----------------------------------
Eval num_timesteps=344500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.266    |
| time/               |          |
|    total_timesteps  | 344500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.83     |
|    n_updates        | 85874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.266    |
| time/               |          |
|    episodes         | 2844     |
|    fps              | 156      |
|    time_elapsed     | 2207     |
|    total_timesteps  | 344536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.82     |
|    n_updates        | 85883    |
----------------------------------
Eval num_timesteps=345000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.263    |
| time/               |          |
|    total_timesteps  | 345000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0282   |
|    n_updates        | 85999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.262    |
| time/               |          |
|    episodes         | 2848     |
|    fps              | 156      |
|    time_elapsed     | 2210     |
|    total_timesteps  | 345024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 86005    |
----------------------------------
Eval num_timesteps=345500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.259    |
| time/               |          |
|    total_timesteps  | 345500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.021    |
|    n_updates        | 86124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.259    |
| time/               |          |
|    episodes         | 2852     |
|    fps              | 156      |
|    time_elapsed     | 2213     |
|    total_timesteps  | 345520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.81     |
|    n_updates        | 86129    |
----------------------------------
Eval num_timesteps=346000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.256    |
| time/               |          |
|    total_timesteps  | 346000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 86249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.256    |
| time/               |          |
|    episodes         | 2856     |
|    fps              | 156      |
|    time_elapsed     | 2216     |
|    total_timesteps  | 346032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 86257    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.253    |
| time/               |          |
|    episodes         | 2860     |
|    fps              | 156      |
|    time_elapsed     | 2216     |
|    total_timesteps  | 346440   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.4      |
|    n_updates        | 86359    |
----------------------------------
Eval num_timesteps=346500, episode_reward=917.92 +/- 566.89
Episode length: 251.48 +/- 135.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 251      |
|    mean_reward      | 918      |
| rollout/            |          |
|    exploration_rate | 0.253    |
| time/               |          |
|    total_timesteps  | 346500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 86374    |
----------------------------------
Eval num_timesteps=347000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.249    |
| time/               |          |
|    total_timesteps  | 347000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 86499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.249    |
| time/               |          |
|    episodes         | 2864     |
|    fps              | 155      |
|    time_elapsed     | 2226     |
|    total_timesteps  | 347008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0408   |
|    n_updates        | 86501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 2868     |
|    fps              | 156      |
|    time_elapsed     | 2227     |
|    total_timesteps  | 347488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0987   |
|    n_updates        | 86621    |
----------------------------------
Eval num_timesteps=347500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.246    |
| time/               |          |
|    total_timesteps  | 347500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 86624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.243    |
| time/               |          |
|    episodes         | 2872     |
|    fps              | 156      |
|    time_elapsed     | 2230     |
|    total_timesteps  | 347960   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.82     |
|    n_updates        | 86739    |
----------------------------------
Eval num_timesteps=348000, episode_reward=320.48 +/- 53.55
Episode length: 105.12 +/- 13.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.242    |
| time/               |          |
|    total_timesteps  | 348000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.81     |
|    n_updates        | 86749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.239    |
| time/               |          |
|    episodes         | 2876     |
|    fps              | 156      |
|    time_elapsed     | 2233     |
|    total_timesteps  | 348464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.83     |
|    n_updates        | 86865    |
----------------------------------
Eval num_timesteps=348500, episode_reward=307.68 +/- 34.35
Episode length: 101.92 +/- 8.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.239    |
| time/               |          |
|    total_timesteps  | 348500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.82     |
|    n_updates        | 86874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.236    |
| time/               |          |
|    episodes         | 2880     |
|    fps              | 156      |
|    time_elapsed     | 2236     |
|    total_timesteps  | 348976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.8      |
|    n_updates        | 86993    |
----------------------------------
Eval num_timesteps=349000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.236    |
| time/               |          |
|    total_timesteps  | 349000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0209   |
|    n_updates        | 86999    |
----------------------------------
Eval num_timesteps=349500, episode_reward=328.16 +/- 37.26
Episode length: 107.04 +/- 9.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 0.232    |
| time/               |          |
|    total_timesteps  | 349500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0202   |
|    n_updates        | 87124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.232    |
| time/               |          |
|    episodes         | 2884     |
|    fps              | 155      |
|    time_elapsed     | 2242     |
|    total_timesteps  | 349528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0366   |
|    n_updates        | 87131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.229    |
| time/               |          |
|    episodes         | 2888     |
|    fps              | 156      |
|    time_elapsed     | 2242     |
|    total_timesteps  | 349936   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.031    |
|    n_updates        | 87233    |
----------------------------------
Eval num_timesteps=350000, episode_reward=406.72 +/- 312.69
Episode length: 126.18 +/- 75.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 407      |
| rollout/            |          |
|    exploration_rate | 0.229    |
| time/               |          |
|    total_timesteps  | 350000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.62     |
|    n_updates        | 87249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.226    |
| time/               |          |
|    episodes         | 2892     |
|    fps              | 155      |
|    time_elapsed     | 2246     |
|    total_timesteps  | 350416   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.66     |
|    n_updates        | 87353    |
----------------------------------
Eval num_timesteps=350500, episode_reward=326.88 +/- 49.00
Episode length: 106.72 +/- 12.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 327      |
| rollout/            |          |
|    exploration_rate | 0.226    |
| time/               |          |
|    total_timesteps  | 350500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0129   |
|    n_updates        | 87374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.223    |
| time/               |          |
|    episodes         | 2896     |
|    fps              | 155      |
|    time_elapsed     | 2249     |
|    total_timesteps  | 350888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0284   |
|    n_updates        | 87471    |
----------------------------------
Eval num_timesteps=351000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.222    |
| time/               |          |
|    total_timesteps  | 351000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0231   |
|    n_updates        | 87499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.22     |
| time/               |          |
|    episodes         | 2900     |
|    fps              | 155      |
|    time_elapsed     | 2252     |
|    total_timesteps  | 351368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0536   |
|    n_updates        | 87591    |
----------------------------------
Eval num_timesteps=351500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.219    |
| time/               |          |
|    total_timesteps  | 351500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 87624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.217    |
| time/               |          |
|    episodes         | 2904     |
|    fps              | 155      |
|    time_elapsed     | 2255     |
|    total_timesteps  | 351824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0253   |
|    n_updates        | 87705    |
----------------------------------
Eval num_timesteps=352000, episode_reward=490.08 +/- 210.73
Episode length: 147.52 +/- 52.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 490      |
| rollout/            |          |
|    exploration_rate | 0.215    |
| time/               |          |
|    total_timesteps  | 352000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0228   |
|    n_updates        | 87749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.213    |
| time/               |          |
|    episodes         | 2908     |
|    fps              | 155      |
|    time_elapsed     | 2260     |
|    total_timesteps  | 352304   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 87825    |
----------------------------------
Eval num_timesteps=352500, episode_reward=314.08 +/- 34.71
Episode length: 103.52 +/- 8.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 0.212    |
| time/               |          |
|    total_timesteps  | 352500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 87874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.21     |
| time/               |          |
|    episodes         | 2912     |
|    fps              | 155      |
|    time_elapsed     | 2263     |
|    total_timesteps  | 352752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.85     |
|    n_updates        | 87937    |
----------------------------------
Eval num_timesteps=353000, episode_reward=316.64 +/- 37.04
Episode length: 104.16 +/- 9.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.209    |
| time/               |          |
|    total_timesteps  | 353000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.67     |
|    n_updates        | 87999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.207    |
| time/               |          |
|    episodes         | 2916     |
|    fps              | 155      |
|    time_elapsed     | 2266     |
|    total_timesteps  | 353296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 88073    |
----------------------------------
Eval num_timesteps=353500, episode_reward=305.76 +/- 33.53
Episode length: 101.44 +/- 8.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 0.205    |
| time/               |          |
|    total_timesteps  | 353500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0244   |
|    n_updates        | 88124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.204    |
| time/               |          |
|    episodes         | 2920     |
|    fps              | 155      |
|    time_elapsed     | 2269     |
|    total_timesteps  | 353680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0216   |
|    n_updates        | 88169    |
----------------------------------
Eval num_timesteps=354000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.202    |
| time/               |          |
|    total_timesteps  | 354000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 88249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 2924     |
|    fps              | 155      |
|    time_elapsed     | 2272     |
|    total_timesteps  | 354256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.83     |
|    n_updates        | 88313    |
----------------------------------
Eval num_timesteps=354500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.198    |
| time/               |          |
|    total_timesteps  | 354500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.68     |
|    n_updates        | 88374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.196    |
| time/               |          |
|    episodes         | 2928     |
|    fps              | 155      |
|    time_elapsed     | 2275     |
|    total_timesteps  | 354784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 88445    |
----------------------------------
Eval num_timesteps=355000, episode_reward=415.20 +/- 167.66
Episode length: 128.80 +/- 41.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 415      |
| rollout/            |          |
|    exploration_rate | 0.195    |
| time/               |          |
|    total_timesteps  | 355000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0191   |
|    n_updates        | 88499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.193    |
| time/               |          |
|    episodes         | 2932     |
|    fps              | 155      |
|    time_elapsed     | 2279     |
|    total_timesteps  | 355256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0239   |
|    n_updates        | 88563    |
----------------------------------
Eval num_timesteps=355500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.191    |
| time/               |          |
|    total_timesteps  | 355500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 88624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.19     |
| time/               |          |
|    episodes         | 2936     |
|    fps              | 155      |
|    time_elapsed     | 2282     |
|    total_timesteps  | 355712   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 88677    |
----------------------------------
Eval num_timesteps=356000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.188    |
| time/               |          |
|    total_timesteps  | 356000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.66     |
|    n_updates        | 88749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.187    |
| time/               |          |
|    episodes         | 2940     |
|    fps              | 155      |
|    time_elapsed     | 2285     |
|    total_timesteps  | 356096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.66     |
|    n_updates        | 88773    |
----------------------------------
Eval num_timesteps=356500, episode_reward=301.28 +/- 30.16
Episode length: 100.32 +/- 7.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 301      |
| rollout/            |          |
|    exploration_rate | 0.185    |
| time/               |          |
|    total_timesteps  | 356500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0385   |
|    n_updates        | 88874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.184    |
| time/               |          |
|    episodes         | 2944     |
|    fps              | 155      |
|    time_elapsed     | 2288     |
|    total_timesteps  | 356592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.85     |
|    n_updates        | 88897    |
----------------------------------
Eval num_timesteps=357000, episode_reward=335.84 +/- 60.34
Episode length: 108.96 +/- 15.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.181    |
| time/               |          |
|    total_timesteps  | 357000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0253   |
|    n_updates        | 88999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.181    |
| time/               |          |
|    episodes         | 2948     |
|    fps              | 155      |
|    time_elapsed     | 2291     |
|    total_timesteps  | 357024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 89005    |
----------------------------------
Eval num_timesteps=357500, episode_reward=341.60 +/- 64.32
Episode length: 110.40 +/- 16.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 342      |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 357500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 89124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.178    |
| time/               |          |
|    episodes         | 2952     |
|    fps              | 155      |
|    time_elapsed     | 2295     |
|    total_timesteps  | 357512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 89127    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.175    |
| time/               |          |
|    episodes         | 2956     |
|    fps              | 155      |
|    time_elapsed     | 2295     |
|    total_timesteps  | 357944   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 89235    |
----------------------------------
Eval num_timesteps=358000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.174    |
| time/               |          |
|    total_timesteps  | 358000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 89249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.172    |
| time/               |          |
|    episodes         | 2960     |
|    fps              | 155      |
|    time_elapsed     | 2298     |
|    total_timesteps  | 358368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.48     |
|    n_updates        | 89341    |
----------------------------------
Eval num_timesteps=358500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.171    |
| time/               |          |
|    total_timesteps  | 358500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 89374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.168    |
| time/               |          |
|    episodes         | 2964     |
|    fps              | 155      |
|    time_elapsed     | 2301     |
|    total_timesteps  | 358848   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 89461    |
----------------------------------
Eval num_timesteps=359000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.167    |
| time/               |          |
|    total_timesteps  | 359000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0229   |
|    n_updates        | 89499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.165    |
| time/               |          |
|    episodes         | 2968     |
|    fps              | 155      |
|    time_elapsed     | 2304     |
|    total_timesteps  | 359392   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0324   |
|    n_updates        | 89597    |
----------------------------------
Eval num_timesteps=359500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.164    |
| time/               |          |
|    total_timesteps  | 359500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.84     |
|    n_updates        | 89624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.161    |
| time/               |          |
|    episodes         | 2972     |
|    fps              | 155      |
|    time_elapsed     | 2307     |
|    total_timesteps  | 359944   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 89735    |
----------------------------------
Eval num_timesteps=360000, episode_reward=312.16 +/- 36.56
Episode length: 103.04 +/- 9.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.16     |
| time/               |          |
|    total_timesteps  | 360000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.83     |
|    n_updates        | 89749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.157    |
| time/               |          |
|    episodes         | 2976     |
|    fps              | 155      |
|    time_elapsed     | 2311     |
|    total_timesteps  | 360424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 89855    |
----------------------------------
Eval num_timesteps=360500, episode_reward=330.72 +/- 55.13
Episode length: 107.68 +/- 13.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 331      |
| rollout/            |          |
|    exploration_rate | 0.157    |
| time/               |          |
|    total_timesteps  | 360500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 89874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.154    |
| time/               |          |
|    episodes         | 2980     |
|    fps              | 155      |
|    time_elapsed     | 2314     |
|    total_timesteps  | 360888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 89971    |
----------------------------------
Eval num_timesteps=361000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.153    |
| time/               |          |
|    total_timesteps  | 361000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 89999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.151    |
| time/               |          |
|    episodes         | 2984     |
|    fps              | 155      |
|    time_elapsed     | 2317     |
|    total_timesteps  | 361376   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.87     |
|    n_updates        | 90093    |
----------------------------------
Eval num_timesteps=361500, episode_reward=287.84 +/- 15.20
Episode length: 96.96 +/- 3.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.15     |
| time/               |          |
|    total_timesteps  | 361500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0177   |
|    n_updates        | 90124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.147    |
| time/               |          |
|    episodes         | 2988     |
|    fps              | 155      |
|    time_elapsed     | 2320     |
|    total_timesteps  | 361880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.034    |
|    n_updates        | 90219    |
----------------------------------
Eval num_timesteps=362000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.146    |
| time/               |          |
|    total_timesteps  | 362000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.9      |
|    n_updates        | 90249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.144    |
| time/               |          |
|    episodes         | 2992     |
|    fps              | 155      |
|    time_elapsed     | 2323     |
|    total_timesteps  | 362312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0433   |
|    n_updates        | 90327    |
----------------------------------
Eval num_timesteps=362500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.143    |
| time/               |          |
|    total_timesteps  | 362500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.88     |
|    n_updates        | 90374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.141    |
| time/               |          |
|    episodes         | 2996     |
|    fps              | 155      |
|    time_elapsed     | 2326     |
|    total_timesteps  | 362704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.95     |
|    n_updates        | 90425    |
----------------------------------
Eval num_timesteps=363000, episode_reward=698.72 +/- 295.09
Episode length: 199.68 +/- 73.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 699      |
| rollout/            |          |
|    exploration_rate | 0.139    |
| time/               |          |
|    total_timesteps  | 363000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.88     |
|    n_updates        | 90499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.138    |
| time/               |          |
|    episodes         | 3000     |
|    fps              | 155      |
|    time_elapsed     | 2332     |
|    total_timesteps  | 363136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 90533    |
----------------------------------
Eval num_timesteps=363500, episode_reward=471.52 +/- 181.70
Episode length: 142.88 +/- 45.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 472      |
| rollout/            |          |
|    exploration_rate | 0.136    |
| time/               |          |
|    total_timesteps  | 363500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0567   |
|    n_updates        | 90624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.136    |
| time/               |          |
|    episodes         | 3004     |
|    fps              | 155      |
|    time_elapsed     | 2336     |
|    total_timesteps  | 363544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.79     |
|    n_updates        | 90635    |
----------------------------------
Eval num_timesteps=364000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.132    |
| time/               |          |
|    total_timesteps  | 364000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 90749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 3008     |
|    fps              | 155      |
|    time_elapsed     | 2339     |
|    total_timesteps  | 364008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 90751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.129    |
| time/               |          |
|    episodes         | 3012     |
|    fps              | 155      |
|    time_elapsed     | 2339     |
|    total_timesteps  | 364456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0419   |
|    n_updates        | 90863    |
----------------------------------
Eval num_timesteps=364500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.129    |
| time/               |          |
|    total_timesteps  | 364500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0307   |
|    n_updates        | 90874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.126    |
| time/               |          |
|    episodes         | 3016     |
|    fps              | 155      |
|    time_elapsed     | 2342     |
|    total_timesteps  | 364840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.69     |
|    n_updates        | 90959    |
----------------------------------
Eval num_timesteps=365000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.125    |
| time/               |          |
|    total_timesteps  | 365000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.88     |
|    n_updates        | 90999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.123    |
| time/               |          |
|    episodes         | 3020     |
|    fps              | 155      |
|    time_elapsed     | 2345     |
|    total_timesteps  | 365288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 91071    |
----------------------------------
Eval num_timesteps=365500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.122    |
| time/               |          |
|    total_timesteps  | 365500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.022    |
|    n_updates        | 91124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.12     |
| time/               |          |
|    episodes         | 3024     |
|    fps              | 155      |
|    time_elapsed     | 2348     |
|    total_timesteps  | 365672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0185   |
|    n_updates        | 91167    |
----------------------------------
Eval num_timesteps=366000, episode_reward=318.56 +/- 49.09
Episode length: 104.64 +/- 12.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.118    |
| time/               |          |
|    total_timesteps  | 366000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0299   |
|    n_updates        | 91249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.117    |
| time/               |          |
|    episodes         | 3028     |
|    fps              | 155      |
|    time_elapsed     | 2351     |
|    total_timesteps  | 366104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0183   |
|    n_updates        | 91275    |
----------------------------------
Eval num_timesteps=366500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.115    |
| time/               |          |
|    total_timesteps  | 366500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.85     |
|    n_updates        | 91374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.114    |
| time/               |          |
|    episodes         | 3032     |
|    fps              | 155      |
|    time_elapsed     | 2354     |
|    total_timesteps  | 366624   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0149   |
|    n_updates        | 91405    |
----------------------------------
Eval num_timesteps=367000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.111    |
| time/               |          |
|    total_timesteps  | 367000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0271   |
|    n_updates        | 91499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.111    |
| time/               |          |
|    episodes         | 3036     |
|    fps              | 155      |
|    time_elapsed     | 2357     |
|    total_timesteps  | 367032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0198   |
|    n_updates        | 91507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 3040     |
|    fps              | 155      |
|    time_elapsed     | 2358     |
|    total_timesteps  | 367416   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 91603    |
----------------------------------
Eval num_timesteps=367500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.108    |
| time/               |          |
|    total_timesteps  | 367500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.85     |
|    n_updates        | 91624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.105    |
| time/               |          |
|    episodes         | 3044     |
|    fps              | 155      |
|    time_elapsed     | 2361     |
|    total_timesteps  | 367832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0225   |
|    n_updates        | 91707    |
----------------------------------
Eval num_timesteps=368000, episode_reward=312.16 +/- 39.78
Episode length: 103.04 +/- 9.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.104    |
| time/               |          |
|    total_timesteps  | 368000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.88     |
|    n_updates        | 91749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.102    |
| time/               |          |
|    episodes         | 3048     |
|    fps              | 155      |
|    time_elapsed     | 2364     |
|    total_timesteps  | 368240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0153   |
|    n_updates        | 91809    |
----------------------------------
Eval num_timesteps=368500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.1      |
| time/               |          |
|    total_timesteps  | 368500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0275   |
|    n_updates        | 91874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.099    |
| time/               |          |
|    episodes         | 3052     |
|    fps              | 155      |
|    time_elapsed     | 2367     |
|    total_timesteps  | 368696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0251   |
|    n_updates        | 91923    |
----------------------------------
Eval num_timesteps=369000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0969   |
| time/               |          |
|    total_timesteps  | 369000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0201   |
|    n_updates        | 91999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0951   |
| time/               |          |
|    episodes         | 3056     |
|    fps              | 155      |
|    time_elapsed     | 2370     |
|    total_timesteps  | 369248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0217   |
|    n_updates        | 92061    |
----------------------------------
Eval num_timesteps=369500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0933   |
| time/               |          |
|    total_timesteps  | 369500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 92124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.092    |
| time/               |          |
|    episodes         | 3060     |
|    fps              | 155      |
|    time_elapsed     | 2373     |
|    total_timesteps  | 369680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 92169    |
----------------------------------
Eval num_timesteps=370000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0897   |
| time/               |          |
|    total_timesteps  | 370000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 92249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 3064     |
|    fps              | 155      |
|    time_elapsed     | 2376     |
|    total_timesteps  | 370160   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0219   |
|    n_updates        | 92289    |
----------------------------------
Eval num_timesteps=370500, episode_reward=315.36 +/- 34.76
Episode length: 103.84 +/- 8.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0861   |
| time/               |          |
|    total_timesteps  | 370500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.74     |
|    n_updates        | 92374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.0846   |
| time/               |          |
|    episodes         | 3068     |
|    fps              | 155      |
|    time_elapsed     | 2379     |
|    total_timesteps  | 370712   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.87     |
|    n_updates        | 92427    |
----------------------------------
Eval num_timesteps=371000, episode_reward=320.48 +/- 41.48
Episode length: 105.12 +/- 10.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.0826   |
| time/               |          |
|    total_timesteps  | 371000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0141   |
|    n_updates        | 92499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0808   |
| time/               |          |
|    episodes         | 3072     |
|    fps              | 155      |
|    time_elapsed     | 2383     |
|    total_timesteps  | 371240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0246   |
|    n_updates        | 92559    |
----------------------------------
Eval num_timesteps=371500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.079    |
| time/               |          |
|    total_timesteps  | 371500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 92624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.0769   |
| time/               |          |
|    episodes         | 3076     |
|    fps              | 155      |
|    time_elapsed     | 2386     |
|    total_timesteps  | 371784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0329   |
|    n_updates        | 92695    |
----------------------------------
Eval num_timesteps=372000, episode_reward=312.16 +/- 44.64
Episode length: 103.04 +/- 11.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0754   |
| time/               |          |
|    total_timesteps  | 372000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0282   |
|    n_updates        | 92749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 3080     |
|    fps              | 155      |
|    time_elapsed     | 2389     |
|    total_timesteps  | 372312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0157   |
|    n_updates        | 92827    |
----------------------------------
Eval num_timesteps=372500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0718   |
| time/               |          |
|    total_timesteps  | 372500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 92874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.0704   |
| time/               |          |
|    episodes         | 3084     |
|    fps              | 155      |
|    time_elapsed     | 2392     |
|    total_timesteps  | 372696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.084    |
|    n_updates        | 92923    |
----------------------------------
Eval num_timesteps=373000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0682   |
| time/               |          |
|    total_timesteps  | 373000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0178   |
|    n_updates        | 92999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 3088     |
|    fps              | 155      |
|    time_elapsed     | 2395     |
|    total_timesteps  | 373248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0244   |
|    n_updates        | 93061    |
----------------------------------
Eval num_timesteps=373500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0646   |
| time/               |          |
|    total_timesteps  | 373500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0304   |
|    n_updates        | 93124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.0631   |
| time/               |          |
|    episodes         | 3092     |
|    fps              | 155      |
|    time_elapsed     | 2398     |
|    total_timesteps  | 373704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0195   |
|    n_updates        | 93175    |
----------------------------------
Eval num_timesteps=374000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.061    |
| time/               |          |
|    total_timesteps  | 374000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.89     |
|    n_updates        | 93249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.06     |
| time/               |          |
|    episodes         | 3096     |
|    fps              | 155      |
|    time_elapsed     | 2401     |
|    total_timesteps  | 374136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 93283    |
----------------------------------
Eval num_timesteps=374500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0573   |
| time/               |          |
|    total_timesteps  | 374500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.88     |
|    n_updates        | 93374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.0568   |
| time/               |          |
|    episodes         | 3100     |
|    fps              | 155      |
|    time_elapsed     | 2404     |
|    total_timesteps  | 374576   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0973   |
|    n_updates        | 93393    |
----------------------------------
Eval num_timesteps=375000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0537   |
| time/               |          |
|    total_timesteps  | 375000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.58     |
|    n_updates        | 93499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.0532   |
| time/               |          |
|    episodes         | 3104     |
|    fps              | 155      |
|    time_elapsed     | 2407     |
|    total_timesteps  | 375072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0393   |
|    n_updates        | 93517    |
----------------------------------
Eval num_timesteps=375500, episode_reward=339.04 +/- 61.07
Episode length: 109.76 +/- 15.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 339      |
| rollout/            |          |
|    exploration_rate | 0.0501   |
| time/               |          |
|    total_timesteps  | 375500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0134   |
|    n_updates        | 93624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0497   |
| time/               |          |
|    episodes         | 3108     |
|    fps              | 155      |
|    time_elapsed     | 2411     |
|    total_timesteps  | 375552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.87     |
|    n_updates        | 93637    |
----------------------------------
Eval num_timesteps=376000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 376000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 93749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3112     |
|    fps              | 155      |
|    time_elapsed     | 2414     |
|    total_timesteps  | 376008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0399   |
|    n_updates        | 93751    |
----------------------------------
Eval num_timesteps=376500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 376500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.89     |
|    n_updates        | 93874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3116     |
|    fps              | 155      |
|    time_elapsed     | 2417     |
|    total_timesteps  | 376688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 93921    |
----------------------------------
Eval num_timesteps=377000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 377000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 93999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3120     |
|    fps              | 155      |
|    time_elapsed     | 2420     |
|    total_timesteps  | 377120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0312   |
|    n_updates        | 94029    |
----------------------------------
Eval num_timesteps=377500, episode_reward=316.00 +/- 42.93
Episode length: 104.00 +/- 10.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 377500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0155   |
|    n_updates        | 94124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3124     |
|    fps              | 155      |
|    time_elapsed     | 2423     |
|    total_timesteps  | 377552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 94137    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3128     |
|    fps              | 155      |
|    time_elapsed     | 2424     |
|    total_timesteps  | 377984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.74     |
|    n_updates        | 94245    |
----------------------------------
Eval num_timesteps=378000, episode_reward=344.16 +/- 66.40
Episode length: 111.04 +/- 16.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 378000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.73     |
|    n_updates        | 94249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3132     |
|    fps              | 155      |
|    time_elapsed     | 2427     |
|    total_timesteps  | 378392   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 94347    |
----------------------------------
Eval num_timesteps=378500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 378500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0544   |
|    n_updates        | 94374    |
----------------------------------
Eval num_timesteps=379000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 379000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 94499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3136     |
|    fps              | 155      |
|    time_elapsed     | 2433     |
|    total_timesteps  | 379000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3140     |
|    fps              | 155      |
|    time_elapsed     | 2433     |
|    total_timesteps  | 379464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 94615    |
----------------------------------
Eval num_timesteps=379500, episode_reward=296.80 +/- 36.77
Episode length: 99.20 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.2     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 379500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 94624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3144     |
|    fps              | 155      |
|    time_elapsed     | 2436     |
|    total_timesteps  | 379896   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 94723    |
----------------------------------
Eval num_timesteps=380000, episode_reward=315.36 +/- 34.76
Episode length: 103.84 +/- 8.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 380000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.86     |
|    n_updates        | 94749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3148     |
|    fps              | 155      |
|    time_elapsed     | 2440     |
|    total_timesteps  | 380360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.77     |
|    n_updates        | 94839    |
----------------------------------
Eval num_timesteps=380500, episode_reward=343.52 +/- 52.39
Episode length: 110.88 +/- 13.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 380500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0182   |
|    n_updates        | 94874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3152     |
|    fps              | 155      |
|    time_elapsed     | 2443     |
|    total_timesteps  | 380760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.93     |
|    n_updates        | 94939    |
----------------------------------
Eval num_timesteps=381000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 381000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0957   |
|    n_updates        | 94999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3156     |
|    fps              | 155      |
|    time_elapsed     | 2446     |
|    total_timesteps  | 381176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0955   |
|    n_updates        | 95043    |
----------------------------------
Eval num_timesteps=381500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 381500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.98     |
|    n_updates        | 95124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3160     |
|    fps              | 155      |
|    time_elapsed     | 2449     |
|    total_timesteps  | 381584   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0543   |
|    n_updates        | 95145    |
----------------------------------
Eval num_timesteps=382000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 382000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.71     |
|    n_updates        | 95249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3164     |
|    fps              | 155      |
|    time_elapsed     | 2452     |
|    total_timesteps  | 382008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 95251    |
----------------------------------
Eval num_timesteps=382500, episode_reward=309.60 +/- 36.20
Episode length: 102.40 +/- 9.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 382500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 95374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3168     |
|    fps              | 155      |
|    time_elapsed     | 2455     |
|    total_timesteps  | 382648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 95411    |
----------------------------------
Eval num_timesteps=383000, episode_reward=582.24 +/- 293.39
Episode length: 170.56 +/- 73.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 171      |
|    mean_reward      | 582      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 383000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.042    |
|    n_updates        | 95499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3172     |
|    fps              | 155      |
|    time_elapsed     | 2460     |
|    total_timesteps  | 383224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.9      |
|    n_updates        | 95555    |
----------------------------------
Eval num_timesteps=383500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 383500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.9      |
|    n_updates        | 95624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3176     |
|    fps              | 155      |
|    time_elapsed     | 2463     |
|    total_timesteps  | 383648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0562   |
|    n_updates        | 95661    |
----------------------------------
Eval num_timesteps=384000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 384000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.9      |
|    n_updates        | 95749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3180     |
|    fps              | 155      |
|    time_elapsed     | 2467     |
|    total_timesteps  | 384176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.016    |
|    n_updates        | 95793    |
----------------------------------
Eval num_timesteps=384500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 384500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0225   |
|    n_updates        | 95874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3184     |
|    fps              | 155      |
|    time_elapsed     | 2470     |
|    total_timesteps  | 384672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.77     |
|    n_updates        | 95917    |
----------------------------------
Eval num_timesteps=385000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 385000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 95999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3188     |
|    fps              | 155      |
|    time_elapsed     | 2473     |
|    total_timesteps  | 385216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0206   |
|    n_updates        | 96053    |
----------------------------------
Eval num_timesteps=385500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 385500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 96124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3192     |
|    fps              | 155      |
|    time_elapsed     | 2476     |
|    total_timesteps  | 385784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 96195    |
----------------------------------
Eval num_timesteps=386000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 386000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.9      |
|    n_updates        | 96249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3196     |
|    fps              | 155      |
|    time_elapsed     | 2479     |
|    total_timesteps  | 386312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.81     |
|    n_updates        | 96327    |
----------------------------------
Eval num_timesteps=386500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 386500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 96374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3200     |
|    fps              | 155      |
|    time_elapsed     | 2482     |
|    total_timesteps  | 386872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0283   |
|    n_updates        | 96467    |
----------------------------------
Eval num_timesteps=387000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 387000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0141   |
|    n_updates        | 96499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3204     |
|    fps              | 155      |
|    time_elapsed     | 2485     |
|    total_timesteps  | 387360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0325   |
|    n_updates        | 96589    |
----------------------------------
Eval num_timesteps=387500, episode_reward=319.20 +/- 43.52
Episode length: 104.80 +/- 10.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 387500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0312   |
|    n_updates        | 96624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3208     |
|    fps              | 155      |
|    time_elapsed     | 2488     |
|    total_timesteps  | 387792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.89     |
|    n_updates        | 96697    |
----------------------------------
Eval num_timesteps=388000, episode_reward=290.40 +/- 23.95
Episode length: 97.60 +/- 5.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 388000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0987   |
|    n_updates        | 96749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3212     |
|    fps              | 155      |
|    time_elapsed     | 2491     |
|    total_timesteps  | 388256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0492   |
|    n_updates        | 96813    |
----------------------------------
Eval num_timesteps=388500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 388500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.96     |
|    n_updates        | 96874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3216     |
|    fps              | 155      |
|    time_elapsed     | 2494     |
|    total_timesteps  | 388688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0367   |
|    n_updates        | 96921    |
----------------------------------
Eval num_timesteps=389000, episode_reward=349.28 +/- 54.67
Episode length: 112.32 +/- 13.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 349      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 389000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 96999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3220     |
|    fps              | 155      |
|    time_elapsed     | 2498     |
|    total_timesteps  | 389120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0185   |
|    n_updates        | 97029    |
----------------------------------
Eval num_timesteps=389500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 389500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0306   |
|    n_updates        | 97124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3224     |
|    fps              | 155      |
|    time_elapsed     | 2501     |
|    total_timesteps  | 389536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.022    |
|    n_updates        | 97133    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3228     |
|    fps              | 155      |
|    time_elapsed     | 2501     |
|    total_timesteps  | 389968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.76     |
|    n_updates        | 97241    |
----------------------------------
Eval num_timesteps=390000, episode_reward=359.52 +/- 52.68
Episode length: 114.88 +/- 13.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 360      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 390000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.89     |
|    n_updates        | 97249    |
----------------------------------
Eval num_timesteps=390500, episode_reward=348.00 +/- 83.69
Episode length: 112.00 +/- 20.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 348      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 390500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 97374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3232     |
|    fps              | 155      |
|    time_elapsed     | 2508     |
|    total_timesteps  | 390648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 97411    |
----------------------------------
Eval num_timesteps=391000, episode_reward=312.80 +/- 35.20
Episode length: 103.20 +/- 8.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 391000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 97499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3236     |
|    fps              | 155      |
|    time_elapsed     | 2511     |
|    total_timesteps  | 391032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0223   |
|    n_updates        | 97507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3240     |
|    fps              | 155      |
|    time_elapsed     | 2511     |
|    total_timesteps  | 391448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 97611    |
----------------------------------
Eval num_timesteps=391500, episode_reward=319.84 +/- 38.21
Episode length: 104.96 +/- 9.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 391500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.031    |
|    n_updates        | 97624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3244     |
|    fps              | 155      |
|    time_elapsed     | 2514     |
|    total_timesteps  | 391880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.67     |
|    n_updates        | 97719    |
----------------------------------
Eval num_timesteps=392000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 392000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.92     |
|    n_updates        | 97749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3248     |
|    fps              | 155      |
|    time_elapsed     | 2518     |
|    total_timesteps  | 392448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.051    |
|    n_updates        | 97861    |
----------------------------------
Eval num_timesteps=392500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 392500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 97874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3252     |
|    fps              | 155      |
|    time_elapsed     | 2521     |
|    total_timesteps  | 392912   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0228   |
|    n_updates        | 97977    |
----------------------------------
Eval num_timesteps=393000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 393000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 97999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3256     |
|    fps              | 155      |
|    time_elapsed     | 2524     |
|    total_timesteps  | 393320   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.95     |
|    n_updates        | 98079    |
----------------------------------
Eval num_timesteps=393500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 393500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.9      |
|    n_updates        | 98124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3260     |
|    fps              | 155      |
|    time_elapsed     | 2527     |
|    total_timesteps  | 393712   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0311   |
|    n_updates        | 98177    |
----------------------------------
Eval num_timesteps=394000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 394000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.82     |
|    n_updates        | 98249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3264     |
|    fps              | 155      |
|    time_elapsed     | 2530     |
|    total_timesteps  | 394256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 98313    |
----------------------------------
Eval num_timesteps=394500, episode_reward=335.20 +/- 49.99
Episode length: 108.80 +/- 12.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 335      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 394500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.084    |
|    n_updates        | 98374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3268     |
|    fps              | 155      |
|    time_elapsed     | 2533     |
|    total_timesteps  | 394856   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 98463    |
----------------------------------
Eval num_timesteps=395000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 395000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 98499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3272     |
|    fps              | 155      |
|    time_elapsed     | 2536     |
|    total_timesteps  | 395312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.034    |
|    n_updates        | 98577    |
----------------------------------
Eval num_timesteps=395500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 395500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.9      |
|    n_updates        | 98624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3276     |
|    fps              | 155      |
|    time_elapsed     | 2539     |
|    total_timesteps  | 395880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0297   |
|    n_updates        | 98719    |
----------------------------------
Eval num_timesteps=396000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 396000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.93     |
|    n_updates        | 98749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3280     |
|    fps              | 155      |
|    time_elapsed     | 2542     |
|    total_timesteps  | 396296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 98823    |
----------------------------------
Eval num_timesteps=396500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 396500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 98874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3284     |
|    fps              | 155      |
|    time_elapsed     | 2545     |
|    total_timesteps  | 396760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0261   |
|    n_updates        | 98939    |
----------------------------------
Eval num_timesteps=397000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 397000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 98999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3288     |
|    fps              | 155      |
|    time_elapsed     | 2548     |
|    total_timesteps  | 397232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.93     |
|    n_updates        | 99057    |
----------------------------------
Eval num_timesteps=397500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 397500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.91     |
|    n_updates        | 99124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3292     |
|    fps              | 155      |
|    time_elapsed     | 2552     |
|    total_timesteps  | 397784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0527   |
|    n_updates        | 99195    |
----------------------------------
Eval num_timesteps=398000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 398000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0193   |
|    n_updates        | 99249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3296     |
|    fps              | 155      |
|    time_elapsed     | 2555     |
|    total_timesteps  | 398288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0188   |
|    n_updates        | 99321    |
----------------------------------
Eval num_timesteps=398500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 398500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0289   |
|    n_updates        | 99374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3300     |
|    fps              | 155      |
|    time_elapsed     | 2558     |
|    total_timesteps  | 398720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0189   |
|    n_updates        | 99429    |
----------------------------------
Eval num_timesteps=399000, episode_reward=311.52 +/- 35.06
Episode length: 102.88 +/- 8.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 399000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0545   |
|    n_updates        | 99499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3304     |
|    fps              | 155      |
|    time_elapsed     | 2561     |
|    total_timesteps  | 399176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0338   |
|    n_updates        | 99543    |
----------------------------------
Eval num_timesteps=399500, episode_reward=308.32 +/- 40.76
Episode length: 102.08 +/- 10.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 399500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.8      |
|    n_updates        | 99624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3308     |
|    fps              | 155      |
|    time_elapsed     | 2564     |
|    total_timesteps  | 399592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0103   |
|    n_updates        | 99647    |
----------------------------------
Eval num_timesteps=400000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 400000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 99749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3312     |
|    fps              | 155      |
|    time_elapsed     | 2567     |
|    total_timesteps  | 400024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.144    |
|    n_updates        | 99755    |
----------------------------------
Eval num_timesteps=400500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 400500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 99874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3316     |
|    fps              | 155      |
|    time_elapsed     | 2570     |
|    total_timesteps  | 400504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0183   |
|    n_updates        | 99875    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3320     |
|    fps              | 155      |
|    time_elapsed     | 2570     |
|    total_timesteps  | 400904   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0143   |
|    n_updates        | 99975    |
----------------------------------
Eval num_timesteps=401000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 401000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.93     |
|    n_updates        | 99999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3324     |
|    fps              | 155      |
|    time_elapsed     | 2573     |
|    total_timesteps  | 401336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 100083   |
----------------------------------
Eval num_timesteps=401500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 401500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0183   |
|    n_updates        | 100124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3328     |
|    fps              | 155      |
|    time_elapsed     | 2576     |
|    total_timesteps  | 401744   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.93     |
|    n_updates        | 100185   |
----------------------------------
Eval num_timesteps=402000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 402000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.84     |
|    n_updates        | 100249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3332     |
|    fps              | 155      |
|    time_elapsed     | 2579     |
|    total_timesteps  | 402216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.92     |
|    n_updates        | 100303   |
----------------------------------
Eval num_timesteps=402500, episode_reward=312.80 +/- 36.35
Episode length: 103.20 +/- 9.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 402500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0231   |
|    n_updates        | 100374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3336     |
|    fps              | 155      |
|    time_elapsed     | 2582     |
|    total_timesteps  | 402696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.92     |
|    n_updates        | 100423   |
----------------------------------
Eval num_timesteps=403000, episode_reward=311.52 +/- 39.46
Episode length: 102.88 +/- 9.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 403000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0214   |
|    n_updates        | 100499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3340     |
|    fps              | 155      |
|    time_elapsed     | 2586     |
|    total_timesteps  | 403128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.024    |
|    n_updates        | 100531   |
----------------------------------
Eval num_timesteps=403500, episode_reward=359.52 +/- 72.90
Episode length: 114.88 +/- 18.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 360      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 403500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.82     |
|    n_updates        | 100624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3344     |
|    fps              | 155      |
|    time_elapsed     | 2589     |
|    total_timesteps  | 403760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.95     |
|    n_updates        | 100689   |
----------------------------------
Eval num_timesteps=404000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 404000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0317   |
|    n_updates        | 100749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3348     |
|    fps              | 155      |
|    time_elapsed     | 2592     |
|    total_timesteps  | 404192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 100797   |
----------------------------------
Eval num_timesteps=404500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 404500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0277   |
|    n_updates        | 100874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3352     |
|    fps              | 155      |
|    time_elapsed     | 2595     |
|    total_timesteps  | 404664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.92     |
|    n_updates        | 100915   |
----------------------------------
Eval num_timesteps=405000, episode_reward=316.00 +/- 35.05
Episode length: 104.00 +/- 8.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 405000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0158   |
|    n_updates        | 100999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3356     |
|    fps              | 155      |
|    time_elapsed     | 2598     |
|    total_timesteps  | 405072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0303   |
|    n_updates        | 101017   |
----------------------------------
Eval num_timesteps=405500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 405500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0214   |
|    n_updates        | 101124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3360     |
|    fps              | 155      |
|    time_elapsed     | 2601     |
|    total_timesteps  | 405528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0228   |
|    n_updates        | 101131   |
----------------------------------
Eval num_timesteps=406000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 406000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.016    |
|    n_updates        | 101249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3364     |
|    fps              | 155      |
|    time_elapsed     | 2605     |
|    total_timesteps  | 406048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 101261   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3368     |
|    fps              | 156      |
|    time_elapsed     | 2605     |
|    total_timesteps  | 406456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0465   |
|    n_updates        | 101363   |
----------------------------------
Eval num_timesteps=406500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 406500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0557   |
|    n_updates        | 101374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3372     |
|    fps              | 156      |
|    time_elapsed     | 2608     |
|    total_timesteps  | 406936   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.87     |
|    n_updates        | 101483   |
----------------------------------
Eval num_timesteps=407000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 407000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.95     |
|    n_updates        | 101499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3376     |
|    fps              | 155      |
|    time_elapsed     | 2611     |
|    total_timesteps  | 407320   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.95     |
|    n_updates        | 101579   |
----------------------------------
Eval num_timesteps=407500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 407500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0252   |
|    n_updates        | 101624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3380     |
|    fps              | 155      |
|    time_elapsed     | 2614     |
|    total_timesteps  | 407776   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0289   |
|    n_updates        | 101693   |
----------------------------------
Eval num_timesteps=408000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 408000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 101749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3384     |
|    fps              | 155      |
|    time_elapsed     | 2617     |
|    total_timesteps  | 408256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.87     |
|    n_updates        | 101813   |
----------------------------------
Eval num_timesteps=408500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 408500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0259   |
|    n_updates        | 101874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3388     |
|    fps              | 155      |
|    time_elapsed     | 2620     |
|    total_timesteps  | 408688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 101921   |
----------------------------------
Eval num_timesteps=409000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 409000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0337   |
|    n_updates        | 101999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3392     |
|    fps              | 155      |
|    time_elapsed     | 2623     |
|    total_timesteps  | 409096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.93     |
|    n_updates        | 102023   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3396     |
|    fps              | 156      |
|    time_elapsed     | 2623     |
|    total_timesteps  | 409480   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0307   |
|    n_updates        | 102119   |
----------------------------------
Eval num_timesteps=409500, episode_reward=313.44 +/- 36.11
Episode length: 103.36 +/- 9.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 409500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0457   |
|    n_updates        | 102124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3400     |
|    fps              | 156      |
|    time_elapsed     | 2627     |
|    total_timesteps  | 409904   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 102225   |
----------------------------------
Eval num_timesteps=410000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 410000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.93     |
|    n_updates        | 102249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3404     |
|    fps              | 156      |
|    time_elapsed     | 2630     |
|    total_timesteps  | 410408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.95     |
|    n_updates        | 102351   |
----------------------------------
Eval num_timesteps=410500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 410500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 11.7     |
|    n_updates        | 102374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3408     |
|    fps              | 156      |
|    time_elapsed     | 2633     |
|    total_timesteps  | 410872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0126   |
|    n_updates        | 102467   |
----------------------------------
Eval num_timesteps=411000, episode_reward=424.80 +/- 171.13
Episode length: 131.20 +/- 42.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 425      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 411000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 102499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3412     |
|    fps              | 155      |
|    time_elapsed     | 2637     |
|    total_timesteps  | 411384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.96     |
|    n_updates        | 102595   |
----------------------------------
Eval num_timesteps=411500, episode_reward=311.52 +/- 36.77
Episode length: 102.88 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 411500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.95     |
|    n_updates        | 102624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3416     |
|    fps              | 155      |
|    time_elapsed     | 2640     |
|    total_timesteps  | 411792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0394   |
|    n_updates        | 102697   |
----------------------------------
Eval num_timesteps=412000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 412000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.96     |
|    n_updates        | 102749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3420     |
|    fps              | 155      |
|    time_elapsed     | 2643     |
|    total_timesteps  | 412248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 102811   |
----------------------------------
Eval num_timesteps=412500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 412500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 102874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3424     |
|    fps              | 155      |
|    time_elapsed     | 2646     |
|    total_timesteps  | 412704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0575   |
|    n_updates        | 102925   |
----------------------------------
Eval num_timesteps=413000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 413000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0362   |
|    n_updates        | 102999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3428     |
|    fps              | 155      |
|    time_elapsed     | 2649     |
|    total_timesteps  | 413208   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0225   |
|    n_updates        | 103051   |
----------------------------------
Eval num_timesteps=413500, episode_reward=310.88 +/- 33.48
Episode length: 102.72 +/- 8.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 413500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 103124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3432     |
|    fps              | 155      |
|    time_elapsed     | 2652     |
|    total_timesteps  | 413760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0493   |
|    n_updates        | 103189   |
----------------------------------
Eval num_timesteps=414000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 414000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.019    |
|    n_updates        | 103249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3436     |
|    fps              | 155      |
|    time_elapsed     | 2655     |
|    total_timesteps  | 414224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 103305   |
----------------------------------
Eval num_timesteps=414500, episode_reward=313.44 +/- 34.37
Episode length: 103.36 +/- 8.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 414500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0149   |
|    n_updates        | 103374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3440     |
|    fps              | 155      |
|    time_elapsed     | 2659     |
|    total_timesteps  | 414696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.021    |
|    n_updates        | 103423   |
----------------------------------
Eval num_timesteps=415000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 415000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 103499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3444     |
|    fps              | 155      |
|    time_elapsed     | 2662     |
|    total_timesteps  | 415168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 103541   |
----------------------------------
Eval num_timesteps=415500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 415500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 103624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3448     |
|    fps              | 155      |
|    time_elapsed     | 2665     |
|    total_timesteps  | 415656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 103663   |
----------------------------------
Eval num_timesteps=416000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 416000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 103749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3452     |
|    fps              | 155      |
|    time_elapsed     | 2668     |
|    total_timesteps  | 416192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 103797   |
----------------------------------
Eval num_timesteps=416500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 416500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0178   |
|    n_updates        | 103874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3456     |
|    fps              | 155      |
|    time_elapsed     | 2671     |
|    total_timesteps  | 416704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.048    |
|    n_updates        | 103925   |
----------------------------------
Eval num_timesteps=417000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 417000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0292   |
|    n_updates        | 103999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3460     |
|    fps              | 155      |
|    time_elapsed     | 2674     |
|    total_timesteps  | 417136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 104033   |
----------------------------------
Eval num_timesteps=417500, episode_reward=344.16 +/- 57.83
Episode length: 111.04 +/- 14.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 417500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0396   |
|    n_updates        | 104124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3464     |
|    fps              | 155      |
|    time_elapsed     | 2677     |
|    total_timesteps  | 417592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0213   |
|    n_updates        | 104147   |
----------------------------------
Eval num_timesteps=418000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 418000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0158   |
|    n_updates        | 104249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3468     |
|    fps              | 155      |
|    time_elapsed     | 2680     |
|    total_timesteps  | 418080   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.89     |
|    n_updates        | 104269   |
----------------------------------
Eval num_timesteps=418500, episode_reward=664.16 +/- 341.53
Episode length: 191.04 +/- 85.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 664      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 418500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 104374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3472     |
|    fps              | 155      |
|    time_elapsed     | 2686     |
|    total_timesteps  | 418560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0134   |
|    n_updates        | 104389   |
----------------------------------
Eval num_timesteps=419000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 419000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0233   |
|    n_updates        | 104499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3476     |
|    fps              | 155      |
|    time_elapsed     | 2689     |
|    total_timesteps  | 419056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0247   |
|    n_updates        | 104513   |
----------------------------------
Eval num_timesteps=419500, episode_reward=309.60 +/- 37.86
Episode length: 102.40 +/- 9.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 419500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0482   |
|    n_updates        | 104624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3480     |
|    fps              | 155      |
|    time_elapsed     | 2692     |
|    total_timesteps  | 419552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0486   |
|    n_updates        | 104637   |
----------------------------------
Eval num_timesteps=420000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 420000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0193   |
|    n_updates        | 104749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3484     |
|    fps              | 155      |
|    time_elapsed     | 2695     |
|    total_timesteps  | 420048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.161    |
|    n_updates        | 104761   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3488     |
|    fps              | 155      |
|    time_elapsed     | 2696     |
|    total_timesteps  | 420480   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0178   |
|    n_updates        | 104869   |
----------------------------------
Eval num_timesteps=420500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 420500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0256   |
|    n_updates        | 104874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3492     |
|    fps              | 155      |
|    time_elapsed     | 2699     |
|    total_timesteps  | 420952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.98     |
|    n_updates        | 104987   |
----------------------------------
Eval num_timesteps=421000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 421000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0616   |
|    n_updates        | 104999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3496     |
|    fps              | 155      |
|    time_elapsed     | 2702     |
|    total_timesteps  | 421408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 105101   |
----------------------------------
Eval num_timesteps=421500, episode_reward=312.16 +/- 32.41
Episode length: 103.04 +/- 8.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 421500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 105124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3500     |
|    fps              | 155      |
|    time_elapsed     | 2705     |
|    total_timesteps  | 421816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0377   |
|    n_updates        | 105203   |
----------------------------------
Eval num_timesteps=422000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 422000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 105249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3504     |
|    fps              | 155      |
|    time_elapsed     | 2708     |
|    total_timesteps  | 422296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 105323   |
----------------------------------
Eval num_timesteps=422500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 422500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 105374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3508     |
|    fps              | 155      |
|    time_elapsed     | 2711     |
|    total_timesteps  | 422776   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0554   |
|    n_updates        | 105443   |
----------------------------------
Eval num_timesteps=423000, episode_reward=315.36 +/- 37.04
Episode length: 103.84 +/- 9.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 423000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 105499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3512     |
|    fps              | 155      |
|    time_elapsed     | 2714     |
|    total_timesteps  | 423288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.9      |
|    n_updates        | 105571   |
----------------------------------
Eval num_timesteps=423500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 423500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0519   |
|    n_updates        | 105624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3516     |
|    fps              | 155      |
|    time_elapsed     | 2717     |
|    total_timesteps  | 423720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0498   |
|    n_updates        | 105679   |
----------------------------------
Eval num_timesteps=424000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 424000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.96     |
|    n_updates        | 105749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3520     |
|    fps              | 155      |
|    time_elapsed     | 2720     |
|    total_timesteps  | 424224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.94     |
|    n_updates        | 105805   |
----------------------------------
Eval num_timesteps=424500, episode_reward=316.64 +/- 39.19
Episode length: 104.16 +/- 9.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 424500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0297   |
|    n_updates        | 105874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3524     |
|    fps              | 155      |
|    time_elapsed     | 2723     |
|    total_timesteps  | 424680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0663   |
|    n_updates        | 105919   |
----------------------------------
Eval num_timesteps=425000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 425000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 105999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3528     |
|    fps              | 155      |
|    time_elapsed     | 2726     |
|    total_timesteps  | 425112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0801   |
|    n_updates        | 106027   |
----------------------------------
Eval num_timesteps=425500, episode_reward=422.24 +/- 143.46
Episode length: 130.56 +/- 35.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 422      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 425500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 106124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3532     |
|    fps              | 155      |
|    time_elapsed     | 2730     |
|    total_timesteps  | 425576   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 106143   |
----------------------------------
Eval num_timesteps=426000, episode_reward=346.08 +/- 85.25
Episode length: 111.52 +/- 21.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 346      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 426000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0215   |
|    n_updates        | 106249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3536     |
|    fps              | 155      |
|    time_elapsed     | 2734     |
|    total_timesteps  | 426056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 106263   |
----------------------------------
Eval num_timesteps=426500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 426500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.91     |
|    n_updates        | 106374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3540     |
|    fps              | 155      |
|    time_elapsed     | 2737     |
|    total_timesteps  | 426648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0132   |
|    n_updates        | 106411   |
----------------------------------
Eval num_timesteps=427000, episode_reward=307.04 +/- 33.28
Episode length: 101.76 +/- 8.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 427000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.054    |
|    n_updates        | 106499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3544     |
|    fps              | 155      |
|    time_elapsed     | 2740     |
|    total_timesteps  | 427072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.96     |
|    n_updates        | 106517   |
----------------------------------
Eval num_timesteps=427500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 427500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 106624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3548     |
|    fps              | 155      |
|    time_elapsed     | 2743     |
|    total_timesteps  | 427568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 106641   |
----------------------------------
Eval num_timesteps=428000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 428000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 106749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3552     |
|    fps              | 155      |
|    time_elapsed     | 2746     |
|    total_timesteps  | 428000   |
----------------------------------
Eval num_timesteps=428500, episode_reward=308.96 +/- 35.22
Episode length: 102.24 +/- 8.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 428500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 106874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3556     |
|    fps              | 155      |
|    time_elapsed     | 2749     |
|    total_timesteps  | 428536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0632   |
|    n_updates        | 106883   |
----------------------------------
Eval num_timesteps=429000, episode_reward=321.12 +/- 42.63
Episode length: 105.28 +/- 10.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 429000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 106999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3560     |
|    fps              | 155      |
|    time_elapsed     | 2752     |
|    total_timesteps  | 429040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.86     |
|    n_updates        | 107009   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3564     |
|    fps              | 155      |
|    time_elapsed     | 2753     |
|    total_timesteps  | 429472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.96     |
|    n_updates        | 107117   |
----------------------------------
Eval num_timesteps=429500, episode_reward=316.00 +/- 43.41
Episode length: 104.00 +/- 10.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 429500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 107124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3568     |
|    fps              | 155      |
|    time_elapsed     | 2756     |
|    total_timesteps  | 429928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0293   |
|    n_updates        | 107231   |
----------------------------------
Eval num_timesteps=430000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 430000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0128   |
|    n_updates        | 107249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3572     |
|    fps              | 155      |
|    time_elapsed     | 2759     |
|    total_timesteps  | 430384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 107345   |
----------------------------------
Eval num_timesteps=430500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 430500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.99     |
|    n_updates        | 107374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3576     |
|    fps              | 156      |
|    time_elapsed     | 2762     |
|    total_timesteps  | 430952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.98     |
|    n_updates        | 107487   |
----------------------------------
Eval num_timesteps=431000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 431000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0142   |
|    n_updates        | 107499   |
----------------------------------
Eval num_timesteps=431500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 431500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0138   |
|    n_updates        | 107624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3580     |
|    fps              | 155      |
|    time_elapsed     | 2768     |
|    total_timesteps  | 431512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0207   |
|    n_updates        | 107627   |
----------------------------------
Eval num_timesteps=432000, episode_reward=297.44 +/- 57.63
Episode length: 99.36 +/- 14.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 432000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.94     |
|    n_updates        | 107749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3584     |
|    fps              | 155      |
|    time_elapsed     | 2771     |
|    total_timesteps  | 432144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.98     |
|    n_updates        | 107785   |
----------------------------------
Eval num_timesteps=432500, episode_reward=314.72 +/- 40.96
Episode length: 103.68 +/- 10.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 432500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 107874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3588     |
|    fps              | 155      |
|    time_elapsed     | 2774     |
|    total_timesteps  | 432592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 107897   |
----------------------------------
Eval num_timesteps=433000, episode_reward=337.76 +/- 94.37
Episode length: 109.44 +/- 23.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 338      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 433000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0307   |
|    n_updates        | 107999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3592     |
|    fps              | 155      |
|    time_elapsed     | 2778     |
|    total_timesteps  | 433096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.93     |
|    n_updates        | 108023   |
----------------------------------
Eval num_timesteps=433500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 433500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0552   |
|    n_updates        | 108124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3596     |
|    fps              | 155      |
|    time_elapsed     | 2781     |
|    total_timesteps  | 433504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.95     |
|    n_updates        | 108125   |
----------------------------------
Eval num_timesteps=434000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 434000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0402   |
|    n_updates        | 108249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3600     |
|    fps              | 155      |
|    time_elapsed     | 2784     |
|    total_timesteps  | 434024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0237   |
|    n_updates        | 108255   |
----------------------------------
Eval num_timesteps=434500, episode_reward=313.44 +/- 38.31
Episode length: 103.36 +/- 9.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 434500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0216   |
|    n_updates        | 108374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3604     |
|    fps              | 155      |
|    time_elapsed     | 2787     |
|    total_timesteps  | 434520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.022    |
|    n_updates        | 108379   |
----------------------------------
Eval num_timesteps=435000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 435000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 108499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3608     |
|    fps              | 155      |
|    time_elapsed     | 2790     |
|    total_timesteps  | 435056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.92     |
|    n_updates        | 108513   |
----------------------------------
Eval num_timesteps=435500, episode_reward=324.32 +/- 69.47
Episode length: 106.08 +/- 17.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 435500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0262   |
|    n_updates        | 108624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3612     |
|    fps              | 155      |
|    time_elapsed     | 2793     |
|    total_timesteps  | 435520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0237   |
|    n_updates        | 108629   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3616     |
|    fps              | 156      |
|    time_elapsed     | 2794     |
|    total_timesteps  | 435952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 108737   |
----------------------------------
Eval num_timesteps=436000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 436000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0132   |
|    n_updates        | 108749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3620     |
|    fps              | 156      |
|    time_elapsed     | 2797     |
|    total_timesteps  | 436408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 108851   |
----------------------------------
Eval num_timesteps=436500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 436500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.98     |
|    n_updates        | 108874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3624     |
|    fps              | 156      |
|    time_elapsed     | 2800     |
|    total_timesteps  | 436864   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0141   |
|    n_updates        | 108965   |
----------------------------------
Eval num_timesteps=437000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 437000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0302   |
|    n_updates        | 108999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3628     |
|    fps              | 155      |
|    time_elapsed     | 2803     |
|    total_timesteps  | 437328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0234   |
|    n_updates        | 109081   |
----------------------------------
Eval num_timesteps=437500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 437500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.026    |
|    n_updates        | 109124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3632     |
|    fps              | 155      |
|    time_elapsed     | 2806     |
|    total_timesteps  | 437712   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 109177   |
----------------------------------
Eval num_timesteps=438000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 438000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 109249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3636     |
|    fps              | 155      |
|    time_elapsed     | 2809     |
|    total_timesteps  | 438216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0238   |
|    n_updates        | 109303   |
----------------------------------
Eval num_timesteps=438500, episode_reward=314.72 +/- 40.46
Episode length: 103.68 +/- 10.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 438500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 109374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3640     |
|    fps              | 156      |
|    time_elapsed     | 2812     |
|    total_timesteps  | 438808   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0226   |
|    n_updates        | 109451   |
----------------------------------
Eval num_timesteps=439000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 439000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 109499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3644     |
|    fps              | 155      |
|    time_elapsed     | 2815     |
|    total_timesteps  | 439256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.88     |
|    n_updates        | 109563   |
----------------------------------
Eval num_timesteps=439500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 439500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 109624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3648     |
|    fps              | 155      |
|    time_elapsed     | 2818     |
|    total_timesteps  | 439664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.92     |
|    n_updates        | 109665   |
----------------------------------
Eval num_timesteps=440000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 440000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.97     |
|    n_updates        | 109749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3652     |
|    fps              | 155      |
|    time_elapsed     | 2821     |
|    total_timesteps  | 440136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 109783   |
----------------------------------
Eval num_timesteps=440500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 440500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0856   |
|    n_updates        | 109874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3656     |
|    fps              | 155      |
|    time_elapsed     | 2824     |
|    total_timesteps  | 440560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0873   |
|    n_updates        | 109889   |
----------------------------------
Eval num_timesteps=441000, episode_reward=360.16 +/- 80.16
Episode length: 115.04 +/- 20.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 360      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 441000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 109999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3660     |
|    fps              | 155      |
|    time_elapsed     | 2828     |
|    total_timesteps  | 441144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.96     |
|    n_updates        | 110035   |
----------------------------------
Eval num_timesteps=441500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 441500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0659   |
|    n_updates        | 110124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3664     |
|    fps              | 155      |
|    time_elapsed     | 2831     |
|    total_timesteps  | 441600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.016    |
|    n_updates        | 110149   |
----------------------------------
Eval num_timesteps=442000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 442000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0191   |
|    n_updates        | 110249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3668     |
|    fps              | 155      |
|    time_elapsed     | 2834     |
|    total_timesteps  | 442056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 110263   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3672     |
|    fps              | 156      |
|    time_elapsed     | 2834     |
|    total_timesteps  | 442472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 110367   |
----------------------------------
Eval num_timesteps=442500, episode_reward=315.36 +/- 38.13
Episode length: 103.84 +/- 9.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 442500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0607   |
|    n_updates        | 110374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3676     |
|    fps              | 156      |
|    time_elapsed     | 2838     |
|    total_timesteps  | 442952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.98     |
|    n_updates        | 110487   |
----------------------------------
Eval num_timesteps=443000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 443000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0435   |
|    n_updates        | 110499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3680     |
|    fps              | 156      |
|    time_elapsed     | 2841     |
|    total_timesteps  | 443400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.01     |
|    n_updates        | 110599   |
----------------------------------
Eval num_timesteps=443500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 443500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 110624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3684     |
|    fps              | 156      |
|    time_elapsed     | 2844     |
|    total_timesteps  | 443952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0328   |
|    n_updates        | 110737   |
----------------------------------
Eval num_timesteps=444000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 444000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 110749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3688     |
|    fps              | 156      |
|    time_elapsed     | 2847     |
|    total_timesteps  | 444360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0313   |
|    n_updates        | 110839   |
----------------------------------
Eval num_timesteps=444500, episode_reward=324.32 +/- 47.38
Episode length: 106.08 +/- 11.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 444500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0274   |
|    n_updates        | 110874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3692     |
|    fps              | 156      |
|    time_elapsed     | 2850     |
|    total_timesteps  | 444816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0441   |
|    n_updates        | 110953   |
----------------------------------
Eval num_timesteps=445000, episode_reward=327.52 +/- 49.89
Episode length: 106.88 +/- 12.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 445000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 110999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3696     |
|    fps              | 156      |
|    time_elapsed     | 2853     |
|    total_timesteps  | 445272   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 111067   |
----------------------------------
Eval num_timesteps=445500, episode_reward=286.56 +/- 14.08
Episode length: 96.64 +/- 3.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 445500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 111124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3700     |
|    fps              | 156      |
|    time_elapsed     | 2856     |
|    total_timesteps  | 445752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0103   |
|    n_updates        | 111187   |
----------------------------------
Eval num_timesteps=446000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 446000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0232   |
|    n_updates        | 111249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3704     |
|    fps              | 156      |
|    time_elapsed     | 2859     |
|    total_timesteps  | 446280   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0247   |
|    n_updates        | 111319   |
----------------------------------
Eval num_timesteps=446500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 446500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0149   |
|    n_updates        | 111374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3708     |
|    fps              | 156      |
|    time_elapsed     | 2863     |
|    total_timesteps  | 446808   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.96     |
|    n_updates        | 111451   |
----------------------------------
Eval num_timesteps=447000, episode_reward=305.12 +/- 26.86
Episode length: 101.28 +/- 6.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 447000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 111499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3712     |
|    fps              | 156      |
|    time_elapsed     | 2866     |
|    total_timesteps  | 447216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0153   |
|    n_updates        | 111553   |
----------------------------------
Eval num_timesteps=447500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 447500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 111624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3716     |
|    fps              | 156      |
|    time_elapsed     | 2869     |
|    total_timesteps  | 447656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0299   |
|    n_updates        | 111663   |
----------------------------------
Eval num_timesteps=448000, episode_reward=310.24 +/- 34.28
Episode length: 102.56 +/- 8.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 448000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0428   |
|    n_updates        | 111749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3720     |
|    fps              | 155      |
|    time_elapsed     | 2872     |
|    total_timesteps  | 448064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0429   |
|    n_updates        | 111765   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3724     |
|    fps              | 156      |
|    time_elapsed     | 2872     |
|    total_timesteps  | 448472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 111867   |
----------------------------------
Eval num_timesteps=448500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 448500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 111874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3728     |
|    fps              | 156      |
|    time_elapsed     | 2875     |
|    total_timesteps  | 448976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 111993   |
----------------------------------
Eval num_timesteps=449000, episode_reward=311.52 +/- 38.41
Episode length: 102.88 +/- 9.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 449000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 111999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3732     |
|    fps              | 156      |
|    time_elapsed     | 2878     |
|    total_timesteps  | 449424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.96     |
|    n_updates        | 112105   |
----------------------------------
Eval num_timesteps=449500, episode_reward=540.64 +/- 269.90
Episode length: 160.16 +/- 67.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 160      |
|    mean_reward      | 541      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 449500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00908  |
|    n_updates        | 112124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3736     |
|    fps              | 156      |
|    time_elapsed     | 2883     |
|    total_timesteps  | 449904   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0226   |
|    n_updates        | 112225   |
----------------------------------
Eval num_timesteps=450000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 450000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0393   |
|    n_updates        | 112249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3740     |
|    fps              | 156      |
|    time_elapsed     | 2886     |
|    total_timesteps  | 450384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.01     |
|    n_updates        | 112345   |
----------------------------------
Eval num_timesteps=450500, episode_reward=321.12 +/- 39.64
Episode length: 105.28 +/- 9.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 450500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 112374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3744     |
|    fps              | 156      |
|    time_elapsed     | 2889     |
|    total_timesteps  | 450864   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 112465   |
----------------------------------
Eval num_timesteps=451000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 451000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 112499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3748     |
|    fps              | 155      |
|    time_elapsed     | 2892     |
|    total_timesteps  | 451288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.051    |
|    n_updates        | 112571   |
----------------------------------
Eval num_timesteps=451500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 451500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 112624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3752     |
|    fps              | 155      |
|    time_elapsed     | 2895     |
|    total_timesteps  | 451720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.97     |
|    n_updates        | 112679   |
----------------------------------
Eval num_timesteps=452000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 452000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 112749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3756     |
|    fps              | 155      |
|    time_elapsed     | 2898     |
|    total_timesteps  | 452184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0202   |
|    n_updates        | 112795   |
----------------------------------
Eval num_timesteps=452500, episode_reward=309.60 +/- 40.48
Episode length: 102.40 +/- 10.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 452500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0183   |
|    n_updates        | 112874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3760     |
|    fps              | 155      |
|    time_elapsed     | 2902     |
|    total_timesteps  | 452664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0129   |
|    n_updates        | 112915   |
----------------------------------
Eval num_timesteps=453000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 453000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0634   |
|    n_updates        | 112999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3764     |
|    fps              | 155      |
|    time_elapsed     | 2905     |
|    total_timesteps  | 453192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.083    |
|    n_updates        | 113047   |
----------------------------------
Eval num_timesteps=453500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 453500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 113124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3768     |
|    fps              | 155      |
|    time_elapsed     | 2908     |
|    total_timesteps  | 453624   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0196   |
|    n_updates        | 113155   |
----------------------------------
Eval num_timesteps=454000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 454000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 113249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3772     |
|    fps              | 155      |
|    time_elapsed     | 2911     |
|    total_timesteps  | 454104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0205   |
|    n_updates        | 113275   |
----------------------------------
Eval num_timesteps=454500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 454500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0402   |
|    n_updates        | 113374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3776     |
|    fps              | 155      |
|    time_elapsed     | 2914     |
|    total_timesteps  | 454584   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 113395   |
----------------------------------
Eval num_timesteps=455000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 455000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 113499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3780     |
|    fps              | 155      |
|    time_elapsed     | 2917     |
|    total_timesteps  | 455072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 113517   |
----------------------------------
Eval num_timesteps=455500, episode_reward=356.96 +/- 61.73
Episode length: 114.24 +/- 15.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 357      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 455500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0332   |
|    n_updates        | 113624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3784     |
|    fps              | 155      |
|    time_elapsed     | 2920     |
|    total_timesteps  | 455552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0843   |
|    n_updates        | 113637   |
----------------------------------
Eval num_timesteps=456000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 456000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 113749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3788     |
|    fps              | 155      |
|    time_elapsed     | 2923     |
|    total_timesteps  | 456024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.99     |
|    n_updates        | 113755   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3792     |
|    fps              | 156      |
|    time_elapsed     | 2924     |
|    total_timesteps  | 456480   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 113869   |
----------------------------------
Eval num_timesteps=456500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 456500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 113874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3796     |
|    fps              | 156      |
|    time_elapsed     | 2927     |
|    total_timesteps  | 456936   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0446   |
|    n_updates        | 113983   |
----------------------------------
Eval num_timesteps=457000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 457000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0335   |
|    n_updates        | 113999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3800     |
|    fps              | 156      |
|    time_elapsed     | 2930     |
|    total_timesteps  | 457456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0244   |
|    n_updates        | 114113   |
----------------------------------
Eval num_timesteps=457500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 457500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0213   |
|    n_updates        | 114124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3804     |
|    fps              | 156      |
|    time_elapsed     | 2933     |
|    total_timesteps  | 457864   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.01     |
|    n_updates        | 114215   |
----------------------------------
Eval num_timesteps=458000, episode_reward=366.56 +/- 79.70
Episode length: 116.64 +/- 19.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 367      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 458000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.99     |
|    n_updates        | 114249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3808     |
|    fps              | 156      |
|    time_elapsed     | 2936     |
|    total_timesteps  | 458272   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.01     |
|    n_updates        | 114317   |
----------------------------------
Eval num_timesteps=458500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 458500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0526   |
|    n_updates        | 114374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3812     |
|    fps              | 156      |
|    time_elapsed     | 2940     |
|    total_timesteps  | 458752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0231   |
|    n_updates        | 114437   |
----------------------------------
Eval num_timesteps=459000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 459000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 114499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3816     |
|    fps              | 156      |
|    time_elapsed     | 2943     |
|    total_timesteps  | 459216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0365   |
|    n_updates        | 114553   |
----------------------------------
Eval num_timesteps=459500, episode_reward=328.16 +/- 56.84
Episode length: 107.04 +/- 14.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 459500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 114624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3820     |
|    fps              | 156      |
|    time_elapsed     | 2946     |
|    total_timesteps  | 459720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0321   |
|    n_updates        | 114679   |
----------------------------------
Eval num_timesteps=460000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 460000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0681   |
|    n_updates        | 114749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3824     |
|    fps              | 156      |
|    time_elapsed     | 2949     |
|    total_timesteps  | 460184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0612   |
|    n_updates        | 114795   |
----------------------------------
Eval num_timesteps=460500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 460500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.01     |
|    n_updates        | 114874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3828     |
|    fps              | 156      |
|    time_elapsed     | 2952     |
|    total_timesteps  | 460592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.08     |
|    n_updates        | 114897   |
----------------------------------
Eval num_timesteps=461000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 461000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0254   |
|    n_updates        | 114999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3832     |
|    fps              | 156      |
|    time_elapsed     | 2955     |
|    total_timesteps  | 461072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.101    |
|    n_updates        | 115017   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3836     |
|    fps              | 156      |
|    time_elapsed     | 2955     |
|    total_timesteps  | 461496   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.02     |
|    n_updates        | 115123   |
----------------------------------
Eval num_timesteps=461500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 461500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.03     |
|    n_updates        | 115124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3840     |
|    fps              | 156      |
|    time_elapsed     | 2959     |
|    total_timesteps  | 461928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.01     |
|    n_updates        | 115231   |
----------------------------------
Eval num_timesteps=462000, episode_reward=321.12 +/- 41.16
Episode length: 105.28 +/- 10.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 462000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 115249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3844     |
|    fps              | 156      |
|    time_elapsed     | 2962     |
|    total_timesteps  | 462448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 115361   |
----------------------------------
Eval num_timesteps=462500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 462500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 115374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3848     |
|    fps              | 156      |
|    time_elapsed     | 2965     |
|    total_timesteps  | 462992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0485   |
|    n_updates        | 115497   |
----------------------------------
Eval num_timesteps=463000, episode_reward=302.56 +/- 40.01
Episode length: 100.64 +/- 10.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 463000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0237   |
|    n_updates        | 115499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3852     |
|    fps              | 156      |
|    time_elapsed     | 2968     |
|    total_timesteps  | 463456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0389   |
|    n_updates        | 115613   |
----------------------------------
Eval num_timesteps=463500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 463500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0253   |
|    n_updates        | 115624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3856     |
|    fps              | 156      |
|    time_elapsed     | 2971     |
|    total_timesteps  | 463904   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 115725   |
----------------------------------
Eval num_timesteps=464000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 464000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0189   |
|    n_updates        | 115749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3860     |
|    fps              | 156      |
|    time_elapsed     | 2974     |
|    total_timesteps  | 464312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 115827   |
----------------------------------
Eval num_timesteps=464500, episode_reward=285.28 +/- 6.27
Episode length: 96.32 +/- 1.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 464500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0128   |
|    n_updates        | 115874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3864     |
|    fps              | 156      |
|    time_elapsed     | 2977     |
|    total_timesteps  | 464840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.028    |
|    n_updates        | 115959   |
----------------------------------
Eval num_timesteps=465000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 465000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.01     |
|    n_updates        | 115999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3868     |
|    fps              | 156      |
|    time_elapsed     | 2980     |
|    total_timesteps  | 465264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6        |
|    n_updates        | 116065   |
----------------------------------
Eval num_timesteps=465500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 465500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 116124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3872     |
|    fps              | 156      |
|    time_elapsed     | 2983     |
|    total_timesteps  | 465648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0299   |
|    n_updates        | 116161   |
----------------------------------
Eval num_timesteps=466000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 466000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0235   |
|    n_updates        | 116249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3876     |
|    fps              | 156      |
|    time_elapsed     | 2986     |
|    total_timesteps  | 466056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0363   |
|    n_updates        | 116263   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3880     |
|    fps              | 156      |
|    time_elapsed     | 2987     |
|    total_timesteps  | 466480   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 116369   |
----------------------------------
Eval num_timesteps=466500, episode_reward=319.84 +/- 62.92
Episode length: 104.96 +/- 15.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 466500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.014    |
|    n_updates        | 116374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3884     |
|    fps              | 156      |
|    time_elapsed     | 2990     |
|    total_timesteps  | 466880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.01     |
|    n_updates        | 116469   |
----------------------------------
Eval num_timesteps=467000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 467000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3        |
|    n_updates        | 116499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3888     |
|    fps              | 156      |
|    time_elapsed     | 2993     |
|    total_timesteps  | 467328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6        |
|    n_updates        | 116581   |
----------------------------------
Eval num_timesteps=467500, episode_reward=307.68 +/- 32.51
Episode length: 101.92 +/- 8.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 467500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.016    |
|    n_updates        | 116624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3892     |
|    fps              | 156      |
|    time_elapsed     | 2996     |
|    total_timesteps  | 467768   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 116691   |
----------------------------------
Eval num_timesteps=468000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 468000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0166   |
|    n_updates        | 116749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3896     |
|    fps              | 156      |
|    time_elapsed     | 2999     |
|    total_timesteps  | 468200   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.02     |
|    n_updates        | 116799   |
----------------------------------
Eval num_timesteps=468500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 468500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0192   |
|    n_updates        | 116874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3900     |
|    fps              | 156      |
|    time_elapsed     | 3002     |
|    total_timesteps  | 468608   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0376   |
|    n_updates        | 116901   |
----------------------------------
Eval num_timesteps=469000, episode_reward=325.60 +/- 46.70
Episode length: 106.40 +/- 11.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 469000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0132   |
|    n_updates        | 116999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3904     |
|    fps              | 156      |
|    time_elapsed     | 3006     |
|    total_timesteps  | 469120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 117029   |
----------------------------------
Eval num_timesteps=469500, episode_reward=356.96 +/- 55.07
Episode length: 114.24 +/- 13.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 357      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 469500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0261   |
|    n_updates        | 117124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3908     |
|    fps              | 156      |
|    time_elapsed     | 3009     |
|    total_timesteps  | 469576   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 117143   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3912     |
|    fps              | 156      |
|    time_elapsed     | 3010     |
|    total_timesteps  | 469992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 117247   |
----------------------------------
Eval num_timesteps=470000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 470000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 117249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3916     |
|    fps              | 156      |
|    time_elapsed     | 3013     |
|    total_timesteps  | 470400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.02     |
|    n_updates        | 117349   |
----------------------------------
Eval num_timesteps=470500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 470500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0215   |
|    n_updates        | 117374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3920     |
|    fps              | 156      |
|    time_elapsed     | 3016     |
|    total_timesteps  | 470832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 117457   |
----------------------------------
Eval num_timesteps=471000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 471000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.052    |
|    n_updates        | 117499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3924     |
|    fps              | 156      |
|    time_elapsed     | 3019     |
|    total_timesteps  | 471312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0364   |
|    n_updates        | 117577   |
----------------------------------
Eval num_timesteps=471500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 471500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 117624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3928     |
|    fps              | 156      |
|    time_elapsed     | 3022     |
|    total_timesteps  | 471792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.02     |
|    n_updates        | 117697   |
----------------------------------
Eval num_timesteps=472000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 472000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 117749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3932     |
|    fps              | 156      |
|    time_elapsed     | 3025     |
|    total_timesteps  | 472248   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.02     |
|    n_updates        | 117811   |
----------------------------------
Eval num_timesteps=472500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 472500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 117874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3936     |
|    fps              | 156      |
|    time_elapsed     | 3028     |
|    total_timesteps  | 472752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.03     |
|    n_updates        | 117937   |
----------------------------------
Eval num_timesteps=473000, episode_reward=344.80 +/- 77.66
Episode length: 111.20 +/- 19.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 345      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 473000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0522   |
|    n_updates        | 117999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3940     |
|    fps              | 156      |
|    time_elapsed     | 3031     |
|    total_timesteps  | 473192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.02     |
|    n_updates        | 118047   |
----------------------------------
Eval num_timesteps=473500, episode_reward=318.56 +/- 41.40
Episode length: 104.64 +/- 10.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 473500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.02     |
|    n_updates        | 118124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3944     |
|    fps              | 156      |
|    time_elapsed     | 3034     |
|    total_timesteps  | 473624   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00932  |
|    n_updates        | 118155   |
----------------------------------
Eval num_timesteps=474000, episode_reward=441.44 +/- 210.60
Episode length: 135.36 +/- 52.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 441      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 474000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0515   |
|    n_updates        | 118249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3948     |
|    fps              | 156      |
|    time_elapsed     | 3038     |
|    total_timesteps  | 474104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 118275   |
----------------------------------
Eval num_timesteps=474500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 474500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 118374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3952     |
|    fps              | 155      |
|    time_elapsed     | 3041     |
|    total_timesteps  | 474520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0213   |
|    n_updates        | 118379   |
----------------------------------
Eval num_timesteps=475000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 475000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.022    |
|    n_updates        | 118499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3956     |
|    fps              | 155      |
|    time_elapsed     | 3044     |
|    total_timesteps  | 475000   |
----------------------------------
Eval num_timesteps=475500, episode_reward=316.64 +/- 39.71
Episode length: 104.16 +/- 9.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 475500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0356   |
|    n_updates        | 118624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3960     |
|    fps              | 156      |
|    time_elapsed     | 3048     |
|    total_timesteps  | 475568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.02     |
|    n_updates        | 118641   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3964     |
|    fps              | 156      |
|    time_elapsed     | 3048     |
|    total_timesteps  | 475960   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 118739   |
----------------------------------
Eval num_timesteps=476000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 476000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 118749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3968     |
|    fps              | 156      |
|    time_elapsed     | 3051     |
|    total_timesteps  | 476368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 118841   |
----------------------------------
Eval num_timesteps=476500, episode_reward=313.44 +/- 31.90
Episode length: 103.36 +/- 7.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 476500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0555   |
|    n_updates        | 118874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3972     |
|    fps              | 156      |
|    time_elapsed     | 3054     |
|    total_timesteps  | 476888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.111    |
|    n_updates        | 118971   |
----------------------------------
Eval num_timesteps=477000, episode_reward=358.88 +/- 122.71
Episode length: 114.72 +/- 30.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 359      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 477000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0256   |
|    n_updates        | 118999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3976     |
|    fps              | 156      |
|    time_elapsed     | 3058     |
|    total_timesteps  | 477304   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 119075   |
----------------------------------
Eval num_timesteps=477500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 477500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0132   |
|    n_updates        | 119124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3980     |
|    fps              | 156      |
|    time_elapsed     | 3061     |
|    total_timesteps  | 477808   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0517   |
|    n_updates        | 119201   |
----------------------------------
Eval num_timesteps=478000, episode_reward=286.56 +/- 14.08
Episode length: 96.64 +/- 3.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 478000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0409   |
|    n_updates        | 119249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3984     |
|    fps              | 156      |
|    time_elapsed     | 3064     |
|    total_timesteps  | 478424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.02     |
|    n_updates        | 119355   |
----------------------------------
Eval num_timesteps=478500, episode_reward=325.60 +/- 35.20
Episode length: 106.40 +/- 8.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 478500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.03     |
|    n_updates        | 119374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3988     |
|    fps              | 156      |
|    time_elapsed     | 3067     |
|    total_timesteps  | 478936   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0307   |
|    n_updates        | 119483   |
----------------------------------
Eval num_timesteps=479000, episode_reward=310.88 +/- 35.26
Episode length: 102.72 +/- 8.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 479000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 119499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3992     |
|    fps              | 156      |
|    time_elapsed     | 3070     |
|    total_timesteps  | 479496   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0305   |
|    n_updates        | 119623   |
----------------------------------
Eval num_timesteps=479500, episode_reward=305.76 +/- 40.70
Episode length: 101.44 +/- 10.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 479500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0488   |
|    n_updates        | 119624   |
----------------------------------
Eval num_timesteps=480000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 480000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.06     |
|    n_updates        | 119749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 3996     |
|    fps              | 156      |
|    time_elapsed     | 3076     |
|    total_timesteps  | 480048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.09     |
|    n_updates        | 119761   |
----------------------------------
Eval num_timesteps=480500, episode_reward=326.88 +/- 43.22
Episode length: 106.72 +/- 10.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 327      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 480500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 119874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4000     |
|    fps              | 155      |
|    time_elapsed     | 3080     |
|    total_timesteps  | 480512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 119877   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4004     |
|    fps              | 156      |
|    time_elapsed     | 3080     |
|    total_timesteps  | 480968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0246   |
|    n_updates        | 119991   |
----------------------------------
Eval num_timesteps=481000, episode_reward=287.84 +/- 16.49
Episode length: 96.96 +/- 4.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 481000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.05     |
|    n_updates        | 119999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4008     |
|    fps              | 156      |
|    time_elapsed     | 3084     |
|    total_timesteps  | 481472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0269   |
|    n_updates        | 120117   |
----------------------------------
Eval num_timesteps=481500, episode_reward=321.12 +/- 42.14
Episode length: 105.28 +/- 10.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 481500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 120124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4012     |
|    fps              | 156      |
|    time_elapsed     | 3087     |
|    total_timesteps  | 481928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 120231   |
----------------------------------
Eval num_timesteps=482000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 482000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 120249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4016     |
|    fps              | 156      |
|    time_elapsed     | 3090     |
|    total_timesteps  | 482408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0193   |
|    n_updates        | 120351   |
----------------------------------
Eval num_timesteps=482500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 482500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0241   |
|    n_updates        | 120374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4020     |
|    fps              | 156      |
|    time_elapsed     | 3093     |
|    total_timesteps  | 482792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.024    |
|    n_updates        | 120447   |
----------------------------------
Eval num_timesteps=483000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 483000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0203   |
|    n_updates        | 120499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4024     |
|    fps              | 156      |
|    time_elapsed     | 3097     |
|    total_timesteps  | 483264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 120565   |
----------------------------------
Eval num_timesteps=483500, episode_reward=320.48 +/- 34.47
Episode length: 105.12 +/- 8.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 483500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0143   |
|    n_updates        | 120624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4028     |
|    fps              | 156      |
|    time_elapsed     | 3100     |
|    total_timesteps  | 483704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 120675   |
----------------------------------
Eval num_timesteps=484000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 484000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 120749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4032     |
|    fps              | 156      |
|    time_elapsed     | 3103     |
|    total_timesteps  | 484232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.05     |
|    n_updates        | 120807   |
----------------------------------
Eval num_timesteps=484500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 484500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.08     |
|    n_updates        | 120874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4036     |
|    fps              | 156      |
|    time_elapsed     | 3106     |
|    total_timesteps  | 484664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.05     |
|    n_updates        | 120915   |
----------------------------------
Eval num_timesteps=485000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 485000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0268   |
|    n_updates        | 120999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4040     |
|    fps              | 156      |
|    time_elapsed     | 3109     |
|    total_timesteps  | 485096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0188   |
|    n_updates        | 121023   |
----------------------------------
Eval num_timesteps=485500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 485500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.05     |
|    n_updates        | 121124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4044     |
|    fps              | 155      |
|    time_elapsed     | 3112     |
|    total_timesteps  | 485512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0257   |
|    n_updates        | 121127   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4048     |
|    fps              | 156      |
|    time_elapsed     | 3113     |
|    total_timesteps  | 485952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0144   |
|    n_updates        | 121237   |
----------------------------------
Eval num_timesteps=486000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 486000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 121249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4052     |
|    fps              | 156      |
|    time_elapsed     | 3115     |
|    total_timesteps  | 486344   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.04     |
|    n_updates        | 121335   |
----------------------------------
Eval num_timesteps=486500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 486500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0363   |
|    n_updates        | 121374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4056     |
|    fps              | 156      |
|    time_elapsed     | 3119     |
|    total_timesteps  | 486888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 121471   |
----------------------------------
Eval num_timesteps=487000, episode_reward=312.16 +/- 34.84
Episode length: 103.04 +/- 8.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 487000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.05     |
|    n_updates        | 121499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4060     |
|    fps              | 156      |
|    time_elapsed     | 3122     |
|    total_timesteps  | 487312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0426   |
|    n_updates        | 121577   |
----------------------------------
Eval num_timesteps=487500, episode_reward=380.64 +/- 193.01
Episode length: 120.16 +/- 48.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 381      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 487500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0214   |
|    n_updates        | 121624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4064     |
|    fps              | 155      |
|    time_elapsed     | 3126     |
|    total_timesteps  | 487720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.05     |
|    n_updates        | 121679   |
----------------------------------
Eval num_timesteps=488000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 488000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0183   |
|    n_updates        | 121749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4068     |
|    fps              | 155      |
|    time_elapsed     | 3129     |
|    total_timesteps  | 488104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0296   |
|    n_updates        | 121775   |
----------------------------------
Eval num_timesteps=488500, episode_reward=302.56 +/- 33.92
Episode length: 100.64 +/- 8.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 488500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 121874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4072     |
|    fps              | 155      |
|    time_elapsed     | 3132     |
|    total_timesteps  | 488600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0297   |
|    n_updates        | 121899   |
----------------------------------
Eval num_timesteps=489000, episode_reward=316.00 +/- 38.40
Episode length: 104.00 +/- 9.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 489000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.03     |
|    n_updates        | 121999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4076     |
|    fps              | 155      |
|    time_elapsed     | 3135     |
|    total_timesteps  | 489032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.027    |
|    n_updates        | 122007   |
----------------------------------
Eval num_timesteps=489500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 489500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.06     |
|    n_updates        | 122124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4080     |
|    fps              | 155      |
|    time_elapsed     | 3138     |
|    total_timesteps  | 489560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.06     |
|    n_updates        | 122139   |
----------------------------------
Eval num_timesteps=490000, episode_reward=550.88 +/- 276.21
Episode length: 162.72 +/- 69.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 163      |
|    mean_reward      | 551      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 490000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0573   |
|    n_updates        | 122249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4084     |
|    fps              | 155      |
|    time_elapsed     | 3143     |
|    total_timesteps  | 490176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.031    |
|    n_updates        | 122293   |
----------------------------------
Eval num_timesteps=490500, episode_reward=326.24 +/- 45.90
Episode length: 106.56 +/- 11.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 490500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0119   |
|    n_updates        | 122374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4088     |
|    fps              | 155      |
|    time_elapsed     | 3146     |
|    total_timesteps  | 490680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.05     |
|    n_updates        | 122419   |
----------------------------------
Eval num_timesteps=491000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 491000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 122499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4092     |
|    fps              | 155      |
|    time_elapsed     | 3150     |
|    total_timesteps  | 491112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.05     |
|    n_updates        | 122527   |
----------------------------------
Eval num_timesteps=491500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 491500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0181   |
|    n_updates        | 122624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4096     |
|    fps              | 155      |
|    time_elapsed     | 3153     |
|    total_timesteps  | 491592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.08     |
|    n_updates        | 122647   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4100     |
|    fps              | 156      |
|    time_elapsed     | 3153     |
|    total_timesteps  | 491976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0417   |
|    n_updates        | 122743   |
----------------------------------
Eval num_timesteps=492000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 492000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.08     |
|    n_updates        | 122749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4104     |
|    fps              | 156      |
|    time_elapsed     | 3156     |
|    total_timesteps  | 492432   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0398   |
|    n_updates        | 122857   |
----------------------------------
Eval num_timesteps=492500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 492500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.12     |
|    n_updates        | 122874   |
----------------------------------
Eval num_timesteps=493000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 493000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 122999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4108     |
|    fps              | 155      |
|    time_elapsed     | 3162     |
|    total_timesteps  | 493032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.026    |
|    n_updates        | 123007   |
----------------------------------
Eval num_timesteps=493500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 493500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 123124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4112     |
|    fps              | 155      |
|    time_elapsed     | 3165     |
|    total_timesteps  | 493520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0194   |
|    n_updates        | 123129   |
----------------------------------
Eval num_timesteps=494000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 494000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 123249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4116     |
|    fps              | 155      |
|    time_elapsed     | 3168     |
|    total_timesteps  | 494032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0138   |
|    n_updates        | 123257   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4120     |
|    fps              | 156      |
|    time_elapsed     | 3168     |
|    total_timesteps  | 494488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0418   |
|    n_updates        | 123371   |
----------------------------------
Eval num_timesteps=494500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 494500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0257   |
|    n_updates        | 123374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4124     |
|    fps              | 156      |
|    time_elapsed     | 3172     |
|    total_timesteps  | 494968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 6.13     |
|    n_updates        | 123491   |
----------------------------------
Eval num_timesteps=495000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 495000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0419   |
|    n_updates        | 123499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4128     |
|    fps              | 156      |
|    time_elapsed     | 3175     |
|    total_timesteps  | 495448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 123611   |
----------------------------------
Eval num_timesteps=495500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 495500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 123624   |
----------------------------------
Eval num_timesteps=496000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 496000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 123749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4132     |
|    fps              | 155      |
|    time_elapsed     | 3180     |
|    total_timesteps  | 496016   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 123753   |
----------------------------------
Eval num_timesteps=496500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 496500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 123874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4136     |
|    fps              | 155      |
|    time_elapsed     | 3183     |
|    total_timesteps  | 496536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0689   |
|    n_updates        | 123883   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4140     |
|    fps              | 156      |
|    time_elapsed     | 3184     |
|    total_timesteps  | 496968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0286   |
|    n_updates        | 123991   |
----------------------------------
Eval num_timesteps=497000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 497000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0356   |
|    n_updates        | 123999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4144     |
|    fps              | 156      |
|    time_elapsed     | 3187     |
|    total_timesteps  | 497376   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0172   |
|    n_updates        | 124093   |
----------------------------------
Eval num_timesteps=497500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 497500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0279   |
|    n_updates        | 124124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4148     |
|    fps              | 156      |
|    time_elapsed     | 3190     |
|    total_timesteps  | 497808   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0187   |
|    n_updates        | 124201   |
----------------------------------
Eval num_timesteps=498000, episode_reward=314.08 +/- 34.11
Episode length: 103.52 +/- 8.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 498000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0383   |
|    n_updates        | 124249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4152     |
|    fps              | 156      |
|    time_elapsed     | 3193     |
|    total_timesteps  | 498264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 9.17     |
|    n_updates        | 124315   |
----------------------------------
Eval num_timesteps=498500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 498500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 124374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4156     |
|    fps              | 156      |
|    time_elapsed     | 3196     |
|    total_timesteps  | 498856   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 124463   |
----------------------------------
Eval num_timesteps=499000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 499000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0425   |
|    n_updates        | 124499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4160     |
|    fps              | 156      |
|    time_elapsed     | 3199     |
|    total_timesteps  | 499384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 124595   |
----------------------------------
Eval num_timesteps=499500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 499500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.05     |
|    n_updates        | 124624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 4164     |
|    fps              | 156      |
|    time_elapsed     | 3202     |
|    total_timesteps  | 499792   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0125   |
|    n_updates        | 124697   |
----------------------------------
Eval num_timesteps=500000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 500000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.04     |
|    n_updates        | 124749   |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/health-gathering/dqn-3/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
