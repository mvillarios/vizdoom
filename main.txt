2024-08-16 03:25:15.613427: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-16 03:25:20.918506: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-16 03:25:33.854021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-0.12 +/- 0.31
Episode length: 198.74 +/- 43.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -0.119   |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-0.09 +/- 0.35
Episode length: 192.28 +/- 53.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | -0.0922  |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-0.05 +/- 0.40
Episode length: 187.78 +/- 56.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 188      |
|    mean_reward     | -0.0477  |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-0.15 +/- 0.26
Episode length: 205.14 +/- 22.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | -0.145   |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 0.00507  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 1        |
|    time_elapsed    | 119      |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0012471342 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -0.248       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00833     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 0.0183       |
------------------------------------------
Eval num_timesteps=3000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | -0.00209 |
| time/              |          |
|    fps             | 16       |
|    iterations      | 2        |
|    time_elapsed    | 248      |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 210           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 4500          |
| train/                  |               |
|    approx_kl            | 0.00043707795 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | 0.00392       |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00312       |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.000674     |
|    value_loss           | 0.0136        |
-------------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | -0.0645  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 3        |
|    time_elapsed    | 360      |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0010338505 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -0.256       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00626     |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00177     |
|    value_loss           | 0.00336      |
------------------------------------------
Eval num_timesteps=7000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 0.0192   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 4        |
|    time_elapsed    | 475      |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-0.19 +/- 0.16
Episode length: 206.82 +/- 22.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 207          |
|    mean_reward          | -0.187       |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0011749556 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.193        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00279      |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 0.0186       |
------------------------------------------
Eval num_timesteps=9000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-0.14 +/- 0.29
Episode length: 197.68 +/- 48.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 0.0144   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 5        |
|    time_elapsed    | 588      |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0013064835 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.241        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0056      |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 0.00995      |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-0.19 +/- 0.16
Episode length: 206.46 +/- 24.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | -0.0176  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 6        |
|    time_elapsed    | 704      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=-0.14 +/- 0.27
Episode length: 201.04 +/- 40.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 201          |
|    mean_reward          | -0.141       |
| time/                   |              |
|    total_timesteps      | 12500        |
| train/                  |              |
|    approx_kl            | 0.0006353685 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -1.96        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00594      |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.000763    |
|    value_loss           | 0.00176      |
------------------------------------------
Eval num_timesteps=13000, episode_reward=-0.14 +/- 0.28
Episode length: 199.62 +/- 41.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | -0.14    |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-0.14 +/- 0.28
Episode length: 198.82 +/- 44.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -0.139   |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | -0.0169  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 7        |
|    time_elapsed    | 833      |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.0007361353 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.392        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000568     |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 0.00633      |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | -0.0399  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 8        |
|    time_elapsed    | 957      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 202           |
|    mean_reward          | -0.162        |
| time/                   |               |
|    total_timesteps      | 16500         |
| train/                  |               |
|    approx_kl            | 0.00080727634 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | -0.727        |
|    learning_rate        | 1e-05         |
|    loss                 | -0.0111       |
|    n_updates            | 80            |
|    policy_gradient_loss | -0.00131      |
|    value_loss           | 0.00182       |
-------------------------------------------
Eval num_timesteps=17000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | -0.0241  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 9        |
|    time_elapsed    | 1079     |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 18500        |
| train/                  |              |
|    approx_kl            | 0.0006083314 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.272        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0016      |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000841    |
|    value_loss           | 0.0119       |
------------------------------------------
Eval num_timesteps=19000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | -0.0333  |
| time/              |          |
|    fps             | 16       |
|    iterations      | 10       |
|    time_elapsed    | 1228     |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.003010424 |
|    clip_fraction        | 0.00264     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.78       |
|    explained_variance   | 0.341       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.000388    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 0.00764     |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | -0.0554  |
| time/              |          |
|    fps             | 16       |
|    iterations      | 11       |
|    time_elapsed    | 1364     |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 0.0030562943 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -1.58        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00105     |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00247     |
|    value_loss           | 0.00165      |
------------------------------------------
Eval num_timesteps=23500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | -0.034   |
| time/              |          |
|    fps             | 16       |
|    iterations      | 12       |
|    time_elapsed    | 1485     |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0028752328 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | 0.373        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0163      |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 0.00543      |
------------------------------------------
Eval num_timesteps=25500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | -0.0673  |
| time/              |          |
|    fps             | 16       |
|    iterations      | 13       |
|    time_elapsed    | 1590     |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0007786666 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -6.33        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00346     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000974    |
|    value_loss           | 0.000985     |
------------------------------------------
Eval num_timesteps=27500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | -0.0781  |
| time/              |          |
|    fps             | 16       |
|    iterations      | 14       |
|    time_elapsed    | 1695     |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 210       |
|    mean_reward          | -0.21     |
| time/                   |           |
|    total_timesteps      | 29000     |
| train/                  |           |
|    approx_kl            | 0.0052258 |
|    clip_fraction        | 0.0173    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.78     |
|    explained_variance   | 0.403     |
|    learning_rate        | 1e-05     |
|    loss                 | 0.0133    |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.00305  |
|    value_loss           | 0.00394   |
---------------------------------------
Eval num_timesteps=29500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 15       |
|    time_elapsed    | 1800     |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=-0.19 +/- 0.17
Episode length: 206.08 +/- 27.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0021816902 |
|    clip_fraction        | 0.00513      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -4.47        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00849     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 0.0011       |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-0.09 +/- 0.35
Episode length: 194.18 +/- 49.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.0942  |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -0.0903  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 16       |
|    time_elapsed    | 1909     |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0029142061 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.0573       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00817      |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00186     |
|    value_loss           | 0.00577      |
------------------------------------------
Eval num_timesteps=33500, episode_reward=-0.19 +/- 0.17
Episode length: 206.42 +/- 25.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -0.1     |
| time/              |          |
|    fps             | 17       |
|    iterations      | 17       |
|    time_elapsed    | 2013     |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0020786854 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.295        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00157     |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 0.00408      |
------------------------------------------
Eval num_timesteps=35500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | -0.0784  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 18       |
|    time_elapsed    | 2117     |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0023887698 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.77        |
|    explained_variance   | 0.352        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0049       |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 0.00872      |
------------------------------------------
Eval num_timesteps=37500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | -0.0669  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 19       |
|    time_elapsed    | 2221     |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.005038088 |
|    clip_fraction        | 0.0438      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.74       |
|    explained_variance   | 0.545       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00913    |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00398    |
|    value_loss           | 0.00947     |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | -0.0445  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 20       |
|    time_elapsed    | 2326     |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 41000        |
| train/                  |              |
|    approx_kl            | 0.0032416317 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.75        |
|    explained_variance   | 0.547        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00921      |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00279     |
|    value_loss           | 0.0111       |
------------------------------------------
Eval num_timesteps=41500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-0.19 +/- 0.17
Episode length: 206.28 +/- 26.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | -0.0441  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 21       |
|    time_elapsed    | 2458     |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.004393816 |
|    clip_fraction        | 0.00845     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.75       |
|    explained_variance   | 0.5         |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0217     |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.00821     |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | -0.0226  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 22       |
|    time_elapsed    | 2566     |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0073727197 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.7         |
|    explained_variance   | 0.577        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00444     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00298     |
|    value_loss           | 0.00571      |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | -0.0224  |
| time/              |          |
|    fps             | 17       |
|    iterations      | 23       |
|    time_elapsed    | 2686     |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.006387693 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.156       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00293    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 0.00657     |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-0.14 +/- 0.28
Episode length: 198.30 +/- 46.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-0.16 +/- 0.23
Episode length: 202.16 +/- 38.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 191       |
|    ep_rew_mean     | -0.000641 |
| time/              |           |
|    fps             | 17        |
|    iterations      | 24        |
|    time_elapsed    | 2786      |
|    total_timesteps | 49152     |
----------------------------------
Eval num_timesteps=49500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0048523843 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.193        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00905      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 0.00948      |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-0.14 +/- 0.29
Episode length: 197.72 +/- 48.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.0449   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 25       |
|    time_elapsed    | 2898     |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.004375227 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.6        |
|    explained_variance   | 0.207       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0141      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00314    |
|    value_loss           | 0.0176      |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-0.19 +/- 0.17
Episode length: 206.30 +/- 25.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-0.14 +/- 0.29
Episode length: 197.74 +/- 48.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0468   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 26       |
|    time_elapsed    | 3000     |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 0.0029096764 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.6         |
|    explained_variance   | 0.198        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00761      |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 0.00656      |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-0.19 +/- 0.16
Episode length: 206.62 +/- 23.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 17       |
|    iterations      | 27       |
|    time_elapsed    | 3104     |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0042926557 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.49        |
|    explained_variance   | 0.418        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0137      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 0.012        |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.0811   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 28       |
|    time_elapsed    | 3212     |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.009283887 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.48       |
|    explained_variance   | 0.415       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0195     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 0.0118      |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-0.19 +/- 0.17
Episode length: 206.30 +/- 25.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.0598   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 29       |
|    time_elapsed    | 3314     |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0034721019 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.4         |
|    explained_variance   | 0.484        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0154      |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 0.00708      |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.0715   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 30       |
|    time_elapsed    | 3416     |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-0.16 +/- 0.23
Episode length: 202.24 +/- 38.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.004729132 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.499       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00122     |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00238    |
|    value_loss           | 0.00908     |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-0.16 +/- 0.23
Episode length: 202.62 +/- 36.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.0607   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 31       |
|    time_elapsed    | 3518     |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.002423002 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.564       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00114    |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00172    |
|    value_loss           | 0.00472     |
-----------------------------------------
Eval num_timesteps=64000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 0.0927   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 32       |
|    time_elapsed    | 3650     |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0051630824 |
|    clip_fraction        | 0.0578       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.35        |
|    explained_variance   | 0.399        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0139       |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00402     |
|    value_loss           | 0.0162       |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 0.0934   |
| time/              |          |
|    fps             | 17       |
|    iterations      | 33       |
|    time_elapsed    | 3757     |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-0.11 +/- 0.33
Episode length: 193.74 +/- 55.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | -0.114      |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.008900542 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.521       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0376      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00468    |
|    value_loss           | 0.00524     |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0569   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 34       |
|    time_elapsed    | 3862     |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0043691252 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | 0.169        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0297      |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00406     |
|    value_loss           | 0.00449      |
------------------------------------------
Eval num_timesteps=70500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-0.14 +/- 0.28
Episode length: 198.54 +/- 45.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -0.139   |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.0902   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 35       |
|    time_elapsed    | 3963     |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 72000      |
| train/                  |            |
|    approx_kl            | 0.00949769 |
|    clip_fraction        | 0.0523     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.603      |
|    learning_rate        | 1e-05      |
|    loss                 | -0.00337   |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.00334   |
|    value_loss           | 0.0132     |
----------------------------------------
Eval num_timesteps=72500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 18       |
|    iterations      | 36       |
|    time_elapsed    | 4069     |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.015715519 |
|    clip_fraction        | 0.071       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.51        |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00636     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0033     |
|    value_loss           | 0.0124      |
-----------------------------------------
Eval num_timesteps=74500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 18       |
|    iterations      | 37       |
|    time_elapsed    | 4178     |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.002548507 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.662       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00361     |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 0.0101      |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 0.0883   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 38       |
|    time_elapsed    | 4284     |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.005434514 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.303       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00862    |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 0.00734     |
-----------------------------------------
Eval num_timesteps=78500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 0.125    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 39       |
|    time_elapsed    | 4396     |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.004321655 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.972      |
|    explained_variance   | 0.569       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00344     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 0.0147      |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-0.16 +/- 0.23
Episode length: 202.58 +/- 36.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 0.0927   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 40       |
|    time_elapsed    | 4507     |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0009458851 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.714       |
|    explained_variance   | 0.321        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00189     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.000924    |
|    value_loss           | 0.00244      |
------------------------------------------
Eval num_timesteps=82500, episode_reward=-0.14 +/- 0.29
Episode length: 197.72 +/- 48.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.0614   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 41       |
|    time_elapsed    | 4611     |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0025919166 |
|    clip_fraction        | 0.0186       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.846       |
|    explained_variance   | 0.364        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0093      |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00185     |
|    value_loss           | 0.00505      |
------------------------------------------
Eval num_timesteps=84500, episode_reward=-0.16 +/- 0.23
Episode length: 202.20 +/- 38.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 0.0865   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 42       |
|    time_elapsed    | 4742     |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 0.0011919807 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.915       |
|    explained_variance   | 0.407        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0123       |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000984    |
|    value_loss           | 0.014        |
------------------------------------------
Eval num_timesteps=87000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.074    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 43       |
|    time_elapsed    | 4847     |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.011704443 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.96       |
|    explained_variance   | 0.393       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00609     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.000957   |
|    value_loss           | 0.00374     |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 0.0621   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 44       |
|    time_elapsed    | 4952     |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0010017395 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.527       |
|    explained_variance   | 0.552        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000952     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00245     |
|    value_loss           | 0.011        |
------------------------------------------
Eval num_timesteps=91000, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-0.11 +/- 0.32
Episode length: 194.64 +/- 52.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.0643   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 45       |
|    time_elapsed    | 5054     |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.002479005 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.542       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00456     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 0.00455     |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-0.19 +/- 0.16
Episode length: 206.50 +/- 24.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-0.11 +/- 0.33
Episode length: 193.48 +/- 56.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 193      |
|    mean_reward     | -0.113   |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 0.0525   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 46       |
|    time_elapsed    | 5174     |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 94500        |
| train/                  |              |
|    approx_kl            | 0.0035434898 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.645       |
|    explained_variance   | 0.716        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00234      |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 0.00782      |
------------------------------------------
Eval num_timesteps=95000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.0536   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 47       |
|    time_elapsed    | 5279     |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0010750869 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.602       |
|    explained_variance   | 0.6          |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000181    |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 0.00578      |
------------------------------------------
Eval num_timesteps=97000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0171   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 48       |
|    time_elapsed    | 5387     |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 0.0018045366 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.348       |
|    explained_variance   | 0.643        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00196      |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00219     |
|    value_loss           | 0.00417      |
------------------------------------------
Eval num_timesteps=99000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 49       |
|    time_elapsed    | 5520     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 100500       |
| train/                  |              |
|    approx_kl            | 0.0013134638 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.501       |
|    explained_variance   | 0.562        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00941     |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 0.00893      |
------------------------------------------
Eval num_timesteps=101000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.061    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 50       |
|    time_elapsed    | 5638     |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 0.0012078114 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.5         |
|    explained_variance   | 0.566        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00328     |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00199     |
|    value_loss           | 0.00731      |
------------------------------------------
Eval num_timesteps=103000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0467   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 51       |
|    time_elapsed    | 5742     |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0028274802 |
|    clip_fraction        | 0.0451       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.59        |
|    explained_variance   | 0.662        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.017        |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00546     |
|    value_loss           | 0.00839      |
------------------------------------------
Eval num_timesteps=105000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0474   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 52       |
|    time_elapsed    | 5844     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0026682452 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.482       |
|    explained_variance   | 0.651        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00152      |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 0.00311      |
------------------------------------------
Eval num_timesteps=107000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 0.0244   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 53       |
|    time_elapsed    | 5975     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 109000     |
| train/                  |            |
|    approx_kl            | 0.00171778 |
|    clip_fraction        | 0.0244     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.702      |
|    learning_rate        | 1e-05      |
|    loss                 | 0.000226   |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.00223   |
|    value_loss           | 0.00455    |
----------------------------------------
Eval num_timesteps=109500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-0.14 +/- 0.29
Episode length: 197.68 +/- 48.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 0.000526 |
| time/              |          |
|    fps             | 18       |
|    iterations      | 54       |
|    time_elapsed    | 6085     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.008643076 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.531      |
|    explained_variance   | 0.703       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0342     |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 0.00549     |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-0.14 +/- 0.29
Episode length: 197.70 +/- 48.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | -0.00997 |
| time/              |          |
|    fps             | 18       |
|    iterations      | 55       |
|    time_elapsed    | 6186     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 0.0012299789 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.649        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00509     |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00198     |
|    value_loss           | 0.00797      |
------------------------------------------
Eval num_timesteps=113500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-0.16 +/- 0.23
Episode length: 202.74 +/- 35.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | -0.0111  |
| time/              |          |
|    fps             | 18       |
|    iterations      | 56       |
|    time_elapsed    | 6303     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0021108284 |
|    clip_fraction        | 0.0169       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.532       |
|    explained_variance   | 0.661        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0021       |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00371     |
|    value_loss           | 0.00801      |
------------------------------------------
Eval num_timesteps=115500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-0.14 +/- 0.28
Episode length: 198.26 +/- 46.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 0.0114   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 57       |
|    time_elapsed    | 6407     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 0.0016580384 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.535       |
|    explained_variance   | 0.429        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00723     |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 0.00639      |
------------------------------------------
Eval num_timesteps=117500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 0.00241  |
| time/              |          |
|    fps             | 18       |
|    iterations      | 58       |
|    time_elapsed    | 6518     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0025687162 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.688       |
|    explained_variance   | 0.475        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0143       |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 0.00694      |
------------------------------------------
Eval num_timesteps=119500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-0.16 +/- 0.23
Episode length: 202.36 +/- 37.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | -0.022   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 59       |
|    time_elapsed    | 6622     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0033913516 |
|    clip_fraction        | 0.041        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.536       |
|    explained_variance   | 0.732        |
|    learning_rate        | 1e-05        |
|    loss                 | -6.1e-05     |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00422     |
|    value_loss           | 0.00373      |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | -0.0337  |
| time/              |          |
|    fps             | 18       |
|    iterations      | 60       |
|    time_elapsed    | 6727     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 206           |
|    mean_reward          | -0.186        |
| time/                   |               |
|    total_timesteps      | 123000        |
| train/                  |               |
|    approx_kl            | 0.00062204764 |
|    clip_fraction        | 0.00381       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.689        |
|    explained_variance   | 0.409         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00245       |
|    n_updates            | 600           |
|    policy_gradient_loss | -0.00132      |
|    value_loss           | 0.0096        |
-------------------------------------------
Eval num_timesteps=123500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | -0.0116  |
| time/              |          |
|    fps             | 18       |
|    iterations      | 61       |
|    time_elapsed    | 6835     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-0.14 +/- 0.28
Episode length: 198.08 +/- 47.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0023612718 |
|    clip_fraction        | 0.0322       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.778       |
|    explained_variance   | 0.419        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0209      |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.0046      |
|    value_loss           | 0.00736      |
------------------------------------------
Eval num_timesteps=125500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 0.000185 |
| time/              |          |
|    fps             | 18       |
|    iterations      | 62       |
|    time_elapsed    | 6935     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0023945405 |
|    clip_fraction        | 0.0263       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.672       |
|    explained_variance   | 0.457        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0187       |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00211     |
|    value_loss           | 0.0116       |
------------------------------------------
Eval num_timesteps=127500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.045    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 63       |
|    time_elapsed    | 7062     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 0.0020229425 |
|    clip_fraction        | 0.0062       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.734       |
|    explained_variance   | 0.6          |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00157     |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00171     |
|    value_loss           | 0.0116       |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-0.16 +/- 0.23
Episode length: 204.02 +/- 31.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | -0.164   |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 0.033    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 64       |
|    time_elapsed    | 7163     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0011511883 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.728       |
|    explained_variance   | 0.492        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00147      |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.000947    |
|    value_loss           | 0.00519      |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-0.16 +/- 0.23
Episode length: 202.18 +/- 38.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 0.0528   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 65       |
|    time_elapsed    | 7264     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0035384358 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.889       |
|    explained_variance   | 0.418        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000182    |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00453     |
|    value_loss           | 0.0131       |
------------------------------------------
Eval num_timesteps=134000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.0852   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 66       |
|    time_elapsed    | 7366     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.003728847 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.828      |
|    explained_variance   | 0.549       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00476    |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00334    |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.108    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 67       |
|    time_elapsed    | 7469     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0042049526 |
|    clip_fraction        | 0.043        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.872       |
|    explained_variance   | 0.615        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000108     |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00617     |
|    value_loss           | 0.0112       |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-0.14 +/- 0.29
Episode length: 197.60 +/- 49.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 0.0984   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 68       |
|    time_elapsed    | 7569     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0043143295 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.443        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00631      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00434     |
|    value_loss           | 0.00671      |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-0.19 +/- 0.16
Episode length: 207.60 +/- 16.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 0.0996   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 69       |
|    time_elapsed    | 7670     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 141500     |
| train/                  |            |
|    approx_kl            | 0.00417988 |
|    clip_fraction        | 0.0217     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.53       |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00752    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.00181   |
|    value_loss           | 0.00688    |
----------------------------------------
Eval num_timesteps=142000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.131    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 70       |
|    time_elapsed    | 7776     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 0.0027097329 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.972       |
|    explained_variance   | 0.606        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0232      |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00464     |
|    value_loss           | 0.0128       |
------------------------------------------
Eval num_timesteps=144000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-0.16 +/- 0.23
Episode length: 202.28 +/- 37.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 0.0983   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 71       |
|    time_elapsed    | 7879     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.001333382 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.451       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00988    |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00186    |
|    value_loss           | 0.00332     |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=-0.19 +/- 0.14
Episode length: 209.66 +/- 2.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.19    |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 0.0983   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 72       |
|    time_elapsed    | 7981     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0030159752 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.844       |
|    explained_variance   | 0.69         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0126       |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 0.00805      |
------------------------------------------
Eval num_timesteps=148000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.111    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 73       |
|    time_elapsed    | 8116     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0011800891 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.735       |
|    explained_variance   | 0.601        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0019       |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00458     |
|    value_loss           | 0.00751      |
------------------------------------------
Eval num_timesteps=150500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.101    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 74       |
|    time_elapsed    | 8226     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0036078228 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.509        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000892    |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.0032      |
|    value_loss           | 0.017        |
------------------------------------------
Eval num_timesteps=152500, episode_reward=-0.16 +/- 0.23
Episode length: 202.40 +/- 37.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 0.113    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 75       |
|    time_elapsed    | 8327     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-0.16 +/- 0.23
Episode length: 202.74 +/- 35.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | -0.163      |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.002293997 |
|    clip_fraction        | 0.0216      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.91       |
|    explained_variance   | 0.551       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0142      |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00092    |
|    value_loss           | 0.0135      |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 0.138    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 76       |
|    time_elapsed    | 8444     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0044161174 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.765       |
|    explained_variance   | 0.674        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00647      |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00346     |
|    value_loss           | 0.0106       |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-0.14 +/- 0.29
Episode length: 197.78 +/- 48.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 77       |
|    time_elapsed    | 8549     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0059001707 |
|    clip_fraction        | 0.0541       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.755       |
|    explained_variance   | 0.648        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0106      |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.0033      |
|    value_loss           | 0.0118       |
------------------------------------------
Eval num_timesteps=158500, episode_reward=-0.19 +/- 0.17
Episode length: 206.24 +/- 26.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-0.16 +/- 0.23
Episode length: 202.90 +/- 35.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 0.161    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 78       |
|    time_elapsed    | 8652     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0015560953 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.62        |
|    explained_variance   | 0.738        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00799     |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.0022      |
|    value_loss           | 0.0092       |
------------------------------------------
Eval num_timesteps=160500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-0.16 +/- 0.23
Episode length: 203.20 +/- 34.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 0.218    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 79       |
|    time_elapsed    | 8755     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 210           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 162000        |
| train/                  |               |
|    approx_kl            | 0.00088591385 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.705        |
|    explained_variance   | 0.613         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00733       |
|    n_updates            | 790           |
|    policy_gradient_loss | -0.00282      |
|    value_loss           | 0.0148        |
-------------------------------------------
Eval num_timesteps=162500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 0.184    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 80       |
|    time_elapsed    | 8932     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0051519577 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.643       |
|    explained_variance   | 0.503        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0326      |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00313     |
|    value_loss           | 0.00225      |
------------------------------------------
Eval num_timesteps=164500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 0.184    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 81       |
|    time_elapsed    | 9042     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0051336717 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.486       |
|    explained_variance   | 0.559        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0114      |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00541     |
|    value_loss           | 0.00654      |
------------------------------------------
Eval num_timesteps=166500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 0.194    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 82       |
|    time_elapsed    | 9145     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-0.19 +/- 0.16
Episode length: 206.66 +/- 23.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 207          |
|    mean_reward          | -0.187       |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0038132288 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.732       |
|    explained_variance   | 0.586        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0164       |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00261     |
|    value_loss           | 0.0132       |
------------------------------------------
Eval num_timesteps=168500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-0.14 +/- 0.29
Episode length: 197.70 +/- 48.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.172    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 83       |
|    time_elapsed    | 9248     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0022600165 |
|    clip_fraction        | 0.0369       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.619       |
|    explained_variance   | 0.57         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00263     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00537     |
|    value_loss           | 0.00983      |
------------------------------------------
Eval num_timesteps=170500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 84       |
|    time_elapsed    | 9381     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0015752523 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.681       |
|    explained_variance   | 0.503        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00171      |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00202     |
|    value_loss           | 0.00771      |
------------------------------------------
Eval num_timesteps=173000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 0.138    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 85       |
|    time_elapsed    | 9485     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 0.0030014953 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.651       |
|    explained_variance   | 0.352        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00209      |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00246     |
|    value_loss           | 0.00721      |
------------------------------------------
Eval num_timesteps=175000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-0.19 +/- 0.16
Episode length: 207.36 +/- 18.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-0.16 +/- 0.23
Episode length: 202.14 +/- 38.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 0.114    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 86       |
|    time_elapsed    | 9590     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0021765444 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.602       |
|    explained_variance   | 0.489        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00233      |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00115     |
|    value_loss           | 0.0104       |
------------------------------------------
Eval num_timesteps=177000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-0.16 +/- 0.23
Episode length: 202.18 +/- 38.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.124    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 87       |
|    time_elapsed    | 9692     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0022094517 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.55        |
|    explained_variance   | 0.667        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00377     |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00252     |
|    value_loss           | 0.00936      |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-0.19 +/- 0.17
Episode length: 206.20 +/- 26.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 0.0889   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 88       |
|    time_elapsed    | 9812     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0026695447 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.65        |
|    explained_variance   | 0.656        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00899     |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00403     |
|    value_loss           | 0.00669      |
------------------------------------------
Eval num_timesteps=181000, episode_reward=-0.11 +/- 0.32
Episode length: 194.22 +/- 53.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-0.19 +/- 0.17
Episode length: 206.18 +/- 26.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.134    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 89       |
|    time_elapsed    | 9912     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.002011567 |
|    clip_fraction        | 0.0135      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.679      |
|    explained_variance   | 0.616       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.000855    |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0022     |
|    value_loss           | 0.0122      |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 0.101    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 90       |
|    time_elapsed    | 10015    |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 184500       |
| train/                  |              |
|    approx_kl            | 0.0017115566 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.433       |
|    explained_variance   | 0.619        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00381     |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 0.00174      |
------------------------------------------
Eval num_timesteps=185000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 0.0678   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 91       |
|    time_elapsed    | 10123    |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 0.0012809241 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.506       |
|    explained_variance   | 0.707        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.015        |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00147     |
|    value_loss           | 0.007        |
------------------------------------------
Eval num_timesteps=187000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0572   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 92       |
|    time_elapsed    | 10226    |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0064427764 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.685        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0124       |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.00375     |
|    value_loss           | 0.00485      |
------------------------------------------
Eval num_timesteps=189000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-0.19 +/- 0.17
Episode length: 206.44 +/- 24.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 0.0665   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 93       |
|    time_elapsed    | 10327    |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-0.14 +/- 0.29
Episode length: 197.62 +/- 49.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0017190936 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.622       |
|    explained_variance   | 0.603        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0161      |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00445     |
|    value_loss           | 0.00908      |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-0.16 +/- 0.23
Episode length: 203.72 +/- 32.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 204      |
|    mean_reward     | -0.164   |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-0.16 +/- 0.23
Episode length: 202.42 +/- 37.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-0.11 +/- 0.33
Episode length: 193.98 +/- 54.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 0.0883   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 94       |
|    time_elapsed    | 10448    |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0014324349 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.716        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00376     |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 0.0101       |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-0.19 +/- 0.17
Episode length: 206.26 +/- 26.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.0656   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 95       |
|    time_elapsed    | 10550    |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 210           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 0.00078609365 |
|    clip_fraction        | 0.00557       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.501        |
|    explained_variance   | 0.743         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00701      |
|    n_updates            | 950           |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 0.00514       |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 0.0426   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 96       |
|    time_elapsed    | 10652    |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0035630388 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.54        |
|    explained_variance   | 0.674        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0224       |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00378     |
|    value_loss           | 0.00643      |
------------------------------------------
Eval num_timesteps=197500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 0.0338   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 97       |
|    time_elapsed    | 10754    |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.003084179 |
|    clip_fraction        | 0.0485      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.452      |
|    explained_variance   | 0.592       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.022       |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 0.00411     |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 0.000379 |
| time/              |          |
|    fps             | 18       |
|    iterations      | 98       |
|    time_elapsed    | 10862    |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0019691116 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.393       |
|    explained_variance   | 0.729        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0258      |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00436     |
|    value_loss           | 0.00378      |
------------------------------------------
Eval num_timesteps=201500, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-0.14 +/- 0.28
Episode length: 199.64 +/- 42.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | -0.14    |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 191       |
|    ep_rew_mean     | -0.000514 |
| time/              |           |
|    fps             | 18        |
|    iterations      | 99        |
|    time_elapsed    | 10963     |
|    total_timesteps | 202752    |
----------------------------------
Eval num_timesteps=203000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.001967107 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.673       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.000174    |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 0.00527     |
-----------------------------------------
Eval num_timesteps=203500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-0.16 +/- 0.23
Episode length: 202.32 +/- 37.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 6.4e-05  |
| time/              |          |
|    fps             | 18       |
|    iterations      | 100      |
|    time_elapsed    | 11075    |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0005335128 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.381       |
|    explained_variance   | 0.636        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.012       |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 0.00397      |
------------------------------------------
Eval num_timesteps=205500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-0.16 +/- 0.23
Episode length: 202.50 +/- 36.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 0.0322   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 101      |
|    time_elapsed    | 11176    |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0010967909 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.455       |
|    explained_variance   | 0.848        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00959      |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00255     |
|    value_loss           | 0.00533      |
------------------------------------------
Eval num_timesteps=207500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 0.0678   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 102      |
|    time_elapsed    | 11299    |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 210           |
|    mean_reward          | -0.21         |
| time/                   |               |
|    total_timesteps      | 209000        |
| train/                  |               |
|    approx_kl            | 0.00072607596 |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.438        |
|    explained_variance   | 0.787         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00236      |
|    n_updates            | 1020          |
|    policy_gradient_loss | -0.00229      |
|    value_loss           | 0.00897       |
-------------------------------------------
Eval num_timesteps=209500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.104    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 103      |
|    time_elapsed    | 11433    |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0019631782 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.485       |
|    explained_variance   | 0.909        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00827      |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00348     |
|    value_loss           | 0.00691      |
------------------------------------------
Eval num_timesteps=211500, episode_reward=-0.16 +/- 0.22
Episode length: 204.58 +/- 30.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | -0.165   |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 0.0948   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 104      |
|    time_elapsed    | 11536    |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0009775573 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.474       |
|    explained_variance   | 0.79         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00671      |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00376     |
|    value_loss           | 0.00547      |
------------------------------------------
Eval num_timesteps=213500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 0.128    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 105      |
|    time_elapsed    | 11663    |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.002056041 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.81        |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0114      |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00302    |
|    value_loss           | 0.00952     |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=-0.16 +/- 0.23
Episode length: 202.08 +/- 38.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-0.19 +/- 0.17
Episode length: 206.14 +/- 27.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.152    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 106      |
|    time_elapsed    | 11765    |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 0.0024298863 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.507       |
|    explained_variance   | 0.67         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00898      |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.00365     |
|    value_loss           | 0.0106       |
------------------------------------------
Eval num_timesteps=218000, episode_reward=-0.16 +/- 0.23
Episode length: 202.48 +/- 37.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.152    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 107      |
|    time_elapsed    | 11913    |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 219500       |
| train/                  |              |
|    approx_kl            | 0.0020449534 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.516       |
|    explained_variance   | 0.689        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00455     |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00259     |
|    value_loss           | 0.00703      |
------------------------------------------
Eval num_timesteps=220000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 0.186    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 108      |
|    time_elapsed    | 12030    |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.003869384 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.501      |
|    explained_variance   | 0.713       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00223    |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00226    |
|    value_loss           | 0.00937     |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 0.199    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 109      |
|    time_elapsed    | 12132    |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-0.19 +/- 0.17
Episode length: 206.20 +/- 26.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 0.0018750803 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | 0.731        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00224      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00355     |
|    value_loss           | 0.00896      |
------------------------------------------
Eval num_timesteps=224000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-0.19 +/- 0.17
Episode length: 206.30 +/- 25.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-0.19 +/- 0.17
Episode length: 206.14 +/- 27.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 0.176    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 110      |
|    time_elapsed    | 12235    |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 0.0022742683 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.807        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00102      |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00387     |
|    value_loss           | 0.00679      |
------------------------------------------
Eval num_timesteps=226000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.142    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 111      |
|    time_elapsed    | 12337    |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-0.19 +/- 0.16
Episode length: 206.44 +/- 24.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 0.0029203114 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.585       |
|    explained_variance   | 0.625        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0125      |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00431     |
|    value_loss           | 0.00864      |
------------------------------------------
Eval num_timesteps=228000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-0.11 +/- 0.33
Episode length: 193.58 +/- 55.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 0.119    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 112      |
|    time_elapsed    | 12438    |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 0.0022728208 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.511       |
|    explained_variance   | 0.679        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00173     |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 0.0047       |
------------------------------------------
Eval num_timesteps=230000, episode_reward=-0.19 +/- 0.16
Episode length: 207.84 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.142    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 113      |
|    time_elapsed    | 12539    |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 0.0038508563 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.645        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0097       |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 0.0096       |
------------------------------------------
Eval num_timesteps=232000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.0943   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 114      |
|    time_elapsed    | 12680    |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 233500       |
| train/                  |              |
|    approx_kl            | 0.0010921379 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.541       |
|    explained_variance   | 0.766        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00148     |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 0.00427      |
------------------------------------------
Eval num_timesteps=234000, episode_reward=-0.19 +/- 0.16
Episode length: 206.94 +/- 21.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 0.106    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 115      |
|    time_elapsed    | 12808    |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0015606255 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.514       |
|    explained_variance   | 0.708        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00237     |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00207     |
|    value_loss           | 0.00866      |
------------------------------------------
Eval num_timesteps=236500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 0.0928   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 116      |
|    time_elapsed    | 12909    |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 0.0018754671 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.61        |
|    explained_variance   | 0.685        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000713     |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00426     |
|    value_loss           | 0.00658      |
------------------------------------------
Eval num_timesteps=238500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 0.106    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 117      |
|    time_elapsed    | 13013    |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0040520877 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.683       |
|    explained_variance   | 0.619        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0129       |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00385     |
|    value_loss           | 0.0147       |
------------------------------------------
Eval num_timesteps=240500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 0.0935   |
| time/              |          |
|    fps             | 18       |
|    iterations      | 118      |
|    time_elapsed    | 13118    |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0017990409 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.489        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00106      |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 0.00837      |
------------------------------------------
Eval num_timesteps=242500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 0.115    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 119      |
|    time_elapsed    | 13237    |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0010377998 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.537       |
|    explained_variance   | 0.662        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00136      |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.000547    |
|    value_loss           | 0.0102       |
------------------------------------------
Eval num_timesteps=244500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 0.136    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 120      |
|    time_elapsed    | 13356    |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0020725997 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.537       |
|    explained_variance   | 0.736        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00265     |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00307     |
|    value_loss           | 0.00758      |
------------------------------------------
Eval num_timesteps=246500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.192    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 121      |
|    time_elapsed    | 13458    |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.001917342 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.545      |
|    explained_variance   | 0.734       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.000157    |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 0.0119      |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-0.19 +/- 0.17
Episode length: 206.28 +/- 26.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 0.204    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 122      |
|    time_elapsed    | 13583    |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-0.16 +/- 0.23
Episode length: 202.64 +/- 36.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 203          |
|    mean_reward          | -0.163       |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0014152311 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.482       |
|    explained_variance   | 0.862        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0158       |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 0.00482      |
------------------------------------------
Eval num_timesteps=250500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 0.193    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 123      |
|    time_elapsed    | 13698    |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0014473898 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | 0.831        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000718    |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 0.00337      |
------------------------------------------
Eval num_timesteps=252500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-0.16 +/- 0.23
Episode length: 202.48 +/- 37.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 0.205    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 124      |
|    time_elapsed    | 13799    |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.002447694 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.783       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0081      |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.0019     |
|    value_loss           | 0.00744     |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-0.16 +/- 0.23
Episode length: 202.16 +/- 38.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 0.169    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 125      |
|    time_elapsed    | 13924    |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 256500       |
| train/                  |              |
|    approx_kl            | 0.0012523219 |
|    clip_fraction        | 0.00942      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.772        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00725      |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 0.0058       |
------------------------------------------
Eval num_timesteps=257000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 0.199    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 126      |
|    time_elapsed    | 14061    |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 0.0020317098 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.548       |
|    explained_variance   | 0.849        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00235      |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00324     |
|    value_loss           | 0.00945      |
------------------------------------------
Eval num_timesteps=259000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-0.19 +/- 0.14
Episode length: 209.88 +/- 0.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.19    |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 0.187    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 127      |
|    time_elapsed    | 14163    |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 0.0025780771 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.512       |
|    explained_variance   | 0.703        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00768     |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00517     |
|    value_loss           | 0.00367      |
------------------------------------------
Eval num_timesteps=261000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 0.189    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 128      |
|    time_elapsed    | 14265    |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 262500       |
| train/                  |              |
|    approx_kl            | 0.0022558768 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.489       |
|    explained_variance   | 0.577        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00246     |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.00466     |
|    value_loss           | 0.0171       |
------------------------------------------
Eval num_timesteps=263000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 129      |
|    time_elapsed    | 14368    |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0019428132 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.465       |
|    explained_variance   | 0.769        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00649     |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00297     |
|    value_loss           | 0.00957      |
------------------------------------------
Eval num_timesteps=265000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-0.19 +/- 0.17
Episode length: 206.20 +/- 26.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 0.205    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 130      |
|    time_elapsed    | 14469    |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0017924944 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | 0.774        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00543      |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00123     |
|    value_loss           | 0.0125       |
------------------------------------------
Eval num_timesteps=267000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-0.19 +/- 0.17
Episode length: 206.34 +/- 25.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 0.227    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 131      |
|    time_elapsed    | 14571    |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 0.0009839829 |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.441       |
|    explained_variance   | 0.748        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00432      |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 0.00887      |
------------------------------------------
Eval num_timesteps=269000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 132      |
|    time_elapsed    | 14674    |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 0.0011041266 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.378       |
|    explained_variance   | 0.835        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00383      |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 0.006        |
------------------------------------------
Eval num_timesteps=271000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 0.241    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 133      |
|    time_elapsed    | 14776    |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 0.0024348877 |
|    clip_fraction        | 0.0351       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.444       |
|    explained_variance   | 0.716        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0141      |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.00417     |
|    value_loss           | 0.00851      |
------------------------------------------
Eval num_timesteps=273000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 0.233    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 134      |
|    time_elapsed    | 14880    |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 274500       |
| train/                  |              |
|    approx_kl            | 0.0043733963 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.371       |
|    explained_variance   | 0.756        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000128     |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 0.00696      |
------------------------------------------
Eval num_timesteps=275000, episode_reward=-0.16 +/- 0.23
Episode length: 202.42 +/- 37.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 0.254    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 135      |
|    time_elapsed    | 14984    |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 276500       |
| train/                  |              |
|    approx_kl            | 0.0017953985 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.446       |
|    explained_variance   | 0.7          |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00341      |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00314     |
|    value_loss           | 0.0111       |
------------------------------------------
Eval num_timesteps=277000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 0.255    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 136      |
|    time_elapsed    | 15154    |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 279000       |
| train/                  |              |
|    approx_kl            | 0.0022380126 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.336       |
|    explained_variance   | 0.728        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0108      |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00213     |
|    value_loss           | 0.00978      |
------------------------------------------
Eval num_timesteps=279500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 0.253    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 137      |
|    time_elapsed    | 15256    |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0019156433 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.434       |
|    explained_variance   | 0.774        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00184      |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00297     |
|    value_loss           | 0.0128       |
------------------------------------------
Eval num_timesteps=281500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-0.19 +/- 0.15
Episode length: 208.24 +/- 12.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 138      |
|    time_elapsed    | 15379    |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.001048454 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.289      |
|    explained_variance   | 0.885       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00147     |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00265    |
|    value_loss           | 0.00451     |
-----------------------------------------
Eval num_timesteps=283500, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 139      |
|    time_elapsed    | 15489    |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0021423176 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.347       |
|    explained_variance   | 0.734        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00535     |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00302     |
|    value_loss           | 0.0075       |
------------------------------------------
Eval num_timesteps=285500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 0.206    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 140      |
|    time_elapsed    | 15593    |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0012107096 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.322       |
|    explained_variance   | 0.839        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00295     |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 0.00481      |
------------------------------------------
Eval num_timesteps=287500, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 0.221    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 141      |
|    time_elapsed    | 15697    |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-0.16 +/- 0.24
Episode length: 201.86 +/- 39.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0018250502 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.358       |
|    explained_variance   | 0.784        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0103      |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00283     |
|    value_loss           | 0.0122       |
------------------------------------------
Eval num_timesteps=289500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 0.209    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 142      |
|    time_elapsed    | 15805    |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0015947499 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0.785        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00429     |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00244     |
|    value_loss           | 0.00555      |
------------------------------------------
Eval num_timesteps=291500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 0.222    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 143      |
|    time_elapsed    | 15916    |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.002005751 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.805       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00907    |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.00299    |
|    value_loss           | 0.00779     |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 0.184    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 144      |
|    time_elapsed    | 16028    |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0017406605 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.367       |
|    explained_variance   | 0.785        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0323       |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00266     |
|    value_loss           | 0.00752      |
------------------------------------------
Eval num_timesteps=295500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 0.183    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 145      |
|    time_elapsed    | 16149    |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 0.0046440586 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | 0.813        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00934      |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.00425     |
|    value_loss           | 0.00986      |
------------------------------------------
Eval num_timesteps=297500, episode_reward=-0.16 +/- 0.23
Episode length: 202.76 +/- 35.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-0.14 +/- 0.28
Episode length: 199.34 +/- 43.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -0.139   |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-0.19 +/- 0.17
Episode length: 206.14 +/- 27.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 146      |
|    time_elapsed    | 16283    |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=-0.19 +/- 0.17
Episode length: 206.36 +/- 25.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0019654573 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.444       |
|    explained_variance   | 0.819        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000392     |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00298     |
|    value_loss           | 0.00903      |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 0.189    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 147      |
|    time_elapsed    | 16397    |
|    total_timesteps | 301056   |
---------------------------------
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/my-way-home/ppo-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
