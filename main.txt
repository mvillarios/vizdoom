2024-08-17 00:55:37.936775: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-17 00:55:46.151085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-17 00:56:02.575118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-0.11 +/- 0.33
Episode length: 193.58 +/- 55.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | -0.103   |
| time/              |          |
|    fps             | 13       |
|    iterations      | 1        |
|    time_elapsed    | 148      |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 2500         |
| train/                  |              |
|    approx_kl            | 0.0012762711 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -1.26        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00418      |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 0.00993      |
------------------------------------------
Eval num_timesteps=3000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | -0.0976  |
| time/              |          |
|    fps             | 14       |
|    iterations      | 2        |
|    time_elapsed    | 281      |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 0.0013346059 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | -0.454       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00833      |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 0.00635      |
------------------------------------------
Eval num_timesteps=5000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | -0.066   |
| time/              |          |
|    fps             | 14       |
|    iterations      | 3        |
|    time_elapsed    | 416      |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0020350062 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.0556       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00354      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 0.00971      |
------------------------------------------
Eval num_timesteps=7000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | -0.0766  |
| time/              |          |
|    fps             | 13       |
|    iterations      | 4        |
|    time_elapsed    | 617      |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0018140201 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.144        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.017        |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 0.0059       |
------------------------------------------
Eval num_timesteps=9000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 12       |
|    iterations      | 5        |
|    time_elapsed    | 818      |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0015088809 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.78        |
|    explained_variance   | -0.693       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0179       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00129     |
|    value_loss           | 0.00203      |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 12       |
|    iterations      | 6        |
|    time_elapsed    | 980      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 12500        |
| train/                  |              |
|    approx_kl            | 0.0030628894 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.79        |
|    explained_variance   | 0.0714       |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000889     |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 0.00456      |
------------------------------------------
Eval num_timesteps=13000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 12       |
|    iterations      | 7        |
|    time_elapsed    | 1123     |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 206           |
|    mean_reward          | -0.186        |
| time/                   |               |
|    total_timesteps      | 14500         |
| train/                  |               |
|    approx_kl            | 0.00017969697 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.79         |
|    explained_variance   | 0.18          |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00468      |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.000406     |
|    value_loss           | 0.00497       |
-------------------------------------------
Eval num_timesteps=15000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 200      |
|    ep_rew_mean     | -0.101   |
| time/              |          |
|    fps             | 12       |
|    iterations      | 8        |
|    time_elapsed    | 1275     |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.009100049 |
|    clip_fraction        | 0.0519      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -0.00115    |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0256     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0055     |
|    value_loss           | 0.00545     |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | -0.0238  |
| time/              |          |
|    fps             | 13       |
|    iterations      | 9        |
|    time_elapsed    | 1412     |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.008510703 |
|    clip_fraction        | 0.0673      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.73       |
|    explained_variance   | 0.277       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00593    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 0.0205      |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-0.16 +/- 0.23
Episode length: 202.30 +/- 37.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | -0.0181  |
| time/              |          |
|    fps             | 13       |
|    iterations      | 10       |
|    time_elapsed    | 1549     |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=-0.19 +/- 0.16
Episode length: 207.04 +/- 20.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 207          |
|    mean_reward          | -0.187       |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0075733075 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.0271       |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0219      |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.0038      |
|    value_loss           | 0.00916      |
------------------------------------------
Eval num_timesteps=21000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-0.14 +/- 0.29
Episode length: 197.70 +/- 48.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | -0.018   |
| time/              |          |
|    fps             | 13       |
|    iterations      | 11       |
|    time_elapsed    | 1683     |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 0.0018008307 |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.71        |
|    explained_variance   | 0.231        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000789    |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00168     |
|    value_loss           | 0.00428      |
------------------------------------------
Eval num_timesteps=23500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | -0.0402  |
| time/              |          |
|    fps             | 13       |
|    iterations      | 12       |
|    time_elapsed    | 1798     |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.00201562 |
|    clip_fraction        | 0.00542    |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.68      |
|    explained_variance   | -5.57      |
|    learning_rate        | 1e-05      |
|    loss                 | -0.00513   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0012    |
|    value_loss           | 0.00117    |
----------------------------------------
Eval num_timesteps=25500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 0.00434  |
| time/              |          |
|    fps             | 13       |
|    iterations      | 13       |
|    time_elapsed    | 1917     |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0044833883 |
|    clip_fraction        | 0.00708      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.69        |
|    explained_variance   | 0.443        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0123       |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00168     |
|    value_loss           | 0.0133       |
------------------------------------------
Eval num_timesteps=27500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 0.0161   |
| time/              |          |
|    fps             | 14       |
|    iterations      | 14       |
|    time_elapsed    | 2033     |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.003886933 |
|    clip_fraction        | 0.0118      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.493       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0107     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00125    |
|    value_loss           | 0.00662     |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.0151   |
| time/              |          |
|    fps             | 14       |
|    iterations      | 15       |
|    time_elapsed    | 2153     |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.006179099 |
|    clip_fraction        | 0.0338      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.72       |
|    explained_variance   | 0.495       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0262      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 0.00427     |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 0.0157   |
| time/              |          |
|    fps             | 14       |
|    iterations      | 16       |
|    time_elapsed    | 2275     |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.009445672 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.63       |
|    explained_variance   | 0.343       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00537    |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00409    |
|    value_loss           | 0.00462     |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.0251   |
| time/              |          |
|    fps             | 14       |
|    iterations      | 17       |
|    time_elapsed    | 2404     |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.19 +/- 0.16
Episode length: 207.26 +/- 19.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | -0.187      |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.007830638 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.67       |
|    explained_variance   | 0.336       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.012      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 0.00696     |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | -0.0334  |
| time/              |          |
|    fps             | 14       |
|    iterations      | 18       |
|    time_elapsed    | 2525     |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0043355348 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.67        |
|    explained_variance   | 0.181        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00577     |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00298     |
|    value_loss           | 0.00501      |
------------------------------------------
Eval num_timesteps=37500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | -0.0324  |
| time/              |          |
|    fps             | 14       |
|    iterations      | 19       |
|    time_elapsed    | 2645     |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.007461275 |
|    clip_fraction        | 0.0465      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.407       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0126     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00519    |
|    value_loss           | 0.0103      |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | -0.0339  |
| time/              |          |
|    fps             | 14       |
|    iterations      | 20       |
|    time_elapsed    | 2775     |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.006532168 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.64       |
|    explained_variance   | 0.35        |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0106     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00261    |
|    value_loss           | 0.00686     |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-0.16 +/- 0.23
Episode length: 202.12 +/- 38.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | -0.023   |
| time/              |          |
|    fps             | 14       |
|    iterations      | 21       |
|    time_elapsed    | 2928     |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | -0.21      |
| time/                   |            |
|    total_timesteps      | 43500      |
| train/                  |            |
|    approx_kl            | 0.00809204 |
|    clip_fraction        | 0.0158     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.55      |
|    explained_variance   | 0.234      |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00514    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.00131   |
|    value_loss           | 0.00435    |
----------------------------------------
Eval num_timesteps=44000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | -0.0133  |
| time/              |          |
|    fps             | 14       |
|    iterations      | 22       |
|    time_elapsed    | 3045     |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.004374884 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.322       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0127     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00324    |
|    value_loss           | 0.0104      |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-0.14 +/- 0.29
Episode length: 197.62 +/- 49.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | -0.0123  |
| time/              |          |
|    fps             | 14       |
|    iterations      | 23       |
|    time_elapsed    | 3163     |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.006537497 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.5        |
|    explained_variance   | 0.582       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.000695    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 0.0121      |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 0.00855  |
| time/              |          |
|    fps             | 14       |
|    iterations      | 24       |
|    time_elapsed    | 3284     |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0082859425 |
|    clip_fraction        | 0.041        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.51        |
|    explained_variance   | 0.41         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0464       |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00442     |
|    value_loss           | 0.0105       |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 0.00895  |
| time/              |          |
|    fps             | 15       |
|    iterations      | 25       |
|    time_elapsed    | 3406     |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 0.0024867256 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.472        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000235     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 0.00636      |
------------------------------------------
Eval num_timesteps=52000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-0.19 +/- 0.16
Episode length: 207.82 +/- 15.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 208      |
|    mean_reward     | -0.188   |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.0549   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 26       |
|    time_elapsed    | 3528     |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.005859766 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.51       |
|    explained_variance   | 0.483       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0229      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00432    |
|    value_loss           | 0.0149      |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 0.0993   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 27       |
|    time_elapsed    | 3660     |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0032807074 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.41        |
|    explained_variance   | 0.568        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0146      |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.0028      |
|    value_loss           | 0.0135       |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 0.076    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 28       |
|    time_elapsed    | 3784     |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0037629453 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.451        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0164       |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00311     |
|    value_loss           | 0.00537      |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-0.19 +/- 0.16
Episode length: 206.46 +/- 24.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-0.14 +/- 0.29
Episode length: 197.62 +/- 49.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 0.086    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 29       |
|    time_elapsed    | 3909     |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-0.19 +/- 0.15
Episode length: 207.88 +/- 14.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | -0.188     |
| time/                   |            |
|    total_timesteps      | 59500      |
| train/                  |            |
|    approx_kl            | 0.01202076 |
|    clip_fraction        | 0.06       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.37       |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00984    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00241   |
|    value_loss           | 0.00968    |
----------------------------------------
Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.107    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 30       |
|    time_elapsed    | 4031     |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-0.19 +/- 0.17
Episode length: 206.28 +/- 26.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 61500       |
| train/                  |             |
|    approx_kl            | 0.007869419 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.578       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.000498   |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0038     |
|    value_loss           | 0.00869     |
-----------------------------------------
Eval num_timesteps=62000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 15       |
|    iterations      | 31       |
|    time_elapsed    | 4153     |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.002015594 |
|    clip_fraction        | 0.00967     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.992      |
|    explained_variance   | 0.594       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00618    |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.000592   |
|    value_loss           | 0.0072      |
-----------------------------------------
Eval num_timesteps=64000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 0.0762   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 32       |
|    time_elapsed    | 4306     |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.003613198 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.508       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.000379   |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00463    |
|    value_loss           | 0.0053      |
-----------------------------------------
Eval num_timesteps=66500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.0753   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 33       |
|    time_elapsed    | 4478     |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.011382894 |
|    clip_fraction        | 0.0545      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.973      |
|    explained_variance   | 0.509       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0153      |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 0.00575     |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 0.0638   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 34       |
|    time_elapsed    | 4609     |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-0.14 +/- 0.29
Episode length: 197.64 +/- 48.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0065065203 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.803       |
|    explained_variance   | 0.531        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00517     |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.0029      |
|    value_loss           | 0.00497      |
------------------------------------------
Eval num_timesteps=70500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-0.19 +/- 0.17
Episode length: 206.24 +/- 26.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-0.19 +/- 0.16
Episode length: 207.16 +/- 19.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 207      |
|    mean_reward     | -0.187   |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 0.0206   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 35       |
|    time_elapsed    | 4733     |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 0.0013181355 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.786       |
|    explained_variance   | 0.364        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00104     |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 0.00436      |
------------------------------------------
Eval num_timesteps=72500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 0.00823  |
| time/              |          |
|    fps             | 15       |
|    iterations      | 36       |
|    time_elapsed    | 4854     |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0070067276 |
|    clip_fraction        | 0.075        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.755       |
|    explained_variance   | 0.455        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00395     |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00267     |
|    value_loss           | 0.00797      |
------------------------------------------
Eval num_timesteps=74500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 0.0315   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 37       |
|    time_elapsed    | 4976     |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0065319003 |
|    clip_fraction        | 0.0494       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.813       |
|    explained_variance   | 0.296        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0216      |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 0.0145       |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 0.0327   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 38       |
|    time_elapsed    | 5099     |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0029430382 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.655       |
|    explained_variance   | 0.501        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00658     |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 0.00838      |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 0.023    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 39       |
|    time_elapsed    | 5240     |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0030039172 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.816       |
|    explained_variance   | 0.464        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00126     |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00393     |
|    value_loss           | 0.00547      |
------------------------------------------
Eval num_timesteps=80500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-0.19 +/- 0.17
Episode length: 206.30 +/- 25.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 0.0442   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 40       |
|    time_elapsed    | 5390     |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-0.16 +/- 0.23
Episode length: 202.12 +/- 38.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0027675522 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.819       |
|    explained_variance   | 0.484        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000154     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00425     |
|    value_loss           | 0.0151       |
------------------------------------------
Eval num_timesteps=82500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 0.0544   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 41       |
|    time_elapsed    | 5510     |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-0.19 +/- 0.16
Episode length: 206.62 +/- 23.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | -0.187      |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.002908424 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.836      |
|    explained_variance   | 0.465       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0105     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 0.00709     |
-----------------------------------------
Eval num_timesteps=84500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-0.14 +/- 0.29
Episode length: 197.58 +/- 49.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 0.0558   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 42       |
|    time_elapsed    | 5692     |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=-0.16 +/- 0.24
Episode length: 201.72 +/- 40.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 0.0015998664 |
|    clip_fraction        | 0.00522      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.881       |
|    explained_variance   | 0.298        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00257      |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 0.00766      |
------------------------------------------
Eval num_timesteps=87000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 0.0886   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 43       |
|    time_elapsed    | 5801     |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0029392662 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.825       |
|    explained_variance   | 0.466        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0179       |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.000754    |
|    value_loss           | 0.0106       |
------------------------------------------
Eval num_timesteps=89000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-0.14 +/- 0.29
Episode length: 197.66 +/- 48.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0769   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 44       |
|    time_elapsed    | 5909     |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 90500       |
| train/                  |             |
|    approx_kl            | 0.005358454 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.844      |
|    explained_variance   | 0.585       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00696    |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 0.00187     |
-----------------------------------------
Eval num_timesteps=91000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-0.09 +/- 0.36
Episode length: 190.62 +/- 58.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -0.0906  |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.0899   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 45       |
|    time_elapsed    | 6015     |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.005742876 |
|    clip_fraction        | 0.0552      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.517       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0201     |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 0.0109      |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0671   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 46       |
|    time_elapsed    | 6128     |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 94500        |
| train/                  |              |
|    approx_kl            | 0.0029664263 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.668       |
|    explained_variance   | 0.493        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0135      |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00119     |
|    value_loss           | 0.00834      |
------------------------------------------
Eval num_timesteps=95000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 0.0895   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 47       |
|    time_elapsed    | 6235     |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0030080287 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.651       |
|    explained_variance   | 0.605        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00375     |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.000463    |
|    value_loss           | 0.012        |
------------------------------------------
Eval num_timesteps=97000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 15       |
|    iterations      | 48       |
|    time_elapsed    | 6347     |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 0.0036508623 |
|    clip_fraction        | 0.0381       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.667       |
|    explained_variance   | 0.556        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00598      |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00205     |
|    value_loss           | 0.00948      |
------------------------------------------
Eval num_timesteps=99000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 0.0548   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 49       |
|    time_elapsed    | 6454     |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 100500       |
| train/                  |              |
|    approx_kl            | 0.0023748311 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.695       |
|    explained_variance   | 0.428        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00474     |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00421     |
|    value_loss           | 0.00502      |
------------------------------------------
Eval num_timesteps=101000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0674   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 50       |
|    time_elapsed    | 6567     |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-0.11 +/- 0.33
Episode length: 193.54 +/- 55.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 194          |
|    mean_reward          | -0.113       |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 0.0015287715 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.631       |
|    explained_variance   | 0.448        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0101      |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.0011      |
|    value_loss           | 0.0102       |
------------------------------------------
Eval num_timesteps=103000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 0.066    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 51       |
|    time_elapsed    | 6684     |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0017370909 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.634       |
|    explained_variance   | 0.42         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0147       |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00283     |
|    value_loss           | 0.0076       |
------------------------------------------
Eval num_timesteps=105000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 0.0672   |
| time/              |          |
|    fps             | 15       |
|    iterations      | 52       |
|    time_elapsed    | 6804     |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-0.16 +/- 0.22
Episode length: 204.90 +/- 29.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 205         |
|    mean_reward          | -0.165      |
| time/                   |             |
|    total_timesteps      | 106500      |
| train/                  |             |
|    approx_kl            | 0.005113395 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.494       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00922     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.00291    |
|    value_loss           | 0.00958     |
-----------------------------------------
Eval num_timesteps=107000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-0.16 +/- 0.23
Episode length: 202.36 +/- 37.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.134    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 53       |
|    time_elapsed    | 6935     |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0024339044 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.796       |
|    explained_variance   | 0.568        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00179     |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00265     |
|    value_loss           | 0.0145       |
------------------------------------------
Eval num_timesteps=109500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 0.135    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 54       |
|    time_elapsed    | 7048     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 0.0034896187 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.767       |
|    explained_variance   | 0.511        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00966     |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 0.0153       |
------------------------------------------
Eval num_timesteps=111500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 0.124    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 55       |
|    time_elapsed    | 7155     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 0.0064453846 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.735       |
|    explained_variance   | 0.346        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00447     |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.000867    |
|    value_loss           | 0.00568      |
------------------------------------------
Eval num_timesteps=113500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-0.16 +/- 0.24
Episode length: 201.76 +/- 40.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 0.112    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 56       |
|    time_elapsed    | 7266     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0041994434 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.66        |
|    explained_variance   | 0.598        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00514      |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 0.00536      |
------------------------------------------
Eval num_timesteps=115500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-0.16 +/- 0.24
Episode length: 201.74 +/- 40.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 0.133    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 57       |
|    time_elapsed    | 7373     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-0.16 +/- 0.23
Episode length: 204.00 +/- 31.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 204          |
|    mean_reward          | -0.164       |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 0.0030963991 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.635       |
|    explained_variance   | 0.624        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00338     |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00124     |
|    value_loss           | 0.00936      |
------------------------------------------
Eval num_timesteps=117500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 0.169    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 58       |
|    time_elapsed    | 7480     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | -0.21       |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.004135664 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.63       |
|    explained_variance   | 0.711       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.000383   |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00397    |
|    value_loss           | 0.0132      |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 15       |
|    iterations      | 59       |
|    time_elapsed    | 7587     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0041561774 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.622       |
|    explained_variance   | 0.648        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0585       |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00154     |
|    value_loss           | 0.0092       |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 0.203    |
| time/              |          |
|    fps             | 15       |
|    iterations      | 60       |
|    time_elapsed    | 7698     |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0041822344 |
|    clip_fraction        | 0.0428       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.721       |
|    explained_variance   | 0.685        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00681      |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.004       |
|    value_loss           | 0.0125       |
------------------------------------------
Eval num_timesteps=123500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.202    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 61       |
|    time_elapsed    | 7804     |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-0.19 +/- 0.17
Episode length: 206.24 +/- 26.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.003704693 |
|    clip_fraction        | 0.0462      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.635       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00365    |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 0.0125      |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=-0.16 +/- 0.24
Episode length: 201.88 +/- 39.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 0.199    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 62       |
|    time_elapsed    | 7910     |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0023707114 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.79        |
|    explained_variance   | 0.61         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0101      |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.0017      |
|    value_loss           | 0.00769      |
------------------------------------------
Eval num_timesteps=127500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-0.19 +/- 0.17
Episode length: 206.22 +/- 26.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 16       |
|    iterations      | 63       |
|    time_elapsed    | 8045     |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-0.16 +/- 0.23
Episode length: 202.24 +/- 38.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | -0.162     |
| time/                   |            |
|    total_timesteps      | 129500     |
| train/                  |            |
|    approx_kl            | 0.00207947 |
|    clip_fraction        | 0.0232     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.714     |
|    explained_variance   | 0.778      |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00783    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.00238   |
|    value_loss           | 0.0111     |
----------------------------------------
Eval num_timesteps=130000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 0.201    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 64       |
|    time_elapsed    | 8155     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0055165594 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.738       |
|    explained_variance   | 0.491        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0387       |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00346     |
|    value_loss           | 0.00474      |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-0.19 +/- 0.17
Episode length: 206.42 +/- 25.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 0.201    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 65       |
|    time_elapsed    | 8263     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0028933515 |
|    clip_fraction        | 0.0377       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.679       |
|    explained_variance   | 0.558        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00824     |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 0.0113       |
------------------------------------------
Eval num_timesteps=134000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 0.176    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 66       |
|    time_elapsed    | 8368     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.002614121 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.678      |
|    explained_variance   | 0.517       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0127      |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00289    |
|    value_loss           | 0.0119      |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=-0.16 +/- 0.23
Episode length: 203.02 +/- 34.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-0.16 +/- 0.24
Episode length: 201.78 +/- 40.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 0.178    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 67       |
|    time_elapsed    | 8475     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 206         |
|    mean_reward          | -0.186      |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.002228496 |
|    clip_fraction        | 0.027       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.632      |
|    explained_variance   | 0.528       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0105      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00126    |
|    value_loss           | 0.0132      |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-0.16 +/- 0.24
Episode length: 201.88 +/- 39.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 0.189    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 68       |
|    time_elapsed    | 8583     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-0.16 +/- 0.24
Episode length: 202.02 +/- 39.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0027489678 |
|    clip_fraction        | 0.0311       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.688       |
|    explained_variance   | 0.503        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00749     |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.0037      |
|    value_loss           | 0.0133       |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-0.16 +/- 0.23
Episode length: 202.14 +/- 38.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.192    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 69       |
|    time_elapsed    | 8692     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 141500       |
| train/                  |              |
|    approx_kl            | 0.0032521368 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.65        |
|    explained_variance   | 0.631        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0142       |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.0038      |
|    value_loss           | 0.0138       |
------------------------------------------
Eval num_timesteps=142000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.202    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 70       |
|    time_elapsed    | 8822     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 0.0018603161 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.667       |
|    explained_variance   | 0.637        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00307      |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.00944      |
------------------------------------------
Eval num_timesteps=144000, episode_reward=-0.19 +/- 0.17
Episode length: 206.12 +/- 27.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-0.19 +/- 0.17
Episode length: 206.18 +/- 26.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 0.193    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 71       |
|    time_elapsed    | 8946     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0036503843 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.758       |
|    explained_variance   | 0.525        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0075       |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00467     |
|    value_loss           | 0.0153       |
------------------------------------------
Eval num_timesteps=146000, episode_reward=-0.16 +/- 0.23
Episode length: 202.14 +/- 38.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 0.201    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 72       |
|    time_elapsed    | 9053     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-0.07 +/- 0.39
Episode length: 186.18 +/- 64.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 186          |
|    mean_reward          | -0.0661      |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0026454171 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.741       |
|    explained_variance   | 0.577        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.01         |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00395     |
|    value_loss           | 0.00809      |
------------------------------------------
New best mean reward!
Eval num_timesteps=148000, episode_reward=-0.16 +/- 0.24
Episode length: 202.04 +/- 39.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-0.16 +/- 0.24
Episode length: 201.94 +/- 39.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-0.14 +/- 0.28
Episode length: 198.24 +/- 46.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-0.16 +/- 0.24
Episode length: 201.82 +/- 40.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 0.203    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 73       |
|    time_elapsed    | 9179     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-0.16 +/- 0.22
Episode length: 204.82 +/- 28.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 205          |
|    mean_reward          | -0.165       |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0024988228 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.709       |
|    explained_variance   | 0.492        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.013        |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.003       |
|    value_loss           | 0.0105       |
------------------------------------------
Eval num_timesteps=150500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 0.237    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 74       |
|    time_elapsed    | 9285     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-0.16 +/- 0.24
Episode length: 201.80 +/- 40.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0025128245 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.73        |
|    explained_variance   | 0.47         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0095       |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 0.0183       |
------------------------------------------
Eval num_timesteps=152500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-0.14 +/- 0.28
Episode length: 198.16 +/- 46.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-0.11 +/- 0.33
Episode length: 193.98 +/- 54.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 0.224    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 75       |
|    time_elapsed    | 9390     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0029125926 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.664       |
|    explained_variance   | 0.515        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00418     |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00298     |
|    value_loss           | 0.0088       |
------------------------------------------
Eval num_timesteps=154500, episode_reward=-0.09 +/- 0.35
Episode length: 192.02 +/- 54.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | -0.092   |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-0.09 +/- 0.36
Episode length: 190.30 +/- 59.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -0.0903  |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-0.16 +/- 0.23
Episode length: 202.04 +/- 39.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 0.269    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 76       |
|    time_elapsed    | 9497     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0033378224 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.6         |
|    explained_variance   | 0.635        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00533      |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 0.0196       |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-0.14 +/- 0.29
Episode length: 197.78 +/- 48.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-0.19 +/- 0.17
Episode length: 206.00 +/- 28.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 16       |
|    iterations      | 77       |
|    time_elapsed    | 9608     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-0.14 +/- 0.29
Episode length: 197.92 +/- 47.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 198        |
|    mean_reward          | -0.138     |
| time/                   |            |
|    total_timesteps      | 158000     |
| train/                  |            |
|    approx_kl            | 0.00116616 |
|    clip_fraction        | 0.00698    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.611      |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00498    |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.000973  |
|    value_loss           | 0.0172     |
----------------------------------------
Eval num_timesteps=158500, episode_reward=-0.16 +/- 0.24
Episode length: 201.90 +/- 39.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-0.14 +/- 0.29
Episode length: 197.78 +/- 48.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-0.11 +/- 0.33
Episode length: 193.74 +/- 55.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 16       |
|    iterations      | 78       |
|    time_elapsed    | 9713     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-0.16 +/- 0.23
Episode length: 202.04 +/- 39.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0034178128 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.561       |
|    explained_variance   | 0.485        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0139       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 0.00729      |
------------------------------------------
Eval num_timesteps=160500, episode_reward=-0.16 +/- 0.23
Episode length: 202.12 +/- 38.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-0.19 +/- 0.17
Episode length: 205.96 +/- 28.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-0.16 +/- 0.24
Episode length: 201.92 +/- 39.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 0.279    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 79       |
|    time_elapsed    | 9817     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-0.16 +/- 0.24
Episode length: 201.96 +/- 39.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 0.0026421282 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.56        |
|    explained_variance   | 0.56         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0029       |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00301     |
|    value_loss           | 0.0153       |
------------------------------------------
Eval num_timesteps=162500, episode_reward=-0.16 +/- 0.24
Episode length: 202.02 +/- 39.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-0.09 +/- 0.36
Episode length: 190.42 +/- 58.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -0.0904  |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-0.11 +/- 0.32
Episode length: 194.38 +/- 53.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 0.291    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 80       |
|    time_elapsed    | 9944     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-0.16 +/- 0.23
Episode length: 202.08 +/- 38.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | -0.162     |
| time/                   |            |
|    total_timesteps      | 164000     |
| train/                  |            |
|    approx_kl            | 0.00186002 |
|    clip_fraction        | 0.0229     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.614     |
|    explained_variance   | 0.487      |
|    learning_rate        | 1e-05      |
|    loss                 | -0.0128    |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.00245   |
|    value_loss           | 0.0115     |
----------------------------------------
Eval num_timesteps=164500, episode_reward=-0.14 +/- 0.28
Episode length: 198.24 +/- 46.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-0.19 +/- 0.17
Episode length: 206.00 +/- 28.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-0.09 +/- 0.36
Episode length: 189.82 +/- 60.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -0.0898  |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 0.289    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 81       |
|    time_elapsed    | 10049    |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | -0.21        |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0013353722 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.602       |
|    explained_variance   | 0.546        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00235      |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 0.0133       |
------------------------------------------
Eval num_timesteps=166500, episode_reward=-0.19 +/- 0.17
Episode length: 206.02 +/- 27.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-0.11 +/- 0.32
Episode length: 194.26 +/- 53.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 0.278    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 82       |
|    time_elapsed    | 10156    |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-0.19 +/- 0.17
Episode length: 206.06 +/- 27.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0057160715 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.641       |
|    explained_variance   | 0.6          |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00517      |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00567     |
|    value_loss           | 0.0164       |
------------------------------------------
Eval num_timesteps=168500, episode_reward=-0.14 +/- 0.28
Episode length: 198.04 +/- 47.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-0.14 +/- 0.28
Episode length: 198.66 +/- 45.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -0.139   |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-0.14 +/- 0.28
Episode length: 198.22 +/- 46.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 16       |
|    iterations      | 83       |
|    time_elapsed    | 10265    |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-0.16 +/- 0.23
Episode length: 202.58 +/- 36.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 203          |
|    mean_reward          | -0.163       |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0038675747 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.589       |
|    explained_variance   | 0.633        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00193     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00232     |
|    value_loss           | 0.0149       |
------------------------------------------
Eval num_timesteps=170500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-0.11 +/- 0.33
Episode length: 193.94 +/- 54.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-0.16 +/- 0.24
Episode length: 202.04 +/- 39.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-0.19 +/- 0.17
Episode length: 205.98 +/- 28.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 0.26     |
| time/              |          |
|    fps             | 16       |
|    iterations      | 84       |
|    time_elapsed    | 10397    |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-0.11 +/- 0.33
Episode length: 193.90 +/- 54.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | -0.114      |
| time/                   |             |
|    total_timesteps      | 172500      |
| train/                  |             |
|    approx_kl            | 0.003929265 |
|    clip_fraction        | 0.0299      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.58       |
|    explained_variance   | 0.614       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.0104     |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 0.0111      |
-----------------------------------------
Eval num_timesteps=173000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-0.19 +/- 0.17
Episode length: 205.86 +/- 28.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-0.14 +/- 0.28
Episode length: 198.46 +/- 45.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 0.261    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 85       |
|    time_elapsed    | 10503    |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-0.16 +/- 0.23
Episode length: 202.30 +/- 37.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | -0.162      |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.001736735 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.667      |
|    explained_variance   | 0.635       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00359     |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 0.0191      |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=-0.16 +/- 0.23
Episode length: 202.18 +/- 38.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-0.11 +/- 0.32
Episode length: 194.56 +/- 52.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-0.16 +/- 0.23
Episode length: 202.36 +/- 37.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 0.274    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 86       |
|    time_elapsed    | 10613    |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0031919111 |
|    clip_fraction        | 0.0544       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.571       |
|    explained_variance   | 0.696        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00912     |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00497     |
|    value_loss           | 0.00784      |
------------------------------------------
Eval num_timesteps=177000, episode_reward=-0.19 +/- 0.17
Episode length: 206.10 +/- 27.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-0.19 +/- 0.17
Episode length: 205.90 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-0.19 +/- 0.17
Episode length: 205.94 +/- 28.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 0.286    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 87       |
|    time_elapsed    | 10723    |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-0.14 +/- 0.28
Episode length: 198.68 +/- 44.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 199          |
|    mean_reward          | -0.139       |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0033979837 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.554       |
|    explained_variance   | 0.676        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00896     |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00342     |
|    value_loss           | 0.00869      |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-0.09 +/- 0.36
Episode length: 191.32 +/- 56.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -0.0913  |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 0.297    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 88       |
|    time_elapsed    | 10827    |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-0.19 +/- 0.17
Episode length: 205.92 +/- 28.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 206          |
|    mean_reward          | -0.186       |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0015756356 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.502       |
|    explained_variance   | 0.67         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00636      |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00208     |
|    value_loss           | 0.0101       |
------------------------------------------
Eval num_timesteps=181000, episode_reward=-0.19 +/- 0.17
Episode length: 206.06 +/- 27.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-0.11 +/- 0.33
Episode length: 194.20 +/- 53.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-0.14 +/- 0.29
Episode length: 198.00 +/- 47.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 0.301    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 89       |
|    time_elapsed    | 10932    |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-0.14 +/- 0.28
Episode length: 198.26 +/- 46.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 0.0009982272 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.612       |
|    explained_variance   | 0.728        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00791     |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00257     |
|    value_loss           | 0.0145       |
------------------------------------------
Eval num_timesteps=183000, episode_reward=-0.11 +/- 0.32
Episode length: 194.70 +/- 51.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-0.14 +/- 0.29
Episode length: 197.76 +/- 48.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-0.16 +/- 0.23
Episode length: 202.46 +/- 36.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 0.323    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 90       |
|    time_elapsed    | 11034    |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-0.09 +/- 0.36
Episode length: 189.72 +/- 60.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | -0.0897     |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.001074234 |
|    clip_fraction        | 0.00825     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.534      |
|    explained_variance   | 0.719       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0119      |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00243    |
|    value_loss           | 0.0146      |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=-0.11 +/- 0.32
Episode length: 194.58 +/- 52.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-0.19 +/- 0.17
Episode length: 206.10 +/- 27.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 0.321    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 91       |
|    time_elapsed    | 11138    |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-0.16 +/- 0.24
Episode length: 201.86 +/- 39.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 0.0010433432 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.517       |
|    explained_variance   | 0.708        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00371      |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00221     |
|    value_loss           | 0.0116       |
------------------------------------------
Eval num_timesteps=187000, episode_reward=-0.14 +/- 0.28
Episode length: 198.28 +/- 46.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-0.09 +/- 0.36
Episode length: 189.96 +/- 60.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -0.0899  |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-0.19 +/- 0.17
Episode length: 206.34 +/- 25.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 0.323    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 92       |
|    time_elapsed    | 11245    |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-0.11 +/- 0.33
Episode length: 193.84 +/- 54.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 194          |
|    mean_reward          | -0.114       |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0039917007 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.672       |
|    explained_variance   | 0.735        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000322    |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.00406     |
|    value_loss           | 0.0125       |
------------------------------------------
Eval num_timesteps=189000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-0.19 +/- 0.17
Episode length: 205.98 +/- 28.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 0.342    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 93       |
|    time_elapsed    | 11353    |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-0.14 +/- 0.28
Episode length: 199.92 +/- 40.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 200         |
|    mean_reward          | -0.14       |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.002239036 |
|    clip_fraction        | 0.0126      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.463      |
|    explained_variance   | 0.698       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0042      |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 0.0116      |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=-0.11 +/- 0.32
Episode length: 194.56 +/- 52.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-0.19 +/- 0.17
Episode length: 206.00 +/- 28.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-0.12 +/- 0.32
Episode length: 196.16 +/- 47.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | -0.116   |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-0.09 +/- 0.36
Episode length: 191.10 +/- 56.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 191      |
|    mean_reward     | -0.0911  |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 0.363    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 94       |
|    time_elapsed    | 11483    |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-0.16 +/- 0.24
Episode length: 201.96 +/- 39.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0041036815 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.58        |
|    explained_variance   | 0.694        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0125       |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00473     |
|    value_loss           | 0.0126       |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-0.11 +/- 0.32
Episode length: 194.24 +/- 53.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-0.19 +/- 0.17
Episode length: 206.08 +/- 27.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-0.19 +/- 0.17
Episode length: 205.88 +/- 28.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 0.386    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 95       |
|    time_elapsed    | 11591    |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-0.09 +/- 0.36
Episode length: 190.86 +/- 57.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 191          |
|    mean_reward          | -0.0908      |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0010597648 |
|    clip_fraction        | 0.0085       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.694        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00261     |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 0.0164       |
------------------------------------------
Eval num_timesteps=195500, episode_reward=-0.02 +/- 0.44
Episode length: 180.18 +/- 68.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | -0.0201  |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
New best mean reward!
Eval num_timesteps=196000, episode_reward=-0.04 +/- 0.41
Episode length: 182.66 +/- 68.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | -0.0426  |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-0.07 +/- 0.39
Episode length: 187.02 +/- 62.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | -0.067   |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 0.364    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 96       |
|    time_elapsed    | 11699    |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-0.14 +/- 0.28
Episode length: 198.88 +/- 44.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 199          |
|    mean_reward          | -0.139       |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0025166124 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.687       |
|    explained_variance   | 0.565        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0058      |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00377     |
|    value_loss           | 0.016        |
------------------------------------------
Eval num_timesteps=197500, episode_reward=-0.12 +/- 0.32
Episode length: 195.66 +/- 49.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 196      |
|    mean_reward     | -0.116   |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-0.07 +/- 0.39
Episode length: 187.02 +/- 62.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | -0.067   |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-0.07 +/- 0.39
Episode length: 186.66 +/- 63.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | -0.0666  |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 0.387    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 97       |
|    time_elapsed    | 11800    |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-0.16 +/- 0.24
Episode length: 202.00 +/- 39.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 0.0034897123 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.589       |
|    explained_variance   | 0.675        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00161     |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00464     |
|    value_loss           | 0.0171       |
------------------------------------------
Eval num_timesteps=199500, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-0.21 +/- 0.00
Episode length: 210.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 210      |
|    mean_reward     | -0.21    |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-0.16 +/- 0.23
Episode length: 202.36 +/- 37.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 0.421    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 98       |
|    time_elapsed    | 11911    |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-0.04 +/- 0.41
Episode length: 182.72 +/- 67.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 183          |
|    mean_reward          | -0.0427      |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0031029175 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.711       |
|    explained_variance   | 0.703        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0046      |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.0032      |
|    value_loss           | 0.0162       |
------------------------------------------
Eval num_timesteps=201500, episode_reward=-0.14 +/- 0.29
Episode length: 197.94 +/- 47.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-0.14 +/- 0.29
Episode length: 198.02 +/- 47.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-0.12 +/- 0.32
Episode length: 196.60 +/- 45.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | -0.117   |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 0.409    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 99       |
|    time_elapsed    | 12012    |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-0.16 +/- 0.23
Episode length: 202.48 +/- 36.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.0014833438 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.511       |
|    explained_variance   | 0.724        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00163     |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.0032      |
|    value_loss           | 0.011        |
------------------------------------------
Eval num_timesteps=203500, episode_reward=-0.14 +/- 0.28
Episode length: 198.10 +/- 47.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-0.04 +/- 0.41
Episode length: 183.02 +/- 67.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | -0.0429  |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-0.09 +/- 0.36
Episode length: 191.58 +/- 55.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | -0.0915  |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 0.486    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 100      |
|    time_elapsed    | 12115    |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-0.16 +/- 0.23
Episode length: 202.14 +/- 38.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 202          |
|    mean_reward          | -0.162       |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0023527294 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.455       |
|    explained_variance   | 0.765        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0228       |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00197     |
|    value_loss           | 0.0194       |
------------------------------------------
Eval num_timesteps=205500, episode_reward=-0.11 +/- 0.33
Episode length: 194.08 +/- 54.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-0.12 +/- 0.32
Episode length: 195.04 +/- 50.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-0.16 +/- 0.24
Episode length: 202.02 +/- 39.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 0.474    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 101      |
|    time_elapsed    | 12250    |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-0.12 +/- 0.32
Episode length: 195.14 +/- 50.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 195          |
|    mean_reward          | -0.115       |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0030757957 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.513       |
|    explained_variance   | 0.748        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00868     |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00275     |
|    value_loss           | 0.0102       |
------------------------------------------
Eval num_timesteps=207500, episode_reward=-0.16 +/- 0.24
Episode length: 201.84 +/- 39.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-0.12 +/- 0.32
Episode length: 195.18 +/- 50.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-0.16 +/- 0.24
Episode length: 201.88 +/- 39.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 0.477    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 102      |
|    time_elapsed    | 12353    |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-0.14 +/- 0.29
Episode length: 197.96 +/- 47.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0021843305 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.588       |
|    explained_variance   | 0.717        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00174      |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00377     |
|    value_loss           | 0.0122       |
------------------------------------------
Eval num_timesteps=209500, episode_reward=-0.16 +/- 0.23
Episode length: 202.60 +/- 36.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-0.14 +/- 0.29
Episode length: 197.96 +/- 47.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-0.14 +/- 0.29
Episode length: 197.94 +/- 47.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 0.469    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 103      |
|    time_elapsed    | 12466    |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-0.07 +/- 0.39
Episode length: 187.36 +/- 61.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 187          |
|    mean_reward          | -0.0673      |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0033112038 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.715       |
|    explained_variance   | 0.776        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.011        |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00161     |
|    value_loss           | 0.0154       |
------------------------------------------
Eval num_timesteps=211500, episode_reward=-0.14 +/- 0.28
Episode length: 198.26 +/- 46.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-0.14 +/- 0.29
Episode length: 197.92 +/- 47.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-0.19 +/- 0.17
Episode length: 206.08 +/- 27.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 0.435    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 104      |
|    time_elapsed    | 12573    |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-0.14 +/- 0.28
Episode length: 198.04 +/- 47.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0032605757 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.476       |
|    explained_variance   | 0.641        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00692     |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00307     |
|    value_loss           | 0.00988      |
------------------------------------------
Eval num_timesteps=213500, episode_reward=-0.16 +/- 0.23
Episode length: 202.32 +/- 37.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-0.04 +/- 0.41
Episode length: 182.62 +/- 67.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | -0.0426  |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-0.14 +/- 0.28
Episode length: 198.80 +/- 44.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 199      |
|    mean_reward     | -0.139   |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-0.14 +/- 0.28
Episode length: 198.14 +/- 46.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 0.455    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 105      |
|    time_elapsed    | 12698    |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-0.14 +/- 0.29
Episode length: 197.96 +/- 47.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | -0.138       |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0011915918 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.669       |
|    explained_variance   | 0.608        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00083      |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00218     |
|    value_loss           | 0.0189       |
------------------------------------------
Eval num_timesteps=216000, episode_reward=-0.16 +/- 0.24
Episode length: 201.90 +/- 39.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-0.07 +/- 0.39
Episode length: 187.24 +/- 61.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | -0.0672  |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-0.16 +/- 0.23
Episode length: 202.40 +/- 37.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 202      |
|    mean_reward     | -0.162   |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 0.407    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 106      |
|    time_elapsed    | 12807    |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-0.14 +/- 0.28
Episode length: 198.84 +/- 44.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 199          |
|    mean_reward          | -0.139       |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 0.0046743713 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.684       |
|    explained_variance   | 0.713        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00029     |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.00316     |
|    value_loss           | 0.0104       |
------------------------------------------
Eval num_timesteps=218000, episode_reward=-0.09 +/- 0.36
Episode length: 191.54 +/- 56.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | -0.0915  |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-0.19 +/- 0.17
Episode length: 206.20 +/- 26.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-0.19 +/- 0.17
Episode length: 206.24 +/- 26.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -0.186   |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 0.358    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 107      |
|    time_elapsed    | 12911    |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-0.12 +/- 0.32
Episode length: 195.06 +/- 50.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 195          |
|    mean_reward          | -0.115       |
| time/                   |              |
|    total_timesteps      | 219500       |
| train/                  |              |
|    approx_kl            | 0.0015536447 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.496       |
|    explained_variance   | 0.694        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00579      |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00189     |
|    value_loss           | 0.0113       |
------------------------------------------
Eval num_timesteps=220000, episode_reward=-0.07 +/- 0.39
Episode length: 186.32 +/- 64.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | -0.0663  |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-0.14 +/- 0.29
Episode length: 197.96 +/- 47.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 198      |
|    mean_reward     | -0.138   |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-0.12 +/- 0.32
Episode length: 195.08 +/- 50.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 0.359    |
| time/              |          |
|    fps             | 16       |
|    iterations      | 108      |
|    time_elapsed    | 13013    |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-0.14 +/- 0.29
Episode length: 197.84 +/- 48.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | -0.138      |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.002471228 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.762       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0126      |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 0.0103      |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=-0.11 +/- 0.33
Episode length: 194.12 +/- 53.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-0.12 +/- 0.32
Episode length: 195.04 +/- 50.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 195      |
|    mean_reward     | -0.115   |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-0.16 +/- 0.23
Episode length: 202.66 +/- 35.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 203      |
|    mean_reward     | -0.163   |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 0.323    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 109      |
|    time_elapsed    | 13118    |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=0.07 +/- 0.50
Episode length: 168.30 +/- 75.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 168          |
|    mean_reward          | 0.0718       |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 0.0037647597 |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.618       |
|    explained_variance   | 0.691        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0282       |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00375     |
|    value_loss           | 0.00795      |
------------------------------------------
New best mean reward!
Eval num_timesteps=224000, episode_reward=0.07 +/- 0.51
Episode length: 165.96 +/- 79.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 0.0742   |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
New best mean reward!
Eval num_timesteps=224500, episode_reward=0.24 +/- 0.57
Episode length: 141.52 +/- 88.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 0.239    |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
New best mean reward!
Eval num_timesteps=225000, episode_reward=-0.02 +/- 0.43
Episode length: 181.98 +/- 65.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | -0.0219  |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 0.288    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 110      |
|    time_elapsed    | 13205    |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=0.03 +/- 0.47
Episode length: 174.54 +/- 73.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 175          |
|    mean_reward          | 0.0255       |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 0.0014473451 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.51         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.012        |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00183     |
|    value_loss           | 0.0157       |
------------------------------------------
Eval num_timesteps=226000, episode_reward=0.05 +/- 0.49
Episode length: 172.06 +/- 72.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 0.048    |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=0.15 +/- 0.55
Episode length: 152.46 +/- 88.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 152      |
|    mean_reward     | 0.148    |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=0.10 +/- 0.52
Episode length: 162.92 +/- 80.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 163      |
|    mean_reward     | 0.0972   |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 0.265    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 111      |
|    time_elapsed    | 13297    |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=0.22 +/- 0.57
Episode length: 143.14 +/- 89.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 143          |
|    mean_reward          | 0.217        |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 0.0019430973 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.585       |
|    explained_variance   | -1.9         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00678      |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00214     |
|    value_loss           | 0.00328      |
------------------------------------------
Eval num_timesteps=228000, episode_reward=0.07 +/- 0.50
Episode length: 171.00 +/- 72.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 0.0691   |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-0.04 +/- 0.41
Episode length: 184.22 +/- 64.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | -0.0442  |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-0.04 +/- 0.41
Episode length: 184.04 +/- 64.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | -0.044   |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 0.268    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 112      |
|    time_elapsed    | 13387    |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-0.05 +/- 0.41
Episode length: 185.70 +/- 61.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 186         |
|    mean_reward          | -0.0457     |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.002001673 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.563       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00409     |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.017       |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=-0.04 +/- 0.41
Episode length: 183.58 +/- 65.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | -0.0435  |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=0.03 +/- 0.47
Episode length: 174.04 +/- 73.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 0.0261   |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=0.12 +/- 0.53
Episode length: 158.94 +/- 83.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 0.121    |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 17       |
|    iterations      | 113      |
|    time_elapsed    | 13480    |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=0.08 +/- 0.51
Episode length: 164.94 +/- 80.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 0.0752      |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.005284557 |
|    clip_fraction        | 0.0366      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.581      |
|    explained_variance   | 0.637       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0104      |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00341    |
|    value_loss           | 0.0215      |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=0.03 +/- 0.47
Episode length: 174.28 +/- 72.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 0.0258   |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=0.00 +/- 0.46
Episode length: 176.68 +/- 71.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 0.00339  |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=0.00 +/- 0.46
Episode length: 176.00 +/- 72.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 0.00405  |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 17       |
|    iterations      | 114      |
|    time_elapsed    | 13575    |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=0.00 +/- 0.45
Episode length: 179.10 +/- 67.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 179          |
|    mean_reward          | 0.000986     |
| time/                   |              |
|    total_timesteps      | 233500       |
| train/                  |              |
|    approx_kl            | 0.0010524174 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.464       |
|    explained_variance   | 0.712        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00756      |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00112     |
|    value_loss           | 0.0169       |
------------------------------------------
Eval num_timesteps=234000, episode_reward=-0.09 +/- 0.35
Episode length: 191.74 +/- 55.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 192      |
|    mean_reward     | -0.0917  |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=0.05 +/- 0.48
Episode length: 172.60 +/- 71.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 173      |
|    mean_reward     | 0.0475   |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-0.05 +/- 0.41
Episode length: 185.44 +/- 61.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | -0.0454  |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=0.05 +/- 0.49
Episode length: 170.66 +/- 74.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 0.0494   |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 17       |
|    iterations      | 115      |
|    time_elapsed    | 13694    |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=0.07 +/- 0.51
Episode length: 165.76 +/- 79.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 0.0743      |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.002400444 |
|    clip_fraction        | 0.022       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.746       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00275    |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 0.0126      |
-----------------------------------------
Eval num_timesteps=236500, episode_reward=0.08 +/- 0.51
Episode length: 164.66 +/- 80.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 0.0754   |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=0.05 +/- 0.49
Episode length: 171.02 +/- 73.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 171      |
|    mean_reward     | 0.0491   |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=0.07 +/- 0.50
Episode length: 167.54 +/- 76.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 0.0726   |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 0.428    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 116      |
|    time_elapsed    | 13783    |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=0.22 +/- 0.57
Episode length: 141.82 +/- 91.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 142          |
|    mean_reward          | 0.218        |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 0.0011398742 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.423       |
|    explained_variance   | 0.769        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00179      |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 0.0153       |
------------------------------------------
Eval num_timesteps=238500, episode_reward=-0.02 +/- 0.43
Episode length: 181.70 +/- 65.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | -0.0216  |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-0.02 +/- 0.43
Episode length: 183.74 +/- 61.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 184      |
|    mean_reward     | -0.0237  |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-0.00 +/- 0.45
Episode length: 180.78 +/- 63.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 181      |
|    mean_reward     | -0.00069 |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 0.471    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 117      |
|    time_elapsed    | 13883    |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=0.28 +/- 0.58
Episode length: 137.88 +/- 87.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 138          |
|    mean_reward          | 0.282        |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0038440118 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.495       |
|    explained_variance   | 0.679        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.016        |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00319     |
|    value_loss           | 0.0152       |
------------------------------------------
New best mean reward!
Eval num_timesteps=240500, episode_reward=0.22 +/- 0.57
Episode length: 144.64 +/- 87.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 145      |
|    mean_reward     | 0.216    |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=0.09 +/- 0.51
Episode length: 166.90 +/- 76.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 0.0932   |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=0.05 +/- 0.49
Episode length: 168.96 +/- 77.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 0.0511   |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 0.461    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 118      |
|    time_elapsed    | 13969    |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=0.09 +/- 0.51
Episode length: 165.24 +/- 76.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 165          |
|    mean_reward          | 0.0948       |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0022020177 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.473       |
|    explained_variance   | 0.648        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0312       |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 0.0117       |
------------------------------------------
Eval num_timesteps=242500, episode_reward=-0.09 +/- 0.36
Episode length: 190.16 +/- 59.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 190      |
|    mean_reward     | -0.0901  |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=0.12 +/- 0.53
Episode length: 159.64 +/- 81.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 0.12     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=0.00 +/- 0.45
Episode length: 177.22 +/- 71.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 0.00283  |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 0.44     |
| time/              |          |
|    fps             | 17       |
|    iterations      | 119      |
|    time_elapsed    | 14059    |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=0.19 +/- 0.56
Episode length: 148.96 +/- 85.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 149          |
|    mean_reward          | 0.191        |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0016509171 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.521       |
|    explained_variance   | 0.664        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000134     |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 0.012        |
------------------------------------------
Eval num_timesteps=244500, episode_reward=0.14 +/- 0.53
Episode length: 160.24 +/- 77.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 0.14     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=0.10 +/- 0.52
Episode length: 162.30 +/- 81.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 0.0978   |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=0.10 +/- 0.52
Episode length: 164.40 +/- 78.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 0.0957   |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 0.372    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 120      |
|    time_elapsed    | 14142    |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=0.10 +/- 0.52
Episode length: 160.58 +/- 83.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 161          |
|    mean_reward          | 0.0995       |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0038794423 |
|    clip_fraction        | 0.0375       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.331       |
|    explained_variance   | 0.651        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00624      |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00396     |
|    value_loss           | 0.0127       |
------------------------------------------
Eval num_timesteps=246500, episode_reward=0.09 +/- 0.51
Episode length: 165.24 +/- 78.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 0.0949   |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=0.03 +/- 0.47
Episode length: 174.26 +/- 71.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 0.0258   |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=0.07 +/- 0.50
Episode length: 169.78 +/- 73.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 170      |
|    mean_reward     | 0.0704   |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 0.372    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 121      |
|    time_elapsed    | 14237    |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=0.07 +/- 0.50
Episode length: 171.22 +/- 71.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 171          |
|    mean_reward          | 0.0689       |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0024920187 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.324       |
|    explained_variance   | 0.723        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00167     |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.0025      |
|    value_loss           | 0.0135       |
------------------------------------------
Eval num_timesteps=248500, episode_reward=0.17 +/- 0.55
Episode length: 155.14 +/- 81.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 0.165    |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=0.04 +/- 0.48
Episode length: 176.64 +/- 65.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 0.0434   |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=0.09 +/- 0.51
Episode length: 165.62 +/- 75.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 0.0945   |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 0.338    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 122      |
|    time_elapsed    | 14328    |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=0.12 +/- 0.53
Episode length: 161.88 +/- 77.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 162          |
|    mean_reward          | 0.118        |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0016734938 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.335       |
|    explained_variance   | 0.678        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00378      |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.000686    |
|    value_loss           | 0.0106       |
------------------------------------------
Eval num_timesteps=250500, episode_reward=0.46 +/- 0.57
Episode length: 119.62 +/- 83.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 0.461    |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
New best mean reward!
Eval num_timesteps=251000, episode_reward=0.17 +/- 0.55
Episode length: 153.56 +/- 83.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 154      |
|    mean_reward     | 0.167    |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=0.09 +/- 0.51
Episode length: 167.22 +/- 74.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 0.0928   |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 0.318    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 123      |
|    time_elapsed    | 14412    |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=0.12 +/- 0.53
Episode length: 161.18 +/- 79.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 161          |
|    mean_reward          | 0.119        |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0025805354 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.413       |
|    explained_variance   | 0.764        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00253      |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00349     |
|    value_loss           | 0.00961      |
------------------------------------------
Eval num_timesteps=252500, episode_reward=0.39 +/- 0.58
Episode length: 131.94 +/- 83.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 0.388    |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=0.12 +/- 0.52
Episode length: 163.54 +/- 76.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 0.117    |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=0.21 +/- 0.56
Episode length: 152.30 +/- 79.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 152      |
|    mean_reward     | 0.208    |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 0.356    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 124      |
|    time_elapsed    | 14494    |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=0.07 +/- 0.50
Episode length: 169.78 +/- 72.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 170          |
|    mean_reward          | 0.0703       |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0041366923 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.438       |
|    explained_variance   | 0.747        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00652      |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.00344     |
|    value_loss           | 0.0179       |
------------------------------------------
Eval num_timesteps=254500, episode_reward=0.14 +/- 0.54
Episode length: 156.18 +/- 82.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 0.144    |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=0.21 +/- 0.56
Episode length: 148.14 +/- 83.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 148      |
|    mean_reward     | 0.212    |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=0.21 +/- 0.56
Episode length: 149.80 +/- 83.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 0.21     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=0.21 +/- 0.57
Episode length: 146.38 +/- 86.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 0.214    |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 0.354    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 125      |
|    time_elapsed    | 14614    |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=0.21 +/- 0.56
Episode length: 151.34 +/- 81.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 0.209       |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.002198151 |
|    clip_fraction        | 0.0118      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.698       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00518    |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 0.0147      |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=0.35 +/- 0.58
Episode length: 134.52 +/- 82.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 0.346    |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=0.21 +/- 0.56
Episode length: 149.76 +/- 81.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 0.21     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=0.12 +/- 0.52
Episode length: 163.62 +/- 75.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 0.116    |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 0.375    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 126      |
|    time_elapsed    | 14694    |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=0.23 +/- 0.56
Episode length: 151.06 +/- 78.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 151          |
|    mean_reward          | 0.229        |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 0.0024846946 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.384       |
|    explained_variance   | 0.716        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00582      |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00268     |
|    value_loss           | 0.0136       |
------------------------------------------
Eval num_timesteps=259000, episode_reward=0.18 +/- 0.55
Episode length: 159.24 +/- 75.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 0.181    |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=0.21 +/- 0.56
Episode length: 149.38 +/- 82.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 0.211    |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=0.07 +/- 0.50
Episode length: 168.50 +/- 74.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 0.0716   |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 0.354    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 127      |
|    time_elapsed    | 14780    |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=0.09 +/- 0.51
Episode length: 165.92 +/- 75.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 166          |
|    mean_reward          | 0.0942       |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 0.0010535978 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | 0.736        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000972    |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00208     |
|    value_loss           | 0.0129       |
------------------------------------------
Eval num_timesteps=261000, episode_reward=0.14 +/- 0.54
Episode length: 157.36 +/- 81.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 157      |
|    mean_reward     | 0.143    |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=0.19 +/- 0.56
Episode length: 147.84 +/- 87.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 148      |
|    mean_reward     | 0.192    |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=0.21 +/- 0.56
Episode length: 149.28 +/- 83.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 0.211    |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 0.39     |
| time/              |          |
|    fps             | 17       |
|    iterations      | 128      |
|    time_elapsed    | 14868    |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=0.03 +/- 0.48
Episode length: 172.04 +/- 76.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 172        |
|    mean_reward          | 0.028      |
| time/                   |            |
|    total_timesteps      | 262500     |
| train/                  |            |
|    approx_kl            | 0.00142018 |
|    clip_fraction        | 0.0152     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.33      |
|    explained_variance   | 0.698      |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00566    |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.00313   |
|    value_loss           | 0.0135     |
----------------------------------------
Eval num_timesteps=263000, episode_reward=0.14 +/- 0.54
Episode length: 159.62 +/- 79.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 0.141    |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=0.14 +/- 0.54
Episode length: 157.70 +/- 81.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 0.142    |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=0.23 +/- 0.56
Episode length: 148.56 +/- 81.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 149      |
|    mean_reward     | 0.232    |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 0.367    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 129      |
|    time_elapsed    | 14955    |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=0.23 +/- 0.56
Episode length: 152.32 +/- 77.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 152          |
|    mean_reward          | 0.228        |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0045621255 |
|    clip_fraction        | 0.041        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.298       |
|    explained_variance   | 0.727        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00107      |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00394     |
|    value_loss           | 0.014        |
------------------------------------------
Eval num_timesteps=265000, episode_reward=0.36 +/- 0.58
Episode length: 135.86 +/- 81.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 0.364    |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=0.44 +/- 0.58
Episode length: 120.08 +/- 84.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 0.44     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=0.35 +/- 0.58
Episode length: 128.52 +/- 87.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 0.352    |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 0.355    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 130      |
|    time_elapsed    | 15029    |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=0.34 +/- 0.57
Episode length: 142.06 +/- 79.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 142          |
|    mean_reward          | 0.338        |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0017799042 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0.702        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00698     |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.0024      |
|    value_loss           | 0.00944      |
------------------------------------------
Eval num_timesteps=267000, episode_reward=0.37 +/- 0.58
Episode length: 128.78 +/- 85.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 0.371    |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=0.28 +/- 0.58
Episode length: 139.24 +/- 87.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 0.281    |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=0.21 +/- 0.55
Episode length: 155.00 +/- 76.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 0.205    |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 0.393    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 131      |
|    time_elapsed    | 15110    |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=0.35 +/- 0.58
Episode length: 134.14 +/- 82.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 0.346       |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.003180984 |
|    clip_fraction        | 0.0205      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.276      |
|    explained_variance   | 0.76        |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00518     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00345    |
|    value_loss           | 0.0168      |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=0.26 +/- 0.57
Episode length: 141.76 +/- 85.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 0.258    |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=0.39 +/- 0.58
Episode length: 131.48 +/- 84.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 0.389    |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=0.40 +/- 0.58
Episode length: 124.30 +/- 85.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 0.396    |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 0.348    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 132      |
|    time_elapsed    | 15186    |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=0.34 +/- 0.57
Episode length: 141.04 +/- 80.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 141          |
|    mean_reward          | 0.339        |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 0.0016556922 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.232       |
|    explained_variance   | 0.627        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00631     |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00357     |
|    value_loss           | 0.0104       |
------------------------------------------
Eval num_timesteps=271000, episode_reward=0.33 +/- 0.58
Episode length: 131.28 +/- 87.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 0.329    |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=0.25 +/- 0.57
Episode length: 148.24 +/- 82.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 148      |
|    mean_reward     | 0.252    |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=0.37 +/- 0.58
Episode length: 128.18 +/- 87.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 0.372    |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 0.425    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 133      |
|    time_elapsed    | 15263    |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=0.36 +/- 0.57
Episode length: 138.10 +/- 82.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 138         |
|    mean_reward          | 0.362       |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.003063911 |
|    clip_fraction        | 0.0306      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.686       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0104      |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00481    |
|    value_loss           | 0.025       |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=0.34 +/- 0.58
Episode length: 136.64 +/- 82.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 0.344    |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=0.51 +/- 0.57
Episode length: 108.14 +/- 84.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.512    |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
New best mean reward!
Eval num_timesteps=274000, episode_reward=0.45 +/- 0.58
Episode length: 114.88 +/- 87.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 0.445    |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 0.402    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 134      |
|    time_elapsed    | 15334    |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=0.38 +/- 0.57
Episode length: 138.00 +/- 76.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 138          |
|    mean_reward          | 0.382        |
| time/                   |              |
|    total_timesteps      | 274500       |
| train/                  |              |
|    approx_kl            | 0.0014268348 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0.7          |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0044      |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00253     |
|    value_loss           | 0.0135       |
------------------------------------------
Eval num_timesteps=275000, episode_reward=0.55 +/- 0.55
Episode length: 108.26 +/- 81.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.552    |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
New best mean reward!
Eval num_timesteps=275500, episode_reward=0.44 +/- 0.58
Episode length: 117.06 +/- 88.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 0.443    |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=0.55 +/- 0.55
Episode length: 105.62 +/- 82.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.555    |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 0.412    |
| time/              |          |
|    fps             | 17       |
|    iterations      | 135      |
|    time_elapsed    | 15400    |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=0.45 +/- 0.56
Episode length: 127.92 +/- 79.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 128          |
|    mean_reward          | 0.452        |
| time/                   |              |
|    total_timesteps      | 276500       |
| train/                  |              |
|    approx_kl            | 0.0007896627 |
|    clip_fraction        | 0.00562      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.232       |
|    explained_variance   | 0.706        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00781      |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00122     |
|    value_loss           | 0.0157       |
------------------------------------------
Eval num_timesteps=277000, episode_reward=0.46 +/- 0.57
Episode length: 121.64 +/- 81.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.459    |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=0.60 +/- 0.53
Episode length: 96.12 +/- 79.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 0.604    |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
New best mean reward!
Eval num_timesteps=278000, episode_reward=0.52 +/- 0.55
Episode length: 117.20 +/- 80.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 0.523    |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=0.41 +/- 0.58
Episode length: 126.88 +/- 84.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 0.413    |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 17       |
|    iterations      | 136      |
|    time_elapsed    | 15484    |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=0.37 +/- 0.58
Episode length: 130.42 +/- 86.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 130         |
|    mean_reward          | 0.37        |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.002457588 |
|    clip_fraction        | 0.0303      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.281      |
|    explained_variance   | 0.778       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00312     |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 0.0174      |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=0.60 +/- 0.53
Episode length: 101.42 +/- 77.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.599    |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=0.39 +/- 0.57
Episode length: 135.18 +/- 81.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 0.385    |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=0.37 +/- 0.58
Episode length: 130.74 +/- 82.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 0.369    |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 0.445    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 137      |
|    time_elapsed    | 15559    |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=0.55 +/- 0.55
Episode length: 113.04 +/- 80.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 113          |
|    mean_reward          | 0.547        |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0011967677 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.271       |
|    explained_variance   | 0.75         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00386      |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00191     |
|    value_loss           | 0.0119       |
------------------------------------------
Eval num_timesteps=281500, episode_reward=0.69 +/- 0.48
Episode length: 91.52 +/- 73.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 0.689    |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
New best mean reward!
Eval num_timesteps=282000, episode_reward=0.57 +/- 0.54
Episode length: 105.40 +/- 81.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.575    |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=0.33 +/- 0.58
Episode length: 133.64 +/- 85.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 0.327    |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 0.452    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 138      |
|    time_elapsed    | 15658    |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=0.51 +/- 0.57
Episode length: 108.36 +/- 85.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 108          |
|    mean_reward          | 0.512        |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 0.0021638684 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.351       |
|    explained_variance   | 0.783        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00431      |
|    n_updates            | 1380         |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 0.0141       |
------------------------------------------
Eval num_timesteps=283500, episode_reward=0.37 +/- 0.58
Episode length: 134.42 +/- 81.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 0.366    |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=0.48 +/- 0.57
Episode length: 118.12 +/- 84.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 0.482    |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=0.43 +/- 0.57
Episode length: 125.44 +/- 82.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 0.435    |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 0.465    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 139      |
|    time_elapsed    | 15729    |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=0.51 +/- 0.57
Episode length: 108.20 +/- 83.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 0.512       |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.001590105 |
|    clip_fraction        | 0.0116      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.764       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00672     |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 0.0183      |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=0.49 +/- 0.57
Episode length: 113.00 +/- 84.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.487    |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=0.50 +/- 0.56
Episode length: 121.24 +/- 81.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 0.499    |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=0.51 +/- 0.56
Episode length: 111.40 +/- 82.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.509    |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 0.463    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 140      |
|    time_elapsed    | 15795    |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=0.51 +/- 0.56
Episode length: 113.78 +/- 85.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 114          |
|    mean_reward          | 0.506        |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0026580917 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.244       |
|    explained_variance   | 0.701        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00683      |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00457     |
|    value_loss           | 0.0161       |
------------------------------------------
Eval num_timesteps=287500, episode_reward=0.56 +/- 0.55
Episode length: 102.84 +/- 85.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 0.557    |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=0.51 +/- 0.56
Episode length: 110.72 +/- 80.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.51     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=0.50 +/- 0.56
Episode length: 115.82 +/- 83.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 0.504    |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 0.476    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 141      |
|    time_elapsed    | 15864    |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=0.48 +/- 0.57
Episode length: 118.54 +/- 81.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 119          |
|    mean_reward          | 0.482        |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0012340969 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.219       |
|    explained_variance   | 0.699        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00346     |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00292     |
|    value_loss           | 0.0129       |
------------------------------------------
Eval num_timesteps=289500, episode_reward=0.40 +/- 0.59
Episode length: 121.54 +/- 87.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.399    |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=0.59 +/- 0.53
Episode length: 105.54 +/- 78.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.595    |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=0.53 +/- 0.55
Episode length: 112.58 +/- 83.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.528    |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 0.445    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 142      |
|    time_elapsed    | 15929    |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=0.64 +/- 0.51
Episode length: 97.64 +/- 78.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.6         |
|    mean_reward          | 0.643        |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0011892776 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.246       |
|    explained_variance   | 0.75         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00573      |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00229     |
|    value_loss           | 0.0152       |
------------------------------------------
Eval num_timesteps=291500, episode_reward=0.48 +/- 0.56
Episode length: 123.34 +/- 79.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 0.477    |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=0.62 +/- 0.52
Episode length: 98.44 +/- 79.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 0.622    |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=0.55 +/- 0.55
Episode length: 108.88 +/- 79.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.551    |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 143      |
|    time_elapsed    | 15991    |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=0.48 +/- 0.57
Episode length: 117.08 +/- 84.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 117          |
|    mean_reward          | 0.483        |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 0.0019554188 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | 0.764        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000373     |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00252     |
|    value_loss           | 0.0199       |
------------------------------------------
Eval num_timesteps=293500, episode_reward=0.57 +/- 0.54
Episode length: 105.72 +/- 80.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.575    |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=0.55 +/- 0.55
Episode length: 111.78 +/- 81.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.549    |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=0.58 +/- 0.54
Episode length: 98.62 +/- 81.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 0.582    |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 0.502    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 144      |
|    time_elapsed    | 16053    |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=0.60 +/- 0.53
Episode length: 103.08 +/- 81.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 103          |
|    mean_reward          | 0.597        |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0017398393 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.244       |
|    explained_variance   | 0.717        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00248      |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.00315     |
|    value_loss           | 0.016        |
------------------------------------------
Eval num_timesteps=295500, episode_reward=0.60 +/- 0.53
Episode length: 98.18 +/- 80.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=0.55 +/- 0.55
Episode length: 108.98 +/- 82.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.551    |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=0.48 +/- 0.56
Episode length: 123.46 +/- 79.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 0.477    |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 0.461    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 145      |
|    time_elapsed    | 16116    |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=0.40 +/- 0.58
Episode length: 123.96 +/- 84.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 124          |
|    mean_reward          | 0.396        |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 0.0036905366 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.203       |
|    explained_variance   | 0.758        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00245     |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.0052      |
|    value_loss           | 0.0132       |
------------------------------------------
Eval num_timesteps=297500, episode_reward=0.54 +/- 0.56
Episode length: 105.06 +/- 83.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.535    |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=0.54 +/- 0.56
Episode length: 100.98 +/- 83.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.539    |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=0.44 +/- 0.58
Episode length: 122.02 +/- 84.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.438    |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=0.42 +/- 0.58
Episode length: 122.34 +/- 85.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.418    |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 0.462    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 146      |
|    time_elapsed    | 16196    |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=0.50 +/- 0.56
Episode length: 116.88 +/- 81.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 117          |
|    mean_reward          | 0.503        |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0020532054 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.217       |
|    explained_variance   | 0.678        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0126       |
|    n_updates            | 1460         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 0.0129       |
------------------------------------------
Eval num_timesteps=300000, episode_reward=0.48 +/- 0.57
Episode length: 116.16 +/- 80.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 0.484    |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=0.62 +/- 0.52
Episode length: 99.32 +/- 76.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | 0.621    |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=0.56 +/- 0.55
Episode length: 104.04 +/- 81.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.556    |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 0.496    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 147      |
|    time_elapsed    | 16258    |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=0.38 +/- 0.57
Episode length: 137.06 +/- 79.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 137          |
|    mean_reward          | 0.383        |
| time/                   |              |
|    total_timesteps      | 301500       |
| train/                  |              |
|    approx_kl            | 0.0013341008 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.174       |
|    explained_variance   | 0.819        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00844      |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00187     |
|    value_loss           | 0.0139       |
------------------------------------------
Eval num_timesteps=302000, episode_reward=0.47 +/- 0.58
Episode length: 114.24 +/- 85.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 0.466    |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=0.41 +/- 0.58
Episode length: 127.52 +/- 84.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 0.413    |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=0.52 +/- 0.55
Episode length: 115.92 +/- 79.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 0.524    |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 0.503    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 148      |
|    time_elapsed    | 16331    |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=0.54 +/- 0.56
Episode length: 104.48 +/- 83.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 0.536        |
| time/                   |              |
|    total_timesteps      | 303500       |
| train/                  |              |
|    approx_kl            | 0.0014617791 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.804        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0128       |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 0.0172       |
------------------------------------------
Eval num_timesteps=304000, episode_reward=0.65 +/- 0.51
Episode length: 94.06 +/- 75.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.1     |
|    mean_reward     | 0.646    |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=0.54 +/- 0.56
Episode length: 102.68 +/- 84.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 0.538    |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=0.58 +/- 0.54
Episode length: 97.72 +/- 85.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | 0.583    |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 18       |
|    iterations      | 149      |
|    time_elapsed    | 16388    |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=0.43 +/- 0.57
Episode length: 127.82 +/- 79.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 128          |
|    mean_reward          | 0.432        |
| time/                   |              |
|    total_timesteps      | 305500       |
| train/                  |              |
|    approx_kl            | 0.0013614735 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.184       |
|    explained_variance   | 0.77         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00365     |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00343     |
|    value_loss           | 0.0123       |
------------------------------------------
Eval num_timesteps=306000, episode_reward=0.62 +/- 0.52
Episode length: 100.18 +/- 76.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.62     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=0.55 +/- 0.55
Episode length: 111.12 +/- 77.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.549    |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=0.58 +/- 0.54
Episode length: 102.12 +/- 80.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 0.493    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 150      |
|    time_elapsed    | 16453    |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=0.51 +/- 0.56
Episode length: 112.40 +/- 84.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 112          |
|    mean_reward          | 0.508        |
| time/                   |              |
|    total_timesteps      | 307500       |
| train/                  |              |
|    approx_kl            | 0.0010281432 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.242       |
|    explained_variance   | 0.75         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0018       |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 0.0182       |
------------------------------------------
Eval num_timesteps=308000, episode_reward=0.41 +/- 0.58
Episode length: 125.72 +/- 82.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 0.415    |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=0.53 +/- 0.56
Episode length: 106.16 +/- 83.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.534    |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=0.55 +/- 0.55
Episode length: 106.06 +/- 79.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.554    |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 0.518    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 151      |
|    time_elapsed    | 16559    |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=0.53 +/- 0.55
Episode length: 114.46 +/- 80.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 114          |
|    mean_reward          | 0.526        |
| time/                   |              |
|    total_timesteps      | 309500       |
| train/                  |              |
|    approx_kl            | 0.0013244413 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.839        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0119       |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.00148     |
|    value_loss           | 0.0135       |
------------------------------------------
Eval num_timesteps=310000, episode_reward=0.50 +/- 0.56
Episode length: 117.14 +/- 79.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 0.503    |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=0.34 +/- 0.58
Episode length: 136.78 +/- 81.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 0.343    |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=0.55 +/- 0.55
Episode length: 105.44 +/- 81.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.555    |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 0.494    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 152      |
|    time_elapsed    | 16635    |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=0.61 +/- 0.52
Episode length: 106.08 +/- 75.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 106           |
|    mean_reward          | 0.614         |
| time/                   |               |
|    total_timesteps      | 311500        |
| train/                  |               |
|    approx_kl            | 0.00058898947 |
|    clip_fraction        | 0.00518       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.105        |
|    explained_variance   | 0.824         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0013        |
|    n_updates            | 1520          |
|    policy_gradient_loss | -0.00125      |
|    value_loss           | 0.00947       |
-------------------------------------------
Eval num_timesteps=312000, episode_reward=0.57 +/- 0.54
Episode length: 107.28 +/- 79.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.573    |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=0.48 +/- 0.56
Episode length: 122.82 +/- 79.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 0.477    |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=0.39 +/- 0.58
Episode length: 126.06 +/- 88.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 0.394    |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 0.497    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 153      |
|    time_elapsed    | 16701    |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=0.70 +/- 0.48
Episode length: 81.74 +/- 75.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 81.7          |
|    mean_reward          | 0.699         |
| time/                   |               |
|    total_timesteps      | 313500        |
| train/                  |               |
|    approx_kl            | 0.00092348375 |
|    clip_fraction        | 0.0124        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.109        |
|    explained_variance   | 0.806         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0112        |
|    n_updates            | 1530          |
|    policy_gradient_loss | -0.00238      |
|    value_loss           | 0.0133        |
-------------------------------------------
New best mean reward!
Eval num_timesteps=314000, episode_reward=0.55 +/- 0.55
Episode length: 105.38 +/- 79.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.555    |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=0.57 +/- 0.54
Episode length: 112.86 +/- 79.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.567    |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=0.53 +/- 0.55
Episode length: 112.76 +/- 80.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.528    |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 0.498    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 154      |
|    time_elapsed    | 16761    |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=0.56 +/- 0.55
Episode length: 100.68 +/- 81.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 0.56        |
| time/                   |             |
|    total_timesteps      | 315500      |
| train/                  |             |
|    approx_kl            | 0.002254053 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.242      |
|    explained_variance   | 0.734       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00564     |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.00118    |
|    value_loss           | 0.0222      |
-----------------------------------------
Eval num_timesteps=316000, episode_reward=0.74 +/- 0.45
Episode length: 82.98 +/- 73.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83       |
|    mean_reward     | 0.737    |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
New best mean reward!
Eval num_timesteps=316500, episode_reward=0.32 +/- 0.58
Episode length: 135.94 +/- 83.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 136      |
|    mean_reward     | 0.324    |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=0.53 +/- 0.56
Episode length: 105.74 +/- 84.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.535    |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 0.486    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 155      |
|    time_elapsed    | 16822    |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=0.58 +/- 0.54
Episode length: 105.10 +/- 81.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | 0.575        |
| time/                   |              |
|    total_timesteps      | 317500       |
| train/                  |              |
|    approx_kl            | 0.0011800722 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 0.81         |
|    learning_rate        | 1e-05        |
|    loss                 | 2.11e-05     |
|    n_updates            | 1550         |
|    policy_gradient_loss | -0.00212     |
|    value_loss           | 0.0141       |
------------------------------------------
Eval num_timesteps=318000, episode_reward=0.50 +/- 0.56
Episode length: 118.72 +/- 79.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 0.502    |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=0.58 +/- 0.54
Episode length: 104.66 +/- 78.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.576    |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=0.53 +/- 0.56
Episode length: 111.24 +/- 80.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.529    |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 0.473    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 156      |
|    time_elapsed    | 16885    |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=0.62 +/- 0.52
Episode length: 101.02 +/- 78.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | 0.619        |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 0.0016424827 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.765        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00478      |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 0.0129       |
------------------------------------------
Eval num_timesteps=320000, episode_reward=0.60 +/- 0.53
Episode length: 100.90 +/- 81.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.599    |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=0.62 +/- 0.52
Episode length: 105.04 +/- 75.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.615    |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=0.60 +/- 0.53
Episode length: 104.52 +/- 75.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.596    |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=0.51 +/- 0.56
Episode length: 110.54 +/- 84.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.51     |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 0.443    |
| time/              |          |
|    fps             | 18       |
|    iterations      | 157      |
|    time_elapsed    | 16959    |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=0.49 +/- 0.57
Episode length: 113.48 +/- 85.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 113          |
|    mean_reward          | 0.487        |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0014946214 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.26        |
|    explained_variance   | 0.687        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00536      |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 0.0184       |
------------------------------------------
Eval num_timesteps=322500, episode_reward=0.53 +/- 0.56
Episode length: 108.46 +/- 82.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.532    |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=0.64 +/- 0.51
Episode length: 99.24 +/- 75.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 0.641    |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=0.64 +/- 0.51
Episode length: 99.58 +/- 76.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 0.641    |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 0.499    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 158      |
|    time_elapsed    | 17020    |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=0.60 +/- 0.53
Episode length: 103.38 +/- 80.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 103           |
|    mean_reward          | 0.597         |
| time/                   |               |
|    total_timesteps      | 324000        |
| train/                  |               |
|    approx_kl            | 0.00095324154 |
|    clip_fraction        | 0.0113        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.202        |
|    explained_variance   | 0.808         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00488       |
|    n_updates            | 1580          |
|    policy_gradient_loss | -0.00154      |
|    value_loss           | 0.0143        |
-------------------------------------------
Eval num_timesteps=324500, episode_reward=0.51 +/- 0.56
Episode length: 111.48 +/- 85.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.509    |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=0.57 +/- 0.54
Episode length: 107.84 +/- 82.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.572    |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=0.59 +/- 0.52
Episode length: 110.72 +/- 74.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.59     |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 0.52     |
| time/              |          |
|    fps             | 19       |
|    iterations      | 159      |
|    time_elapsed    | 17106    |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=0.59 +/- 0.53
Episode length: 108.46 +/- 76.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 108          |
|    mean_reward          | 0.592        |
| time/                   |              |
|    total_timesteps      | 326000       |
| train/                  |              |
|    approx_kl            | 0.0025227335 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.222       |
|    explained_variance   | 0.763        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00433      |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.00334     |
|    value_loss           | 0.0182       |
------------------------------------------
Eval num_timesteps=326500, episode_reward=0.62 +/- 0.52
Episode length: 96.72 +/- 77.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | 0.624    |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=0.64 +/- 0.51
Episode length: 98.10 +/- 80.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.1     |
|    mean_reward     | 0.642    |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=0.51 +/- 0.56
Episode length: 113.24 +/- 83.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.507    |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 0.484    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 160      |
|    time_elapsed    | 17187    |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=0.62 +/- 0.52
Episode length: 104.20 +/- 76.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 0.616        |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0012888351 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.198       |
|    explained_variance   | 0.723        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00425      |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 0.014        |
------------------------------------------
Eval num_timesteps=328500, episode_reward=0.56 +/- 0.55
Episode length: 102.30 +/- 84.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.558    |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=0.60 +/- 0.53
Episode length: 95.88 +/- 82.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.9     |
|    mean_reward     | 0.604    |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=0.42 +/- 0.58
Episode length: 119.60 +/- 85.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 0.421    |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 0.484    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 161      |
|    time_elapsed    | 17256    |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=0.55 +/- 0.55
Episode length: 112.08 +/- 81.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 112          |
|    mean_reward          | 0.548        |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0006990671 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 0.779        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00668      |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.0138       |
------------------------------------------
Eval num_timesteps=330500, episode_reward=0.46 +/- 0.57
Episode length: 120.32 +/- 80.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 0.46     |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=0.60 +/- 0.53
Episode length: 95.92 +/- 79.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.9     |
|    mean_reward     | 0.604    |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=0.55 +/- 0.55
Episode length: 109.08 +/- 78.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.551    |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 0.534    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 162      |
|    time_elapsed    | 17317    |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=0.53 +/- 0.56
Episode length: 105.42 +/- 82.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | 0.535        |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0010671578 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0.764        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0167       |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 0.018        |
------------------------------------------
Eval num_timesteps=332500, episode_reward=0.63 +/- 0.53
Episode length: 88.42 +/- 81.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.4     |
|    mean_reward     | 0.632    |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=0.70 +/- 0.48
Episode length: 82.42 +/- 75.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 0.698    |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=0.43 +/- 0.57
Episode length: 125.62 +/- 83.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 0.435    |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 0.513    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 163      |
|    time_elapsed    | 17374    |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=0.52 +/- 0.55
Episode length: 118.78 +/- 81.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 119           |
|    mean_reward          | 0.522         |
| time/                   |               |
|    total_timesteps      | 334000        |
| train/                  |               |
|    approx_kl            | 0.00086784613 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.177        |
|    explained_variance   | 0.826         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00359       |
|    n_updates            | 1630          |
|    policy_gradient_loss | -0.00236      |
|    value_loss           | 0.0132        |
-------------------------------------------
Eval num_timesteps=334500, episode_reward=0.60 +/- 0.53
Episode length: 102.92 +/- 79.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 0.597    |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=0.51 +/- 0.56
Episode length: 110.34 +/- 83.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 0.51     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=0.59 +/- 0.53
Episode length: 106.90 +/- 75.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.593    |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 0.479    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 164      |
|    time_elapsed    | 17435    |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=0.60 +/- 0.53
Episode length: 95.58 +/- 78.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.6         |
|    mean_reward          | 0.605        |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0011748646 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.807        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00658      |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 0.0112       |
------------------------------------------
Eval num_timesteps=336500, episode_reward=0.63 +/- 0.52
Episode length: 94.46 +/- 78.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.5     |
|    mean_reward     | 0.626    |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=0.46 +/- 0.57
Episode length: 118.24 +/- 83.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 0.462    |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=0.53 +/- 0.55
Episode length: 113.24 +/- 79.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.527    |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 0.476    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 165      |
|    time_elapsed    | 17497    |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=0.46 +/- 0.57
Episode length: 121.50 +/- 84.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 122          |
|    mean_reward          | 0.459        |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0011961216 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.29        |
|    explained_variance   | 0.703        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00543      |
|    n_updates            | 1650         |
|    policy_gradient_loss | -0.00217     |
|    value_loss           | 0.0182       |
------------------------------------------
Eval num_timesteps=338500, episode_reward=0.50 +/- 0.56
Episode length: 117.26 +/- 82.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 0.503    |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=0.62 +/- 0.52
Episode length: 99.84 +/- 80.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.8     |
|    mean_reward     | 0.62     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=0.48 +/- 0.57
Episode length: 116.36 +/- 81.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 0.484    |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 19       |
|    iterations      | 166      |
|    time_elapsed    | 17562    |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=0.46 +/- 0.57
Episode length: 120.40 +/- 84.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 120           |
|    mean_reward          | 0.46          |
| time/                   |               |
|    total_timesteps      | 340000        |
| train/                  |               |
|    approx_kl            | 0.00072123273 |
|    clip_fraction        | 0.00771       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.125        |
|    explained_variance   | 0.827         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00396      |
|    n_updates            | 1660          |
|    policy_gradient_loss | -0.000795     |
|    value_loss           | 0.0117        |
-------------------------------------------
Eval num_timesteps=340500, episode_reward=0.57 +/- 0.54
Episode length: 106.32 +/- 80.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.574    |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=0.64 +/- 0.51
Episode length: 98.50 +/- 76.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | 0.642    |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=0.58 +/- 0.54
Episode length: 96.64 +/- 80.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 0.584    |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=0.67 +/- 0.49
Episode length: 92.06 +/- 75.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.1     |
|    mean_reward     | 0.668    |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 0.526    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 167      |
|    time_elapsed    | 17637    |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=0.48 +/- 0.57
Episode length: 115.94 +/- 84.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 116          |
|    mean_reward          | 0.484        |
| time/                   |              |
|    total_timesteps      | 342500       |
| train/                  |              |
|    approx_kl            | 0.0013396393 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.157       |
|    explained_variance   | 0.835        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00406     |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 0.0119       |
------------------------------------------
Eval num_timesteps=343000, episode_reward=0.48 +/- 0.56
Episode length: 120.28 +/- 80.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 0.48     |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=0.66 +/- 0.49
Episode length: 105.14 +/- 74.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.655    |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=0.44 +/- 0.58
Episode length: 119.44 +/- 87.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 0.441    |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 0.504    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 168      |
|    time_elapsed    | 17704    |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=0.75 +/- 0.45
Episode length: 73.02 +/- 72.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73           |
|    mean_reward          | 0.747        |
| time/                   |              |
|    total_timesteps      | 344500       |
| train/                  |              |
|    approx_kl            | 0.0014792869 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0822      |
|    explained_variance   | 0.865        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00362     |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.000572    |
|    value_loss           | 0.00801      |
------------------------------------------
New best mean reward!
Eval num_timesteps=345000, episode_reward=0.55 +/- 0.54
Episode length: 115.24 +/- 77.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 0.545    |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=0.34 +/- 0.58
Episode length: 137.12 +/- 83.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 0.343    |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=0.60 +/- 0.53
Episode length: 98.54 +/- 81.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 0.529    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 169      |
|    time_elapsed    | 17788    |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=0.72 +/- 0.47
Episode length: 81.86 +/- 74.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 81.9          |
|    mean_reward          | 0.718         |
| time/                   |               |
|    total_timesteps      | 346500        |
| train/                  |               |
|    approx_kl            | 0.00073895964 |
|    clip_fraction        | 0.00776       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0979       |
|    explained_variance   | 0.839         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0032        |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.00245      |
|    value_loss           | 0.0108        |
-------------------------------------------
Eval num_timesteps=347000, episode_reward=0.59 +/- 0.53
Episode length: 105.50 +/- 75.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.595    |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=0.61 +/- 0.52
Episode length: 106.14 +/- 80.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.614    |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=0.64 +/- 0.51
Episode length: 99.32 +/- 76.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | 0.641    |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 0.517    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 170      |
|    time_elapsed    | 17847    |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=0.46 +/- 0.57
Episode length: 122.32 +/- 83.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 122          |
|    mean_reward          | 0.458        |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 0.0010016665 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.831        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00326      |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 0.0161       |
------------------------------------------
Eval num_timesteps=349000, episode_reward=0.69 +/- 0.48
Episode length: 94.22 +/- 72.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | 0.686    |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=0.58 +/- 0.54
Episode length: 100.20 +/- 82.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.58     |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=0.55 +/- 0.55
Episode length: 110.90 +/- 79.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.549    |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 0.485    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 171      |
|    time_elapsed    | 17909    |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=0.57 +/- 0.54
Episode length: 105.82 +/- 81.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 106          |
|    mean_reward          | 0.574        |
| time/                   |              |
|    total_timesteps      | 350500       |
| train/                  |              |
|    approx_kl            | 0.0013117507 |
|    clip_fraction        | 0.01         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.166       |
|    explained_variance   | 0.815        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0142       |
|    n_updates            | 1710         |
|    policy_gradient_loss | -0.00341     |
|    value_loss           | 0.014        |
------------------------------------------
Eval num_timesteps=351000, episode_reward=0.60 +/- 0.53
Episode length: 100.14 +/- 79.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=0.48 +/- 0.56
Episode length: 122.40 +/- 79.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.478    |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=0.74 +/- 0.45
Episode length: 79.32 +/- 72.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 0.741    |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 0.47     |
| time/              |          |
|    fps             | 19       |
|    iterations      | 172      |
|    time_elapsed    | 17982    |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=0.66 +/- 0.49
Episode length: 95.56 +/- 75.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.6         |
|    mean_reward          | 0.665        |
| time/                   |              |
|    total_timesteps      | 352500       |
| train/                  |              |
|    approx_kl            | 0.0008441628 |
|    clip_fraction        | 0.00889      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.221       |
|    explained_variance   | 0.803        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0131       |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00232     |
|    value_loss           | 0.0148       |
------------------------------------------
Eval num_timesteps=353000, episode_reward=0.63 +/- 0.52
Episode length: 92.38 +/- 82.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 0.628    |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=0.61 +/- 0.51
Episode length: 113.04 +/- 76.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.607    |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=0.62 +/- 0.52
Episode length: 96.46 +/- 77.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | 0.624    |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 0.516    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 173      |
|    time_elapsed    | 18041    |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=0.60 +/- 0.53
Episode length: 96.94 +/- 84.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.9         |
|    mean_reward          | 0.603        |
| time/                   |              |
|    total_timesteps      | 354500       |
| train/                  |              |
|    approx_kl            | 0.0015443498 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.211       |
|    explained_variance   | 0.77         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0133      |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.00308     |
|    value_loss           | 0.018        |
------------------------------------------
Eval num_timesteps=355000, episode_reward=0.65 +/- 0.51
Episode length: 94.10 +/- 74.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.1     |
|    mean_reward     | 0.646    |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=0.56 +/- 0.55
Episode length: 99.64 +/- 81.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 0.561    |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=0.55 +/- 0.54
Episode length: 114.84 +/- 81.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 0.545    |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 0.547    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 174      |
|    time_elapsed    | 18101    |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=0.61 +/- 0.52
Episode length: 105.92 +/- 75.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 106          |
|    mean_reward          | 0.614        |
| time/                   |              |
|    total_timesteps      | 356500       |
| train/                  |              |
|    approx_kl            | 0.0009590068 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.245       |
|    explained_variance   | 0.782        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00459     |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.000287    |
|    value_loss           | 0.0149       |
------------------------------------------
Eval num_timesteps=357000, episode_reward=0.60 +/- 0.53
Episode length: 98.00 +/- 81.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98       |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=0.62 +/- 0.52
Episode length: 100.58 +/- 77.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.62     |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=0.64 +/- 0.50
Episode length: 104.00 +/- 71.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.636    |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 0.536    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 175      |
|    time_elapsed    | 18162    |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=0.56 +/- 0.55
Episode length: 104.22 +/- 85.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 0.556        |
| time/                   |              |
|    total_timesteps      | 358500       |
| train/                  |              |
|    approx_kl            | 0.0014600607 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.204       |
|    explained_variance   | 0.782        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00747     |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00173     |
|    value_loss           | 0.0153       |
------------------------------------------
Eval num_timesteps=359000, episode_reward=0.66 +/- 0.49
Episode length: 100.64 +/- 77.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.66     |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=0.65 +/- 0.51
Episode length: 93.74 +/- 76.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.7     |
|    mean_reward     | 0.647    |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=0.62 +/- 0.52
Episode length: 99.54 +/- 77.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.5     |
|    mean_reward     | 0.621    |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 0.558    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 176      |
|    time_elapsed    | 18220    |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=0.67 +/- 0.49
Episode length: 94.86 +/- 81.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.9         |
|    mean_reward          | 0.665        |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 0.0014808152 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.768        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00679      |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.0026      |
|    value_loss           | 0.0159       |
------------------------------------------
Eval num_timesteps=361000, episode_reward=0.76 +/- 0.43
Episode length: 78.74 +/- 68.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | 0.762    |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
New best mean reward!
Eval num_timesteps=361500, episode_reward=0.67 +/- 0.50
Episode length: 91.46 +/- 75.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.5     |
|    mean_reward     | 0.669    |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=0.67 +/- 0.50
Episode length: 88.26 +/- 76.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.3     |
|    mean_reward     | 0.672    |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 0.579    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 177      |
|    time_elapsed    | 18294    |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=0.72 +/- 0.46
Episode length: 84.04 +/- 72.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84           |
|    mean_reward          | 0.716        |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0014408148 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.273       |
|    explained_variance   | 0.697        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00782      |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 0.0212       |
------------------------------------------
Eval num_timesteps=363000, episode_reward=0.64 +/- 0.50
Episode length: 102.42 +/- 73.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.638    |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=0.61 +/- 0.51
Episode length: 109.10 +/- 77.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.611    |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=0.66 +/- 0.49
Episode length: 98.98 +/- 77.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 0.661    |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=0.71 +/- 0.46
Episode length: 88.94 +/- 79.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.9     |
|    mean_reward     | 0.711    |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 0.582    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 178      |
|    time_elapsed    | 18373    |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=0.73 +/- 0.44
Episode length: 86.60 +/- 68.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 86.6         |
|    mean_reward          | 0.734        |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 0.0014901626 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.794        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00265     |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 0.0134       |
------------------------------------------
Eval num_timesteps=365500, episode_reward=0.59 +/- 0.53
Episode length: 109.28 +/- 76.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.591    |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=0.62 +/- 0.52
Episode length: 97.42 +/- 79.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.4     |
|    mean_reward     | 0.623    |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=0.69 +/- 0.48
Episode length: 87.82 +/- 82.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.8     |
|    mean_reward     | 0.692    |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 0.572    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 179      |
|    time_elapsed    | 18453    |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=0.57 +/- 0.54
Episode length: 111.50 +/- 76.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 112          |
|    mean_reward          | 0.569        |
| time/                   |              |
|    total_timesteps      | 367000       |
| train/                  |              |
|    approx_kl            | 0.0007953953 |
|    clip_fraction        | 0.00552      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.78         |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00501     |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.000698    |
|    value_loss           | 0.0163       |
------------------------------------------
Eval num_timesteps=367500, episode_reward=0.71 +/- 0.46
Episode length: 86.32 +/- 74.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 0.714    |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=0.58 +/- 0.54
Episode length: 103.74 +/- 80.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.577    |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=0.70 +/- 0.49
Episode length: 77.44 +/- 75.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | 0.703    |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 0.539    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 180      |
|    time_elapsed    | 18513    |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=0.74 +/- 0.44
Episode length: 84.98 +/- 66.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 85            |
|    mean_reward          | 0.735         |
| time/                   |               |
|    total_timesteps      | 369000        |
| train/                  |               |
|    approx_kl            | 0.00083562837 |
|    clip_fraction        | 0.0103        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.133        |
|    explained_variance   | 0.792         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00262       |
|    n_updates            | 1800          |
|    policy_gradient_loss | -0.00161      |
|    value_loss           | 0.0125        |
-------------------------------------------
Eval num_timesteps=369500, episode_reward=0.67 +/- 0.50
Episode length: 86.18 +/- 75.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.2     |
|    mean_reward     | 0.674    |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=0.66 +/- 0.52
Episode length: 80.58 +/- 80.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 0.66     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=0.59 +/- 0.53
Episode length: 109.34 +/- 74.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.591    |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 0.564    |
| time/              |          |
|    fps             | 19       |
|    iterations      | 181      |
|    time_elapsed    | 18569    |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=0.64 +/- 0.51
Episode length: 100.30 +/- 80.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 0.64         |
| time/                   |              |
|    total_timesteps      | 371000       |
| train/                  |              |
|    approx_kl            | 0.0019723896 |
|    clip_fraction        | 0.021        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.2         |
|    explained_variance   | 0.822        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00211     |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00242     |
|    value_loss           | 0.0174       |
------------------------------------------
Eval num_timesteps=371500, episode_reward=0.62 +/- 0.52
Episode length: 103.56 +/- 76.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.617    |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=0.53 +/- 0.56
Episode length: 106.10 +/- 82.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.534    |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=0.49 +/- 0.57
Episode length: 109.34 +/- 89.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.491    |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 0.577    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 182      |
|    time_elapsed    | 18631    |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=0.69 +/- 0.48
Episode length: 91.40 +/- 71.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 91.4         |
|    mean_reward          | 0.689        |
| time/                   |              |
|    total_timesteps      | 373000       |
| train/                  |              |
|    approx_kl            | 0.0017102745 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.27        |
|    explained_variance   | 0.756        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00204     |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 0.0169       |
------------------------------------------
Eval num_timesteps=373500, episode_reward=0.62 +/- 0.52
Episode length: 101.56 +/- 82.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.619    |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=0.57 +/- 0.54
Episode length: 107.64 +/- 81.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.573    |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=0.63 +/- 0.52
Episode length: 92.54 +/- 77.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 0.628    |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.597    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 183      |
|    time_elapsed    | 18690    |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=0.57 +/- 0.53
Episode length: 113.16 +/- 78.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 0.567       |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.001345272 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.807       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00878     |
|    n_updates            | 1830        |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 0.0167      |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=0.53 +/- 0.56
Episode length: 106.94 +/- 82.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.533    |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=0.64 +/- 0.51
Episode length: 95.56 +/- 77.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 0.645    |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=0.60 +/- 0.53
Episode length: 96.78 +/- 81.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 0.603    |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 0.615    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 184      |
|    time_elapsed    | 18753    |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=0.67 +/- 0.50
Episode length: 90.58 +/- 79.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.6         |
|    mean_reward          | 0.67         |
| time/                   |              |
|    total_timesteps      | 377000       |
| train/                  |              |
|    approx_kl            | 0.0015790983 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.349       |
|    explained_variance   | 0.643        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00724      |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.00222     |
|    value_loss           | 0.0219       |
------------------------------------------
Eval num_timesteps=377500, episode_reward=0.67 +/- 0.50
Episode length: 90.02 +/- 75.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90       |
|    mean_reward     | 0.67     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=0.43 +/- 0.57
Episode length: 125.74 +/- 81.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 0.434    |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=0.46 +/- 0.57
Episode length: 116.00 +/- 84.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 0.464    |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 0.602    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 185      |
|    time_elapsed    | 18818    |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=0.51 +/- 0.56
Episode length: 112.14 +/- 86.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 112          |
|    mean_reward          | 0.508        |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0011649582 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0.784        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.006        |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.00277     |
|    value_loss           | 0.00932      |
------------------------------------------
Eval num_timesteps=379500, episode_reward=0.56 +/- 0.55
Episode length: 98.60 +/- 83.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 0.562    |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=0.56 +/- 0.55
Episode length: 102.28 +/- 81.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.558    |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=0.65 +/- 0.51
Episode length: 94.34 +/- 78.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.3     |
|    mean_reward     | 0.646    |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 0.579    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 186      |
|    time_elapsed    | 18877    |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=0.54 +/- 0.56
Episode length: 103.00 +/- 83.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 103          |
|    mean_reward          | 0.537        |
| time/                   |              |
|    total_timesteps      | 381000       |
| train/                  |              |
|    approx_kl            | 0.0013298866 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.187       |
|    explained_variance   | 0.815        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00302      |
|    n_updates            | 1860         |
|    policy_gradient_loss | -0.00317     |
|    value_loss           | 0.015        |
------------------------------------------
Eval num_timesteps=381500, episode_reward=0.73 +/- 0.44
Episode length: 93.46 +/- 72.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.5     |
|    mean_reward     | 0.727    |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=0.52 +/- 0.55
Episode length: 120.28 +/- 78.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 0.52     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=0.58 +/- 0.54
Episode length: 100.12 +/- 80.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.58     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 0.615    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 187      |
|    time_elapsed    | 18940    |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=0.49 +/- 0.57
Episode length: 113.16 +/- 82.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 0.487       |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.001135234 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.834       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0124      |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00221    |
|    value_loss           | 0.0134      |
-----------------------------------------
Eval num_timesteps=383500, episode_reward=0.58 +/- 0.54
Episode length: 98.58 +/- 80.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 0.582    |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=0.52 +/- 0.57
Episode length: 99.56 +/- 88.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 0.521    |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=0.51 +/- 0.57
Episode length: 107.12 +/- 83.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.513    |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=0.59 +/- 0.55
Episode length: 90.66 +/- 84.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 0.59     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 0.574    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 188      |
|    time_elapsed    | 19021    |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=0.52 +/- 0.57
Episode length: 103.14 +/- 86.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 103          |
|    mean_reward          | 0.517        |
| time/                   |              |
|    total_timesteps      | 385500       |
| train/                  |              |
|    approx_kl            | 0.0017352088 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.114       |
|    explained_variance   | 0.801        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00587      |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00336     |
|    value_loss           | 0.014        |
------------------------------------------
Eval num_timesteps=386000, episode_reward=0.49 +/- 0.57
Episode length: 108.76 +/- 84.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.491    |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=0.40 +/- 0.58
Episode length: 122.84 +/- 85.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 0.397    |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=0.50 +/- 0.58
Episode length: 102.14 +/- 88.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.498    |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 0.555    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 189      |
|    time_elapsed    | 19085    |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=0.41 +/- 0.58
Episode length: 128.26 +/- 84.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 0.412       |
| time/                   |             |
|    total_timesteps      | 387500      |
| train/                  |             |
|    approx_kl            | 0.001498256 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.232      |
|    explained_variance   | 0.839       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00593     |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 0.0114      |
-----------------------------------------
Eval num_timesteps=388000, episode_reward=0.60 +/- 0.53
Episode length: 100.28 +/- 80.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=0.65 +/- 0.51
Episode length: 89.16 +/- 78.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.2     |
|    mean_reward     | 0.651    |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=0.63 +/- 0.52
Episode length: 91.82 +/- 78.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 0.629    |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.7     |
|    ep_rew_mean     | 0.591    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 190      |
|    time_elapsed    | 19147    |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=0.35 +/- 0.58
Episode length: 128.60 +/- 86.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 129          |
|    mean_reward          | 0.352        |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0016537937 |
|    clip_fraction        | 0.00928      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.191       |
|    explained_variance   | 0.816        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00117      |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 0.015        |
------------------------------------------
Eval num_timesteps=390000, episode_reward=0.40 +/- 0.58
Episode length: 123.34 +/- 86.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 0.397    |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=0.63 +/- 0.52
Episode length: 90.50 +/- 81.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.5     |
|    mean_reward     | 0.63     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=0.30 +/- 0.58
Episode length: 137.94 +/- 84.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 0.302    |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 0.585    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 191      |
|    time_elapsed    | 19218    |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=0.47 +/- 0.58
Episode length: 108.60 +/- 87.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 109          |
|    mean_reward          | 0.472        |
| time/                   |              |
|    total_timesteps      | 391500       |
| train/                  |              |
|    approx_kl            | 0.0013491921 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.237       |
|    explained_variance   | 0.726        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00815      |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.0014      |
|    value_loss           | 0.0149       |
------------------------------------------
Eval num_timesteps=392000, episode_reward=0.56 +/- 0.55
Episode length: 103.70 +/- 82.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.557    |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=0.51 +/- 0.56
Episode length: 113.36 +/- 82.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.507    |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=0.60 +/- 0.53
Episode length: 99.58 +/- 81.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 0.601    |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 0.548    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 192      |
|    time_elapsed    | 19279    |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=0.67 +/- 0.50
Episode length: 91.22 +/- 77.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 91.2       |
|    mean_reward          | 0.669      |
| time/                   |            |
|    total_timesteps      | 393500     |
| train/                  |            |
|    approx_kl            | 0.00204097 |
|    clip_fraction        | 0.0219     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.223     |
|    explained_variance   | 0.717      |
|    learning_rate        | 1e-05      |
|    loss                 | 0.00392    |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.00293   |
|    value_loss           | 0.0141     |
----------------------------------------
Eval num_timesteps=394000, episode_reward=0.68 +/- 0.50
Episode length: 81.32 +/- 77.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.3     |
|    mean_reward     | 0.679    |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=0.46 +/- 0.57
Episode length: 117.48 +/- 84.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 0.463    |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=0.74 +/- 0.45
Episode length: 75.44 +/- 70.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 0.745    |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 0.534    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 193      |
|    time_elapsed    | 19334    |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=0.72 +/- 0.46
Episode length: 83.72 +/- 71.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 83.7         |
|    mean_reward          | 0.717        |
| time/                   |              |
|    total_timesteps      | 395500       |
| train/                  |              |
|    approx_kl            | 0.0017289417 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.348       |
|    explained_variance   | 0.678        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00398      |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 0.0168       |
------------------------------------------
Eval num_timesteps=396000, episode_reward=0.59 +/- 0.52
Episode length: 111.78 +/- 76.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.589    |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=0.69 +/- 0.48
Episode length: 89.34 +/- 74.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.3     |
|    mean_reward     | 0.691    |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=0.42 +/- 0.58
Episode length: 122.44 +/- 85.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.418    |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 0.555    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 194      |
|    time_elapsed    | 19422    |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=0.62 +/- 0.52
Episode length: 98.82 +/- 76.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.8         |
|    mean_reward          | 0.622        |
| time/                   |              |
|    total_timesteps      | 397500       |
| train/                  |              |
|    approx_kl            | 0.0014524008 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.802        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00714      |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00287     |
|    value_loss           | 0.0147       |
------------------------------------------
Eval num_timesteps=398000, episode_reward=0.65 +/- 0.51
Episode length: 89.22 +/- 77.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.2     |
|    mean_reward     | 0.651    |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=0.50 +/- 0.56
Episode length: 119.22 +/- 81.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 0.501    |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=0.55 +/- 0.55
Episode length: 108.24 +/- 81.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.552    |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 0.544    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 195      |
|    time_elapsed    | 19484    |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=0.58 +/- 0.54
Episode length: 99.60 +/- 80.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.6         |
|    mean_reward          | 0.581        |
| time/                   |              |
|    total_timesteps      | 399500       |
| train/                  |              |
|    approx_kl            | 0.0012682509 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0.737        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0069       |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.000837    |
|    value_loss           | 0.0178       |
------------------------------------------
Eval num_timesteps=400000, episode_reward=0.63 +/- 0.52
Episode length: 94.04 +/- 79.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | 0.626    |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=0.60 +/- 0.53
Episode length: 97.24 +/- 79.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 0.603    |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=0.69 +/- 0.48
Episode length: 88.68 +/- 77.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.7     |
|    mean_reward     | 0.692    |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 0.581    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 196      |
|    time_elapsed    | 19569    |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=0.60 +/- 0.53
Episode length: 97.78 +/- 79.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.8         |
|    mean_reward          | 0.603        |
| time/                   |              |
|    total_timesteps      | 401500       |
| train/                  |              |
|    approx_kl            | 0.0019199172 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0.765        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00503      |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 0.0168       |
------------------------------------------
Eval num_timesteps=402000, episode_reward=0.69 +/- 0.48
Episode length: 94.10 +/- 73.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.1     |
|    mean_reward     | 0.686    |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=0.60 +/- 0.53
Episode length: 102.14 +/- 80.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.598    |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=0.72 +/- 0.47
Episode length: 80.30 +/- 70.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.3     |
|    mean_reward     | 0.72     |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 0.604    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 197      |
|    time_elapsed    | 19625    |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=0.69 +/- 0.48
Episode length: 85.92 +/- 73.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 85.9         |
|    mean_reward          | 0.694        |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 0.0019744176 |
|    clip_fraction        | 0.00869      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.791        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0127       |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00144     |
|    value_loss           | 0.015        |
------------------------------------------
Eval num_timesteps=404000, episode_reward=0.61 +/- 0.52
Episode length: 105.98 +/- 77.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.614    |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=0.64 +/- 0.51
Episode length: 98.04 +/- 78.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98       |
|    mean_reward     | 0.642    |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=0.72 +/- 0.47
Episode length: 77.10 +/- 75.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 0.723    |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=0.85 +/- 0.32
Episode length: 66.80 +/- 62.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.8     |
|    mean_reward     | 0.854    |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 0.604    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 198      |
|    time_elapsed    | 19691    |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=0.76 +/- 0.43
Episode length: 78.92 +/- 68.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.9         |
|    mean_reward          | 0.761        |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0021696328 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.179       |
|    explained_variance   | 0.751        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0119      |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.004       |
|    value_loss           | 0.0162       |
------------------------------------------
Eval num_timesteps=406500, episode_reward=0.71 +/- 0.46
Episode length: 92.32 +/- 75.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.3     |
|    mean_reward     | 0.708    |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=0.69 +/- 0.48
Episode length: 87.94 +/- 72.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 0.692    |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=0.62 +/- 0.52
Episode length: 102.14 +/- 79.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.618    |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 0.572    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 199      |
|    time_elapsed    | 19766    |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=0.67 +/- 0.50
Episode length: 87.48 +/- 77.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 87.5          |
|    mean_reward          | 0.673         |
| time/                   |               |
|    total_timesteps      | 408000        |
| train/                  |               |
|    approx_kl            | 0.00048171502 |
|    clip_fraction        | 0.0062        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0898       |
|    explained_variance   | 0.842         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00576       |
|    n_updates            | 1990          |
|    policy_gradient_loss | -0.000698     |
|    value_loss           | 0.00974       |
-------------------------------------------
Eval num_timesteps=408500, episode_reward=0.69 +/- 0.48
Episode length: 90.16 +/- 73.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=0.71 +/- 0.46
Episode length: 93.70 +/- 68.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.7     |
|    mean_reward     | 0.707    |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=0.69 +/- 0.48
Episode length: 91.00 +/- 77.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91       |
|    mean_reward     | 0.689    |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 0.595    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 200      |
|    time_elapsed    | 19833    |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=0.71 +/- 0.46
Episode length: 90.38 +/- 73.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.4         |
|    mean_reward          | 0.71         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0015910873 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.171       |
|    explained_variance   | 0.832        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00524      |
|    n_updates            | 2000         |
|    policy_gradient_loss | -0.00147     |
|    value_loss           | 0.0152       |
------------------------------------------
Eval num_timesteps=410500, episode_reward=0.68 +/- 0.48
Episode length: 99.94 +/- 75.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | 0.68     |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=0.64 +/- 0.51
Episode length: 98.06 +/- 75.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.1     |
|    mean_reward     | 0.642    |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=0.64 +/- 0.53
Episode length: 84.20 +/- 81.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 0.636    |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 0.568    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 201      |
|    time_elapsed    | 19910    |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=0.66 +/- 0.49
Episode length: 101.76 +/- 75.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 0.659       |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.002463672 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.74        |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0195      |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 0.019       |
-----------------------------------------
Eval num_timesteps=412500, episode_reward=0.76 +/- 0.42
Episode length: 84.12 +/- 67.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.1     |
|    mean_reward     | 0.756    |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=0.59 +/- 0.53
Episode length: 105.64 +/- 76.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.595    |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=0.65 +/- 0.51
Episode length: 89.82 +/- 76.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.8     |
|    mean_reward     | 0.651    |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 0.571    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 202      |
|    time_elapsed    | 19967    |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=0.51 +/- 0.57
Episode length: 109.06 +/- 84.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 109          |
|    mean_reward          | 0.511        |
| time/                   |              |
|    total_timesteps      | 414000       |
| train/                  |              |
|    approx_kl            | 0.0012573136 |
|    clip_fraction        | 0.00884      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.823        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00241      |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 0.013        |
------------------------------------------
Eval num_timesteps=414500, episode_reward=0.72 +/- 0.46
Episode length: 84.56 +/- 70.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | 0.716    |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=0.61 +/- 0.51
Episode length: 108.32 +/- 77.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.612    |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=0.71 +/- 0.46
Episode length: 93.34 +/- 72.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.3     |
|    mean_reward     | 0.707    |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 0.581    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 203      |
|    time_elapsed    | 20028    |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=0.67 +/- 0.49
Episode length: 93.42 +/- 78.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93.4         |
|    mean_reward          | 0.667        |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0014275049 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.779        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00813      |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00162     |
|    value_loss           | 0.0169       |
------------------------------------------
Eval num_timesteps=416500, episode_reward=0.67 +/- 0.50
Episode length: 90.56 +/- 77.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | 0.67     |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=0.61 +/- 0.51
Episode length: 111.90 +/- 73.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.608    |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=0.69 +/- 0.48
Episode length: 87.04 +/- 72.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87       |
|    mean_reward     | 0.693    |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 0.648    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 204      |
|    time_elapsed    | 20086    |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=0.63 +/- 0.53
Episode length: 87.72 +/- 79.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.7         |
|    mean_reward          | 0.633        |
| time/                   |              |
|    total_timesteps      | 418000       |
| train/                  |              |
|    approx_kl            | 0.0010660915 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.182       |
|    explained_variance   | 0.779        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00242      |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 0.0192       |
------------------------------------------
Eval num_timesteps=418500, episode_reward=0.60 +/- 0.53
Episode length: 98.18 +/- 80.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=0.64 +/- 0.50
Episode length: 103.02 +/- 74.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 0.637    |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=0.60 +/- 0.53
Episode length: 99.60 +/- 77.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 0.601    |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.6     |
|    ep_rew_mean     | 0.661    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 205      |
|    time_elapsed    | 20145    |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=0.69 +/- 0.48
Episode length: 89.46 +/- 76.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.5        |
|    mean_reward          | 0.691       |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.001078876 |
|    clip_fraction        | 0.0112      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.238      |
|    explained_variance   | 0.769       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0155      |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00174    |
|    value_loss           | 0.0162      |
-----------------------------------------
Eval num_timesteps=420500, episode_reward=0.65 +/- 0.51
Episode length: 91.56 +/- 78.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 0.649    |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=0.63 +/- 0.52
Episode length: 93.06 +/- 80.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 0.627    |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=0.72 +/- 0.47
Episode length: 78.30 +/- 72.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 0.722    |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.3     |
|    ep_rew_mean     | 0.72     |
| time/              |          |
|    fps             | 20       |
|    iterations      | 206      |
|    time_elapsed    | 20199    |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=0.48 +/- 0.57
Episode length: 116.84 +/- 83.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 117           |
|    mean_reward          | 0.483         |
| time/                   |               |
|    total_timesteps      | 422000        |
| train/                  |               |
|    approx_kl            | 0.00051678414 |
|    clip_fraction        | 0.00537       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.183        |
|    explained_variance   | 0.771         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00669       |
|    n_updates            | 2060          |
|    policy_gradient_loss | -0.000578     |
|    value_loss           | 0.0181        |
-------------------------------------------
Eval num_timesteps=422500, episode_reward=0.65 +/- 0.51
Episode length: 86.86 +/- 80.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.9     |
|    mean_reward     | 0.653    |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=0.74 +/- 0.45
Episode length: 79.40 +/- 69.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.4     |
|    mean_reward     | 0.741    |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=0.67 +/- 0.50
Episode length: 89.64 +/- 78.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.6     |
|    mean_reward     | 0.671    |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94       |
|    ep_rew_mean     | 0.686    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 207      |
|    time_elapsed    | 20258    |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=0.71 +/- 0.46
Episode length: 86.82 +/- 71.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 86.8         |
|    mean_reward          | 0.714        |
| time/                   |              |
|    total_timesteps      | 424000       |
| train/                  |              |
|    approx_kl            | 0.0069572143 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.788        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.012        |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.0019      |
|    value_loss           | 0.0133       |
------------------------------------------
Eval num_timesteps=424500, episode_reward=0.69 +/- 0.48
Episode length: 90.08 +/- 74.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.1     |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=0.69 +/- 0.48
Episode length: 93.84 +/- 74.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 0.687    |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=0.59 +/- 0.53
Episode length: 105.68 +/- 79.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.595    |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.4     |
|    ep_rew_mean     | 0.668    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 208      |
|    time_elapsed    | 20315    |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=0.67 +/- 0.50
Episode length: 88.52 +/- 76.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88.5         |
|    mean_reward          | 0.672        |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0028931233 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0.772        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00565      |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.00133     |
|    value_loss           | 0.0154       |
------------------------------------------
Eval num_timesteps=426500, episode_reward=0.58 +/- 0.54
Episode length: 101.74 +/- 82.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.579    |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=0.67 +/- 0.50
Episode length: 87.26 +/- 75.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.3     |
|    mean_reward     | 0.673    |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=0.53 +/- 0.56
Episode length: 110.42 +/- 82.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 0.53     |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=0.50 +/- 0.56
Episode length: 116.28 +/- 80.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 0.504    |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.6     |
|    ep_rew_mean     | 0.622    |
| time/              |          |
|    fps             | 20       |
|    iterations      | 209      |
|    time_elapsed    | 20386    |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=0.56 +/- 0.55
Episode length: 100.56 +/- 84.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | 0.56         |
| time/                   |              |
|    total_timesteps      | 428500       |
| train/                  |              |
|    approx_kl            | 0.0019833064 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.832        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00521      |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00136     |
|    value_loss           | 0.015        |
------------------------------------------
Eval num_timesteps=429000, episode_reward=0.53 +/- 0.56
Episode length: 110.94 +/- 84.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.529    |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=0.58 +/- 0.54
Episode length: 102.14 +/- 80.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=0.56 +/- 0.55
Episode length: 99.12 +/- 81.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 0.561    |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.3     |
|    ep_rew_mean     | 0.635    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 210      |
|    time_elapsed    | 20446    |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=0.65 +/- 0.51
Episode length: 93.00 +/- 76.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93           |
|    mean_reward          | 0.647        |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 0.0008269014 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.848        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0048      |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 0.0119       |
------------------------------------------
Eval num_timesteps=431000, episode_reward=0.60 +/- 0.53
Episode length: 98.24 +/- 78.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=0.62 +/- 0.52
Episode length: 96.56 +/- 81.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.6     |
|    mean_reward     | 0.624    |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=0.57 +/- 0.53
Episode length: 114.52 +/- 78.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 0.566    |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | 0.613    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 211      |
|    time_elapsed    | 20505    |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=0.55 +/- 0.55
Episode length: 106.20 +/- 79.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 0.554       |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.002019615 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.089      |
|    explained_variance   | 0.843       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0152      |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 0.0147      |
-----------------------------------------
Eval num_timesteps=433000, episode_reward=0.56 +/- 0.55
Episode length: 102.02 +/- 85.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.558    |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=0.66 +/- 0.49
Episode length: 98.46 +/- 77.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | 0.662    |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=0.67 +/- 0.49
Episode length: 93.16 +/- 79.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 0.667    |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.3     |
|    ep_rew_mean     | 0.664    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 212      |
|    time_elapsed    | 20565    |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=0.44 +/- 0.57
Episode length: 125.28 +/- 83.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 0.435       |
| time/                   |             |
|    total_timesteps      | 434500      |
| train/                  |             |
|    approx_kl            | 0.001404002 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.834       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0034      |
|    n_updates            | 2120        |
|    policy_gradient_loss | -0.00334    |
|    value_loss           | 0.0176      |
-----------------------------------------
Eval num_timesteps=435000, episode_reward=0.41 +/- 0.57
Episode length: 134.84 +/- 76.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 0.405    |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=0.55 +/- 0.55
Episode length: 107.82 +/- 80.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.552    |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=0.49 +/- 0.57
Episode length: 113.84 +/- 83.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 0.486    |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.9     |
|    ep_rew_mean     | 0.615    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 213      |
|    time_elapsed    | 20634    |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=0.51 +/- 0.57
Episode length: 108.42 +/- 85.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 108          |
|    mean_reward          | 0.512        |
| time/                   |              |
|    total_timesteps      | 436500       |
| train/                  |              |
|    approx_kl            | 0.0064079044 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.14        |
|    explained_variance   | 0.818        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00245     |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.000768    |
|    value_loss           | 0.012        |
------------------------------------------
Eval num_timesteps=437000, episode_reward=0.60 +/- 0.53
Episode length: 101.56 +/- 79.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.599    |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=0.53 +/- 0.56
Episode length: 105.88 +/- 81.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.534    |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=0.59 +/- 0.53
Episode length: 108.16 +/- 78.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.592    |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.4     |
|    ep_rew_mean     | 0.625    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 214      |
|    time_elapsed    | 20695    |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=0.67 +/- 0.50
Episode length: 89.72 +/- 74.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.7         |
|    mean_reward          | 0.671        |
| time/                   |              |
|    total_timesteps      | 438500       |
| train/                  |              |
|    approx_kl            | 0.0013968502 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.159       |
|    explained_variance   | 0.873        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00251     |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00231     |
|    value_loss           | 0.0109       |
------------------------------------------
Eval num_timesteps=439000, episode_reward=0.60 +/- 0.53
Episode length: 98.58 +/- 78.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=0.53 +/- 0.56
Episode length: 110.48 +/- 84.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 0.53     |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=0.58 +/- 0.54
Episode length: 102.52 +/- 82.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | 0.623    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 215      |
|    time_elapsed    | 20755    |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=0.74 +/- 0.44
Episode length: 83.74 +/- 69.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 83.7         |
|    mean_reward          | 0.737        |
| time/                   |              |
|    total_timesteps      | 440500       |
| train/                  |              |
|    approx_kl            | 0.0013809593 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 0.799        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0051       |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.000922    |
|    value_loss           | 0.02         |
------------------------------------------
Eval num_timesteps=441000, episode_reward=0.65 +/- 0.51
Episode length: 95.02 +/- 78.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 0.645    |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=0.48 +/- 0.57
Episode length: 118.94 +/- 82.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 0.481    |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=0.52 +/- 0.55
Episode length: 117.60 +/- 78.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 0.523    |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.6     |
|    ep_rew_mean     | 0.633    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 216      |
|    time_elapsed    | 20817    |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=0.56 +/- 0.55
Episode length: 105.26 +/- 84.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | 0.555        |
| time/                   |              |
|    total_timesteps      | 442500       |
| train/                  |              |
|    approx_kl            | 0.0011978067 |
|    clip_fraction        | 0.00835      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.817        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00346     |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 0.0147       |
------------------------------------------
Eval num_timesteps=443000, episode_reward=0.68 +/- 0.48
Episode length: 96.74 +/- 77.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.7     |
|    mean_reward     | 0.684    |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=0.59 +/- 0.52
Episode length: 113.60 +/- 79.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 0.587    |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=0.47 +/- 0.56
Episode length: 126.68 +/- 77.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 0.474    |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 0.561    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 217      |
|    time_elapsed    | 20883    |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=0.66 +/- 0.49
Episode length: 104.82 +/- 73.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | 0.656        |
| time/                   |              |
|    total_timesteps      | 444500       |
| train/                  |              |
|    approx_kl            | 0.0017466127 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.108       |
|    explained_variance   | 0.808        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00664     |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00196     |
|    value_loss           | 0.00928      |
------------------------------------------
Eval num_timesteps=445000, episode_reward=0.55 +/- 0.55
Episode length: 108.66 +/- 81.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.552    |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=0.71 +/- 0.46
Episode length: 88.70 +/- 71.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.7     |
|    mean_reward     | 0.712    |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=0.60 +/- 0.53
Episode length: 104.22 +/- 79.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.596    |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 0.583    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 218      |
|    time_elapsed    | 20952    |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=0.67 +/- 0.49
Episode length: 93.22 +/- 76.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93.2         |
|    mean_reward          | 0.667        |
| time/                   |              |
|    total_timesteps      | 446500       |
| train/                  |              |
|    approx_kl            | 0.0012645806 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | 0.837        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00156     |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.000766    |
|    value_loss           | 0.0136       |
------------------------------------------
Eval num_timesteps=447000, episode_reward=0.80 +/- 0.38
Episode length: 79.58 +/- 64.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.6     |
|    mean_reward     | 0.801    |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=0.69 +/- 0.48
Episode length: 90.02 +/- 73.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90       |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=0.55 +/- 0.55
Episode length: 106.86 +/- 81.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.553    |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=0.74 +/- 0.45
Episode length: 75.70 +/- 71.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | 0.745    |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 0.599    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 219      |
|    time_elapsed    | 21036    |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=0.61 +/- 0.54
Episode length: 90.12 +/- 82.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.1         |
|    mean_reward          | 0.61         |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0010076052 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0971      |
|    explained_variance   | 0.843        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00122      |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 0.0129       |
------------------------------------------
Eval num_timesteps=449500, episode_reward=0.62 +/- 0.52
Episode length: 98.62 +/- 78.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 0.622    |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=0.58 +/- 0.54
Episode length: 101.80 +/- 80.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=0.73 +/- 0.44
Episode length: 86.86 +/- 70.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.9     |
|    mean_reward     | 0.734    |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 0.587    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 220      |
|    time_elapsed    | 21093    |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=0.69 +/- 0.48
Episode length: 88.54 +/- 72.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88.5         |
|    mean_reward          | 0.692        |
| time/                   |              |
|    total_timesteps      | 451000       |
| train/                  |              |
|    approx_kl            | 0.0017859754 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.132       |
|    explained_variance   | 0.841        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00209     |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 0.0139       |
------------------------------------------
Eval num_timesteps=451500, episode_reward=0.69 +/- 0.48
Episode length: 89.96 +/- 75.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90       |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=0.50 +/- 0.56
Episode length: 117.52 +/- 77.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 0.503    |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=0.59 +/- 0.55
Episode length: 94.72 +/- 81.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.7     |
|    mean_reward     | 0.586    |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.596    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 221      |
|    time_elapsed    | 21150    |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=0.70 +/- 0.48
Episode length: 85.02 +/- 73.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 85           |
|    mean_reward          | 0.695        |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0013051541 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.824        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00454      |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00342     |
|    value_loss           | 0.0152       |
------------------------------------------
Eval num_timesteps=453500, episode_reward=0.64 +/- 0.51
Episode length: 100.70 +/- 80.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.64     |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=0.64 +/- 0.50
Episode length: 103.98 +/- 74.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.636    |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=0.63 +/- 0.52
Episode length: 94.86 +/- 80.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 0.626    |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 0.654    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 222      |
|    time_elapsed    | 21207    |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=0.65 +/- 0.51
Episode length: 87.42 +/- 76.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.4         |
|    mean_reward          | 0.653        |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0012469258 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0898      |
|    explained_variance   | 0.838        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00348     |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 0.0155       |
------------------------------------------
Eval num_timesteps=455500, episode_reward=0.70 +/- 0.48
Episode length: 85.24 +/- 75.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.2     |
|    mean_reward     | 0.695    |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=0.73 +/- 0.44
Episode length: 86.14 +/- 71.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.1     |
|    mean_reward     | 0.734    |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=0.67 +/- 0.50
Episode length: 90.72 +/- 75.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 0.67     |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94       |
|    ep_rew_mean     | 0.646    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 223      |
|    time_elapsed    | 21264    |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=0.69 +/- 0.48
Episode length: 85.78 +/- 75.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 85.8         |
|    mean_reward          | 0.695        |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0013732288 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0988      |
|    explained_variance   | 0.841        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.000584     |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.00227     |
|    value_loss           | 0.0135       |
------------------------------------------
Eval num_timesteps=457500, episode_reward=0.53 +/- 0.56
Episode length: 109.72 +/- 82.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 0.531    |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=0.55 +/- 0.55
Episode length: 107.46 +/- 81.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.553    |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=0.70 +/- 0.48
Episode length: 79.62 +/- 75.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.6     |
|    mean_reward     | 0.701    |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 0.668    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 224      |
|    time_elapsed    | 21322    |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=0.68 +/- 0.48
Episode length: 97.92 +/- 70.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.9         |
|    mean_reward          | 0.682        |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0025363439 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.835        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0063       |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00175     |
|    value_loss           | 0.0147       |
------------------------------------------
Eval num_timesteps=459500, episode_reward=0.57 +/- 0.54
Episode length: 108.10 +/- 74.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.572    |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=0.64 +/- 0.51
Episode length: 95.88 +/- 74.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.9     |
|    mean_reward     | 0.644    |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=0.69 +/- 0.48
Episode length: 91.40 +/- 73.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.4     |
|    mean_reward     | 0.689    |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94       |
|    ep_rew_mean     | 0.656    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 225      |
|    time_elapsed    | 21391    |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=0.74 +/- 0.45
Episode length: 75.66 +/- 73.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.7        |
|    mean_reward          | 0.745       |
| time/                   |             |
|    total_timesteps      | 461000      |
| train/                  |             |
|    approx_kl            | 0.001212615 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.806       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0119      |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.000876   |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=461500, episode_reward=0.73 +/- 0.44
Episode length: 94.22 +/- 66.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | 0.726    |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=0.70 +/- 0.48
Episode length: 84.60 +/- 75.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | 0.696    |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=0.67 +/- 0.50
Episode length: 92.62 +/- 78.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.6     |
|    mean_reward     | 0.668    |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90       |
|    ep_rew_mean     | 0.68     |
| time/              |          |
|    fps             | 21       |
|    iterations      | 226      |
|    time_elapsed    | 21461    |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=0.67 +/- 0.49
Episode length: 94.14 +/- 77.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.1         |
|    mean_reward          | 0.666        |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0014768988 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.178       |
|    explained_variance   | 0.808        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0122       |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00158     |
|    value_loss           | 0.0167       |
------------------------------------------
Eval num_timesteps=463500, episode_reward=0.59 +/- 0.52
Episode length: 110.74 +/- 74.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 0.59     |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=0.67 +/- 0.50
Episode length: 90.68 +/- 78.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 0.67     |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=0.74 +/- 0.44
Episode length: 84.54 +/- 69.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 0.736    |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.2     |
|    ep_rew_mean     | 0.69     |
| time/              |          |
|    fps             | 21       |
|    iterations      | 227      |
|    time_elapsed    | 21520    |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=0.69 +/- 0.48
Episode length: 92.78 +/- 73.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.8         |
|    mean_reward          | 0.688        |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0014283637 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | 0.826        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0164       |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 0.0151       |
------------------------------------------
Eval num_timesteps=465500, episode_reward=0.62 +/- 0.52
Episode length: 102.80 +/- 79.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 0.618    |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=0.67 +/- 0.50
Episode length: 88.86 +/- 78.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.9     |
|    mean_reward     | 0.671    |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=0.67 +/- 0.50
Episode length: 90.44 +/- 76.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.4     |
|    mean_reward     | 0.67     |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95       |
|    ep_rew_mean     | 0.675    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 228      |
|    time_elapsed    | 21578    |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=0.81 +/- 0.38
Episode length: 65.66 +/- 61.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 65.7         |
|    mean_reward          | 0.815        |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0007289152 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0973      |
|    explained_variance   | 0.872        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.005        |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.000873    |
|    value_loss           | 0.0113       |
------------------------------------------
Eval num_timesteps=467500, episode_reward=0.71 +/- 0.46
Episode length: 87.38 +/- 75.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.4     |
|    mean_reward     | 0.713    |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=0.53 +/- 0.55
Episode length: 114.04 +/- 77.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 0.526    |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=0.68 +/- 0.48
Episode length: 97.58 +/- 74.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 0.683    |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.636    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 229      |
|    time_elapsed    | 21632    |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=0.69 +/- 0.48
Episode length: 92.24 +/- 77.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.2         |
|    mean_reward          | 0.688        |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0015149013 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.814        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00344      |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00179     |
|    value_loss           | 0.0141       |
------------------------------------------
Eval num_timesteps=469500, episode_reward=0.60 +/- 0.53
Episode length: 99.24 +/- 79.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | 0.601    |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=0.76 +/- 0.42
Episode length: 84.78 +/- 67.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.8     |
|    mean_reward     | 0.756    |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=0.67 +/- 0.49
Episode length: 93.80 +/- 75.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 0.667    |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=0.65 +/- 0.51
Episode length: 94.62 +/- 74.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.6     |
|    mean_reward     | 0.646    |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 0.654    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 230      |
|    time_elapsed    | 21702    |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=0.60 +/- 0.53
Episode length: 100.24 +/- 81.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.001877935 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.8         |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00355     |
|    n_updates            | 2300        |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.0153      |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=0.52 +/- 0.57
Episode length: 103.86 +/- 85.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.516    |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=0.62 +/- 0.52
Episode length: 98.14 +/- 79.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.1     |
|    mean_reward     | 0.622    |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=0.70 +/- 0.48
Episode length: 82.28 +/- 73.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 0.698    |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.8     |
|    ep_rew_mean     | 0.674    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 231      |
|    time_elapsed    | 21761    |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=0.72 +/- 0.46
Episode length: 84.64 +/- 72.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.6        |
|    mean_reward          | 0.716       |
| time/                   |             |
|    total_timesteps      | 473500      |
| train/                  |             |
|    approx_kl            | 0.001780443 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.787       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00623     |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00256    |
|    value_loss           | 0.0167      |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=0.84 +/- 0.35
Episode length: 65.16 +/- 58.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.2     |
|    mean_reward     | 0.835    |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=0.76 +/- 0.43
Episode length: 79.66 +/- 68.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 0.761    |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=0.58 +/- 0.54
Episode length: 100.68 +/- 81.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.58     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | 0.673    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 232      |
|    time_elapsed    | 21816    |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=0.60 +/- 0.53
Episode length: 101.32 +/- 80.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | 0.599        |
| time/                   |              |
|    total_timesteps      | 475500       |
| train/                  |              |
|    approx_kl            | 0.0015814248 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.698        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00124     |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00206     |
|    value_loss           | 0.0207       |
------------------------------------------
Eval num_timesteps=476000, episode_reward=0.57 +/- 0.54
Episode length: 106.76 +/- 79.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.574    |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=0.70 +/- 0.46
Episode length: 100.72 +/- 73.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=0.69 +/- 0.48
Episode length: 93.44 +/- 73.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | 0.687    |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.8     |
|    ep_rew_mean     | 0.72     |
| time/              |          |
|    fps             | 21       |
|    iterations      | 233      |
|    time_elapsed    | 21874    |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=0.68 +/- 0.48
Episode length: 95.36 +/- 71.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.4         |
|    mean_reward          | 0.685        |
| time/                   |              |
|    total_timesteps      | 477500       |
| train/                  |              |
|    approx_kl            | 0.0015741626 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0935      |
|    explained_variance   | 0.831        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0332       |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.00214     |
|    value_loss           | 0.0119       |
------------------------------------------
Eval num_timesteps=478000, episode_reward=0.66 +/- 0.49
Episode length: 95.86 +/- 73.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.9     |
|    mean_reward     | 0.664    |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=0.78 +/- 0.40
Episode length: 84.48 +/- 66.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.5     |
|    mean_reward     | 0.776    |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=0.67 +/- 0.50
Episode length: 89.24 +/- 74.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.2     |
|    mean_reward     | 0.671    |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89       |
|    ep_rew_mean     | 0.731    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 234      |
|    time_elapsed    | 21928    |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=0.74 +/- 0.45
Episode length: 80.94 +/- 73.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 80.9         |
|    mean_reward          | 0.739        |
| time/                   |              |
|    total_timesteps      | 479500       |
| train/                  |              |
|    approx_kl            | 0.0015511189 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.148       |
|    explained_variance   | 0.79         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00179      |
|    n_updates            | 2340         |
|    policy_gradient_loss | -0.00135     |
|    value_loss           | 0.016        |
------------------------------------------
Eval num_timesteps=480000, episode_reward=0.69 +/- 0.48
Episode length: 89.78 +/- 76.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.8     |
|    mean_reward     | 0.691    |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=0.59 +/- 0.53
Episode length: 110.34 +/- 81.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 0.59     |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=0.70 +/- 0.48
Episode length: 83.26 +/- 74.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.3     |
|    mean_reward     | 0.697    |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | 0.714    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 235      |
|    time_elapsed    | 21983    |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=0.61 +/- 0.51
Episode length: 110.24 +/- 74.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 0.61        |
| time/                   |             |
|    total_timesteps      | 481500      |
| train/                  |             |
|    approx_kl            | 0.005135298 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.713       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.0023      |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00474    |
|    value_loss           | 0.015       |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=0.71 +/- 0.46
Episode length: 92.40 +/- 71.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.4     |
|    mean_reward     | 0.708    |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=0.84 +/- 0.35
Episode length: 61.62 +/- 57.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.6     |
|    mean_reward     | 0.839    |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=0.60 +/- 0.53
Episode length: 99.08 +/- 75.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.1     |
|    mean_reward     | 0.601    |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 0.668    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 236      |
|    time_elapsed    | 22041    |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=0.60 +/- 0.53
Episode length: 96.02 +/- 82.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | 0.604        |
| time/                   |              |
|    total_timesteps      | 483500       |
| train/                  |              |
|    approx_kl            | 0.0024612013 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0541      |
|    explained_variance   | 0.855        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000151    |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.00311     |
|    value_loss           | 0.00885      |
------------------------------------------
Eval num_timesteps=484000, episode_reward=0.60 +/- 0.53
Episode length: 98.56 +/- 81.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=0.69 +/- 0.48
Episode length: 90.92 +/- 72.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 0.689    |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=0.68 +/- 0.50
Episode length: 83.90 +/- 77.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.9     |
|    mean_reward     | 0.676    |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 0.644    |
| time/              |          |
|    fps             | 21       |
|    iterations      | 237      |
|    time_elapsed    | 22097    |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=0.71 +/- 0.46
Episode length: 94.64 +/- 69.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.6         |
|    mean_reward          | 0.706        |
| time/                   |              |
|    total_timesteps      | 485500       |
| train/                  |              |
|    approx_kl            | 0.0028849503 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.838        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0104      |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00212     |
|    value_loss           | 0.0115       |
------------------------------------------
Eval num_timesteps=486000, episode_reward=0.73 +/- 0.44
Episode length: 89.38 +/- 71.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.4     |
|    mean_reward     | 0.731    |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=0.65 +/- 0.51
Episode length: 93.58 +/- 75.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 0.647    |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=0.76 +/- 0.43
Episode length: 80.04 +/- 68.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80       |
|    mean_reward     | 0.76     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 0.658    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 238      |
|    time_elapsed    | 22150    |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=0.60 +/- 0.53
Episode length: 105.06 +/- 74.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 0.595       |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.003878979 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.791       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00235    |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00248    |
|    value_loss           | 0.0169      |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=0.78 +/- 0.40
Episode length: 77.94 +/- 67.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 0.783    |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=0.59 +/- 0.52
Episode length: 115.28 +/- 73.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 0.585    |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=0.69 +/- 0.48
Episode length: 86.34 +/- 79.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 0.694    |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 0.621    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 239      |
|    time_elapsed    | 22212    |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=0.72 +/- 0.47
Episode length: 78.12 +/- 72.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.1         |
|    mean_reward          | 0.722        |
| time/                   |              |
|    total_timesteps      | 489500       |
| train/                  |              |
|    approx_kl            | 0.0024064681 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0984      |
|    explained_variance   | 0.838        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00053     |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.0029      |
|    value_loss           | 0.012        |
------------------------------------------
Eval num_timesteps=490000, episode_reward=0.69 +/- 0.48
Episode length: 92.20 +/- 73.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.2     |
|    mean_reward     | 0.688    |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=0.64 +/- 0.51
Episode length: 98.26 +/- 77.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 0.642    |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=0.63 +/- 0.50
Episode length: 105.70 +/- 72.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.635    |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=0.56 +/- 0.55
Episode length: 101.50 +/- 82.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.559    |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 0.612    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 240      |
|    time_elapsed    | 22319    |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=0.69 +/- 0.48
Episode length: 89.40 +/- 77.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 89.4          |
|    mean_reward          | 0.691         |
| time/                   |               |
|    total_timesteps      | 492000        |
| train/                  |               |
|    approx_kl            | 0.00088645436 |
|    clip_fraction        | 0.0121        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.112        |
|    explained_variance   | 0.731         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00499       |
|    n_updates            | 2400          |
|    policy_gradient_loss | -0.000173     |
|    value_loss           | 0.0172        |
-------------------------------------------
Eval num_timesteps=492500, episode_reward=0.55 +/- 0.55
Episode length: 106.20 +/- 82.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.554    |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=0.77 +/- 0.43
Episode length: 73.44 +/- 67.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.4     |
|    mean_reward     | 0.767    |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=0.71 +/- 0.46
Episode length: 89.50 +/- 69.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | 0.711    |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 0.67     |
| time/              |          |
|    fps             | 22       |
|    iterations      | 241      |
|    time_elapsed    | 22392    |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=0.75 +/- 0.42
Episode length: 89.70 +/- 68.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.7         |
|    mean_reward          | 0.751        |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0012529388 |
|    clip_fraction        | 0.00967      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.768        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00057     |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.00155     |
|    value_loss           | 0.0167       |
------------------------------------------
Eval num_timesteps=494500, episode_reward=0.74 +/- 0.45
Episode length: 84.16 +/- 71.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.2     |
|    mean_reward     | 0.736    |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=0.78 +/- 0.40
Episode length: 79.32 +/- 68.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 0.781    |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=0.76 +/- 0.43
Episode length: 77.70 +/- 69.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 0.763    |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.2     |
|    ep_rew_mean     | 0.705    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 242      |
|    time_elapsed    | 22444    |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=0.60 +/- 0.53
Episode length: 97.34 +/- 81.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.3       |
|    mean_reward          | 0.603      |
| time/                   |            |
|    total_timesteps      | 496000     |
| train/                  |            |
|    approx_kl            | 0.01268447 |
|    clip_fraction        | 0.0775     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.748      |
|    learning_rate        | 1e-05      |
|    loss                 | -0.0128    |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.00602   |
|    value_loss           | 0.0187     |
----------------------------------------
Eval num_timesteps=496500, episode_reward=0.76 +/- 0.43
Episode length: 77.26 +/- 69.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.3     |
|    mean_reward     | 0.763    |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=0.69 +/- 0.48
Episode length: 87.58 +/- 73.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.6     |
|    mean_reward     | 0.693    |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=0.73 +/- 0.44
Episode length: 89.36 +/- 69.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.4     |
|    mean_reward     | 0.731    |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.7     |
|    ep_rew_mean     | 0.684    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 243      |
|    time_elapsed    | 22505    |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=0.79 +/- 0.40
Episode length: 75.26 +/- 65.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.3        |
|    mean_reward          | 0.785       |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.001010865 |
|    clip_fraction        | 0.00977     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.855       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00383     |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.000858   |
|    value_loss           | 0.0132      |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=0.69 +/- 0.48
Episode length: 87.86 +/- 71.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.9     |
|    mean_reward     | 0.692    |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=0.80 +/- 0.38
Episode length: 76.52 +/- 67.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.5     |
|    mean_reward     | 0.804    |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=0.60 +/- 0.53
Episode length: 98.32 +/- 77.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.3     |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.2     |
|    ep_rew_mean     | 0.745    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 244      |
|    time_elapsed    | 22556    |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=0.68 +/- 0.50
Episode length: 82.68 +/- 74.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 82.7         |
|    mean_reward          | 0.678        |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0023950944 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.799        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00432      |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00254     |
|    value_loss           | 0.0178       |
------------------------------------------
Eval num_timesteps=500500, episode_reward=0.64 +/- 0.50
Episode length: 101.74 +/- 73.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.639    |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=0.73 +/- 0.44
Episode length: 91.26 +/- 70.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.3     |
|    mean_reward     | 0.729    |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=0.74 +/- 0.45
Episode length: 83.32 +/- 72.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.3     |
|    mean_reward     | 0.737    |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.1     |
|    ep_rew_mean     | 0.736    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 245      |
|    time_elapsed    | 22615    |
|    total_timesteps | 501760   |
---------------------------------
Eval num_timesteps=502000, episode_reward=0.69 +/- 0.48
Episode length: 92.34 +/- 73.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.3         |
|    mean_reward          | 0.688        |
| time/                   |              |
|    total_timesteps      | 502000       |
| train/                  |              |
|    approx_kl            | 0.0015546996 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0751      |
|    explained_variance   | 0.85         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0014       |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.000461    |
|    value_loss           | 0.0111       |
------------------------------------------
Eval num_timesteps=502500, episode_reward=0.73 +/- 0.44
Episode length: 87.96 +/- 72.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88       |
|    mean_reward     | 0.732    |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=0.55 +/- 0.54
Episode length: 113.52 +/- 76.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 0.547    |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=0.61 +/- 0.51
Episode length: 107.24 +/- 73.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.613    |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.3     |
|    ep_rew_mean     | 0.712    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 246      |
|    time_elapsed    | 22676    |
|    total_timesteps | 503808   |
---------------------------------
Eval num_timesteps=504000, episode_reward=0.68 +/- 0.50
Episode length: 84.36 +/- 77.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.4         |
|    mean_reward          | 0.676        |
| time/                   |              |
|    total_timesteps      | 504000       |
| train/                  |              |
|    approx_kl            | 0.0027181872 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.783        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00199     |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 0.0162       |
------------------------------------------
Eval num_timesteps=504500, episode_reward=0.75 +/- 0.42
Episode length: 90.72 +/- 69.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.7     |
|    mean_reward     | 0.75     |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=0.66 +/- 0.49
Episode length: 96.80 +/- 72.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 0.664    |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=0.68 +/- 0.48
Episode length: 97.06 +/- 70.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 0.683    |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.2     |
|    ep_rew_mean     | 0.739    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 247      |
|    time_elapsed    | 22734    |
|    total_timesteps | 505856   |
---------------------------------
Eval num_timesteps=506000, episode_reward=0.66 +/- 0.49
Episode length: 96.48 +/- 73.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.5         |
|    mean_reward          | 0.664        |
| time/                   |              |
|    total_timesteps      | 506000       |
| train/                  |              |
|    approx_kl            | 0.0013180206 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.833        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00518      |
|    n_updates            | 2470         |
|    policy_gradient_loss | -0.00121     |
|    value_loss           | 0.0156       |
------------------------------------------
Eval num_timesteps=506500, episode_reward=0.80 +/- 0.38
Episode length: 78.30 +/- 65.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 0.802    |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=0.69 +/- 0.48
Episode length: 91.96 +/- 71.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | 0.688    |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=0.74 +/- 0.45
Episode length: 79.66 +/- 71.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | 0.741    |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 0.727    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 248      |
|    time_elapsed    | 22789    |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=508000, episode_reward=0.69 +/- 0.48
Episode length: 92.52 +/- 69.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.5         |
|    mean_reward          | 0.688        |
| time/                   |              |
|    total_timesteps      | 508000       |
| train/                  |              |
|    approx_kl            | 0.0019118239 |
|    clip_fraction        | 0.00825      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0568      |
|    explained_variance   | 0.857        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00487      |
|    n_updates            | 2480         |
|    policy_gradient_loss | 0.000353     |
|    value_loss           | 0.0108       |
------------------------------------------
Eval num_timesteps=508500, episode_reward=0.66 +/- 0.49
Episode length: 96.24 +/- 78.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.2     |
|    mean_reward     | 0.664    |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=0.71 +/- 0.46
Episode length: 85.54 +/- 71.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.5     |
|    mean_reward     | 0.715    |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=0.76 +/- 0.43
Episode length: 76.98 +/- 67.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 0.763    |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.5     |
|    ep_rew_mean     | 0.681    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 249      |
|    time_elapsed    | 22845    |
|    total_timesteps | 509952   |
---------------------------------
Eval num_timesteps=510000, episode_reward=0.69 +/- 0.48
Episode length: 87.36 +/- 71.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 87.4          |
|    mean_reward          | 0.693         |
| time/                   |               |
|    total_timesteps      | 510000        |
| train/                  |               |
|    approx_kl            | 0.00061474973 |
|    clip_fraction        | 0.0104        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0727       |
|    explained_variance   | 0.868         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.0049        |
|    n_updates            | 2490          |
|    policy_gradient_loss | -0.000881     |
|    value_loss           | 0.01          |
-------------------------------------------
Eval num_timesteps=510500, episode_reward=0.58 +/- 0.54
Episode length: 104.12 +/- 81.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.576    |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=0.73 +/- 0.44
Episode length: 87.54 +/- 73.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.5     |
|    mean_reward     | 0.733    |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=0.72 +/- 0.46
Episode length: 83.78 +/- 73.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.8     |
|    mean_reward     | 0.717    |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=0.70 +/- 0.48
Episode length: 84.66 +/- 72.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.7     |
|    mean_reward     | 0.696    |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.9     |
|    ep_rew_mean     | 0.717    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 250      |
|    time_elapsed    | 22910    |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=512500, episode_reward=0.64 +/- 0.50
Episode length: 104.60 +/- 76.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | 0.636        |
| time/                   |              |
|    total_timesteps      | 512500       |
| train/                  |              |
|    approx_kl            | 0.0037376245 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0652      |
|    explained_variance   | 0.886        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00322      |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00259     |
|    value_loss           | 0.0119       |
------------------------------------------
Eval num_timesteps=513000, episode_reward=0.71 +/- 0.46
Episode length: 88.66 +/- 72.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.7     |
|    mean_reward     | 0.712    |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=0.81 +/- 0.38
Episode length: 72.02 +/- 64.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72       |
|    mean_reward     | 0.808    |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=0.62 +/- 0.52
Episode length: 99.00 +/- 75.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | 0.621    |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.2     |
|    ep_rew_mean     | 0.692    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 251      |
|    time_elapsed    | 22966    |
|    total_timesteps | 514048   |
---------------------------------
Eval num_timesteps=514500, episode_reward=0.70 +/- 0.48
Episode length: 84.26 +/- 71.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.3         |
|    mean_reward          | 0.696        |
| time/                   |              |
|    total_timesteps      | 514500       |
| train/                  |              |
|    approx_kl            | 0.0017280458 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0953      |
|    explained_variance   | 0.859        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.019        |
|    n_updates            | 2510         |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 0.0125       |
------------------------------------------
Eval num_timesteps=515000, episode_reward=0.81 +/- 0.38
Episode length: 69.70 +/- 60.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.7     |
|    mean_reward     | 0.811    |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=0.66 +/- 0.49
Episode length: 100.10 +/- 74.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.66     |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=0.66 +/- 0.49
Episode length: 97.64 +/- 71.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 0.663    |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.1     |
|    ep_rew_mean     | 0.699    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 252      |
|    time_elapsed    | 23019    |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=516500, episode_reward=0.67 +/- 0.49
Episode length: 93.20 +/- 72.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 93.2          |
|    mean_reward          | 0.667         |
| time/                   |               |
|    total_timesteps      | 516500        |
| train/                  |               |
|    approx_kl            | 0.00093330507 |
|    clip_fraction        | 0.0119        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0918       |
|    explained_variance   | 0.839         |
|    learning_rate        | 1e-05         |
|    loss                 | -0.00393      |
|    n_updates            | 2520          |
|    policy_gradient_loss | -0.00127      |
|    value_loss           | 0.014         |
-------------------------------------------
Eval num_timesteps=517000, episode_reward=0.65 +/- 0.51
Episode length: 93.80 +/- 77.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | 0.647    |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=0.67 +/- 0.49
Episode length: 94.26 +/- 72.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.3     |
|    mean_reward     | 0.666    |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=0.74 +/- 0.45
Episode length: 80.84 +/- 69.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.8     |
|    mean_reward     | 0.74     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.6     |
|    ep_rew_mean     | 0.72     |
| time/              |          |
|    fps             | 22       |
|    iterations      | 253      |
|    time_elapsed    | 23074    |
|    total_timesteps | 518144   |
---------------------------------
Eval num_timesteps=518500, episode_reward=0.71 +/- 0.46
Episode length: 89.36 +/- 69.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.4        |
|    mean_reward          | 0.711       |
| time/                   |             |
|    total_timesteps      | 518500      |
| train/                  |             |
|    approx_kl            | 0.001844355 |
|    clip_fraction        | 0.0199      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.861       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00772     |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00141    |
|    value_loss           | 0.0137      |
-----------------------------------------
Eval num_timesteps=519000, episode_reward=0.81 +/- 0.38
Episode length: 74.48 +/- 61.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 0.806    |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=0.81 +/- 0.38
Episode length: 71.78 +/- 63.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | 0.809    |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=0.55 +/- 0.55
Episode length: 105.86 +/- 81.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.554    |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 0.704    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 254      |
|    time_elapsed    | 23125    |
|    total_timesteps | 520192   |
---------------------------------
Eval num_timesteps=520500, episode_reward=0.70 +/- 0.48
Episode length: 83.72 +/- 73.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 83.7         |
|    mean_reward          | 0.697        |
| time/                   |              |
|    total_timesteps      | 520500       |
| train/                  |              |
|    approx_kl            | 0.0023787995 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.103       |
|    explained_variance   | 0.838        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0143      |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.00368     |
|    value_loss           | 0.0131       |
------------------------------------------
Eval num_timesteps=521000, episode_reward=0.67 +/- 0.49
Episode length: 94.02 +/- 77.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | 0.666    |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=0.68 +/- 0.48
Episode length: 95.78 +/- 75.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.8     |
|    mean_reward     | 0.685    |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=0.71 +/- 0.46
Episode length: 90.92 +/- 70.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 0.709    |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 0.667    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 255      |
|    time_elapsed    | 23180    |
|    total_timesteps | 522240   |
---------------------------------
Eval num_timesteps=522500, episode_reward=0.71 +/- 0.46
Episode length: 89.82 +/- 71.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.8         |
|    mean_reward          | 0.711        |
| time/                   |              |
|    total_timesteps      | 522500       |
| train/                  |              |
|    approx_kl            | 0.0013249076 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0945      |
|    explained_variance   | 0.826        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000929    |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00182     |
|    value_loss           | 0.0113       |
------------------------------------------
Eval num_timesteps=523000, episode_reward=0.72 +/- 0.44
Episode length: 95.46 +/- 69.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | 0.725    |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=0.57 +/- 0.53
Episode length: 115.14 +/- 78.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 0.565    |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=0.53 +/- 0.55
Episode length: 112.08 +/- 80.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.528    |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.646    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 256      |
|    time_elapsed    | 23242    |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=524500, episode_reward=0.55 +/- 0.55
Episode length: 105.86 +/- 81.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 106          |
|    mean_reward          | 0.554        |
| time/                   |              |
|    total_timesteps      | 524500       |
| train/                  |              |
|    approx_kl            | 0.0018835914 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0684      |
|    explained_variance   | 0.869        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00259      |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.00297     |
|    value_loss           | 0.0101       |
------------------------------------------
Eval num_timesteps=525000, episode_reward=0.52 +/- 0.55
Episode length: 117.68 +/- 76.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 0.523    |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=0.66 +/- 0.49
Episode length: 97.10 +/- 75.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 0.663    |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=0.43 +/- 0.57
Episode length: 129.04 +/- 81.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 0.431    |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.5     |
|    ep_rew_mean     | 0.651    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 257      |
|    time_elapsed    | 23307    |
|    total_timesteps | 526336   |
---------------------------------
Eval num_timesteps=526500, episode_reward=0.57 +/- 0.54
Episode length: 110.82 +/- 75.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 111          |
|    mean_reward          | 0.569        |
| time/                   |              |
|    total_timesteps      | 526500       |
| train/                  |              |
|    approx_kl            | 0.0020147795 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0972      |
|    explained_variance   | 0.772        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0106       |
|    n_updates            | 2570         |
|    policy_gradient_loss | -0.00252     |
|    value_loss           | 0.017        |
------------------------------------------
Eval num_timesteps=527000, episode_reward=0.63 +/- 0.52
Episode length: 95.04 +/- 77.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 0.625    |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=0.46 +/- 0.57
Episode length: 121.24 +/- 84.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 0.459    |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=0.67 +/- 0.49
Episode length: 95.12 +/- 76.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.1     |
|    mean_reward     | 0.665    |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 0.639    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 258      |
|    time_elapsed    | 23372    |
|    total_timesteps | 528384   |
---------------------------------
Eval num_timesteps=528500, episode_reward=0.55 +/- 0.55
Episode length: 110.80 +/- 81.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 0.55        |
| time/                   |             |
|    total_timesteps      | 528500      |
| train/                  |             |
|    approx_kl            | 0.002828347 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.831       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00726     |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.000522   |
|    value_loss           | 0.0121      |
-----------------------------------------
Eval num_timesteps=529000, episode_reward=0.53 +/- 0.55
Episode length: 113.32 +/- 78.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.527    |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=0.67 +/- 0.50
Episode length: 90.82 +/- 76.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.8     |
|    mean_reward     | 0.669    |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=0.53 +/- 0.56
Episode length: 108.94 +/- 82.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 0.531    |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.9     |
|    ep_rew_mean     | 0.675    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 259      |
|    time_elapsed    | 23434    |
|    total_timesteps | 530432   |
---------------------------------
Eval num_timesteps=530500, episode_reward=0.62 +/- 0.52
Episode length: 98.04 +/- 75.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98           |
|    mean_reward          | 0.622        |
| time/                   |              |
|    total_timesteps      | 530500       |
| train/                  |              |
|    approx_kl            | 0.0016921489 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.703        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00634      |
|    n_updates            | 2590         |
|    policy_gradient_loss | 0.000227     |
|    value_loss           | 0.017        |
------------------------------------------
Eval num_timesteps=531000, episode_reward=0.76 +/- 0.40
Episode length: 97.16 +/- 67.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 0.763    |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=0.65 +/- 0.51
Episode length: 91.76 +/- 73.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | 0.649    |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=0.57 +/- 0.54
Episode length: 107.36 +/- 81.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.573    |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.2     |
|    ep_rew_mean     | 0.652    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 260      |
|    time_elapsed    | 23513    |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=532500, episode_reward=0.61 +/- 0.51
Episode length: 110.32 +/- 77.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 110          |
|    mean_reward          | 0.61         |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0022680908 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.08        |
|    explained_variance   | 0.811        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00266      |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 0.0129       |
------------------------------------------
Eval num_timesteps=533000, episode_reward=0.62 +/- 0.52
Episode length: 95.60 +/- 79.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 0.625    |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=0.60 +/- 0.53
Episode length: 100.94 +/- 81.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.599    |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=0.66 +/- 0.49
Episode length: 104.02 +/- 73.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.656    |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=0.64 +/- 0.51
Episode length: 96.92 +/- 78.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | 0.643    |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | 0.672    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 261      |
|    time_elapsed    | 23588    |
|    total_timesteps | 534528   |
---------------------------------
Eval num_timesteps=535000, episode_reward=0.64 +/- 0.51
Episode length: 96.64 +/- 75.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.6         |
|    mean_reward          | 0.644        |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0020684297 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.13        |
|    explained_variance   | 0.795        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000696    |
|    n_updates            | 2610         |
|    policy_gradient_loss | -0.000159    |
|    value_loss           | 0.0146       |
------------------------------------------
Eval num_timesteps=535500, episode_reward=0.64 +/- 0.51
Episode length: 97.76 +/- 76.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | 0.643    |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=0.67 +/- 0.49
Episode length: 94.16 +/- 74.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | 0.666    |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=0.71 +/- 0.46
Episode length: 94.92 +/- 74.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | 0.705    |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 0.648    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 262      |
|    time_elapsed    | 23650    |
|    total_timesteps | 536576   |
---------------------------------
Eval num_timesteps=537000, episode_reward=0.73 +/- 0.44
Episode length: 95.34 +/- 68.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.3         |
|    mean_reward          | 0.725        |
| time/                   |              |
|    total_timesteps      | 537000       |
| train/                  |              |
|    approx_kl            | 0.0022519128 |
|    clip_fraction        | 0.0149       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.808        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0143       |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.000862    |
|    value_loss           | 0.0129       |
------------------------------------------
Eval num_timesteps=537500, episode_reward=0.59 +/- 0.53
Episode length: 108.12 +/- 77.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.592    |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=0.64 +/- 0.51
Episode length: 97.22 +/- 79.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | 0.643    |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=0.72 +/- 0.47
Episode length: 82.88 +/- 73.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.9     |
|    mean_reward     | 0.717    |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.657    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 263      |
|    time_elapsed    | 23711    |
|    total_timesteps | 538624   |
---------------------------------
Eval num_timesteps=539000, episode_reward=0.72 +/- 0.46
Episode length: 84.46 +/- 71.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.5         |
|    mean_reward          | 0.716        |
| time/                   |              |
|    total_timesteps      | 539000       |
| train/                  |              |
|    approx_kl            | 0.0014999942 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.116       |
|    explained_variance   | 0.819        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0333       |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.000971    |
|    value_loss           | 0.0141       |
------------------------------------------
Eval num_timesteps=539500, episode_reward=0.52 +/- 0.55
Episode length: 121.74 +/- 78.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.519    |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=0.55 +/- 0.55
Episode length: 112.48 +/- 79.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.548    |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=0.64 +/- 0.51
Episode length: 97.92 +/- 74.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.9     |
|    mean_reward     | 0.642    |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.646    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 264      |
|    time_elapsed    | 23772    |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=541000, episode_reward=0.63 +/- 0.50
Episode length: 112.64 +/- 73.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 113          |
|    mean_reward          | 0.628        |
| time/                   |              |
|    total_timesteps      | 541000       |
| train/                  |              |
|    approx_kl            | 0.0014981123 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0815      |
|    explained_variance   | 0.866        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00525      |
|    n_updates            | 2640         |
|    policy_gradient_loss | -0.00167     |
|    value_loss           | 0.00843      |
------------------------------------------
Eval num_timesteps=541500, episode_reward=0.71 +/- 0.46
Episode length: 90.36 +/- 73.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.4     |
|    mean_reward     | 0.71     |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=0.63 +/- 0.50
Episode length: 112.50 +/- 73.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.628    |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=0.53 +/- 0.56
Episode length: 108.20 +/- 82.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.532    |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.676    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 265      |
|    time_elapsed    | 23844    |
|    total_timesteps | 542720   |
---------------------------------
Eval num_timesteps=543000, episode_reward=0.69 +/- 0.48
Episode length: 89.84 +/- 74.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.8         |
|    mean_reward          | 0.69         |
| time/                   |              |
|    total_timesteps      | 543000       |
| train/                  |              |
|    approx_kl            | 0.0013912856 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.119       |
|    explained_variance   | 0.828        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00601      |
|    n_updates            | 2650         |
|    policy_gradient_loss | -0.000887    |
|    value_loss           | 0.0151       |
------------------------------------------
Eval num_timesteps=543500, episode_reward=0.68 +/- 0.48
Episode length: 98.90 +/- 75.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.9     |
|    mean_reward     | 0.681    |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=0.63 +/- 0.50
Episode length: 105.48 +/- 71.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.635    |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=0.66 +/- 0.49
Episode length: 103.20 +/- 71.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 0.657    |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.667    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 266      |
|    time_elapsed    | 23903    |
|    total_timesteps | 544768   |
---------------------------------
Eval num_timesteps=545000, episode_reward=0.51 +/- 0.56
Episode length: 112.44 +/- 82.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 112          |
|    mean_reward          | 0.508        |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 0.0023330357 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.832        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0109       |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.00193     |
|    value_loss           | 0.0134       |
------------------------------------------
Eval num_timesteps=545500, episode_reward=0.54 +/- 0.54
Episode length: 117.10 +/- 75.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 0.543    |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=0.63 +/- 0.52
Episode length: 93.62 +/- 77.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 0.627    |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=0.47 +/- 0.56
Episode length: 125.48 +/- 79.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 0.475    |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 0.679    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 267      |
|    time_elapsed    | 23967    |
|    total_timesteps | 546816   |
---------------------------------
Eval num_timesteps=547000, episode_reward=0.52 +/- 0.55
Episode length: 117.38 +/- 77.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 117         |
|    mean_reward          | 0.523       |
| time/                   |             |
|    total_timesteps      | 547000      |
| train/                  |             |
|    approx_kl            | 0.002016799 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.815       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00241    |
|    n_updates            | 2670        |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 0.0128      |
-----------------------------------------
Eval num_timesteps=547500, episode_reward=0.52 +/- 0.57
Episode length: 102.10 +/- 86.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.518    |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=0.55 +/- 0.55
Episode length: 107.54 +/- 83.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.553    |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=0.69 +/- 0.48
Episode length: 92.50 +/- 75.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 0.688    |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 0.635    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 268      |
|    time_elapsed    | 24027    |
|    total_timesteps | 548864   |
---------------------------------
Eval num_timesteps=549000, episode_reward=0.74 +/- 0.44
Episode length: 85.24 +/- 69.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 85.2         |
|    mean_reward          | 0.735        |
| time/                   |              |
|    total_timesteps      | 549000       |
| train/                  |              |
|    approx_kl            | 0.0014424345 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.866        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00232     |
|    n_updates            | 2680         |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 0.0114       |
------------------------------------------
Eval num_timesteps=549500, episode_reward=0.62 +/- 0.52
Episode length: 98.64 +/- 76.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.6     |
|    mean_reward     | 0.622    |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=0.69 +/- 0.48
Episode length: 92.92 +/- 73.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.9     |
|    mean_reward     | 0.687    |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=0.76 +/- 0.42
Episode length: 82.48 +/- 71.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.5     |
|    mean_reward     | 0.758    |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.3     |
|    ep_rew_mean     | 0.671    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 269      |
|    time_elapsed    | 24082    |
|    total_timesteps | 550912   |
---------------------------------
Eval num_timesteps=551000, episode_reward=0.61 +/- 0.52
Episode length: 105.46 +/- 76.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | 0.615        |
| time/                   |              |
|    total_timesteps      | 551000       |
| train/                  |              |
|    approx_kl            | 0.0015013481 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.83         |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00871      |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00165     |
|    value_loss           | 0.0162       |
------------------------------------------
Eval num_timesteps=551500, episode_reward=0.69 +/- 0.48
Episode length: 89.86 +/- 72.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 0.691    |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=0.63 +/- 0.52
Episode length: 95.04 +/- 81.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95       |
|    mean_reward     | 0.625    |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=0.45 +/- 0.56
Episode length: 132.88 +/- 75.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 0.447    |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 0.629    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 270      |
|    time_elapsed    | 24146    |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=553000, episode_reward=0.61 +/- 0.52
Episode length: 106.24 +/- 75.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 106          |
|    mean_reward          | 0.614        |
| time/                   |              |
|    total_timesteps      | 553000       |
| train/                  |              |
|    approx_kl            | 0.0017473621 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.096       |
|    explained_variance   | 0.875        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.000939    |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.000627    |
|    value_loss           | 0.0127       |
------------------------------------------
Eval num_timesteps=553500, episode_reward=0.58 +/- 0.54
Episode length: 101.88 +/- 79.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.578    |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=0.56 +/- 0.55
Episode length: 102.06 +/- 82.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 0.558    |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=0.57 +/- 0.53
Episode length: 114.22 +/- 77.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 0.566    |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=0.46 +/- 0.57
Episode length: 119.88 +/- 82.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 0.46     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 0.615    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 271      |
|    time_elapsed    | 24222    |
|    total_timesteps | 555008   |
---------------------------------
Eval num_timesteps=555500, episode_reward=0.65 +/- 0.51
Episode length: 94.14 +/- 80.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.1         |
|    mean_reward          | 0.646        |
| time/                   |              |
|    total_timesteps      | 555500       |
| train/                  |              |
|    approx_kl            | 0.0015505612 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.843        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0133       |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.000672    |
|    value_loss           | 0.0131       |
------------------------------------------
Eval num_timesteps=556000, episode_reward=0.49 +/- 0.57
Episode length: 111.68 +/- 84.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.489    |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=0.51 +/- 0.57
Episode length: 107.84 +/- 86.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.512    |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=0.58 +/- 0.54
Episode length: 103.92 +/- 81.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 0.576    |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 0.614    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 272      |
|    time_elapsed    | 24288    |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=557500, episode_reward=0.40 +/- 0.57
Episode length: 136.50 +/- 80.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 136          |
|    mean_reward          | 0.404        |
| time/                   |              |
|    total_timesteps      | 557500       |
| train/                  |              |
|    approx_kl            | 0.0018733467 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.113       |
|    explained_variance   | 0.87         |
|    learning_rate        | 1e-05        |
|    loss                 | -3.01e-05    |
|    n_updates            | 2720         |
|    policy_gradient_loss | -0.00134     |
|    value_loss           | 0.00888      |
------------------------------------------
Eval num_timesteps=558000, episode_reward=0.50 +/- 0.56
Episode length: 117.92 +/- 80.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 0.502    |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=0.66 +/- 0.49
Episode length: 96.78 +/- 75.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.8     |
|    mean_reward     | 0.664    |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=0.58 +/- 0.54
Episode length: 100.20 +/- 80.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.58     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 0.591    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 273      |
|    time_elapsed    | 24353    |
|    total_timesteps | 559104   |
---------------------------------
Eval num_timesteps=559500, episode_reward=0.66 +/- 0.49
Episode length: 99.60 +/- 73.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.6        |
|    mean_reward          | 0.661       |
| time/                   |             |
|    total_timesteps      | 559500      |
| train/                  |             |
|    approx_kl            | 0.004328008 |
|    clip_fraction        | 0.0163      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0638     |
|    explained_variance   | 0.851       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00731     |
|    n_updates            | 2730        |
|    policy_gradient_loss | 0.00138     |
|    value_loss           | 0.011       |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=0.53 +/- 0.55
Episode length: 111.66 +/- 81.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.529    |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=0.56 +/- 0.53
Episode length: 116.74 +/- 77.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 0.564    |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=0.67 +/- 0.50
Episode length: 92.08 +/- 77.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.1     |
|    mean_reward     | 0.668    |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 0.587    |
| time/              |          |
|    fps             | 22       |
|    iterations      | 274      |
|    time_elapsed    | 24416    |
|    total_timesteps | 561152   |
---------------------------------
Eval num_timesteps=561500, episode_reward=0.71 +/- 0.46
Episode length: 87.68 +/- 72.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 87.7         |
|    mean_reward          | 0.713        |
| time/                   |              |
|    total_timesteps      | 561500       |
| train/                  |              |
|    approx_kl            | 0.0057331733 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0838      |
|    explained_variance   | 0.835        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0109       |
|    n_updates            | 2740         |
|    policy_gradient_loss | -4.66e-05    |
|    value_loss           | 0.0123       |
------------------------------------------
Eval num_timesteps=562000, episode_reward=0.67 +/- 0.50
Episode length: 87.20 +/- 76.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.2     |
|    mean_reward     | 0.673    |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=0.61 +/- 0.52
Episode length: 105.60 +/- 78.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.615    |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=0.67 +/- 0.50
Episode length: 91.12 +/- 80.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.1     |
|    mean_reward     | 0.669    |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 0.564    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 275      |
|    time_elapsed    | 24473    |
|    total_timesteps | 563200   |
---------------------------------
Eval num_timesteps=563500, episode_reward=0.46 +/- 0.57
Episode length: 116.66 +/- 85.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 117          |
|    mean_reward          | 0.464        |
| time/                   |              |
|    total_timesteps      | 563500       |
| train/                  |              |
|    approx_kl            | 0.0032791041 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0689      |
|    explained_variance   | 0.856        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00715      |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00235     |
|    value_loss           | 0.0148       |
------------------------------------------
Eval num_timesteps=564000, episode_reward=0.46 +/- 0.57
Episode length: 122.20 +/- 83.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.458    |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=0.53 +/- 0.55
Episode length: 112.94 +/- 81.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 0.527    |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=0.54 +/- 0.56
Episode length: 104.82 +/- 82.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.536    |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 0.563    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 276      |
|    time_elapsed    | 24539    |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=565500, episode_reward=0.65 +/- 0.51
Episode length: 92.28 +/- 82.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.3         |
|    mean_reward          | 0.648        |
| time/                   |              |
|    total_timesteps      | 565500       |
| train/                  |              |
|    approx_kl            | 0.0012717184 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.861        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00466      |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.000869    |
|    value_loss           | 0.0126       |
------------------------------------------
Eval num_timesteps=566000, episode_reward=0.69 +/- 0.48
Episode length: 90.02 +/- 69.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90       |
|    mean_reward     | 0.69     |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=0.56 +/- 0.55
Episode length: 103.02 +/- 83.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 0.557    |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=0.59 +/- 0.53
Episode length: 106.48 +/- 73.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.594    |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 0.601    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 277      |
|    time_elapsed    | 24604    |
|    total_timesteps | 567296   |
---------------------------------
Eval num_timesteps=567500, episode_reward=0.69 +/- 0.48
Episode length: 90.58 +/- 76.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.6         |
|    mean_reward          | 0.69         |
| time/                   |              |
|    total_timesteps      | 567500       |
| train/                  |              |
|    approx_kl            | 0.0038043798 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0995      |
|    explained_variance   | 0.879        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00149      |
|    n_updates            | 2770         |
|    policy_gradient_loss | -0.000938    |
|    value_loss           | 0.0113       |
------------------------------------------
Eval num_timesteps=568000, episode_reward=0.55 +/- 0.55
Episode length: 112.36 +/- 80.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.548    |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=0.59 +/- 0.53
Episode length: 110.10 +/- 79.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 0.59     |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=0.53 +/- 0.55
Episode length: 112.04 +/- 81.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.528    |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 23       |
|    iterations      | 278      |
|    time_elapsed    | 24668    |
|    total_timesteps | 569344   |
---------------------------------
Eval num_timesteps=569500, episode_reward=0.44 +/- 0.58
Episode length: 121.66 +/- 85.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 0.439       |
| time/                   |             |
|    total_timesteps      | 569500      |
| train/                  |             |
|    approx_kl            | 0.002467134 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0797     |
|    explained_variance   | 0.868       |
|    learning_rate        | 1e-05       |
|    loss                 | -0.00253    |
|    n_updates            | 2780        |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 0.0105      |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=0.53 +/- 0.56
Episode length: 106.80 +/- 84.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 107      |
|    mean_reward     | 0.533    |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=0.66 +/- 0.49
Episode length: 105.18 +/- 72.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 105      |
|    mean_reward     | 0.655    |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=0.45 +/- 0.56
Episode length: 130.42 +/- 81.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 0.45     |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 0.626    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 279      |
|    time_elapsed    | 24736    |
|    total_timesteps | 571392   |
---------------------------------
Eval num_timesteps=571500, episode_reward=0.67 +/- 0.50
Episode length: 88.80 +/- 76.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88.8         |
|    mean_reward          | 0.672        |
| time/                   |              |
|    total_timesteps      | 571500       |
| train/                  |              |
|    approx_kl            | 0.0012342581 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | 0.789        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0102       |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.000766    |
|    value_loss           | 0.0174       |
------------------------------------------
Eval num_timesteps=572000, episode_reward=0.59 +/- 0.53
Episode length: 111.62 +/- 80.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 0.589    |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=0.67 +/- 0.49
Episode length: 95.20 +/- 78.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | 0.665    |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=0.61 +/- 0.51
Episode length: 108.10 +/- 78.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | 0.612    |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.7     |
|    ep_rew_mean     | 0.678    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 280      |
|    time_elapsed    | 24795    |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=573500, episode_reward=0.57 +/- 0.54
Episode length: 107.20 +/- 80.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 107           |
|    mean_reward          | 0.573         |
| time/                   |               |
|    total_timesteps      | 573500        |
| train/                  |               |
|    approx_kl            | 0.00097262335 |
|    clip_fraction        | 0.0123        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.126        |
|    explained_variance   | 0.834         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00178       |
|    n_updates            | 2800          |
|    policy_gradient_loss | -0.000815     |
|    value_loss           | 0.0156        |
-------------------------------------------
Eval num_timesteps=574000, episode_reward=0.63 +/- 0.52
Episode length: 90.90 +/- 82.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.9     |
|    mean_reward     | 0.629    |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=0.60 +/- 0.53
Episode length: 99.70 +/- 78.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.7     |
|    mean_reward     | 0.601    |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=0.63 +/- 0.50
Episode length: 105.66 +/- 78.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.635    |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 0.63     |
| time/              |          |
|    fps             | 23       |
|    iterations      | 281      |
|    time_elapsed    | 24856    |
|    total_timesteps | 575488   |
---------------------------------
Eval num_timesteps=575500, episode_reward=0.55 +/- 0.55
Episode length: 106.18 +/- 79.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 106           |
|    mean_reward          | 0.554         |
| time/                   |               |
|    total_timesteps      | 575500        |
| train/                  |               |
|    approx_kl            | 0.00064162584 |
|    clip_fraction        | 0.0109        |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0704       |
|    explained_variance   | 0.856         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00905       |
|    n_updates            | 2810          |
|    policy_gradient_loss | -0.0012       |
|    value_loss           | 0.00958       |
-------------------------------------------
Eval num_timesteps=576000, episode_reward=0.60 +/- 0.53
Episode length: 98.18 +/- 77.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.2     |
|    mean_reward     | 0.602    |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=0.60 +/- 0.53
Episode length: 101.08 +/- 77.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.599    |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=0.57 +/- 0.54
Episode length: 106.24 +/- 80.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 0.574    |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=0.64 +/- 0.51
Episode length: 100.24 +/- 74.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.64     |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.2     |
|    ep_rew_mean     | 0.652    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 282      |
|    time_elapsed    | 24940    |
|    total_timesteps | 577536   |
---------------------------------
Eval num_timesteps=578000, episode_reward=0.56 +/- 0.55
Episode length: 103.64 +/- 82.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 0.557        |
| time/                   |              |
|    total_timesteps      | 578000       |
| train/                  |              |
|    approx_kl            | 0.0010612246 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.104       |
|    explained_variance   | 0.813        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00167     |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00183     |
|    value_loss           | 0.0154       |
------------------------------------------
Eval num_timesteps=578500, episode_reward=0.72 +/- 0.46
Episode length: 85.08 +/- 71.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.1     |
|    mean_reward     | 0.715    |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=0.50 +/- 0.56
Episode length: 121.80 +/- 78.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0.499    |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=0.81 +/- 0.38
Episode length: 70.40 +/- 64.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.4     |
|    mean_reward     | 0.81     |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.8     |
|    ep_rew_mean     | 0.651    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 283      |
|    time_elapsed    | 25021    |
|    total_timesteps | 579584   |
---------------------------------
Eval num_timesteps=580000, episode_reward=0.60 +/- 0.53
Episode length: 100.16 +/- 83.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 0.6          |
| time/                   |              |
|    total_timesteps      | 580000       |
| train/                  |              |
|    approx_kl            | 0.0047350186 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.794        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00621      |
|    n_updates            | 2830         |
|    policy_gradient_loss | 0.000722     |
|    value_loss           | 0.0151       |
------------------------------------------
Eval num_timesteps=580500, episode_reward=0.60 +/- 0.53
Episode length: 99.42 +/- 79.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | 0.601    |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=0.62 +/- 0.52
Episode length: 101.46 +/- 78.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 0.619    |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=0.64 +/- 0.51
Episode length: 95.62 +/- 77.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.6     |
|    mean_reward     | 0.645    |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.3     |
|    ep_rew_mean     | 0.641    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 284      |
|    time_elapsed    | 25081    |
|    total_timesteps | 581632   |
---------------------------------
Eval num_timesteps=582000, episode_reward=0.67 +/- 0.49
Episode length: 95.04 +/- 73.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 95            |
|    mean_reward          | 0.665         |
| time/                   |               |
|    total_timesteps      | 582000        |
| train/                  |               |
|    approx_kl            | 0.00052456255 |
|    clip_fraction        | 0.00601       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0602       |
|    explained_variance   | 0.884         |
|    learning_rate        | 1e-05         |
|    loss                 | 0.00162       |
|    n_updates            | 2840          |
|    policy_gradient_loss | -0.000133     |
|    value_loss           | 0.00875       |
-------------------------------------------
Eval num_timesteps=582500, episode_reward=0.64 +/- 0.51
Episode length: 97.14 +/- 74.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.1     |
|    mean_reward     | 0.643    |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=0.65 +/- 0.51
Episode length: 85.30 +/- 77.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.3     |
|    mean_reward     | 0.655    |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=0.72 +/- 0.47
Episode length: 81.58 +/- 72.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.6     |
|    mean_reward     | 0.719    |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 0.627    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 285      |
|    time_elapsed    | 25136    |
|    total_timesteps | 583680   |
---------------------------------
Eval num_timesteps=584000, episode_reward=0.70 +/- 0.49
Episode length: 78.66 +/- 75.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 78.7         |
|    mean_reward          | 0.702        |
| time/                   |              |
|    total_timesteps      | 584000       |
| train/                  |              |
|    approx_kl            | 0.0022573196 |
|    clip_fraction        | 0.0176       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.106       |
|    explained_variance   | 0.8          |
|    learning_rate        | 1e-05        |
|    loss                 | -0.00902     |
|    n_updates            | 2850         |
|    policy_gradient_loss | -0.00248     |
|    value_loss           | 0.0183       |
------------------------------------------
Eval num_timesteps=584500, episode_reward=0.64 +/- 0.51
Episode length: 97.66 +/- 72.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | 0.643    |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=0.66 +/- 0.51
Episode length: 84.28 +/- 79.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.3     |
|    mean_reward     | 0.656    |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=0.72 +/- 0.47
Episode length: 81.64 +/- 74.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.6     |
|    mean_reward     | 0.719    |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.2     |
|    ep_rew_mean     | 0.677    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 286      |
|    time_elapsed    | 25189    |
|    total_timesteps | 585728   |
---------------------------------
Eval num_timesteps=586000, episode_reward=0.60 +/- 0.53
Episode length: 98.04 +/- 80.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98           |
|    mean_reward          | 0.602        |
| time/                   |              |
|    total_timesteps      | 586000       |
| train/                  |              |
|    approx_kl            | 0.0012076367 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0638      |
|    explained_variance   | 0.876        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.00711      |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.000966    |
|    value_loss           | 0.0114       |
------------------------------------------
Eval num_timesteps=586500, episode_reward=0.74 +/- 0.45
Episode length: 79.34 +/- 70.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 0.741    |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=0.73 +/- 0.44
Episode length: 88.44 +/- 67.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.4     |
|    mean_reward     | 0.732    |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=0.73 +/- 0.44
Episode length: 85.96 +/- 66.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86       |
|    mean_reward     | 0.734    |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.3     |
|    ep_rew_mean     | 0.669    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 287      |
|    time_elapsed    | 25241    |
|    total_timesteps | 587776   |
---------------------------------
Eval num_timesteps=588000, episode_reward=0.60 +/- 0.53
Episode length: 98.62 +/- 76.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.6         |
|    mean_reward          | 0.602        |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0020232166 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.103       |
|    explained_variance   | 0.854        |
|    learning_rate        | 1e-05        |
|    loss                 | 7.43e-05     |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.00201     |
|    value_loss           | 0.0135       |
------------------------------------------
Eval num_timesteps=588500, episode_reward=0.76 +/- 0.42
Episode length: 80.60 +/- 67.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 0.76     |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=0.91 +/- 0.23
Episode length: 54.36 +/- 51.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | 0.906    |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
New best mean reward!
Eval num_timesteps=589500, episode_reward=0.72 +/- 0.47
Episode length: 83.40 +/- 77.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.4     |
|    mean_reward     | 0.717    |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.9     |
|    ep_rew_mean     | 0.654    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 288      |
|    time_elapsed    | 25290    |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=590000, episode_reward=0.69 +/- 0.48
Episode length: 86.86 +/- 75.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 86.9         |
|    mean_reward          | 0.693        |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0015370866 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.799        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0085       |
|    n_updates            | 2880         |
|    policy_gradient_loss | -0.00101     |
|    value_loss           | 0.0156       |
------------------------------------------
Eval num_timesteps=590500, episode_reward=0.77 +/- 0.43
Episode length: 70.00 +/- 67.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70       |
|    mean_reward     | 0.77     |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=0.71 +/- 0.46
Episode length: 89.84 +/- 72.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.8     |
|    mean_reward     | 0.711    |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=0.73 +/- 0.44
Episode length: 88.08 +/- 69.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | 0.732    |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | 0.698    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 289      |
|    time_elapsed    | 25341    |
|    total_timesteps | 591872   |
---------------------------------
Eval num_timesteps=592000, episode_reward=0.62 +/- 0.52
Episode length: 96.96 +/- 79.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97          |
|    mean_reward          | 0.623       |
| time/                   |             |
|    total_timesteps      | 592000      |
| train/                  |             |
|    approx_kl            | 0.001110899 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0805     |
|    explained_variance   | 0.863       |
|    learning_rate        | 1e-05       |
|    loss                 | 0.00683     |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00232    |
|    value_loss           | 0.0123      |
-----------------------------------------
Eval num_timesteps=592500, episode_reward=0.85 +/- 0.31
Episode length: 74.40 +/- 56.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 0.846    |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=0.70 +/- 0.48
Episode length: 82.74 +/- 72.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.7     |
|    mean_reward     | 0.698    |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=0.63 +/- 0.52
Episode length: 94.24 +/- 75.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | 0.626    |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.2     |
|    ep_rew_mean     | 0.685    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 290      |
|    time_elapsed    | 25394    |
|    total_timesteps | 593920   |
---------------------------------
Eval num_timesteps=594000, episode_reward=0.73 +/- 0.44
Episode length: 85.58 +/- 72.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 85.6         |
|    mean_reward          | 0.735        |
| time/                   |              |
|    total_timesteps      | 594000       |
| train/                  |              |
|    approx_kl            | 0.0013507433 |
|    clip_fraction        | 0.0136       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.726        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0285       |
|    n_updates            | 2900         |
|    policy_gradient_loss | -0.00129     |
|    value_loss           | 0.02         |
------------------------------------------
Eval num_timesteps=594500, episode_reward=0.73 +/- 0.44
Episode length: 86.44 +/- 71.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.4     |
|    mean_reward     | 0.734    |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=0.62 +/- 0.52
Episode length: 100.32 +/- 76.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.62     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=0.76 +/- 0.42
Episode length: 82.36 +/- 67.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 0.758    |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94       |
|    ep_rew_mean     | 0.696    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 291      |
|    time_elapsed    | 25447    |
|    total_timesteps | 595968   |
---------------------------------
Eval num_timesteps=596000, episode_reward=0.75 +/- 0.45
Episode length: 73.48 +/- 73.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73.5         |
|    mean_reward          | 0.747        |
| time/                   |              |
|    total_timesteps      | 596000       |
| train/                  |              |
|    approx_kl            | 0.0020542548 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0845      |
|    explained_variance   | 0.825        |
|    learning_rate        | 1e-05        |
|    loss                 | 0.0117       |
|    n_updates            | 2910         |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 0.0123       |
------------------------------------------
Eval num_timesteps=596500, episode_reward=0.67 +/- 0.49
Episode length: 91.60 +/- 70.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 0.669    |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=0.75 +/- 0.42
Episode length: 85.98 +/- 67.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86       |
|    mean_reward     | 0.754    |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=0.68 +/- 0.48
Episode length: 96.96 +/- 75.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 0.683    |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=0.73 +/- 0.44
Episode length: 89.48 +/- 67.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | 0.731    |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.3     |
|    ep_rew_mean     | 0.705    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 292      |
|    time_elapsed    | 25511    |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=598500, episode_reward=0.66 +/- 0.49
Episode length: 100.62 +/- 72.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | 0.66         |
| time/                   |              |
|    total_timesteps      | 598500       |
| train/                  |              |
|    approx_kl            | 0.0024276618 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.097       |
|    explained_variance   | 0.851        |
|    learning_rate        | 1e-05        |
|    loss                 | -0.0126      |
|    n_updates            | 2920         |
|    policy_gradient_loss | -0.00278     |
|    value_loss           | 0.0125       |
------------------------------------------
Eval num_timesteps=599000, episode_reward=0.69 +/- 0.48
Episode length: 89.86 +/- 73.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 0.691    |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=0.58 +/- 0.54
Episode length: 99.56 +/- 80.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.6     |
|    mean_reward     | 0.581    |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=0.60 +/- 0.53
Episode length: 100.26 +/- 83.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 0.6      |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.2     |
|    ep_rew_mean     | 0.707    |
| time/              |          |
|    fps             | 23       |
|    iterations      | 293      |
|    time_elapsed    | 25569    |
|    total_timesteps | 600064   |
---------------------------------
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/my-way-home/ppo-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
