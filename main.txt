2024-09-24 01:27:32.923909: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-24 01:27:39.504563: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-24 01:27:52.891051: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.8     |
|    ep_rew_mean      | 1        |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 757      |
|    time_elapsed     | 0        |
|    total_timesteps  | 351      |
----------------------------------
Eval num_timesteps=500, episode_reward=1.92 +/- 2.76
Episode length: 91.54 +/- 28.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.5     |
|    mean_reward      | 1.92     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 1.12     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 8        |
|    fps              | 38       |
|    time_elapsed     | 16       |
|    total_timesteps  | 623      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 2.17     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 12       |
|    fps              | 58       |
|    time_elapsed     | 16       |
|    total_timesteps  | 981      |
----------------------------------
Eval num_timesteps=1000, episode_reward=1.62 +/- 2.55
Episode length: 92.30 +/- 36.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.3     |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 1.62     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 16       |
|    fps              | 48       |
|    time_elapsed     | 28       |
|    total_timesteps  | 1395     |
----------------------------------
Eval num_timesteps=1500, episode_reward=1.90 +/- 3.28
Episode length: 100.50 +/- 32.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 1.9      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.5     |
|    ep_rew_mean      | 1.85     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 20       |
|    fps              | 43       |
|    time_elapsed     | 41       |
|    total_timesteps  | 1809     |
----------------------------------
Eval num_timesteps=2000, episode_reward=0.82 +/- 1.34
Episode length: 87.96 +/- 32.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 0.82     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 1.83     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 24       |
|    fps              | 42       |
|    time_elapsed     | 53       |
|    total_timesteps  | 2255     |
----------------------------------
Eval num_timesteps=2500, episode_reward=2.00 +/- 3.07
Episode length: 94.62 +/- 33.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 2        |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | 1.96     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 28       |
|    fps              | 40       |
|    time_elapsed     | 65       |
|    total_timesteps  | 2677     |
----------------------------------
Eval num_timesteps=3000, episode_reward=1.62 +/- 2.12
Episode length: 99.98 +/- 32.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 2.12     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 32       |
|    fps              | 39       |
|    time_elapsed     | 78       |
|    total_timesteps  | 3075     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 2.5      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 36       |
|    fps              | 44       |
|    time_elapsed     | 78       |
|    total_timesteps  | 3462     |
----------------------------------
Eval num_timesteps=3500, episode_reward=1.02 +/- 1.44
Episode length: 104.58 +/- 37.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 1.02     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.45     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 40       |
|    fps              | 41       |
|    time_elapsed     | 91       |
|    total_timesteps  | 3802     |
----------------------------------
Eval num_timesteps=4000, episode_reward=1.56 +/- 2.56
Episode length: 99.46 +/- 33.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.5     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 2.32     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 44       |
|    fps              | 39       |
|    time_elapsed     | 103      |
|    total_timesteps  | 4090     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 2.33     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 48       |
|    fps              | 42       |
|    time_elapsed     | 104      |
|    total_timesteps  | 4465     |
----------------------------------
Eval num_timesteps=4500, episode_reward=1.54 +/- 2.62
Episode length: 91.80 +/- 35.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.8     |
|    mean_reward      | 1.54     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.6     |
|    ep_rew_mean      | 2.17     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 52       |
|    fps              | 41       |
|    time_elapsed     | 116      |
|    total_timesteps  | 4814     |
----------------------------------
Eval num_timesteps=5000, episode_reward=1.52 +/- 2.17
Episode length: 94.04 +/- 33.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 1.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.29     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 56       |
|    fps              | 41       |
|    time_elapsed     | 128      |
|    total_timesteps  | 5262     |
----------------------------------
Eval num_timesteps=5500, episode_reward=1.36 +/- 2.48
Episode length: 88.02 +/- 33.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 1.36     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.9     |
|    ep_rew_mean      | 2.23     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 60       |
|    fps              | 39       |
|    time_elapsed     | 139      |
|    total_timesteps  | 5512     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 2.3      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 64       |
|    fps              | 42       |
|    time_elapsed     | 139      |
|    total_timesteps  | 5959     |
----------------------------------
Eval num_timesteps=6000, episode_reward=1.14 +/- 2.38
Episode length: 85.52 +/- 25.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.5     |
|    mean_reward      | 1.14     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 2.19     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 68       |
|    fps              | 41       |
|    time_elapsed     | 150      |
|    total_timesteps  | 6259     |
----------------------------------
Eval num_timesteps=6500, episode_reward=1.96 +/- 2.89
Episode length: 93.80 +/- 34.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 2.1      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 72       |
|    fps              | 40       |
|    time_elapsed     | 162      |
|    total_timesteps  | 6574     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.1     |
|    ep_rew_mean      | 1.99     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 76       |
|    fps              | 42       |
|    time_elapsed     | 163      |
|    total_timesteps  | 6927     |
----------------------------------
Eval num_timesteps=7000, episode_reward=1.36 +/- 1.88
Episode length: 87.98 +/- 28.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 1.36     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 2        |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 80       |
|    fps              | 42       |
|    time_elapsed     | 174      |
|    total_timesteps  | 7362     |
----------------------------------
Eval num_timesteps=7500, episode_reward=1.56 +/- 2.79
Episode length: 97.10 +/- 37.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.1     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 2.11     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 84       |
|    fps              | 41       |
|    time_elapsed     | 187      |
|    total_timesteps  | 7728     |
----------------------------------
Eval num_timesteps=8000, episode_reward=1.66 +/- 3.22
Episode length: 96.14 +/- 29.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.1     |
|    mean_reward      | 1.66     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.4     |
|    ep_rew_mean      | 2.24     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 88       |
|    fps              | 41       |
|    time_elapsed     | 199      |
|    total_timesteps  | 8219     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.9     |
|    ep_rew_mean      | 2.14     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 92       |
|    fps              | 42       |
|    time_elapsed     | 200      |
|    total_timesteps  | 8452     |
----------------------------------
Eval num_timesteps=8500, episode_reward=2.02 +/- 2.71
Episode length: 99.30 +/- 30.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 2.02     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.9     |
|    ep_rew_mean      | 2.12     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 96       |
|    fps              | 41       |
|    time_elapsed     | 212      |
|    total_timesteps  | 8826     |
----------------------------------
Eval num_timesteps=9000, episode_reward=1.80 +/- 2.93
Episode length: 97.04 +/- 35.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 2.16     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 100      |
|    fps              | 40       |
|    time_elapsed     | 224      |
|    total_timesteps  | 9152     |
----------------------------------
Eval num_timesteps=9500, episode_reward=1.64 +/- 2.15
Episode length: 93.76 +/- 35.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 1.64     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.2     |
|    ep_rew_mean      | 2.5      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 104      |
|    fps              | 41       |
|    time_elapsed     | 236      |
|    total_timesteps  | 9766     |
----------------------------------
Eval num_timesteps=10000, episode_reward=2.40 +/- 3.52
Episode length: 95.66 +/- 38.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.7     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 108      |
|    fps              | 40       |
|    time_elapsed     | 249      |
|    total_timesteps  | 10188    |
----------------------------------
Eval num_timesteps=10500, episode_reward=2.16 +/- 3.25
Episode length: 99.80 +/- 34.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.8     |
|    mean_reward      | 2.16     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 2.57     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 112      |
|    fps              | 40       |
|    time_elapsed     | 261      |
|    total_timesteps  | 10531    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 2.57     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 116      |
|    fps              | 41       |
|    time_elapsed     | 262      |
|    total_timesteps  | 10859    |
----------------------------------
Eval num_timesteps=11000, episode_reward=1.72 +/- 2.97
Episode length: 93.02 +/- 34.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93       |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 2.53     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 120      |
|    fps              | 40       |
|    time_elapsed     | 274      |
|    total_timesteps  | 11190    |
----------------------------------
Eval num_timesteps=11500, episode_reward=1.24 +/- 2.50
Episode length: 97.78 +/- 32.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.8     |
|    mean_reward      | 1.24     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 2.63     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 124      |
|    fps              | 40       |
|    time_elapsed     | 286      |
|    total_timesteps  | 11535    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 2.54     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 128      |
|    fps              | 41       |
|    time_elapsed     | 287      |
|    total_timesteps  | 11958    |
----------------------------------
Eval num_timesteps=12000, episode_reward=1.56 +/- 2.33
Episode length: 97.52 +/- 37.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.5     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.7     |
|    ep_rew_mean      | 2.44     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 132      |
|    fps              | 40       |
|    time_elapsed     | 299      |
|    total_timesteps  | 12245    |
----------------------------------
Eval num_timesteps=12500, episode_reward=1.52 +/- 2.34
Episode length: 94.36 +/- 37.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 1.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 2.33     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 136      |
|    fps              | 40       |
|    time_elapsed     | 312      |
|    total_timesteps  | 12588    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 140      |
|    fps              | 41       |
|    time_elapsed     | 312      |
|    total_timesteps  | 12897    |
----------------------------------
Eval num_timesteps=13000, episode_reward=1.06 +/- 1.98
Episode length: 85.16 +/- 26.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.2     |
|    mean_reward      | 1.06     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 2.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 144      |
|    fps              | 40       |
|    time_elapsed     | 325      |
|    total_timesteps  | 13189    |
----------------------------------
Eval num_timesteps=13500, episode_reward=0.84 +/- 1.53
Episode length: 88.48 +/- 34.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.5     |
|    mean_reward      | 0.84     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.5     |
|    ep_rew_mean      | 2.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 148      |
|    fps              | 39       |
|    time_elapsed     | 339      |
|    total_timesteps  | 13519    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90       |
|    ep_rew_mean      | 2.47     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 152      |
|    fps              | 40       |
|    time_elapsed     | 339      |
|    total_timesteps  | 13812    |
----------------------------------
Eval num_timesteps=14000, episode_reward=1.48 +/- 2.49
Episode length: 91.22 +/- 36.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.2     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.8     |
|    ep_rew_mean      | 2.48     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 156      |
|    fps              | 40       |
|    time_elapsed     | 353      |
|    total_timesteps  | 14241    |
----------------------------------
Eval num_timesteps=14500, episode_reward=1.60 +/- 2.50
Episode length: 89.36 +/- 28.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.4     |
|    mean_reward      | 1.6      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.6     |
|    ep_rew_mean      | 2.49     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 160      |
|    fps              | 39       |
|    time_elapsed     | 368      |
|    total_timesteps  | 14675    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.4     |
|    ep_rew_mean      | 2.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 164      |
|    fps              | 40       |
|    time_elapsed     | 368      |
|    total_timesteps  | 14998    |
----------------------------------
Eval num_timesteps=15000, episode_reward=1.84 +/- 2.96
Episode length: 100.28 +/- 31.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 1.84     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 2.55     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 168      |
|    fps              | 40       |
|    time_elapsed     | 381      |
|    total_timesteps  | 15359    |
----------------------------------
Eval num_timesteps=15500, episode_reward=1.36 +/- 2.30
Episode length: 92.14 +/- 27.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.1     |
|    mean_reward      | 1.36     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.8     |
|    ep_rew_mean      | 2.62     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 172      |
|    fps              | 39       |
|    time_elapsed     | 393      |
|    total_timesteps  | 15656    |
----------------------------------
Eval num_timesteps=16000, episode_reward=2.26 +/- 3.47
Episode length: 95.18 +/- 30.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.2     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 176      |
|    fps              | 39       |
|    time_elapsed     | 405      |
|    total_timesteps  | 16072    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 180      |
|    fps              | 40       |
|    time_elapsed     | 406      |
|    total_timesteps  | 16464    |
----------------------------------
Eval num_timesteps=16500, episode_reward=0.70 +/- 1.12
Episode length: 90.74 +/- 27.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.7     |
|    mean_reward      | 0.7      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 2.58     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 184      |
|    fps              | 40       |
|    time_elapsed     | 417      |
|    total_timesteps  | 16932    |
----------------------------------
Eval num_timesteps=17000, episode_reward=1.44 +/- 2.62
Episode length: 91.18 +/- 33.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.2     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.5     |
|    ep_rew_mean      | 2.52     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 188      |
|    fps              | 40       |
|    time_elapsed     | 430      |
|    total_timesteps  | 17270    |
----------------------------------
Eval num_timesteps=17500, episode_reward=1.98 +/- 3.22
Episode length: 90.68 +/- 28.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.7     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 2.56     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 192      |
|    fps              | 39       |
|    time_elapsed     | 442      |
|    total_timesteps  | 17601    |
----------------------------------
Eval num_timesteps=18000, episode_reward=1.16 +/- 1.88
Episode length: 94.22 +/- 30.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.2     |
|    mean_reward      | 1.16     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 196      |
|    fps              | 39       |
|    time_elapsed     | 455      |
|    total_timesteps  | 18132    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 2.63     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 200      |
|    fps              | 40       |
|    time_elapsed     | 455      |
|    total_timesteps  | 18455    |
----------------------------------
Eval num_timesteps=18500, episode_reward=2.12 +/- 2.72
Episode length: 103.90 +/- 33.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.12     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 2.46     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 204      |
|    fps              | 40       |
|    time_elapsed     | 468      |
|    total_timesteps  | 18897    |
----------------------------------
Eval num_timesteps=19000, episode_reward=1.94 +/- 2.76
Episode length: 101.98 +/- 35.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 2.28     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 208      |
|    fps              | 40       |
|    time_elapsed     | 481      |
|    total_timesteps  | 19323    |
----------------------------------
Eval num_timesteps=19500, episode_reward=1.98 +/- 3.19
Episode length: 91.56 +/- 33.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.6     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.1     |
|    ep_rew_mean      | 2.29     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 212      |
|    fps              | 40       |
|    time_elapsed     | 493      |
|    total_timesteps  | 19742    |
----------------------------------
Eval num_timesteps=20000, episode_reward=1.46 +/- 2.08
Episode length: 86.46 +/- 28.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.5     |
|    mean_reward      | 1.46     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 2.45     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 216      |
|    fps              | 40       |
|    time_elapsed     | 504      |
|    total_timesteps  | 20210    |
----------------------------------
Eval num_timesteps=20500, episode_reward=1.48 +/- 2.01
Episode length: 95.38 +/- 28.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.4     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.1     |
|    ep_rew_mean      | 2.48     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 220      |
|    fps              | 39       |
|    time_elapsed     | 516      |
|    total_timesteps  | 20601    |
----------------------------------
Eval num_timesteps=21000, episode_reward=1.92 +/- 2.67
Episode length: 97.26 +/- 40.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 1.92     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.9     |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 224      |
|    fps              | 39       |
|    time_elapsed     | 529      |
|    total_timesteps  | 21026    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 2.34     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 228      |
|    fps              | 40       |
|    time_elapsed     | 529      |
|    total_timesteps  | 21262    |
----------------------------------
Eval num_timesteps=21500, episode_reward=1.96 +/- 3.35
Episode length: 88.56 +/- 26.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.6     |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 2.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 232      |
|    fps              | 39       |
|    time_elapsed     | 542      |
|    total_timesteps  | 21623    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 2.37     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 236      |
|    fps              | 40       |
|    time_elapsed     | 542      |
|    total_timesteps  | 21972    |
----------------------------------
Eval num_timesteps=22000, episode_reward=1.58 +/- 2.44
Episode length: 99.80 +/- 36.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.8     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 2.32     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 240      |
|    fps              | 39       |
|    time_elapsed     | 558      |
|    total_timesteps  | 22242    |
----------------------------------
Eval num_timesteps=22500, episode_reward=1.82 +/- 2.45
Episode length: 99.58 +/- 30.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.6     |
|    mean_reward      | 1.82     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 244      |
|    fps              | 39       |
|    time_elapsed     | 571      |
|    total_timesteps  | 22594    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.2     |
|    ep_rew_mean      | 2.34     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 248      |
|    fps              | 40       |
|    time_elapsed     | 571      |
|    total_timesteps  | 22941    |
----------------------------------
Eval num_timesteps=23000, episode_reward=1.96 +/- 3.12
Episode length: 99.00 +/- 39.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99       |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 2.34     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 252      |
|    fps              | 40       |
|    time_elapsed     | 583      |
|    total_timesteps  | 23361    |
----------------------------------
Eval num_timesteps=23500, episode_reward=1.30 +/- 1.96
Episode length: 91.08 +/- 29.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.1     |
|    mean_reward      | 1.3      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 2.24     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 256      |
|    fps              | 39       |
|    time_elapsed     | 595      |
|    total_timesteps  | 23722    |
----------------------------------
Eval num_timesteps=24000, episode_reward=1.30 +/- 1.73
Episode length: 99.94 +/- 36.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.9     |
|    mean_reward      | 1.3      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 2.23     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 260      |
|    fps              | 39       |
|    time_elapsed     | 608      |
|    total_timesteps  | 24041    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 2.31     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 264      |
|    fps              | 40       |
|    time_elapsed     | 608      |
|    total_timesteps  | 24384    |
----------------------------------
Eval num_timesteps=24500, episode_reward=0.98 +/- 1.96
Episode length: 87.94 +/- 31.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.9     |
|    mean_reward      | 0.98     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 2.13     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 268      |
|    fps              | 39       |
|    time_elapsed     | 619      |
|    total_timesteps  | 24634    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 2.19     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 272      |
|    fps              | 40       |
|    time_elapsed     | 620      |
|    total_timesteps  | 24940    |
----------------------------------
Eval num_timesteps=25000, episode_reward=1.76 +/- 2.56
Episode length: 93.10 +/- 28.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.1     |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.9     |
|    ep_rew_mean      | 2.08     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 276      |
|    fps              | 40       |
|    time_elapsed     | 632      |
|    total_timesteps  | 25361    |
----------------------------------
Eval num_timesteps=25500, episode_reward=1.16 +/- 2.01
Episode length: 84.80 +/- 30.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.8     |
|    mean_reward      | 1.16     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 2.11     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 280      |
|    fps              | 39       |
|    time_elapsed     | 646      |
|    total_timesteps  | 25733    |
----------------------------------
Eval num_timesteps=26000, episode_reward=1.94 +/- 3.44
Episode length: 87.04 +/- 31.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87       |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.2     |
|    ep_rew_mean      | 2.16     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 284      |
|    fps              | 39       |
|    time_elapsed     | 657      |
|    total_timesteps  | 26150    |
----------------------------------
Eval num_timesteps=26500, episode_reward=1.38 +/- 2.15
Episode length: 89.00 +/- 29.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89       |
|    mean_reward      | 1.38     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.2     |
|    ep_rew_mean      | 2.07     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 288      |
|    fps              | 39       |
|    time_elapsed     | 669      |
|    total_timesteps  | 26687    |
----------------------------------
Eval num_timesteps=27000, episode_reward=1.80 +/- 2.58
Episode length: 100.24 +/- 43.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.04     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 292      |
|    fps              | 39       |
|    time_elapsed     | 684      |
|    total_timesteps  | 27003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.8     |
|    ep_rew_mean      | 1.88     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 296      |
|    fps              | 39       |
|    time_elapsed     | 685      |
|    total_timesteps  | 27210    |
----------------------------------
Eval num_timesteps=27500, episode_reward=1.58 +/- 2.76
Episode length: 98.30 +/- 37.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.3     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 1.86     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 300      |
|    fps              | 39       |
|    time_elapsed     | 700      |
|    total_timesteps  | 27550    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.3     |
|    ep_rew_mean      | 1.83     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 304      |
|    fps              | 39       |
|    time_elapsed     | 701      |
|    total_timesteps  | 27931    |
----------------------------------
Eval num_timesteps=28000, episode_reward=1.44 +/- 2.84
Episode length: 89.94 +/- 34.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.9     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | 1.9      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 308      |
|    fps              | 39       |
|    time_elapsed     | 712      |
|    total_timesteps  | 28297    |
----------------------------------
Eval num_timesteps=28500, episode_reward=1.48 +/- 2.13
Episode length: 85.36 +/- 25.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.4     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.8     |
|    ep_rew_mean      | 1.95     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 312      |
|    fps              | 39       |
|    time_elapsed     | 724      |
|    total_timesteps  | 28821    |
----------------------------------
Eval num_timesteps=29000, episode_reward=1.78 +/- 2.39
Episode length: 95.32 +/- 33.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.3     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.6     |
|    ep_rew_mean      | 1.92     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 316      |
|    fps              | 39       |
|    time_elapsed     | 736      |
|    total_timesteps  | 29167    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 2.04     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 320      |
|    fps              | 40       |
|    time_elapsed     | 736      |
|    total_timesteps  | 29492    |
----------------------------------
Eval num_timesteps=29500, episode_reward=1.40 +/- 1.92
Episode length: 98.88 +/- 37.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 1.4      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | 2.07     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 324      |
|    fps              | 39       |
|    time_elapsed     | 749      |
|    total_timesteps  | 29938    |
----------------------------------
Eval num_timesteps=30000, episode_reward=1.34 +/- 2.85
Episode length: 93.40 +/- 31.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.4     |
|    mean_reward      | 1.34     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.2     |
|    ep_rew_mean      | 2.23     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 328      |
|    fps              | 39       |
|    time_elapsed     | 761      |
|    total_timesteps  | 30387    |
----------------------------------
Eval num_timesteps=30500, episode_reward=1.58 +/- 2.15
Episode length: 101.14 +/- 40.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.9     |
|    ep_rew_mean      | 2.44     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 332      |
|    fps              | 39       |
|    time_elapsed     | 774      |
|    total_timesteps  | 30811    |
----------------------------------
Eval num_timesteps=31000, episode_reward=1.22 +/- 1.68
Episode length: 92.46 +/- 30.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.5     |
|    mean_reward      | 1.22     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.1     |
|    ep_rew_mean      | 2.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 336      |
|    fps              | 39       |
|    time_elapsed     | 785      |
|    total_timesteps  | 31182    |
----------------------------------
Eval num_timesteps=31500, episode_reward=1.52 +/- 2.95
Episode length: 91.48 +/- 36.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.5     |
|    mean_reward      | 1.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 340      |
|    fps              | 39       |
|    time_elapsed     | 797      |
|    total_timesteps  | 31646    |
----------------------------------
Eval num_timesteps=32000, episode_reward=2.76 +/- 3.74
Episode length: 105.56 +/- 41.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.76     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32000    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 2.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 344      |
|    fps              | 39       |
|    time_elapsed     | 811      |
|    total_timesteps  | 32074    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 348      |
|    fps              | 39       |
|    time_elapsed     | 811      |
|    total_timesteps  | 32342    |
----------------------------------
Eval num_timesteps=32500, episode_reward=0.94 +/- 2.01
Episode length: 84.80 +/- 32.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.8     |
|    mean_reward      | 0.94     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.3     |
|    ep_rew_mean      | 2.48     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 352      |
|    fps              | 39       |
|    time_elapsed     | 822      |
|    total_timesteps  | 32696    |
----------------------------------
Eval num_timesteps=33000, episode_reward=1.88 +/- 2.52
Episode length: 88.02 +/- 33.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 2.5      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 356      |
|    fps              | 39       |
|    time_elapsed     | 833      |
|    total_timesteps  | 33024    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.59     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 360      |
|    fps              | 40       |
|    time_elapsed     | 834      |
|    total_timesteps  | 33436    |
----------------------------------
Eval num_timesteps=33500, episode_reward=1.64 +/- 2.40
Episode length: 89.74 +/- 31.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.7     |
|    mean_reward      | 1.64     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 2.68     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 364      |
|    fps              | 40       |
|    time_elapsed     | 846      |
|    total_timesteps  | 33890    |
----------------------------------
Eval num_timesteps=34000, episode_reward=1.10 +/- 1.58
Episode length: 97.72 +/- 34.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.7     |
|    mean_reward      | 1.1      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 368      |
|    fps              | 39       |
|    time_elapsed     | 859      |
|    total_timesteps  | 34364    |
----------------------------------
Eval num_timesteps=34500, episode_reward=1.38 +/- 2.40
Episode length: 100.96 +/- 48.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 1.38     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 372      |
|    fps              | 39       |
|    time_elapsed     | 872      |
|    total_timesteps  | 34780    |
----------------------------------
Eval num_timesteps=35000, episode_reward=1.28 +/- 1.95
Episode length: 94.32 +/- 31.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.3     |
|    mean_reward      | 1.28     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 376      |
|    fps              | 39       |
|    time_elapsed     | 884      |
|    total_timesteps  | 35128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 2.84     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 380      |
|    fps              | 40       |
|    time_elapsed     | 884      |
|    total_timesteps  | 35469    |
----------------------------------
Eval num_timesteps=35500, episode_reward=1.10 +/- 1.76
Episode length: 89.52 +/- 32.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 1.1      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 384      |
|    fps              | 40       |
|    time_elapsed     | 896      |
|    total_timesteps  | 35847    |
----------------------------------
Eval num_timesteps=36000, episode_reward=1.48 +/- 2.00
Episode length: 96.94 +/- 33.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.9     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 388      |
|    fps              | 39       |
|    time_elapsed     | 908      |
|    total_timesteps  | 36287    |
----------------------------------
Eval num_timesteps=36500, episode_reward=1.06 +/- 2.24
Episode length: 82.54 +/- 33.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.5     |
|    mean_reward      | 1.06     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 392      |
|    fps              | 40       |
|    time_elapsed     | 919      |
|    total_timesteps  | 36903    |
----------------------------------
Eval num_timesteps=37000, episode_reward=1.64 +/- 3.00
Episode length: 103.10 +/- 41.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 1.64     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 396      |
|    fps              | 39       |
|    time_elapsed     | 932      |
|    total_timesteps  | 37248    |
----------------------------------
Eval num_timesteps=37500, episode_reward=1.98 +/- 2.89
Episode length: 89.26 +/- 33.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.3     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 400      |
|    fps              | 39       |
|    time_elapsed     | 944      |
|    total_timesteps  | 37683    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 404      |
|    fps              | 40       |
|    time_elapsed     | 944      |
|    total_timesteps  | 37989    |
----------------------------------
Eval num_timesteps=38000, episode_reward=1.60 +/- 3.01
Episode length: 91.94 +/- 34.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.9     |
|    mean_reward      | 1.6      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.12     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 408      |
|    fps              | 40       |
|    time_elapsed     | 956      |
|    total_timesteps  | 38429    |
----------------------------------
Eval num_timesteps=38500, episode_reward=1.38 +/- 1.79
Episode length: 93.98 +/- 28.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 1.38     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.98     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 412      |
|    fps              | 40       |
|    time_elapsed     | 968      |
|    total_timesteps  | 38821    |
----------------------------------
Eval num_timesteps=39000, episode_reward=1.98 +/- 3.87
Episode length: 84.76 +/- 31.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.8     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.19     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 416      |
|    fps              | 40       |
|    time_elapsed     | 979      |
|    total_timesteps  | 39265    |
----------------------------------
Eval num_timesteps=39500, episode_reward=1.16 +/- 2.66
Episode length: 85.66 +/- 33.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.7     |
|    mean_reward      | 1.16     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 420      |
|    fps              | 40       |
|    time_elapsed     | 990      |
|    total_timesteps  | 39737    |
----------------------------------
Eval num_timesteps=40000, episode_reward=2.72 +/- 4.35
Episode length: 95.84 +/- 35.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.8     |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.23     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 424      |
|    fps              | 40       |
|    time_elapsed     | 1003     |
|    total_timesteps  | 40159    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 428      |
|    fps              | 40       |
|    time_elapsed     | 1003     |
|    total_timesteps  | 40459    |
----------------------------------
Eval num_timesteps=40500, episode_reward=1.80 +/- 3.24
Episode length: 95.22 +/- 36.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.2     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 3        |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 432      |
|    fps              | 40       |
|    time_elapsed     | 1015     |
|    total_timesteps  | 40791    |
----------------------------------
Eval num_timesteps=41000, episode_reward=2.08 +/- 3.15
Episode length: 95.12 +/- 38.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.1     |
|    mean_reward      | 2.08     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 436      |
|    fps              | 40       |
|    time_elapsed     | 1028     |
|    total_timesteps  | 41259    |
----------------------------------
Eval num_timesteps=41500, episode_reward=1.76 +/- 3.25
Episode length: 98.84 +/- 40.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.8     |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 440      |
|    fps              | 39       |
|    time_elapsed     | 1040     |
|    total_timesteps  | 41597    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 444      |
|    fps              | 40       |
|    time_elapsed     | 1041     |
|    total_timesteps  | 41991    |
----------------------------------
Eval num_timesteps=42000, episode_reward=1.44 +/- 2.33
Episode length: 92.12 +/- 28.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.1     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 42000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 448      |
|    fps              | 40       |
|    time_elapsed     | 1053     |
|    total_timesteps  | 42367    |
----------------------------------
Eval num_timesteps=42500, episode_reward=1.50 +/- 2.17
Episode length: 89.48 +/- 35.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 1.5      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 42500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.98     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 452      |
|    fps              | 40       |
|    time_elapsed     | 1066     |
|    total_timesteps  | 42828    |
----------------------------------
Eval num_timesteps=43000, episode_reward=1.74 +/- 2.35
Episode length: 91.46 +/- 29.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.5     |
|    mean_reward      | 1.74     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 43000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 456      |
|    fps              | 40       |
|    time_elapsed     | 1079     |
|    total_timesteps  | 43354    |
----------------------------------
Eval num_timesteps=43500, episode_reward=1.70 +/- 3.04
Episode length: 94.24 +/- 31.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.2     |
|    mean_reward      | 1.7      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 43500    |
----------------------------------
Eval num_timesteps=44000, episode_reward=1.68 +/- 3.13
Episode length: 94.60 +/- 40.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 44000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 460      |
|    fps              | 39       |
|    time_elapsed     | 1103     |
|    total_timesteps  | 44040    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 464      |
|    fps              | 40       |
|    time_elapsed     | 1103     |
|    total_timesteps  | 44478    |
----------------------------------
Eval num_timesteps=44500, episode_reward=1.56 +/- 2.25
Episode length: 100.36 +/- 33.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 44500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 468      |
|    fps              | 40       |
|    time_elapsed     | 1116     |
|    total_timesteps  | 44822    |
----------------------------------
Eval num_timesteps=45000, episode_reward=1.82 +/- 2.94
Episode length: 96.42 +/- 34.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.4     |
|    mean_reward      | 1.82     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 45000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 472      |
|    fps              | 40       |
|    time_elapsed     | 1128     |
|    total_timesteps  | 45212    |
----------------------------------
Eval num_timesteps=45500, episode_reward=1.90 +/- 3.15
Episode length: 105.56 +/- 38.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 1.9      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 45500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.12     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 476      |
|    fps              | 39       |
|    time_elapsed     | 1141     |
|    total_timesteps  | 45634    |
----------------------------------
Eval num_timesteps=46000, episode_reward=1.68 +/- 2.89
Episode length: 91.00 +/- 37.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91       |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 46000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 480      |
|    fps              | 39       |
|    time_elapsed     | 1153     |
|    total_timesteps  | 46076    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.12     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 484      |
|    fps              | 40       |
|    time_elapsed     | 1153     |
|    total_timesteps  | 46417    |
----------------------------------
Eval num_timesteps=46500, episode_reward=1.68 +/- 2.87
Episode length: 90.76 +/- 29.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 46500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 488      |
|    fps              | 40       |
|    time_elapsed     | 1165     |
|    total_timesteps  | 46754    |
----------------------------------
Eval num_timesteps=47000, episode_reward=2.44 +/- 3.11
Episode length: 99.18 +/- 34.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.2     |
|    mean_reward      | 2.44     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 47000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.06     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 492      |
|    fps              | 39       |
|    time_elapsed     | 1177     |
|    total_timesteps  | 47094    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 496      |
|    fps              | 40       |
|    time_elapsed     | 1178     |
|    total_timesteps  | 47476    |
----------------------------------
Eval num_timesteps=47500, episode_reward=1.68 +/- 2.15
Episode length: 87.30 +/- 25.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.3     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 47500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 500      |
|    fps              | 40       |
|    time_elapsed     | 1189     |
|    total_timesteps  | 47777    |
----------------------------------
Eval num_timesteps=48000, episode_reward=1.46 +/- 1.89
Episode length: 94.74 +/- 36.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 1.46     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 48000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.26     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 504      |
|    fps              | 40       |
|    time_elapsed     | 1201     |
|    total_timesteps  | 48128    |
----------------------------------
Eval num_timesteps=48500, episode_reward=1.82 +/- 2.57
Episode length: 98.12 +/- 31.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.1     |
|    mean_reward      | 1.82     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 48500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 508      |
|    fps              | 39       |
|    time_elapsed     | 1215     |
|    total_timesteps  | 48534    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.4     |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 512      |
|    fps              | 40       |
|    time_elapsed     | 1215     |
|    total_timesteps  | 48765    |
----------------------------------
Eval num_timesteps=49000, episode_reward=1.48 +/- 3.60
Episode length: 97.76 +/- 39.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.8     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 49000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 516      |
|    fps              | 39       |
|    time_elapsed     | 1227     |
|    total_timesteps  | 49070    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.1     |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 520      |
|    fps              | 40       |
|    time_elapsed     | 1228     |
|    total_timesteps  | 49446    |
----------------------------------
Eval num_timesteps=49500, episode_reward=1.88 +/- 3.55
Episode length: 91.84 +/- 38.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.8     |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 49500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 2.82     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 524      |
|    fps              | 40       |
|    time_elapsed     | 1242     |
|    total_timesteps  | 49857    |
----------------------------------
Eval num_timesteps=50000, episode_reward=2.52 +/- 4.61
Episode length: 92.38 +/- 36.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.4     |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 50000    |
----------------------------------
Eval num_timesteps=50500, episode_reward=2.62 +/- 3.86
Episode length: 101.26 +/- 31.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 50500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 528      |
|    fps              | 39       |
|    time_elapsed     | 1270     |
|    total_timesteps  | 50500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.99     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 532      |
|    fps              | 39       |
|    time_elapsed     | 1270     |
|    total_timesteps  | 50823    |
----------------------------------
Eval num_timesteps=51000, episode_reward=2.28 +/- 3.54
Episode length: 95.28 +/- 42.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.3     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 51000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 536      |
|    fps              | 39       |
|    time_elapsed     | 1284     |
|    total_timesteps  | 51220    |
----------------------------------
Eval num_timesteps=51500, episode_reward=1.56 +/- 2.78
Episode length: 99.90 +/- 29.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.9     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 51500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 2.97     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 540      |
|    fps              | 39       |
|    time_elapsed     | 1297     |
|    total_timesteps  | 51542    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 2.85     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 544      |
|    fps              | 39       |
|    time_elapsed     | 1297     |
|    total_timesteps  | 51890    |
----------------------------------
Eval num_timesteps=52000, episode_reward=1.16 +/- 1.92
Episode length: 91.96 +/- 30.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | 1.16     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 52000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 548      |
|    fps              | 39       |
|    time_elapsed     | 1309     |
|    total_timesteps  | 52266    |
----------------------------------
Eval num_timesteps=52500, episode_reward=1.68 +/- 2.10
Episode length: 101.72 +/- 37.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 52500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 552      |
|    fps              | 39       |
|    time_elapsed     | 1321     |
|    total_timesteps  | 52714    |
----------------------------------
Eval num_timesteps=53000, episode_reward=1.20 +/- 2.19
Episode length: 85.94 +/- 27.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.9     |
|    mean_reward      | 1.2      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 53000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 2.73     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 556      |
|    fps              | 39       |
|    time_elapsed     | 1333     |
|    total_timesteps  | 53083    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 2.47     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 560      |
|    fps              | 40       |
|    time_elapsed     | 1333     |
|    total_timesteps  | 53417    |
----------------------------------
Eval num_timesteps=53500, episode_reward=1.92 +/- 3.27
Episode length: 88.84 +/- 27.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.8     |
|    mean_reward      | 1.92     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 53500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 2.49     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 564      |
|    fps              | 40       |
|    time_elapsed     | 1344     |
|    total_timesteps  | 53830    |
----------------------------------
Eval num_timesteps=54000, episode_reward=1.52 +/- 2.00
Episode length: 93.76 +/- 28.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 1.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 54000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.51     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 568      |
|    fps              | 39       |
|    time_elapsed     | 1356     |
|    total_timesteps  | 54225    |
----------------------------------
Eval num_timesteps=54500, episode_reward=1.18 +/- 2.13
Episode length: 91.38 +/- 32.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.4     |
|    mean_reward      | 1.18     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 54500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.4     |
|    ep_rew_mean      | 2.49     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 572      |
|    fps              | 39       |
|    time_elapsed     | 1368     |
|    total_timesteps  | 54555    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.9     |
|    ep_rew_mean      | 2.57     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 576      |
|    fps              | 40       |
|    time_elapsed     | 1368     |
|    total_timesteps  | 54927    |
----------------------------------
Eval num_timesteps=55000, episode_reward=1.60 +/- 2.31
Episode length: 94.46 +/- 32.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.5     |
|    mean_reward      | 1.6      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 55000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.6     |
|    ep_rew_mean      | 2.41     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 580      |
|    fps              | 40       |
|    time_elapsed     | 1380     |
|    total_timesteps  | 55236    |
----------------------------------
Eval num_timesteps=55500, episode_reward=1.72 +/- 2.40
Episode length: 95.44 +/- 28.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.4     |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 55500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 2.5      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 584      |
|    fps              | 39       |
|    time_elapsed     | 1392     |
|    total_timesteps  | 55566    |
----------------------------------
Eval num_timesteps=56000, episode_reward=1.34 +/- 1.86
Episode length: 86.50 +/- 24.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.5     |
|    mean_reward      | 1.34     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 56000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 2.57     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 588      |
|    fps              | 39       |
|    time_elapsed     | 1403     |
|    total_timesteps  | 56035    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.3     |
|    ep_rew_mean      | 2.69     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 592      |
|    fps              | 40       |
|    time_elapsed     | 1403     |
|    total_timesteps  | 56427    |
----------------------------------
Eval num_timesteps=56500, episode_reward=1.80 +/- 2.38
Episode length: 99.42 +/- 37.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 56500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 596      |
|    fps              | 40       |
|    time_elapsed     | 1416     |
|    total_timesteps  | 56871    |
----------------------------------
Eval num_timesteps=57000, episode_reward=1.22 +/- 1.96
Episode length: 89.88 +/- 29.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.9     |
|    mean_reward      | 1.22     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 57000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.5     |
|    ep_rew_mean      | 2.69     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 600      |
|    fps              | 40       |
|    time_elapsed     | 1427     |
|    total_timesteps  | 57222    |
----------------------------------
Eval num_timesteps=57500, episode_reward=1.48 +/- 2.80
Episode length: 98.58 +/- 30.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 57500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 604      |
|    fps              | 40       |
|    time_elapsed     | 1439     |
|    total_timesteps  | 57631    |
----------------------------------
Eval num_timesteps=58000, episode_reward=1.08 +/- 1.68
Episode length: 88.32 +/- 24.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.3     |
|    mean_reward      | 1.08     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 58000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 608      |
|    fps              | 40       |
|    time_elapsed     | 1450     |
|    total_timesteps  | 58064    |
----------------------------------
Eval num_timesteps=58500, episode_reward=1.50 +/- 2.26
Episode length: 99.82 +/- 37.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.8     |
|    mean_reward      | 1.5      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 58500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 2.89     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 612      |
|    fps              | 39       |
|    time_elapsed     | 1462     |
|    total_timesteps  | 58501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 616      |
|    fps              | 40       |
|    time_elapsed     | 1463     |
|    total_timesteps  | 58992    |
----------------------------------
Eval num_timesteps=59000, episode_reward=1.56 +/- 2.68
Episode length: 99.30 +/- 32.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 59000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 2.98     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 620      |
|    fps              | 40       |
|    time_elapsed     | 1475     |
|    total_timesteps  | 59362    |
----------------------------------
Eval num_timesteps=59500, episode_reward=1.56 +/- 2.38
Episode length: 89.38 +/- 31.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.4     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 59500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.27     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 624      |
|    fps              | 40       |
|    time_elapsed     | 1486     |
|    total_timesteps  | 59912    |
----------------------------------
Eval num_timesteps=60000, episode_reward=1.90 +/- 2.99
Episode length: 87.10 +/- 28.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.1     |
|    mean_reward      | 1.9      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 60000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.1     |
|    ep_rew_mean      | 2.96     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 628      |
|    fps              | 40       |
|    time_elapsed     | 1497     |
|    total_timesteps  | 60213    |
----------------------------------
Eval num_timesteps=60500, episode_reward=1.52 +/- 2.08
Episode length: 98.92 +/- 31.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 1.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 60500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 3.04     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 632      |
|    fps              | 40       |
|    time_elapsed     | 1509     |
|    total_timesteps  | 60570    |
----------------------------------
Eval num_timesteps=61000, episode_reward=1.62 +/- 2.86
Episode length: 93.80 +/- 28.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 61000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 636      |
|    fps              | 40       |
|    time_elapsed     | 1521     |
|    total_timesteps  | 61101    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 640      |
|    fps              | 40       |
|    time_elapsed     | 1521     |
|    total_timesteps  | 61384    |
----------------------------------
Eval num_timesteps=61500, episode_reward=1.46 +/- 2.87
Episode length: 92.28 +/- 22.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.3     |
|    mean_reward      | 1.46     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 61500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 644      |
|    fps              | 40       |
|    time_elapsed     | 1533     |
|    total_timesteps  | 61781    |
----------------------------------
Eval num_timesteps=62000, episode_reward=1.94 +/- 2.76
Episode length: 91.88 +/- 29.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.9     |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 62000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 3.44     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 648      |
|    fps              | 40       |
|    time_elapsed     | 1544     |
|    total_timesteps  | 62132    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 652      |
|    fps              | 40       |
|    time_elapsed     | 1545     |
|    total_timesteps  | 62436    |
----------------------------------
Eval num_timesteps=62500, episode_reward=1.72 +/- 3.03
Episode length: 87.32 +/- 28.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.3     |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 62500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 3.54     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 656      |
|    fps              | 40       |
|    time_elapsed     | 1557     |
|    total_timesteps  | 62888    |
----------------------------------
Eval num_timesteps=63000, episode_reward=1.12 +/- 1.91
Episode length: 94.48 +/- 34.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.5     |
|    mean_reward      | 1.12     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 63000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.55     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 660      |
|    fps              | 40       |
|    time_elapsed     | 1571     |
|    total_timesteps  | 63434    |
----------------------------------
Eval num_timesteps=63500, episode_reward=2.28 +/- 3.07
Episode length: 92.92 +/- 31.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.9     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 63500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 3.49     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 664      |
|    fps              | 40       |
|    time_elapsed     | 1585     |
|    total_timesteps  | 63747    |
----------------------------------
Eval num_timesteps=64000, episode_reward=1.68 +/- 2.37
Episode length: 96.84 +/- 29.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.8     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 64000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 3.47     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 668      |
|    fps              | 40       |
|    time_elapsed     | 1597     |
|    total_timesteps  | 64048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 3.53     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 672      |
|    fps              | 40       |
|    time_elapsed     | 1597     |
|    total_timesteps  | 64396    |
----------------------------------
Eval num_timesteps=64500, episode_reward=1.54 +/- 2.27
Episode length: 89.98 +/- 24.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90       |
|    mean_reward      | 1.54     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 64500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 3.44     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 676      |
|    fps              | 40       |
|    time_elapsed     | 1608     |
|    total_timesteps  | 64771    |
----------------------------------
Eval num_timesteps=65000, episode_reward=1.20 +/- 2.16
Episode length: 85.08 +/- 22.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.1     |
|    mean_reward      | 1.2      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 65000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.37     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 680      |
|    fps              | 40       |
|    time_elapsed     | 1619     |
|    total_timesteps  | 65289    |
----------------------------------
Eval num_timesteps=65500, episode_reward=1.76 +/- 2.34
Episode length: 102.32 +/- 40.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 65500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 684      |
|    fps              | 40       |
|    time_elapsed     | 1632     |
|    total_timesteps  | 65657    |
----------------------------------
Eval num_timesteps=66000, episode_reward=1.54 +/- 2.11
Episode length: 86.28 +/- 24.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.3     |
|    mean_reward      | 1.54     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 66000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.55     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 688      |
|    fps              | 40       |
|    time_elapsed     | 1643     |
|    total_timesteps  | 66140    |
----------------------------------
Eval num_timesteps=66500, episode_reward=1.48 +/- 2.56
Episode length: 98.24 +/- 32.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.2     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 66500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.47     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 692      |
|    fps              | 40       |
|    time_elapsed     | 1655     |
|    total_timesteps  | 66592    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 696      |
|    fps              | 40       |
|    time_elapsed     | 1656     |
|    total_timesteps  | 66844    |
----------------------------------
Eval num_timesteps=67000, episode_reward=1.62 +/- 2.63
Episode length: 98.30 +/- 36.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.3     |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 67000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 700      |
|    fps              | 40       |
|    time_elapsed     | 1668     |
|    total_timesteps  | 67210    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 704      |
|    fps              | 40       |
|    time_elapsed     | 1668     |
|    total_timesteps  | 67464    |
----------------------------------
Eval num_timesteps=67500, episode_reward=1.78 +/- 2.26
Episode length: 94.24 +/- 29.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.2     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 67500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 708      |
|    fps              | 40       |
|    time_elapsed     | 1680     |
|    total_timesteps  | 67858    |
----------------------------------
Eval num_timesteps=68000, episode_reward=2.20 +/- 2.97
Episode length: 98.12 +/- 26.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.1     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 68000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 712      |
|    fps              | 40       |
|    time_elapsed     | 1692     |
|    total_timesteps  | 68241    |
----------------------------------
Eval num_timesteps=68500, episode_reward=2.00 +/- 3.01
Episode length: 93.58 +/- 36.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.6     |
|    mean_reward      | 2        |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 68500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 3.38     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 716      |
|    fps              | 40       |
|    time_elapsed     | 1703     |
|    total_timesteps  | 68748    |
----------------------------------
Eval num_timesteps=69000, episode_reward=1.68 +/- 2.35
Episode length: 85.68 +/- 32.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.7     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 69000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 3.37     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 720      |
|    fps              | 40       |
|    time_elapsed     | 1714     |
|    total_timesteps  | 69193    |
----------------------------------
Eval num_timesteps=69500, episode_reward=1.56 +/- 2.51
Episode length: 88.60 +/- 35.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.6     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 69500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 724      |
|    fps              | 40       |
|    time_elapsed     | 1725     |
|    total_timesteps  | 69530    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 2.96     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 728      |
|    fps              | 40       |
|    time_elapsed     | 1726     |
|    total_timesteps  | 69894    |
----------------------------------
Eval num_timesteps=70000, episode_reward=2.20 +/- 4.54
Episode length: 92.24 +/- 31.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.2     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 70000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 2.8      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 732      |
|    fps              | 40       |
|    time_elapsed     | 1739     |
|    total_timesteps  | 70249    |
----------------------------------
Eval num_timesteps=70500, episode_reward=0.96 +/- 1.61
Episode length: 100.94 +/- 28.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 0.96     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 70500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 2.27     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 736      |
|    fps              | 40       |
|    time_elapsed     | 1754     |
|    total_timesteps  | 70641    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 2.32     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 740      |
|    fps              | 40       |
|    time_elapsed     | 1755     |
|    total_timesteps  | 70966    |
----------------------------------
Eval num_timesteps=71000, episode_reward=1.54 +/- 2.31
Episode length: 99.88 +/- 33.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.9     |
|    mean_reward      | 1.54     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 71000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.1     |
|    ep_rew_mean      | 2.59     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 744      |
|    fps              | 40       |
|    time_elapsed     | 1769     |
|    total_timesteps  | 71488    |
----------------------------------
Eval num_timesteps=71500, episode_reward=1.26 +/- 2.99
Episode length: 93.52 +/- 30.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.5     |
|    mean_reward      | 1.26     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 71500    |
----------------------------------
Eval num_timesteps=72000, episode_reward=2.02 +/- 3.11
Episode length: 94.30 +/- 24.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.3     |
|    mean_reward      | 2.02     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 72000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 2.84     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 748      |
|    fps              | 40       |
|    time_elapsed     | 1794     |
|    total_timesteps  | 72094    |
----------------------------------
Eval num_timesteps=72500, episode_reward=1.72 +/- 2.64
Episode length: 98.88 +/- 46.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 72500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 752      |
|    fps              | 40       |
|    time_elapsed     | 1807     |
|    total_timesteps  | 72539    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.73     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 756      |
|    fps              | 40       |
|    time_elapsed     | 1807     |
|    total_timesteps  | 72983    |
----------------------------------
Eval num_timesteps=73000, episode_reward=2.34 +/- 3.23
Episode length: 95.10 +/- 32.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.1     |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 73000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.4     |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 760      |
|    fps              | 40       |
|    time_elapsed     | 1819     |
|    total_timesteps  | 73378    |
----------------------------------
Eval num_timesteps=73500, episode_reward=1.30 +/- 2.32
Episode length: 88.86 +/- 29.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.9     |
|    mean_reward      | 1.3      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 73500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 764      |
|    fps              | 40       |
|    time_elapsed     | 1830     |
|    total_timesteps  | 73764    |
----------------------------------
Eval num_timesteps=74000, episode_reward=1.40 +/- 2.08
Episode length: 86.88 +/- 27.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.9     |
|    mean_reward      | 1.4      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 74000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.98     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 768      |
|    fps              | 40       |
|    time_elapsed     | 1841     |
|    total_timesteps  | 74130    |
----------------------------------
Eval num_timesteps=74500, episode_reward=1.70 +/- 2.51
Episode length: 100.00 +/- 33.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 1.7      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 74500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 772      |
|    fps              | 40       |
|    time_elapsed     | 1854     |
|    total_timesteps  | 74688    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 776      |
|    fps              | 40       |
|    time_elapsed     | 1854     |
|    total_timesteps  | 74946    |
----------------------------------
Eval num_timesteps=75000, episode_reward=1.42 +/- 2.13
Episode length: 92.52 +/- 29.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.5     |
|    mean_reward      | 1.42     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 75000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 3.19     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 780      |
|    fps              | 40       |
|    time_elapsed     | 1865     |
|    total_timesteps  | 75235    |
----------------------------------
Eval num_timesteps=75500, episode_reward=2.16 +/- 3.70
Episode length: 104.24 +/- 45.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.16     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 75500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 784      |
|    fps              | 40       |
|    time_elapsed     | 1878     |
|    total_timesteps  | 75663    |
----------------------------------
Eval num_timesteps=76000, episode_reward=1.50 +/- 2.33
Episode length: 91.34 +/- 26.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.3     |
|    mean_reward      | 1.5      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 76000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 788      |
|    fps              | 40       |
|    time_elapsed     | 1889     |
|    total_timesteps  | 76190    |
----------------------------------
Eval num_timesteps=76500, episode_reward=2.00 +/- 2.90
Episode length: 94.98 +/- 39.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95       |
|    mean_reward      | 2        |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 76500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 792      |
|    fps              | 40       |
|    time_elapsed     | 1901     |
|    total_timesteps  | 76503    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.27     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 796      |
|    fps              | 40       |
|    time_elapsed     | 1902     |
|    total_timesteps  | 76968    |
----------------------------------
Eval num_timesteps=77000, episode_reward=1.40 +/- 2.37
Episode length: 99.44 +/- 36.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 1.4      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 77000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 800      |
|    fps              | 40       |
|    time_elapsed     | 1914     |
|    total_timesteps  | 77394    |
----------------------------------
Eval num_timesteps=77500, episode_reward=1.42 +/- 2.23
Episode length: 85.80 +/- 33.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.8     |
|    mean_reward      | 1.42     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 77500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 804      |
|    fps              | 40       |
|    time_elapsed     | 1924     |
|    total_timesteps  | 77818    |
----------------------------------
Eval num_timesteps=78000, episode_reward=1.20 +/- 2.12
Episode length: 97.26 +/- 30.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 1.2      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 78000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.47     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 808      |
|    fps              | 40       |
|    time_elapsed     | 1937     |
|    total_timesteps  | 78273    |
----------------------------------
Eval num_timesteps=78500, episode_reward=1.88 +/- 2.80
Episode length: 100.38 +/- 30.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 78500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 812      |
|    fps              | 40       |
|    time_elapsed     | 1949     |
|    total_timesteps  | 78665    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.49     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 816      |
|    fps              | 40       |
|    time_elapsed     | 1949     |
|    total_timesteps  | 78991    |
----------------------------------
Eval num_timesteps=79000, episode_reward=1.78 +/- 2.93
Episode length: 101.10 +/- 38.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 79000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 820      |
|    fps              | 40       |
|    time_elapsed     | 1962     |
|    total_timesteps  | 79321    |
----------------------------------
Eval num_timesteps=79500, episode_reward=1.72 +/- 2.62
Episode length: 91.68 +/- 35.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.7     |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 79500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 824      |
|    fps              | 40       |
|    time_elapsed     | 1973     |
|    total_timesteps  | 79721    |
----------------------------------
Eval num_timesteps=80000, episode_reward=1.48 +/- 2.03
Episode length: 97.80 +/- 35.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.8     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 80000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.54     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 828      |
|    fps              | 40       |
|    time_elapsed     | 1986     |
|    total_timesteps  | 80125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.61     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 832      |
|    fps              | 40       |
|    time_elapsed     | 1986     |
|    total_timesteps  | 80444    |
----------------------------------
Eval num_timesteps=80500, episode_reward=1.34 +/- 2.11
Episode length: 94.44 +/- 39.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 1.34     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 80500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 836      |
|    fps              | 40       |
|    time_elapsed     | 1998     |
|    total_timesteps  | 80856    |
----------------------------------
Eval num_timesteps=81000, episode_reward=2.00 +/- 2.83
Episode length: 95.24 +/- 34.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.2     |
|    mean_reward      | 2        |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 81000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 840      |
|    fps              | 40       |
|    time_elapsed     | 2010     |
|    total_timesteps  | 81313    |
----------------------------------
Eval num_timesteps=81500, episode_reward=1.70 +/- 2.60
Episode length: 95.10 +/- 34.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.1     |
|    mean_reward      | 1.7      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 81500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 844      |
|    fps              | 40       |
|    time_elapsed     | 2021     |
|    total_timesteps  | 81611    |
----------------------------------
Eval num_timesteps=82000, episode_reward=1.94 +/- 2.18
Episode length: 103.24 +/- 30.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 82000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 848      |
|    fps              | 40       |
|    time_elapsed     | 2034     |
|    total_timesteps  | 82026    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 3.38     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 852      |
|    fps              | 40       |
|    time_elapsed     | 2034     |
|    total_timesteps  | 82375    |
----------------------------------
Eval num_timesteps=82500, episode_reward=1.52 +/- 2.87
Episode length: 101.94 +/- 43.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 1.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 82500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 3.43     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 856      |
|    fps              | 40       |
|    time_elapsed     | 2049     |
|    total_timesteps  | 82914    |
----------------------------------
Eval num_timesteps=83000, episode_reward=2.06 +/- 2.83
Episode length: 94.02 +/- 31.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 83000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 860      |
|    fps              | 40       |
|    time_elapsed     | 2061     |
|    total_timesteps  | 83228    |
----------------------------------
Eval num_timesteps=83500, episode_reward=1.64 +/- 3.52
Episode length: 93.34 +/- 35.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.3     |
|    mean_reward      | 1.64     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 83500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 864      |
|    fps              | 40       |
|    time_elapsed     | 2073     |
|    total_timesteps  | 83659    |
----------------------------------
Eval num_timesteps=84000, episode_reward=1.02 +/- 1.66
Episode length: 86.08 +/- 24.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.1     |
|    mean_reward      | 1.02     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 84000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 3.23     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 868      |
|    fps              | 40       |
|    time_elapsed     | 2083     |
|    total_timesteps  | 84096    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 872      |
|    fps              | 40       |
|    time_elapsed     | 2084     |
|    total_timesteps  | 84430    |
----------------------------------
Eval num_timesteps=84500, episode_reward=2.10 +/- 3.11
Episode length: 97.38 +/- 34.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.4     |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 84500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 876      |
|    fps              | 40       |
|    time_elapsed     | 2096     |
|    total_timesteps  | 84843    |
----------------------------------
Eval num_timesteps=85000, episode_reward=2.24 +/- 3.12
Episode length: 100.06 +/- 35.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 2.24     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 85000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.4     |
|    ep_rew_mean      | 3.27     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 880      |
|    fps              | 40       |
|    time_elapsed     | 2108     |
|    total_timesteps  | 85171    |
----------------------------------
Eval num_timesteps=85500, episode_reward=2.08 +/- 3.29
Episode length: 97.28 +/- 39.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 2.08     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 85500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.56     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 884      |
|    fps              | 40       |
|    time_elapsed     | 2120     |
|    total_timesteps  | 85813    |
----------------------------------
Eval num_timesteps=86000, episode_reward=1.30 +/- 3.07
Episode length: 95.62 +/- 35.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.6     |
|    mean_reward      | 1.3      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 86000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 3.42     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 888      |
|    fps              | 40       |
|    time_elapsed     | 2132     |
|    total_timesteps  | 86086    |
----------------------------------
Eval num_timesteps=86500, episode_reward=1.56 +/- 2.42
Episode length: 90.74 +/- 32.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.7     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 86500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.46     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 892      |
|    fps              | 40       |
|    time_elapsed     | 2143     |
|    total_timesteps  | 86508    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 896      |
|    fps              | 40       |
|    time_elapsed     | 2144     |
|    total_timesteps  | 86855    |
----------------------------------
Eval num_timesteps=87000, episode_reward=1.82 +/- 2.38
Episode length: 98.90 +/- 36.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 1.82     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 87000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 3.56     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 900      |
|    fps              | 40       |
|    time_elapsed     | 2156     |
|    total_timesteps  | 87370    |
----------------------------------
Eval num_timesteps=87500, episode_reward=1.50 +/- 2.59
Episode length: 87.58 +/- 31.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.6     |
|    mean_reward      | 1.5      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 87500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 3.37     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 904      |
|    fps              | 40       |
|    time_elapsed     | 2167     |
|    total_timesteps  | 87729    |
----------------------------------
Eval num_timesteps=88000, episode_reward=2.10 +/- 3.08
Episode length: 100.74 +/- 42.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 88000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 908      |
|    fps              | 40       |
|    time_elapsed     | 2179     |
|    total_timesteps  | 88037    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 912      |
|    fps              | 40       |
|    time_elapsed     | 2179     |
|    total_timesteps  | 88385    |
----------------------------------
Eval num_timesteps=88500, episode_reward=2.36 +/- 3.42
Episode length: 102.36 +/- 32.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 88500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 916      |
|    fps              | 40       |
|    time_elapsed     | 2192     |
|    total_timesteps  | 88668    |
----------------------------------
Eval num_timesteps=89000, episode_reward=1.54 +/- 3.05
Episode length: 95.40 +/- 28.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.4     |
|    mean_reward      | 1.54     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 89000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 920      |
|    fps              | 40       |
|    time_elapsed     | 2203     |
|    total_timesteps  | 89171    |
----------------------------------
Eval num_timesteps=89500, episode_reward=1.38 +/- 2.17
Episode length: 94.86 +/- 37.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.9     |
|    mean_reward      | 1.38     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 89500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 924      |
|    fps              | 40       |
|    time_elapsed     | 2215     |
|    total_timesteps  | 89529    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 928      |
|    fps              | 40       |
|    time_elapsed     | 2215     |
|    total_timesteps  | 89971    |
----------------------------------
Eval num_timesteps=90000, episode_reward=1.72 +/- 3.03
Episode length: 88.42 +/- 27.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.4     |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 90000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 932      |
|    fps              | 40       |
|    time_elapsed     | 2226     |
|    total_timesteps  | 90319    |
----------------------------------
Eval num_timesteps=90500, episode_reward=2.00 +/- 2.62
Episode length: 93.56 +/- 34.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.6     |
|    mean_reward      | 2        |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 90500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 3.27     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 936      |
|    fps              | 40       |
|    time_elapsed     | 2237     |
|    total_timesteps  | 90765    |
----------------------------------
Eval num_timesteps=91000, episode_reward=1.96 +/- 3.17
Episode length: 96.32 +/- 36.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 91000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 940      |
|    fps              | 40       |
|    time_elapsed     | 2249     |
|    total_timesteps  | 91231    |
----------------------------------
Eval num_timesteps=91500, episode_reward=1.56 +/- 2.51
Episode length: 93.62 +/- 39.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.6     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 91500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 944      |
|    fps              | 40       |
|    time_elapsed     | 2261     |
|    total_timesteps  | 91730    |
----------------------------------
Eval num_timesteps=92000, episode_reward=2.26 +/- 2.97
Episode length: 99.20 +/- 27.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.2     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 92000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 948      |
|    fps              | 40       |
|    time_elapsed     | 2273     |
|    total_timesteps  | 92088    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.28     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 952      |
|    fps              | 40       |
|    time_elapsed     | 2274     |
|    total_timesteps  | 92370    |
----------------------------------
Eval num_timesteps=92500, episode_reward=1.50 +/- 1.88
Episode length: 97.08 +/- 43.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.1     |
|    mean_reward      | 1.5      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 92500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 956      |
|    fps              | 40       |
|    time_elapsed     | 2286     |
|    total_timesteps  | 92822    |
----------------------------------
Eval num_timesteps=93000, episode_reward=2.18 +/- 3.73
Episode length: 91.32 +/- 29.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.3     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 93000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 960      |
|    fps              | 40       |
|    time_elapsed     | 2297     |
|    total_timesteps  | 93205    |
----------------------------------
Eval num_timesteps=93500, episode_reward=2.26 +/- 3.33
Episode length: 93.22 +/- 30.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.2     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 93500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 964      |
|    fps              | 40       |
|    time_elapsed     | 2309     |
|    total_timesteps  | 93664    |
----------------------------------
Eval num_timesteps=94000, episode_reward=1.72 +/- 2.98
Episode length: 98.70 +/- 37.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 94000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 3.59     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 968      |
|    fps              | 40       |
|    time_elapsed     | 2321     |
|    total_timesteps  | 94088    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 972      |
|    fps              | 40       |
|    time_elapsed     | 2321     |
|    total_timesteps  | 94420    |
----------------------------------
Eval num_timesteps=94500, episode_reward=2.20 +/- 3.39
Episode length: 90.36 +/- 34.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.4     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 94500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 3.31     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 976      |
|    fps              | 40       |
|    time_elapsed     | 2332     |
|    total_timesteps  | 94734    |
----------------------------------
Eval num_timesteps=95000, episode_reward=1.68 +/- 2.48
Episode length: 96.20 +/- 28.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 95000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 3.28     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 980      |
|    fps              | 40       |
|    time_elapsed     | 2344     |
|    total_timesteps  | 95141    |
----------------------------------
Eval num_timesteps=95500, episode_reward=0.94 +/- 1.46
Episode length: 91.42 +/- 31.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.4     |
|    mean_reward      | 0.94     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 95500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.99     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 984      |
|    fps              | 40       |
|    time_elapsed     | 2356     |
|    total_timesteps  | 95893    |
----------------------------------
Eval num_timesteps=96000, episode_reward=1.60 +/- 2.32
Episode length: 97.34 +/- 27.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 1.6      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 96000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.08     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 988      |
|    fps              | 40       |
|    time_elapsed     | 2368     |
|    total_timesteps  | 96342    |
----------------------------------
Eval num_timesteps=96500, episode_reward=1.28 +/- 2.22
Episode length: 91.68 +/- 30.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.7     |
|    mean_reward      | 1.28     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 96500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.19     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 992      |
|    fps              | 40       |
|    time_elapsed     | 2379     |
|    total_timesteps  | 96934    |
----------------------------------
Eval num_timesteps=97000, episode_reward=1.82 +/- 3.54
Episode length: 93.92 +/- 34.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.9     |
|    mean_reward      | 1.82     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 97000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 996      |
|    fps              | 40       |
|    time_elapsed     | 2391     |
|    total_timesteps  | 97217    |
----------------------------------
Eval num_timesteps=97500, episode_reward=1.32 +/- 2.54
Episode length: 92.88 +/- 36.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.9     |
|    mean_reward      | 1.32     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 97500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 40       |
|    time_elapsed     | 2403     |
|    total_timesteps  | 97555    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 40       |
|    time_elapsed     | 2403     |
|    total_timesteps  | 97980    |
----------------------------------
Eval num_timesteps=98000, episode_reward=1.64 +/- 3.13
Episode length: 95.74 +/- 33.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.7     |
|    mean_reward      | 1.64     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 98000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 40       |
|    time_elapsed     | 2415     |
|    total_timesteps  | 98382    |
----------------------------------
Eval num_timesteps=98500, episode_reward=1.78 +/- 2.51
Episode length: 99.46 +/- 33.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.5     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 98500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.52     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 40       |
|    time_elapsed     | 2427     |
|    total_timesteps  | 98903    |
----------------------------------
Eval num_timesteps=99000, episode_reward=2.14 +/- 3.27
Episode length: 92.30 +/- 33.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.3     |
|    mean_reward      | 2.14     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 99000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 40       |
|    time_elapsed     | 2439     |
|    total_timesteps  | 99200    |
----------------------------------
Eval num_timesteps=99500, episode_reward=2.10 +/- 3.14
Episode length: 93.36 +/- 39.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.4     |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 99500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 40       |
|    time_elapsed     | 2450     |
|    total_timesteps  | 99668    |
----------------------------------
Eval num_timesteps=100000, episode_reward=1.86 +/- 3.24
Episode length: 94.86 +/- 36.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.9     |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 100000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 40       |
|    time_elapsed     | 2469     |
|    total_timesteps  | 100083   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0546   |
|    n_updates        | 20       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 40       |
|    time_elapsed     | 2470     |
|    total_timesteps  | 100407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00764  |
|    n_updates        | 101      |
----------------------------------
Eval num_timesteps=100500, episode_reward=2.06 +/- 2.83
Episode length: 99.12 +/- 33.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.1     |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 7.1e-05  |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 40       |
|    time_elapsed     | 2482     |
|    total_timesteps  | 100677   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0393   |
|    n_updates        | 169      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.06     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 40       |
|    time_elapsed     | 2483     |
|    total_timesteps  | 100907   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0542   |
|    n_updates        | 226      |
----------------------------------
Eval num_timesteps=101000, episode_reward=2.24 +/- 3.20
Episode length: 73.60 +/- 22.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.6     |
|    mean_reward      | 2.24     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 249      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 2.93     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 40       |
|    time_elapsed     | 2492     |
|    total_timesteps  | 101216   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00806  |
|    n_updates        | 303      |
----------------------------------
Eval num_timesteps=101500, episode_reward=1.48 +/- 2.77
Episode length: 81.10 +/- 30.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.1     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0926   |
|    n_updates        | 374      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 2.96     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 40       |
|    time_elapsed     | 2502     |
|    total_timesteps  | 101633   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0546   |
|    n_updates        | 408      |
----------------------------------
Eval num_timesteps=102000, episode_reward=1.52 +/- 2.50
Episode length: 88.10 +/- 28.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.1     |
|    mean_reward      | 1.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 499      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.4     |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 40       |
|    time_elapsed     | 2513     |
|    total_timesteps  | 102025   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000269 |
|    n_updates        | 506      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 40       |
|    time_elapsed     | 2514     |
|    total_timesteps  | 102442   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00808  |
|    n_updates        | 610      |
----------------------------------
Eval num_timesteps=102500, episode_reward=3.46 +/- 4.34
Episode length: 109.36 +/- 43.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.46     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 8.96e-05 |
|    n_updates        | 624      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 40       |
|    time_elapsed     | 2527     |
|    total_timesteps  | 102897   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0392   |
|    n_updates        | 724      |
----------------------------------
Eval num_timesteps=103000, episode_reward=2.56 +/- 3.37
Episode length: 89.50 +/- 28.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000369 |
|    n_updates        | 749      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 40       |
|    time_elapsed     | 2539     |
|    total_timesteps  | 103255   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 813      |
----------------------------------
Eval num_timesteps=103500, episode_reward=3.00 +/- 3.93
Episode length: 95.34 +/- 37.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.3     |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0383   |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 2.96     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 40       |
|    time_elapsed     | 2550     |
|    total_timesteps  | 103630   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00757  |
|    n_updates        | 907      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 40       |
|    time_elapsed     | 2551     |
|    total_timesteps  | 103961   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000407 |
|    n_updates        | 990      |
----------------------------------
Eval num_timesteps=104000, episode_reward=2.94 +/- 4.91
Episode length: 103.78 +/- 43.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000188 |
|    n_updates        | 999      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 2.84     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 40       |
|    time_elapsed     | 2563     |
|    total_timesteps  | 104377   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000156 |
|    n_updates        | 1094     |
----------------------------------
Eval num_timesteps=104500, episode_reward=2.14 +/- 5.04
Episode length: 92.08 +/- 40.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.1     |
|    mean_reward      | 2.14     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0462   |
|    n_updates        | 1124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.91     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 40       |
|    time_elapsed     | 2574     |
|    total_timesteps  | 104817   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.053    |
|    n_updates        | 1204     |
----------------------------------
Eval num_timesteps=105000, episode_reward=1.98 +/- 2.45
Episode length: 90.68 +/- 29.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.7     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00707  |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.04     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 40       |
|    time_elapsed     | 2585     |
|    total_timesteps  | 105247   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 1311     |
----------------------------------
Eval num_timesteps=105500, episode_reward=1.56 +/- 2.52
Episode length: 73.86 +/- 21.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.9     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000176 |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 2.89     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 40       |
|    time_elapsed     | 2595     |
|    total_timesteps  | 105643   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00686  |
|    n_updates        | 1410     |
----------------------------------
Eval num_timesteps=106000, episode_reward=2.10 +/- 3.69
Episode length: 86.10 +/- 34.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.1     |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000196 |
|    n_updates        | 1499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 3.19     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 40       |
|    time_elapsed     | 2606     |
|    total_timesteps  | 106168   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 1541     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 2.99     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 40       |
|    time_elapsed     | 2607     |
|    total_timesteps  | 106466   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0385   |
|    n_updates        | 1616     |
----------------------------------
Eval num_timesteps=106500, episode_reward=1.44 +/- 2.12
Episode length: 87.38 +/- 30.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.4     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00757  |
|    n_updates        | 1624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 40       |
|    time_elapsed     | 2619     |
|    total_timesteps  | 106889   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0774   |
|    n_updates        | 1722     |
----------------------------------
Eval num_timesteps=107000, episode_reward=2.26 +/- 2.41
Episode length: 94.04 +/- 31.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000164 |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 3        |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 40       |
|    time_elapsed     | 2631     |
|    total_timesteps  | 107284   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0773   |
|    n_updates        | 1820     |
----------------------------------
Eval num_timesteps=107500, episode_reward=1.68 +/- 2.39
Episode length: 97.42 +/- 33.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.4     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0387   |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.9     |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 40       |
|    time_elapsed     | 2643     |
|    total_timesteps  | 107670   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.044    |
|    n_updates        | 1917     |
----------------------------------
Eval num_timesteps=108000, episode_reward=2.18 +/- 3.59
Episode length: 77.34 +/- 26.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.3     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000237 |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 40       |
|    time_elapsed     | 2653     |
|    total_timesteps  | 108171   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00035  |
|    n_updates        | 2042     |
----------------------------------
Eval num_timesteps=108500, episode_reward=2.34 +/- 3.31
Episode length: 95.64 +/- 34.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.6     |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000429 |
|    n_updates        | 2124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.1     |
|    ep_rew_mean      | 2.99     |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 40       |
|    time_elapsed     | 2665     |
|    total_timesteps  | 108611   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000254 |
|    n_updates        | 2152     |
----------------------------------
Eval num_timesteps=109000, episode_reward=1.86 +/- 2.65
Episode length: 89.54 +/- 34.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000289 |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.44     |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 40       |
|    time_elapsed     | 2676     |
|    total_timesteps  | 109404   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0458   |
|    n_updates        | 2350     |
----------------------------------
Eval num_timesteps=109500, episode_reward=1.62 +/- 2.72
Episode length: 83.42 +/- 27.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.4     |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00766  |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.42     |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 40       |
|    time_elapsed     | 2687     |
|    total_timesteps  | 109748   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0383   |
|    n_updates        | 2436     |
----------------------------------
Eval num_timesteps=110000, episode_reward=1.68 +/- 2.19
Episode length: 99.32 +/- 38.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000353 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.42     |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 40       |
|    time_elapsed     | 2699     |
|    total_timesteps  | 110176   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0495   |
|    n_updates        | 2543     |
----------------------------------
Eval num_timesteps=110500, episode_reward=2.06 +/- 2.98
Episode length: 90.30 +/- 27.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.3     |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000688 |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.53     |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 40       |
|    time_elapsed     | 2710     |
|    total_timesteps  | 110631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0065   |
|    n_updates        | 2657     |
----------------------------------
Eval num_timesteps=111000, episode_reward=1.94 +/- 2.98
Episode length: 88.74 +/- 36.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.7     |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000399 |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 40       |
|    time_elapsed     | 2720     |
|    total_timesteps  | 111010   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0463   |
|    n_updates        | 2752     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 40       |
|    time_elapsed     | 2722     |
|    total_timesteps  | 111454   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.045    |
|    n_updates        | 2863     |
----------------------------------
Eval num_timesteps=111500, episode_reward=2.66 +/- 3.17
Episode length: 86.24 +/- 25.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.2     |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000968 |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 40       |
|    time_elapsed     | 2733     |
|    total_timesteps  | 111835   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0377   |
|    n_updates        | 2958     |
----------------------------------
Eval num_timesteps=112000, episode_reward=1.68 +/- 2.56
Episode length: 74.04 +/- 20.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74       |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0393   |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 40       |
|    time_elapsed     | 2742     |
|    total_timesteps  | 112238   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.039    |
|    n_updates        | 3059     |
----------------------------------
Eval num_timesteps=112500, episode_reward=2.18 +/- 3.64
Episode length: 102.40 +/- 49.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0531   |
|    n_updates        | 3124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 40       |
|    time_elapsed     | 2754     |
|    total_timesteps  | 112611   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00102  |
|    n_updates        | 3152     |
----------------------------------
Eval num_timesteps=113000, episode_reward=1.62 +/- 2.39
Episode length: 84.34 +/- 33.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.3     |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000884 |
|    n_updates        | 3249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 40       |
|    time_elapsed     | 2765     |
|    total_timesteps  | 113056   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00086  |
|    n_updates        | 3263     |
----------------------------------
Eval num_timesteps=113500, episode_reward=1.66 +/- 2.98
Episode length: 87.56 +/- 33.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.6     |
|    mean_reward      | 1.66     |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.038    |
|    n_updates        | 3374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 40       |
|    time_elapsed     | 2776     |
|    total_timesteps  | 113559   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00735  |
|    n_updates        | 3389     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 41       |
|    time_elapsed     | 2777     |
|    total_timesteps  | 113945   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 3486     |
----------------------------------
Eval num_timesteps=114000, episode_reward=2.04 +/- 2.53
Episode length: 74.08 +/- 17.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.1     |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00105  |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 41       |
|    time_elapsed     | 2786     |
|    total_timesteps  | 114262   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00723  |
|    n_updates        | 3565     |
----------------------------------
Eval num_timesteps=114500, episode_reward=2.22 +/- 3.06
Episode length: 87.74 +/- 26.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.7     |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 3624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 41       |
|    time_elapsed     | 2797     |
|    total_timesteps  | 114948   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000973 |
|    n_updates        | 3736     |
----------------------------------
Eval num_timesteps=115000, episode_reward=1.86 +/- 2.32
Episode length: 86.42 +/- 30.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.4     |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0768   |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 41       |
|    time_elapsed     | 2807     |
|    total_timesteps  | 115256   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00107  |
|    n_updates        | 3813     |
----------------------------------
Eval num_timesteps=115500, episode_reward=2.88 +/- 3.82
Episode length: 117.14 +/- 57.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00247  |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 40       |
|    time_elapsed     | 2825     |
|    total_timesteps  | 115529   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 3882     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.51     |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 41       |
|    time_elapsed     | 2826     |
|    total_timesteps  | 115899   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0777   |
|    n_updates        | 3974     |
----------------------------------
Eval num_timesteps=116000, episode_reward=2.26 +/- 3.36
Episode length: 92.24 +/- 32.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.2     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0445   |
|    n_updates        | 3999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.51     |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 40       |
|    time_elapsed     | 2840     |
|    total_timesteps  | 116237   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000692 |
|    n_updates        | 4059     |
----------------------------------
Eval num_timesteps=116500, episode_reward=2.84 +/- 3.81
Episode length: 99.10 +/- 49.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.1     |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00525  |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 40       |
|    time_elapsed     | 2855     |
|    total_timesteps  | 116533   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00618  |
|    n_updates        | 4133     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 40       |
|    time_elapsed     | 2856     |
|    total_timesteps  | 116831   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00109  |
|    n_updates        | 4207     |
----------------------------------
Eval num_timesteps=117000, episode_reward=2.44 +/- 3.91
Episode length: 90.50 +/- 35.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.5     |
|    mean_reward      | 2.44     |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000667 |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 40       |
|    time_elapsed     | 2870     |
|    total_timesteps  | 117354   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0768   |
|    n_updates        | 4338     |
----------------------------------
Eval num_timesteps=117500, episode_reward=1.64 +/- 2.40
Episode length: 82.04 +/- 28.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82       |
|    mean_reward      | 1.64     |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 4374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 40       |
|    time_elapsed     | 2881     |
|    total_timesteps  | 117698   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00853  |
|    n_updates        | 4424     |
----------------------------------
Eval num_timesteps=118000, episode_reward=3.00 +/- 2.89
Episode length: 93.60 +/- 29.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.6     |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 40       |
|    time_elapsed     | 2893     |
|    total_timesteps  | 118071   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0376   |
|    n_updates        | 4517     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.12     |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 40       |
|    time_elapsed     | 2893     |
|    total_timesteps  | 118387   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0338   |
|    n_updates        | 4596     |
----------------------------------
Eval num_timesteps=118500, episode_reward=4.02 +/- 9.41
Episode length: 124.94 +/- 77.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00103  |
|    n_updates        | 4624     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 40       |
|    time_elapsed     | 2908     |
|    total_timesteps  | 118814   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00727  |
|    n_updates        | 4703     |
----------------------------------
Eval num_timesteps=119000, episode_reward=1.46 +/- 2.37
Episode length: 84.80 +/- 31.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.8     |
|    mean_reward      | 1.46     |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0328   |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 2.65     |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 40       |
|    time_elapsed     | 2918     |
|    total_timesteps  | 119305   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00101  |
|    n_updates        | 4826     |
----------------------------------
Eval num_timesteps=119500, episode_reward=1.80 +/- 3.51
Episode length: 92.70 +/- 38.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.7     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0324   |
|    n_updates        | 4874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 40       |
|    time_elapsed     | 2929     |
|    total_timesteps  | 119683   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000874 |
|    n_updates        | 4920     |
----------------------------------
Eval num_timesteps=120000, episode_reward=2.02 +/- 3.13
Episode length: 87.80 +/- 27.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.8     |
|    mean_reward      | 2.02     |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0508   |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 2.78     |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 40       |
|    time_elapsed     | 2940     |
|    total_timesteps  | 120036   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0019   |
|    n_updates        | 5008     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 2.63     |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 40       |
|    time_elapsed     | 2941     |
|    total_timesteps  | 120297   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00371  |
|    n_updates        | 5074     |
----------------------------------
Eval num_timesteps=120500, episode_reward=3.52 +/- 4.91
Episode length: 122.06 +/- 56.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00808  |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 40       |
|    time_elapsed     | 2955     |
|    total_timesteps  | 120807   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00176  |
|    n_updates        | 5201     |
----------------------------------
Eval num_timesteps=121000, episode_reward=2.20 +/- 2.94
Episode length: 93.12 +/- 37.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.1     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00785  |
|    n_updates        | 5249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 40       |
|    time_elapsed     | 2966     |
|    total_timesteps  | 121048   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 5261     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 40       |
|    time_elapsed     | 2967     |
|    total_timesteps  | 121381   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 5345     |
----------------------------------
Eval num_timesteps=121500, episode_reward=2.16 +/- 3.32
Episode length: 92.84 +/- 33.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 2.16     |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0399   |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 40       |
|    time_elapsed     | 2978     |
|    total_timesteps  | 121806   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00649  |
|    n_updates        | 5451     |
----------------------------------
Eval num_timesteps=122000, episode_reward=3.70 +/- 6.12
Episode length: 115.36 +/- 64.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0399   |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.2     |
|    ep_rew_mean      | 3.11     |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 40       |
|    time_elapsed     | 2991     |
|    total_timesteps  | 122031   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0416   |
|    n_updates        | 5507     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 40       |
|    time_elapsed     | 2992     |
|    total_timesteps  | 122335   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0791   |
|    n_updates        | 5583     |
----------------------------------
Eval num_timesteps=122500, episode_reward=2.20 +/- 3.05
Episode length: 100.60 +/- 47.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0406   |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.5     |
|    ep_rew_mean      | 2.86     |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 40       |
|    time_elapsed     | 3005     |
|    total_timesteps  | 122609   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.091    |
|    n_updates        | 5652     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89       |
|    ep_rew_mean      | 2.82     |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 40       |
|    time_elapsed     | 3005     |
|    total_timesteps  | 122843   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0702   |
|    n_updates        | 5710     |
----------------------------------
Eval num_timesteps=123000, episode_reward=2.48 +/- 5.46
Episode length: 102.48 +/- 45.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0049   |
|    n_updates        | 5749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.1     |
|    ep_rew_mean      | 2.78     |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 40       |
|    time_elapsed     | 3017     |
|    total_timesteps  | 123073   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.041    |
|    n_updates        | 5768     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.3     |
|    ep_rew_mean      | 2.63     |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 40       |
|    time_elapsed     | 3018     |
|    total_timesteps  | 123482   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000868 |
|    n_updates        | 5870     |
----------------------------------
Eval num_timesteps=123500, episode_reward=1.58 +/- 2.38
Episode length: 94.24 +/- 38.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.2     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00247  |
|    n_updates        | 5874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 40       |
|    time_elapsed     | 3030     |
|    total_timesteps  | 123753   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00328  |
|    n_updates        | 5938     |
----------------------------------
Eval num_timesteps=124000, episode_reward=2.26 +/- 3.43
Episode length: 100.32 +/- 43.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00227  |
|    n_updates        | 5999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 2.69     |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 40       |
|    time_elapsed     | 3043     |
|    total_timesteps  | 124277   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00192  |
|    n_updates        | 6069     |
----------------------------------
Eval num_timesteps=124500, episode_reward=4.18 +/- 4.58
Episode length: 125.04 +/- 68.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00531  |
|    n_updates        | 6124     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 2.86     |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 40       |
|    time_elapsed     | 3058     |
|    total_timesteps  | 124786   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 6196     |
----------------------------------
Eval num_timesteps=125000, episode_reward=2.80 +/- 4.61
Episode length: 107.10 +/- 61.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0832   |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 40       |
|    time_elapsed     | 3070     |
|    total_timesteps  | 125122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00203  |
|    n_updates        | 6280     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.3     |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 40       |
|    time_elapsed     | 3071     |
|    total_timesteps  | 125464   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.039    |
|    n_updates        | 6365     |
----------------------------------
Eval num_timesteps=125500, episode_reward=2.68 +/- 3.39
Episode length: 94.98 +/- 35.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95       |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0596   |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | 2.93     |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 40       |
|    time_elapsed     | 3082     |
|    total_timesteps  | 125774   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00145  |
|    n_updates        | 6443     |
----------------------------------
Eval num_timesteps=126000, episode_reward=2.68 +/- 4.08
Episode length: 112.30 +/- 42.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00808  |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.1     |
|    ep_rew_mean      | 2.82     |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 40       |
|    time_elapsed     | 3095     |
|    total_timesteps  | 126066   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00149  |
|    n_updates        | 6516     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 2.89     |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 40       |
|    time_elapsed     | 3096     |
|    total_timesteps  | 126446   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00106  |
|    n_updates        | 6611     |
----------------------------------
Eval num_timesteps=126500, episode_reward=2.96 +/- 3.48
Episode length: 107.24 +/- 41.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00115  |
|    n_updates        | 6624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 40       |
|    time_elapsed     | 3109     |
|    total_timesteps  | 126795   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00501  |
|    n_updates        | 6698     |
----------------------------------
Eval num_timesteps=127000, episode_reward=3.10 +/- 4.54
Episode length: 96.80 +/- 42.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.8     |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00129  |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 2.57     |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 40       |
|    time_elapsed     | 3120     |
|    total_timesteps  | 127108   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0724   |
|    n_updates        | 6776     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 2.46     |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 40       |
|    time_elapsed     | 3121     |
|    total_timesteps  | 127437   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0545   |
|    n_updates        | 6859     |
----------------------------------
Eval num_timesteps=127500, episode_reward=2.52 +/- 3.16
Episode length: 91.66 +/- 31.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.7     |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0379   |
|    n_updates        | 6874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 2.44     |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 40       |
|    time_elapsed     | 3132     |
|    total_timesteps  | 127759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.04     |
|    n_updates        | 6939     |
----------------------------------
Eval num_timesteps=128000, episode_reward=2.36 +/- 3.89
Episode length: 107.78 +/- 47.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00137  |
|    n_updates        | 6999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 2.4      |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 40       |
|    time_elapsed     | 3145     |
|    total_timesteps  | 128114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.079    |
|    n_updates        | 7028     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 2.38     |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 40       |
|    time_elapsed     | 3146     |
|    total_timesteps  | 128464   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00128  |
|    n_updates        | 7115     |
----------------------------------
Eval num_timesteps=128500, episode_reward=2.18 +/- 2.75
Episode length: 98.36 +/- 35.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00149  |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 2.48     |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 40       |
|    time_elapsed     | 3158     |
|    total_timesteps  | 128800   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 7199     |
----------------------------------
Eval num_timesteps=129000, episode_reward=2.14 +/- 3.13
Episode length: 111.14 +/- 45.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.14     |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00693  |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 2.29     |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 40       |
|    time_elapsed     | 3170     |
|    total_timesteps  | 129144   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000879 |
|    n_updates        | 7285     |
----------------------------------
Eval num_timesteps=129500, episode_reward=2.28 +/- 2.82
Episode length: 104.52 +/- 56.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00199  |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 2.4      |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 40       |
|    time_elapsed     | 3183     |
|    total_timesteps  | 129548   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00907  |
|    n_updates        | 7386     |
----------------------------------
Eval num_timesteps=130000, episode_reward=2.92 +/- 4.39
Episode length: 105.54 +/- 47.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00101  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.9     |
|    ep_rew_mean      | 2.44     |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 40       |
|    time_elapsed     | 3196     |
|    total_timesteps  | 130073   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00492  |
|    n_updates        | 7518     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 1.97     |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 40       |
|    time_elapsed     | 3196     |
|    total_timesteps  | 130359   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 7589     |
----------------------------------
Eval num_timesteps=130500, episode_reward=3.66 +/- 4.61
Episode length: 113.46 +/- 46.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0437   |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.5     |
|    ep_rew_mean      | 2.04     |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 40       |
|    time_elapsed     | 3209     |
|    total_timesteps  | 130682   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00288  |
|    n_updates        | 7670     |
----------------------------------
Eval num_timesteps=131000, episode_reward=1.44 +/- 2.75
Episode length: 89.34 +/- 32.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.3     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 2.08     |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 40       |
|    time_elapsed     | 3220     |
|    total_timesteps  | 131070   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00417  |
|    n_updates        | 7767     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.9     |
|    ep_rew_mean      | 2.09     |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 40       |
|    time_elapsed     | 3221     |
|    total_timesteps  | 131396   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00236  |
|    n_updates        | 7848     |
----------------------------------
Eval num_timesteps=131500, episode_reward=1.88 +/- 2.95
Episode length: 86.86 +/- 26.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.9     |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0425   |
|    n_updates        | 7874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | 2.08     |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 40       |
|    time_elapsed     | 3232     |
|    total_timesteps  | 131726   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00356  |
|    n_updates        | 7931     |
----------------------------------
Eval num_timesteps=132000, episode_reward=3.06 +/- 4.08
Episode length: 118.64 +/- 63.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00213  |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.1     |
|    ep_rew_mean      | 2.22     |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 40       |
|    time_elapsed     | 3246     |
|    total_timesteps  | 132081   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0051   |
|    n_updates        | 8020     |
----------------------------------
Eval num_timesteps=132500, episode_reward=1.68 +/- 2.71
Episode length: 91.56 +/- 28.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.6     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0407   |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 40       |
|    time_elapsed     | 3257     |
|    total_timesteps  | 132611   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 8152     |
----------------------------------
Eval num_timesteps=133000, episode_reward=3.14 +/- 5.67
Episode length: 103.90 +/- 47.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00902  |
|    n_updates        | 8249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.9     |
|    ep_rew_mean      | 2.48     |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 40       |
|    time_elapsed     | 3269     |
|    total_timesteps  | 133040   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00277  |
|    n_updates        | 8259     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 2.51     |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 40       |
|    time_elapsed     | 3270     |
|    total_timesteps  | 133422   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0385   |
|    n_updates        | 8355     |
----------------------------------
Eval num_timesteps=133500, episode_reward=2.62 +/- 5.16
Episode length: 94.72 +/- 53.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00245  |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | 2.38     |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 40       |
|    time_elapsed     | 3281     |
|    total_timesteps  | 133751   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00145  |
|    n_updates        | 8437     |
----------------------------------
Eval num_timesteps=134000, episode_reward=2.68 +/- 3.66
Episode length: 102.60 +/- 41.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00171  |
|    n_updates        | 8499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90       |
|    ep_rew_mean      | 2.27     |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 40       |
|    time_elapsed     | 3293     |
|    total_timesteps  | 134122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0478   |
|    n_updates        | 8530     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.9     |
|    ep_rew_mean      | 2.33     |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 40       |
|    time_elapsed     | 3294     |
|    total_timesteps  | 134455   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 8613     |
----------------------------------
Eval num_timesteps=134500, episode_reward=2.72 +/- 4.23
Episode length: 99.50 +/- 45.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.5     |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0452   |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.2     |
|    ep_rew_mean      | 2.32     |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 40       |
|    time_elapsed     | 3306     |
|    total_timesteps  | 134889   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.002    |
|    n_updates        | 8722     |
----------------------------------
Eval num_timesteps=135000, episode_reward=4.64 +/- 5.52
Episode length: 118.16 +/- 58.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00166  |
|    n_updates        | 8749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 2.31     |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 40       |
|    time_elapsed     | 3320     |
|    total_timesteps  | 135192   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00531  |
|    n_updates        | 8797     |
----------------------------------
Eval num_timesteps=135500, episode_reward=4.64 +/- 8.77
Episode length: 113.58 +/- 58.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00312  |
|    n_updates        | 8874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.6     |
|    ep_rew_mean      | 2.4      |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 40       |
|    time_elapsed     | 3333     |
|    total_timesteps  | 135710   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0379   |
|    n_updates        | 8927     |
----------------------------------
Eval num_timesteps=136000, episode_reward=2.80 +/- 3.54
Episode length: 88.80 +/- 41.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.8     |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0461   |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 2.41     |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 40       |
|    time_elapsed     | 3343     |
|    total_timesteps  | 136102   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0358   |
|    n_updates        | 9025     |
----------------------------------
Eval num_timesteps=136500, episode_reward=2.22 +/- 2.56
Episode length: 92.62 +/- 33.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.98     |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 40       |
|    time_elapsed     | 3355     |
|    total_timesteps  | 136575   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0348   |
|    n_updates        | 9143     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 40       |
|    time_elapsed     | 3356     |
|    total_timesteps  | 136873   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0787   |
|    n_updates        | 9218     |
----------------------------------
Eval num_timesteps=137000, episode_reward=5.04 +/- 7.97
Episode length: 132.54 +/- 70.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 5.04     |
| rollout/            |          |
|    exploration_rate | 0.98     |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00166  |
|    n_updates        | 9249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 40       |
|    time_elapsed     | 3375     |
|    total_timesteps  | 137292   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 9322     |
----------------------------------
Eval num_timesteps=137500, episode_reward=3.40 +/- 5.39
Episode length: 101.80 +/- 61.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0396   |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.2     |
|    ep_rew_mean      | 2.58     |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 40       |
|    time_elapsed     | 3389     |
|    total_timesteps  | 137629   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00191  |
|    n_updates        | 9407     |
----------------------------------
Eval num_timesteps=138000, episode_reward=2.26 +/- 3.72
Episode length: 79.98 +/- 33.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80       |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 9499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 2.72     |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 40       |
|    time_elapsed     | 3398     |
|    total_timesteps  | 138097   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00198  |
|    n_updates        | 9524     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 2.67     |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 40       |
|    time_elapsed     | 3399     |
|    total_timesteps  | 138423   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00357  |
|    n_updates        | 9605     |
----------------------------------
Eval num_timesteps=138500, episode_reward=3.54 +/- 5.33
Episode length: 112.04 +/- 73.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.54     |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00234  |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 2.78     |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 40       |
|    time_elapsed     | 3412     |
|    total_timesteps  | 138915   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00558  |
|    n_updates        | 9728     |
----------------------------------
Eval num_timesteps=139000, episode_reward=1.86 +/- 2.33
Episode length: 84.40 +/- 27.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.4     |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0521   |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.8     |
|    ep_rew_mean      | 2.84     |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 40       |
|    time_elapsed     | 3423     |
|    total_timesteps  | 139326   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0445   |
|    n_updates        | 9831     |
----------------------------------
Eval num_timesteps=139500, episode_reward=2.88 +/- 3.52
Episode length: 102.78 +/- 40.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0567   |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 2.85     |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 40       |
|    time_elapsed     | 3438     |
|    total_timesteps  | 139693   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00343  |
|    n_updates        | 9923     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 40       |
|    time_elapsed     | 3439     |
|    total_timesteps  | 139983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00162  |
|    n_updates        | 9995     |
----------------------------------
Eval num_timesteps=140000, episode_reward=3.24 +/- 6.02
Episode length: 118.70 +/- 68.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.977    |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0452   |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 40       |
|    time_elapsed     | 3452     |
|    total_timesteps  | 140274   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0757   |
|    n_updates        | 10068    |
----------------------------------
Eval num_timesteps=140500, episode_reward=4.18 +/- 6.02
Episode length: 106.02 +/- 82.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.977    |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0039   |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 40       |
|    time_elapsed     | 3465     |
|    total_timesteps  | 140643   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00479  |
|    n_updates        | 10160    |
----------------------------------
Eval num_timesteps=141000, episode_reward=2.50 +/- 3.93
Episode length: 114.56 +/- 82.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.977    |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.041    |
|    n_updates        | 10249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 40       |
|    time_elapsed     | 3483     |
|    total_timesteps  | 141222   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0442   |
|    n_updates        | 10305    |
----------------------------------
Eval num_timesteps=141500, episode_reward=1.76 +/- 2.89
Episode length: 93.86 +/- 34.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.9     |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00803  |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 3.19     |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 40       |
|    time_elapsed     | 3496     |
|    total_timesteps  | 141581   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00412  |
|    n_updates        | 10395    |
----------------------------------
Eval num_timesteps=142000, episode_reward=2.62 +/- 3.68
Episode length: 98.58 +/- 44.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00321  |
|    n_updates        | 10499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 40       |
|    time_elapsed     | 3508     |
|    total_timesteps  | 142027   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0338   |
|    n_updates        | 10506    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 40       |
|    time_elapsed     | 3509     |
|    total_timesteps  | 142307   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0204   |
|    n_updates        | 10576    |
----------------------------------
Eval num_timesteps=142500, episode_reward=2.06 +/- 2.55
Episode length: 103.84 +/- 39.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00342  |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 40       |
|    time_elapsed     | 3524     |
|    total_timesteps  | 142619   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 10654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 40       |
|    time_elapsed     | 3525     |
|    total_timesteps  | 142964   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0613   |
|    n_updates        | 10740    |
----------------------------------
Eval num_timesteps=143000, episode_reward=2.10 +/- 2.93
Episode length: 88.22 +/- 35.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.2     |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0687   |
|    n_updates        | 10749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 2.74     |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 40       |
|    time_elapsed     | 3540     |
|    total_timesteps  | 143472   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0686   |
|    n_updates        | 10867    |
----------------------------------
Eval num_timesteps=143500, episode_reward=3.94 +/- 6.18
Episode length: 106.30 +/- 54.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.974    |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00656  |
|    n_updates        | 10874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 2.79     |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 40       |
|    time_elapsed     | 3552     |
|    total_timesteps  | 143763   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00349  |
|    n_updates        | 10940    |
----------------------------------
Eval num_timesteps=144000, episode_reward=2.32 +/- 3.46
Episode length: 92.84 +/- 38.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.974    |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0578   |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 40       |
|    time_elapsed     | 3563     |
|    total_timesteps  | 144191   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0043   |
|    n_updates        | 11047    |
----------------------------------
Eval num_timesteps=144500, episode_reward=2.92 +/- 4.57
Episode length: 112.70 +/- 83.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0722   |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 3.01     |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 40       |
|    time_elapsed     | 3576     |
|    total_timesteps  | 144610   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00754  |
|    n_updates        | 11152    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.8     |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 40       |
|    time_elapsed     | 3577     |
|    total_timesteps  | 144972   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0809   |
|    n_updates        | 11242    |
----------------------------------
Eval num_timesteps=145000, episode_reward=2.26 +/- 2.57
Episode length: 91.00 +/- 24.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91       |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00528  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.8     |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 40       |
|    time_elapsed     | 3592     |
|    total_timesteps  | 145485   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0058   |
|    n_updates        | 11371    |
----------------------------------
Eval num_timesteps=145500, episode_reward=2.78 +/- 5.59
Episode length: 97.24 +/- 39.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.2     |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 11374    |
----------------------------------
Eval num_timesteps=146000, episode_reward=2.82 +/- 4.77
Episode length: 98.70 +/- 46.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.972    |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0889   |
|    n_updates        | 11499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 40       |
|    time_elapsed     | 3614     |
|    total_timesteps  | 146006   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0426   |
|    n_updates        | 11501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 40       |
|    time_elapsed     | 3615     |
|    total_timesteps  | 146403   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 11600    |
----------------------------------
Eval num_timesteps=146500, episode_reward=2.28 +/- 2.87
Episode length: 100.24 +/- 40.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.972    |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00353  |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 40       |
|    time_elapsed     | 3627     |
|    total_timesteps  | 146759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0791   |
|    n_updates        | 11689    |
----------------------------------
Eval num_timesteps=147000, episode_reward=2.34 +/- 3.85
Episode length: 85.38 +/- 30.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.4     |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.971    |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0723   |
|    n_updates        | 11749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 40       |
|    time_elapsed     | 3637     |
|    total_timesteps  | 147040   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 11759    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.8     |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 40       |
|    time_elapsed     | 3638     |
|    total_timesteps  | 147410   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0427   |
|    n_updates        | 11852    |
----------------------------------
Eval num_timesteps=147500, episode_reward=1.82 +/- 2.42
Episode length: 85.46 +/- 25.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.5     |
|    mean_reward      | 1.82     |
| rollout/            |          |
|    exploration_rate | 0.971    |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0454   |
|    n_updates        | 11874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 40       |
|    time_elapsed     | 3649     |
|    total_timesteps  | 147841   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00403  |
|    n_updates        | 11960    |
----------------------------------
Eval num_timesteps=148000, episode_reward=1.54 +/- 2.14
Episode length: 83.42 +/- 22.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.4     |
|    mean_reward      | 1.54     |
| rollout/            |          |
|    exploration_rate | 0.97     |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0517   |
|    n_updates        | 11999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 40       |
|    time_elapsed     | 3659     |
|    total_timesteps  | 148216   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00449  |
|    n_updates        | 12053    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.2     |
|    ep_rew_mean      | 3        |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 40       |
|    time_elapsed     | 3660     |
|    total_timesteps  | 148436   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 12108    |
----------------------------------
Eval num_timesteps=148500, episode_reward=1.56 +/- 2.48
Episode length: 86.04 +/- 35.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86       |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 0.97     |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0469   |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 2.89     |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 40       |
|    time_elapsed     | 3670     |
|    total_timesteps  | 148759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0629   |
|    n_updates        | 12189    |
----------------------------------
Eval num_timesteps=149000, episode_reward=2.54 +/- 3.79
Episode length: 95.52 +/- 35.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.5     |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 12249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 40       |
|    time_elapsed     | 3681     |
|    total_timesteps  | 149080   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0407   |
|    n_updates        | 12269    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 2.88     |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 40       |
|    time_elapsed     | 3682     |
|    total_timesteps  | 149427   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 12356    |
----------------------------------
Eval num_timesteps=149500, episode_reward=2.92 +/- 3.47
Episode length: 95.78 +/- 32.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.8     |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.04     |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 40       |
|    time_elapsed     | 3693     |
|    total_timesteps  | 149710   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00403  |
|    n_updates        | 12427    |
----------------------------------
Eval num_timesteps=150000, episode_reward=1.76 +/- 3.14
Episode length: 95.44 +/- 37.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.4     |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 0.968    |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0816   |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 2.95     |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 40       |
|    time_elapsed     | 3705     |
|    total_timesteps  | 150072   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0116   |
|    n_updates        | 12517    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.8     |
|    ep_rew_mean      | 2.53     |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 40       |
|    time_elapsed     | 3706     |
|    total_timesteps  | 150397   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00646  |
|    n_updates        | 12599    |
----------------------------------
Eval num_timesteps=150500, episode_reward=2.34 +/- 3.32
Episode length: 88.04 +/- 29.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.968    |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 2.68     |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 40       |
|    time_elapsed     | 3716     |
|    total_timesteps  | 150851   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00367  |
|    n_updates        | 12712    |
----------------------------------
Eval num_timesteps=151000, episode_reward=1.68 +/- 2.72
Episode length: 90.70 +/- 27.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.7     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 12749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.6     |
|    ep_rew_mean      | 2.97     |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 40       |
|    time_elapsed     | 3728     |
|    total_timesteps  | 151386   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00741  |
|    n_updates        | 12846    |
----------------------------------
Eval num_timesteps=151500, episode_reward=3.02 +/- 3.91
Episode length: 85.14 +/- 31.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.1     |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.047    |
|    n_updates        | 12874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 3.04     |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 40       |
|    time_elapsed     | 3738     |
|    total_timesteps  | 151790   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0489   |
|    n_updates        | 12947    |
----------------------------------
Eval num_timesteps=152000, episode_reward=2.52 +/- 3.27
Episode length: 95.84 +/- 34.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.8     |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 0.966    |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 40       |
|    time_elapsed     | 3750     |
|    total_timesteps  | 152319   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0495   |
|    n_updates        | 13079    |
----------------------------------
Eval num_timesteps=152500, episode_reward=2.00 +/- 2.84
Episode length: 94.02 +/- 27.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 2        |
| rollout/            |          |
|    exploration_rate | 0.966    |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00608  |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 40       |
|    time_elapsed     | 3762     |
|    total_timesteps  | 152639   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0151   |
|    n_updates        | 13159    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 3.12     |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 40       |
|    time_elapsed     | 3763     |
|    total_timesteps  | 152971   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0417   |
|    n_updates        | 13242    |
----------------------------------
Eval num_timesteps=153000, episode_reward=1.80 +/- 2.79
Episode length: 81.50 +/- 29.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.5     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.966    |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0085   |
|    n_updates        | 13249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 40       |
|    time_elapsed     | 3776     |
|    total_timesteps  | 153385   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00733  |
|    n_updates        | 13346    |
----------------------------------
Eval num_timesteps=153500, episode_reward=1.42 +/- 2.24
Episode length: 83.02 +/- 26.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83       |
|    mean_reward      | 1.42     |
| rollout/            |          |
|    exploration_rate | 0.965    |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 40       |
|    time_elapsed     | 3789     |
|    total_timesteps  | 153786   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00939  |
|    n_updates        | 13446    |
----------------------------------
Eval num_timesteps=154000, episode_reward=2.20 +/- 3.24
Episode length: 90.82 +/- 28.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.965    |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0463   |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.94     |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 40       |
|    time_elapsed     | 3800     |
|    total_timesteps  | 154111   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0451   |
|    n_updates        | 13527    |
----------------------------------
Eval num_timesteps=154500, episode_reward=2.04 +/- 3.08
Episode length: 83.42 +/- 30.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.4     |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 13624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 40       |
|    time_elapsed     | 3810     |
|    total_timesteps  | 154550   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 13637    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 40       |
|    time_elapsed     | 3811     |
|    total_timesteps  | 154959   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00494  |
|    n_updates        | 13739    |
----------------------------------
Eval num_timesteps=155000, episode_reward=2.74 +/- 4.97
Episode length: 93.08 +/- 43.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.1     |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00899  |
|    n_updates        | 13749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.4     |
|    ep_rew_mean      | 3        |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 40       |
|    time_elapsed     | 3823     |
|    total_timesteps  | 155344   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0437   |
|    n_updates        | 13835    |
----------------------------------
Eval num_timesteps=155500, episode_reward=2.00 +/- 3.09
Episode length: 89.46 +/- 30.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 2        |
| rollout/            |          |
|    exploration_rate | 0.963    |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00452  |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 2.97     |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 40       |
|    time_elapsed     | 3833     |
|    total_timesteps  | 155711   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00438  |
|    n_updates        | 13927    |
----------------------------------
Eval num_timesteps=156000, episode_reward=2.22 +/- 3.06
Episode length: 88.12 +/- 32.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.1     |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.963    |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 13999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | 2.97     |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 40       |
|    time_elapsed     | 3844     |
|    total_timesteps  | 156074   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0295   |
|    n_updates        | 14018    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.4     |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 40       |
|    time_elapsed     | 3845     |
|    total_timesteps  | 156379   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00466  |
|    n_updates        | 14094    |
----------------------------------
Eval num_timesteps=156500, episode_reward=2.38 +/- 4.02
Episode length: 90.24 +/- 34.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.2     |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00519  |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 40       |
|    time_elapsed     | 3856     |
|    total_timesteps  | 156812   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0394   |
|    n_updates        | 14202    |
----------------------------------
Eval num_timesteps=157000, episode_reward=2.78 +/- 4.28
Episode length: 114.08 +/- 83.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0589   |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.2     |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 40       |
|    time_elapsed     | 3869     |
|    total_timesteps  | 157265   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 14316    |
----------------------------------
Eval num_timesteps=157500, episode_reward=3.36 +/- 4.21
Episode length: 98.42 +/- 37.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 14374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 40       |
|    time_elapsed     | 3881     |
|    total_timesteps  | 157644   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0389   |
|    n_updates        | 14410    |
----------------------------------
Eval num_timesteps=158000, episode_reward=2.10 +/- 2.82
Episode length: 96.16 +/- 38.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.1     |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 40       |
|    time_elapsed     | 3893     |
|    total_timesteps  | 158148   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.051    |
|    n_updates        | 14536    |
----------------------------------
Eval num_timesteps=158500, episode_reward=1.80 +/- 3.58
Episode length: 85.50 +/- 23.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.5     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.96     |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 40       |
|    time_elapsed     | 3904     |
|    total_timesteps  | 158500   |
----------------------------------
Eval num_timesteps=159000, episode_reward=2.66 +/- 3.63
Episode length: 80.30 +/- 32.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.3     |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.96     |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00374  |
|    n_updates        | 14749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 40       |
|    time_elapsed     | 3915     |
|    total_timesteps  | 159028   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.068    |
|    n_updates        | 14756    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 3.43     |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 40       |
|    time_elapsed     | 3916     |
|    total_timesteps  | 159395   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00547  |
|    n_updates        | 14848    |
----------------------------------
Eval num_timesteps=159500, episode_reward=1.90 +/- 2.91
Episode length: 91.78 +/- 35.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.8     |
|    mean_reward      | 1.9      |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 40       |
|    time_elapsed     | 3927     |
|    total_timesteps  | 159665   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00382  |
|    n_updates        | 14916    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 3.27     |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 40       |
|    time_elapsed     | 3927     |
|    total_timesteps  | 159938   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.043    |
|    n_updates        | 14984    |
----------------------------------
Eval num_timesteps=160000, episode_reward=2.60 +/- 4.56
Episode length: 93.18 +/- 36.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.2     |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.958    |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0415   |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 40       |
|    time_elapsed     | 3939     |
|    total_timesteps  | 160484   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00884  |
|    n_updates        | 15120    |
----------------------------------
Eval num_timesteps=160500, episode_reward=3.08 +/- 3.77
Episode length: 85.56 +/- 31.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.6     |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.958    |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 15124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 40       |
|    time_elapsed     | 3949     |
|    total_timesteps  | 160816   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 15203    |
----------------------------------
Eval num_timesteps=161000, episode_reward=2.08 +/- 3.25
Episode length: 82.30 +/- 29.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.3     |
|    mean_reward      | 2.08     |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0677   |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 3.26     |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 40       |
|    time_elapsed     | 3960     |
|    total_timesteps  | 161349   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 15337    |
----------------------------------
Eval num_timesteps=161500, episode_reward=2.18 +/- 2.81
Episode length: 99.30 +/- 34.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0454   |
|    n_updates        | 15374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 40       |
|    time_elapsed     | 3972     |
|    total_timesteps  | 161863   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00716  |
|    n_updates        | 15465    |
----------------------------------
Eval num_timesteps=162000, episode_reward=2.64 +/- 5.00
Episode length: 80.72 +/- 31.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.7     |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.956    |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 40       |
|    time_elapsed     | 3983     |
|    total_timesteps  | 162287   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0886   |
|    n_updates        | 15571    |
----------------------------------
Eval num_timesteps=162500, episode_reward=1.78 +/- 2.78
Episode length: 79.80 +/- 25.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.8     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 0.956    |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0116   |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 40       |
|    time_elapsed     | 3992     |
|    total_timesteps  | 162550   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0429   |
|    n_updates        | 15637    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 40       |
|    time_elapsed     | 3993     |
|    total_timesteps  | 162852   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00468  |
|    n_updates        | 15712    |
----------------------------------
Eval num_timesteps=163000, episode_reward=3.40 +/- 4.10
Episode length: 90.76 +/- 30.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.955    |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00786  |
|    n_updates        | 15749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 40       |
|    time_elapsed     | 4006     |
|    total_timesteps  | 163227   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00741  |
|    n_updates        | 15806    |
----------------------------------
Eval num_timesteps=163500, episode_reward=2.38 +/- 3.45
Episode length: 98.60 +/- 70.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.955    |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0384   |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 3.08     |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 40       |
|    time_elapsed     | 4018     |
|    total_timesteps  | 163604   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0776   |
|    n_updates        | 15900    |
----------------------------------
Eval num_timesteps=164000, episode_reward=1.96 +/- 2.31
Episode length: 93.74 +/- 30.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.7     |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 0.954    |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0463   |
|    n_updates        | 15999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 40       |
|    time_elapsed     | 4030     |
|    total_timesteps  | 164132   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00486  |
|    n_updates        | 16032    |
----------------------------------
Eval num_timesteps=164500, episode_reward=2.86 +/- 3.49
Episode length: 96.44 +/- 34.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.4     |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.954    |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 40       |
|    time_elapsed     | 4041     |
|    total_timesteps  | 164506   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0414   |
|    n_updates        | 16126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 2.8      |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 40       |
|    time_elapsed     | 4042     |
|    total_timesteps  | 164813   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 16203    |
----------------------------------
Eval num_timesteps=165000, episode_reward=2.88 +/- 3.99
Episode length: 78.54 +/- 28.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.5     |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.953    |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0489   |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 2.84     |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 40       |
|    time_elapsed     | 4052     |
|    total_timesteps  | 165187   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 16296    |
----------------------------------
Eval num_timesteps=165500, episode_reward=2.90 +/- 3.05
Episode length: 94.40 +/- 27.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.953    |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00373  |
|    n_updates        | 16374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 40       |
|    time_elapsed     | 4064     |
|    total_timesteps  | 165551   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 16387    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 40       |
|    time_elapsed     | 4065     |
|    total_timesteps  | 165949   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0408   |
|    n_updates        | 16487    |
----------------------------------
Eval num_timesteps=166000, episode_reward=3.10 +/- 4.76
Episode length: 99.10 +/- 40.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.1     |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.952    |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0295   |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.86     |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 40       |
|    time_elapsed     | 4079     |
|    total_timesteps  | 166398   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 16599    |
----------------------------------
Eval num_timesteps=166500, episode_reward=1.56 +/- 2.07
Episode length: 83.92 +/- 25.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.9     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 0.952    |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0935   |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.73     |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 40       |
|    time_elapsed     | 4089     |
|    total_timesteps  | 166807   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.043    |
|    n_updates        | 16701    |
----------------------------------
Eval num_timesteps=167000, episode_reward=2.56 +/- 3.49
Episode length: 85.14 +/- 32.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.1     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.951    |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00214  |
|    n_updates        | 16749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 40       |
|    time_elapsed     | 4100     |
|    total_timesteps  | 167280   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 16819    |
----------------------------------
Eval num_timesteps=167500, episode_reward=2.48 +/- 3.86
Episode length: 89.84 +/- 33.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.8     |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.95     |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.056    |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 2.72     |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 40       |
|    time_elapsed     | 4111     |
|    total_timesteps  | 167602   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00826  |
|    n_updates        | 16900    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 2.54     |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 40       |
|    time_elapsed     | 4111     |
|    total_timesteps  | 167826   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0479   |
|    n_updates        | 16956    |
----------------------------------
Eval num_timesteps=168000, episode_reward=2.04 +/- 3.29
Episode length: 83.30 +/- 29.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.3     |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.95     |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00604  |
|    n_updates        | 16999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 40       |
|    time_elapsed     | 4123     |
|    total_timesteps  | 168331   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0458   |
|    n_updates        | 17082    |
----------------------------------
Eval num_timesteps=168500, episode_reward=2.24 +/- 3.74
Episode length: 86.94 +/- 31.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.9     |
|    mean_reward      | 2.24     |
| rollout/            |          |
|    exploration_rate | 0.949    |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00469  |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 2.67     |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 40       |
|    time_elapsed     | 4134     |
|    total_timesteps  | 168708   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 17176    |
----------------------------------
Eval num_timesteps=169000, episode_reward=2.54 +/- 4.07
Episode length: 84.00 +/- 27.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84       |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.949    |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 17249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.8     |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 40       |
|    time_elapsed     | 4146     |
|    total_timesteps  | 169172   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0785   |
|    n_updates        | 17292    |
----------------------------------
Eval num_timesteps=169500, episode_reward=2.50 +/- 6.04
Episode length: 92.70 +/- 42.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.7     |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.948    |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0455   |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 40       |
|    time_elapsed     | 4157     |
|    total_timesteps  | 169685   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00766  |
|    n_updates        | 17421    |
----------------------------------
Eval num_timesteps=170000, episode_reward=1.58 +/- 2.80
Episode length: 83.16 +/- 32.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.2     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 0.948    |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0408   |
|    n_updates        | 17499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 40       |
|    time_elapsed     | 4168     |
|    total_timesteps  | 170201   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0576   |
|    n_updates        | 17550    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.04     |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 40       |
|    time_elapsed     | 4169     |
|    total_timesteps  | 170490   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0353   |
|    n_updates        | 17622    |
----------------------------------
Eval num_timesteps=170500, episode_reward=1.98 +/- 2.22
Episode length: 80.62 +/- 22.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.6     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 0.947    |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 40       |
|    time_elapsed     | 4179     |
|    total_timesteps  | 170976   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0947   |
|    n_updates        | 17743    |
----------------------------------
Eval num_timesteps=171000, episode_reward=2.22 +/- 2.73
Episode length: 86.62 +/- 29.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.6     |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.947    |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0618   |
|    n_updates        | 17749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 40       |
|    time_elapsed     | 4189     |
|    total_timesteps  | 171329   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0714   |
|    n_updates        | 17832    |
----------------------------------
Eval num_timesteps=171500, episode_reward=1.86 +/- 2.46
Episode length: 94.32 +/- 40.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.3     |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 0.946    |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.056    |
|    n_updates        | 17874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 40       |
|    time_elapsed     | 4200     |
|    total_timesteps  | 171680   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00606  |
|    n_updates        | 17919    |
----------------------------------
Eval num_timesteps=172000, episode_reward=2.38 +/- 3.54
Episode length: 88.52 +/- 31.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.5     |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.945    |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00589  |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 2.8      |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 40       |
|    time_elapsed     | 4211     |
|    total_timesteps  | 172020   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0697   |
|    n_updates        | 18004    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 2.74     |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 40       |
|    time_elapsed     | 4211     |
|    total_timesteps  | 172306   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00596  |
|    n_updates        | 18076    |
----------------------------------
Eval num_timesteps=172500, episode_reward=3.64 +/- 8.42
Episode length: 103.74 +/- 60.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.945    |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0787   |
|    n_updates        | 18124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 2.74     |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 40       |
|    time_elapsed     | 4224     |
|    total_timesteps  | 172646   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00749  |
|    n_updates        | 18161    |
----------------------------------
Eval num_timesteps=173000, episode_reward=1.92 +/- 2.76
Episode length: 84.66 +/- 26.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.7     |
|    mean_reward      | 1.92     |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00761  |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 40       |
|    time_elapsed     | 4235     |
|    total_timesteps  | 173055   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 18263    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 2.73     |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 40       |
|    time_elapsed     | 4237     |
|    total_timesteps  | 173460   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0483   |
|    n_updates        | 18364    |
----------------------------------
Eval num_timesteps=173500, episode_reward=2.54 +/- 3.16
Episode length: 101.66 +/- 36.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.085    |
|    n_updates        | 18374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 40       |
|    time_elapsed     | 4253     |
|    total_timesteps  | 173919   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 18479    |
----------------------------------
Eval num_timesteps=174000, episode_reward=3.22 +/- 3.05
Episode length: 88.38 +/- 27.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.4     |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.943    |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 2.77     |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 40       |
|    time_elapsed     | 4264     |
|    total_timesteps  | 174310   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0445   |
|    n_updates        | 18577    |
----------------------------------
Eval num_timesteps=174500, episode_reward=3.00 +/- 3.83
Episode length: 100.96 +/- 43.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.943    |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0346   |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 40       |
|    time_elapsed     | 4276     |
|    total_timesteps  | 174732   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0069   |
|    n_updates        | 18682    |
----------------------------------
Eval num_timesteps=175000, episode_reward=1.88 +/- 2.51
Episode length: 89.02 +/- 31.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89       |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 0.942    |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0448   |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 40       |
|    time_elapsed     | 4287     |
|    total_timesteps  | 175150   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 18787    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 2.85     |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 40       |
|    time_elapsed     | 4288     |
|    total_timesteps  | 175471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0614   |
|    n_updates        | 18867    |
----------------------------------
Eval num_timesteps=175500, episode_reward=3.44 +/- 4.86
Episode length: 99.20 +/- 34.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.2     |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00353  |
|    n_updates        | 18874    |
----------------------------------
Eval num_timesteps=176000, episode_reward=2.98 +/- 3.80
Episode length: 99.78 +/- 42.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.8     |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00652  |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.06     |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 40       |
|    time_elapsed     | 4311     |
|    total_timesteps  | 176050   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.035    |
|    n_updates        | 19012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 40       |
|    time_elapsed     | 4312     |
|    total_timesteps  | 176462   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 19115    |
----------------------------------
Eval num_timesteps=176500, episode_reward=2.66 +/- 4.14
Episode length: 96.76 +/- 35.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.8     |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.94     |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00481  |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 2.88     |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 40       |
|    time_elapsed     | 4324     |
|    total_timesteps  | 176764   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.017    |
|    n_updates        | 19190    |
----------------------------------
Eval num_timesteps=177000, episode_reward=2.02 +/- 2.94
Episode length: 92.62 +/- 34.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 2.02     |
| rollout/            |          |
|    exploration_rate | 0.94     |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 19249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 40       |
|    time_elapsed     | 4335     |
|    total_timesteps  | 177120   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 19279    |
----------------------------------
Eval num_timesteps=177500, episode_reward=2.56 +/- 2.77
Episode length: 88.54 +/- 28.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.5     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.939    |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 3.01     |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 40       |
|    time_elapsed     | 4345     |
|    total_timesteps  | 177535   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00573  |
|    n_updates        | 19383    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 40       |
|    time_elapsed     | 4347     |
|    total_timesteps  | 177966   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0119   |
|    n_updates        | 19491    |
----------------------------------
Eval num_timesteps=178000, episode_reward=3.74 +/- 5.87
Episode length: 94.96 +/- 41.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95       |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.938    |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0397   |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 40       |
|    time_elapsed     | 4358     |
|    total_timesteps  | 178312   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0669   |
|    n_updates        | 19577    |
----------------------------------
Eval num_timesteps=178500, episode_reward=3.72 +/- 4.28
Episode length: 101.12 +/- 35.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.72     |
| rollout/            |          |
|    exploration_rate | 0.938    |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0471   |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 3.01     |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 40       |
|    time_elapsed     | 4370     |
|    total_timesteps  | 178670   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 19667    |
----------------------------------
Eval num_timesteps=179000, episode_reward=2.46 +/- 3.22
Episode length: 111.18 +/- 62.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.937    |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0659   |
|    n_updates        | 19749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 40       |
|    time_elapsed     | 4382     |
|    total_timesteps  | 179017   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0481   |
|    n_updates        | 19754    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 2.73     |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 40       |
|    time_elapsed     | 4383     |
|    total_timesteps  | 179369   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0444   |
|    n_updates        | 19842    |
----------------------------------
Eval num_timesteps=179500, episode_reward=2.62 +/- 4.35
Episode length: 110.50 +/- 67.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.937    |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.77     |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 40       |
|    time_elapsed     | 4396     |
|    total_timesteps  | 179702   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00641  |
|    n_updates        | 19925    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 2.78     |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 40       |
|    time_elapsed     | 4397     |
|    total_timesteps  | 179949   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0499   |
|    n_updates        | 19987    |
----------------------------------
Eval num_timesteps=180000, episode_reward=2.14 +/- 3.00
Episode length: 94.18 +/- 31.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.2     |
|    mean_reward      | 2.14     |
| rollout/            |          |
|    exploration_rate | 0.936    |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0747   |
|    n_updates        | 19999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 40       |
|    time_elapsed     | 4408     |
|    total_timesteps  | 180292   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00763  |
|    n_updates        | 20072    |
----------------------------------
Eval num_timesteps=180500, episode_reward=3.00 +/- 4.76
Episode length: 92.90 +/- 43.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.9     |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 40       |
|    time_elapsed     | 4419     |
|    total_timesteps  | 180641   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0434   |
|    n_updates        | 20160    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | 2.73     |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 40       |
|    time_elapsed     | 4420     |
|    total_timesteps  | 180997   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0581   |
|    n_updates        | 20249    |
----------------------------------
Eval num_timesteps=181000, episode_reward=3.38 +/- 4.06
Episode length: 107.42 +/- 47.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 181000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 40       |
|    time_elapsed     | 4433     |
|    total_timesteps  | 181385   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 20346    |
----------------------------------
Eval num_timesteps=181500, episode_reward=3.16 +/- 5.77
Episode length: 86.42 +/- 28.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.4     |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.934    |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.2     |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 40       |
|    time_elapsed     | 4444     |
|    total_timesteps  | 181724   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 20430    |
----------------------------------
Eval num_timesteps=182000, episode_reward=1.78 +/- 2.89
Episode length: 80.18 +/- 28.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.2     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 0.934    |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0536   |
|    n_updates        | 20499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 40       |
|    time_elapsed     | 4454     |
|    total_timesteps  | 182205   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 20551    |
----------------------------------
Eval num_timesteps=182500, episode_reward=1.78 +/- 2.82
Episode length: 81.52 +/- 38.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.5     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0618   |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 40       |
|    time_elapsed     | 4465     |
|    total_timesteps  | 182674   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00935  |
|    n_updates        | 20668    |
----------------------------------
Eval num_timesteps=183000, episode_reward=2.16 +/- 2.79
Episode length: 86.30 +/- 45.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.3     |
|    mean_reward      | 2.16     |
| rollout/            |          |
|    exploration_rate | 0.932    |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00959  |
|    n_updates        | 20749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 2.88     |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 40       |
|    time_elapsed     | 4475     |
|    total_timesteps  | 183002   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00652  |
|    n_updates        | 20750    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 40       |
|    time_elapsed     | 4476     |
|    total_timesteps  | 183418   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 20854    |
----------------------------------
Eval num_timesteps=183500, episode_reward=1.76 +/- 2.18
Episode length: 90.86 +/- 44.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.9     |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 0.932    |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.034    |
|    n_updates        | 20874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.93     |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 40       |
|    time_elapsed     | 4487     |
|    total_timesteps  | 183811   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0247   |
|    n_updates        | 20952    |
----------------------------------
Eval num_timesteps=184000, episode_reward=1.42 +/- 2.00
Episode length: 88.98 +/- 36.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89       |
|    mean_reward      | 1.42     |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 20999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 2.8      |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 40       |
|    time_elapsed     | 4498     |
|    total_timesteps  | 184087   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0119   |
|    n_updates        | 21021    |
----------------------------------
Eval num_timesteps=184500, episode_reward=1.84 +/- 2.43
Episode length: 89.22 +/- 34.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.2     |
|    mean_reward      | 1.84     |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00635  |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 2.67     |
|    exploration_rate | 0.931    |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 40       |
|    time_elapsed     | 4508     |
|    total_timesteps  | 184527   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 21131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 41       |
|    time_elapsed     | 4510     |
|    total_timesteps  | 184972   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.044    |
|    n_updates        | 21242    |
----------------------------------
Eval num_timesteps=185000, episode_reward=2.88 +/- 4.54
Episode length: 95.60 +/- 33.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.6     |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.93     |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.2     |
|    ep_rew_mean      | 2.68     |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 41       |
|    time_elapsed     | 4522     |
|    total_timesteps  | 185465   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 21366    |
----------------------------------
Eval num_timesteps=185500, episode_reward=2.30 +/- 3.79
Episode length: 103.30 +/- 81.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0381   |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.64     |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 40       |
|    time_elapsed     | 4534     |
|    total_timesteps  | 185865   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.034    |
|    n_updates        | 21466    |
----------------------------------
Eval num_timesteps=186000, episode_reward=2.56 +/- 3.77
Episode length: 92.20 +/- 36.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.2     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0544   |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.2     |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 40       |
|    time_elapsed     | 4545     |
|    total_timesteps  | 186185   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0162   |
|    n_updates        | 21546    |
----------------------------------
Eval num_timesteps=186500, episode_reward=4.56 +/- 7.00
Episode length: 129.86 +/- 62.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.56     |
| rollout/            |          |
|    exploration_rate | 0.928    |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 21624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 40       |
|    time_elapsed     | 4559     |
|    total_timesteps  | 186525   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00856  |
|    n_updates        | 21631    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.4     |
|    ep_rew_mean      | 2.67     |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 40       |
|    time_elapsed     | 4560     |
|    total_timesteps  | 186874   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.036    |
|    n_updates        | 21718    |
----------------------------------
Eval num_timesteps=187000, episode_reward=2.40 +/- 4.82
Episode length: 82.62 +/- 32.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.6     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.927    |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 21749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 2.57     |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 40       |
|    time_elapsed     | 4570     |
|    total_timesteps  | 187279   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 21819    |
----------------------------------
Eval num_timesteps=187500, episode_reward=3.10 +/- 4.04
Episode length: 91.08 +/- 26.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.1     |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.927    |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00678  |
|    n_updates        | 21874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.6     |
|    ep_rew_mean      | 2.47     |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 40       |
|    time_elapsed     | 4581     |
|    total_timesteps  | 187568   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00687  |
|    n_updates        | 21891    |
----------------------------------
Eval num_timesteps=188000, episode_reward=2.90 +/- 4.69
Episode length: 90.30 +/- 44.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.3     |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.926    |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00959  |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.1     |
|    ep_rew_mean      | 2.53     |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 40       |
|    time_elapsed     | 4593     |
|    total_timesteps  | 188082   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 22020    |
----------------------------------
Eval num_timesteps=188500, episode_reward=2.08 +/- 2.79
Episode length: 91.48 +/- 36.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.5     |
|    mean_reward      | 2.08     |
| rollout/            |          |
|    exploration_rate | 0.926    |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0525   |
|    n_updates        | 22124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 2.55     |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 40       |
|    time_elapsed     | 4605     |
|    total_timesteps  | 188596   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0078   |
|    n_updates        | 22148    |
----------------------------------
Eval num_timesteps=189000, episode_reward=2.96 +/- 5.00
Episode length: 99.68 +/- 53.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.7     |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.925    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0486   |
|    n_updates        | 22249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 2.62     |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 40       |
|    time_elapsed     | 4618     |
|    total_timesteps  | 189100   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 22274    |
----------------------------------
Eval num_timesteps=189500, episode_reward=1.96 +/- 2.86
Episode length: 80.62 +/- 24.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.6     |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 0.924    |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0471   |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 2.63     |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 40       |
|    time_elapsed     | 4628     |
|    total_timesteps  | 189600   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 22399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.7      |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 41       |
|    time_elapsed     | 4629     |
|    total_timesteps  | 189957   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 22489    |
----------------------------------
Eval num_timesteps=190000, episode_reward=2.12 +/- 3.25
Episode length: 88.74 +/- 29.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.7     |
|    mean_reward      | 2.12     |
| rollout/            |          |
|    exploration_rate | 0.924    |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.65     |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 41       |
|    time_elapsed     | 4640     |
|    total_timesteps  | 190334   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 22583    |
----------------------------------
Eval num_timesteps=190500, episode_reward=2.62 +/- 3.24
Episode length: 89.70 +/- 27.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.7     |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.923    |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0514   |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 41       |
|    time_elapsed     | 4651     |
|    total_timesteps  | 190742   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 22685    |
----------------------------------
Eval num_timesteps=191000, episode_reward=2.26 +/- 3.02
Episode length: 98.88 +/- 54.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.922    |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0843   |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.61     |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 40       |
|    time_elapsed     | 4663     |
|    total_timesteps  | 191136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 22783    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.56     |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 41       |
|    time_elapsed     | 4664     |
|    total_timesteps  | 191403   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 22850    |
----------------------------------
Eval num_timesteps=191500, episode_reward=2.02 +/- 2.66
Episode length: 97.88 +/- 30.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.9     |
|    mean_reward      | 2.02     |
| rollout/            |          |
|    exploration_rate | 0.922    |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0279   |
|    n_updates        | 22874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 41       |
|    time_elapsed     | 4676     |
|    total_timesteps  | 191983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0606   |
|    n_updates        | 22995    |
----------------------------------
Eval num_timesteps=192000, episode_reward=2.64 +/- 4.01
Episode length: 87.58 +/- 38.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.6     |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 22999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.74     |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 41       |
|    time_elapsed     | 4686     |
|    total_timesteps  | 192380   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00789  |
|    n_updates        | 23094    |
----------------------------------
Eval num_timesteps=192500, episode_reward=3.00 +/- 3.47
Episode length: 89.18 +/- 29.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.2     |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 41       |
|    time_elapsed     | 4697     |
|    total_timesteps  | 192843   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0522   |
|    n_updates        | 23210    |
----------------------------------
Eval num_timesteps=193000, episode_reward=1.58 +/- 4.28
Episode length: 93.64 +/- 37.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.6     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 0.92     |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.057    |
|    n_updates        | 23249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 41       |
|    time_elapsed     | 4708     |
|    total_timesteps  | 193180   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 23294    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.89     |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 41       |
|    time_elapsed     | 4709     |
|    total_timesteps  | 193495   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 23373    |
----------------------------------
Eval num_timesteps=193500, episode_reward=1.80 +/- 2.80
Episode length: 89.42 +/- 34.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.4     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.919    |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.78     |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 41       |
|    time_elapsed     | 4723     |
|    total_timesteps  | 193926   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00661  |
|    n_updates        | 23481    |
----------------------------------
Eval num_timesteps=194000, episode_reward=2.96 +/- 3.97
Episode length: 96.14 +/- 33.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.1     |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.919    |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0385   |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 41       |
|    time_elapsed     | 4734     |
|    total_timesteps  | 194180   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0044   |
|    n_updates        | 23544    |
----------------------------------
Eval num_timesteps=194500, episode_reward=2.38 +/- 3.60
Episode length: 90.82 +/- 35.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.918    |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 23624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 40       |
|    time_elapsed     | 4745     |
|    total_timesteps  | 194523   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00892  |
|    n_updates        | 23630    |
----------------------------------
Eval num_timesteps=195000, episode_reward=1.88 +/- 2.98
Episode length: 84.96 +/- 28.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85       |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 0.917    |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00703  |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 41       |
|    time_elapsed     | 4755     |
|    total_timesteps  | 195054   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 23763    |
----------------------------------
Eval num_timesteps=195500, episode_reward=1.90 +/- 3.53
Episode length: 97.58 +/- 34.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 1.9      |
| rollout/            |          |
|    exploration_rate | 0.917    |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 23874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.86     |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 41       |
|    time_elapsed     | 4767     |
|    total_timesteps  | 195527   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0175   |
|    n_updates        | 23881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 41       |
|    time_elapsed     | 4768     |
|    total_timesteps  | 195846   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 23961    |
----------------------------------
Eval num_timesteps=196000, episode_reward=1.88 +/- 2.38
Episode length: 85.82 +/- 33.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.8     |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 0.916    |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00548  |
|    n_updates        | 23999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 2.78     |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 41       |
|    time_elapsed     | 4779     |
|    total_timesteps  | 196148   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00532  |
|    n_updates        | 24036    |
----------------------------------
Eval num_timesteps=196500, episode_reward=2.96 +/- 3.62
Episode length: 92.86 +/- 41.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.9     |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.915    |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0618   |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.82     |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 41       |
|    time_elapsed     | 4790     |
|    total_timesteps  | 196537   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 24134    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.86     |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 41       |
|    time_elapsed     | 4791     |
|    total_timesteps  | 196948   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 24236    |
----------------------------------
Eval num_timesteps=197000, episode_reward=2.78 +/- 4.00
Episode length: 98.26 +/- 40.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.3     |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.915    |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0144   |
|    n_updates        | 24249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.1      |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 41       |
|    time_elapsed     | 4803     |
|    total_timesteps  | 197494   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0286   |
|    n_updates        | 24373    |
----------------------------------
Eval num_timesteps=197500, episode_reward=2.94 +/- 3.33
Episode length: 87.22 +/- 30.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.2     |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.914    |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0496   |
|    n_updates        | 24374    |
----------------------------------
Eval num_timesteps=198000, episode_reward=2.04 +/- 2.95
Episode length: 93.36 +/- 33.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.4     |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.913    |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.913    |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 41       |
|    time_elapsed     | 4825     |
|    total_timesteps  | 198136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 24533    |
----------------------------------
Eval num_timesteps=198500, episode_reward=2.06 +/- 2.52
Episode length: 91.86 +/- 36.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.9     |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.913    |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0547   |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.37     |
|    exploration_rate | 0.913    |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 41       |
|    time_elapsed     | 4836     |
|    total_timesteps  | 198605   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0263   |
|    n_updates        | 24651    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 41       |
|    time_elapsed     | 4837     |
|    total_timesteps  | 198939   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0104   |
|    n_updates        | 24734    |
----------------------------------
Eval num_timesteps=199000, episode_reward=2.34 +/- 3.07
Episode length: 85.14 +/- 27.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.1     |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.912    |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0556   |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 41       |
|    time_elapsed     | 4848     |
|    total_timesteps  | 199413   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0405   |
|    n_updates        | 24853    |
----------------------------------
Eval num_timesteps=199500, episode_reward=2.84 +/- 4.72
Episode length: 92.02 +/- 40.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.911    |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0075   |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.28     |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 41       |
|    time_elapsed     | 4859     |
|    total_timesteps  | 199748   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0175   |
|    n_updates        | 24936    |
----------------------------------
Eval num_timesteps=200000, episode_reward=2.94 +/- 4.86
Episode length: 96.26 +/- 34.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.911    |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0489   |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.46     |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 41       |
|    time_elapsed     | 4870     |
|    total_timesteps  | 200182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0761   |
|    n_updates        | 25045    |
----------------------------------
Eval num_timesteps=200500, episode_reward=2.42 +/- 3.07
Episode length: 87.24 +/- 41.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.2     |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.91     |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.035    |
|    n_updates        | 25124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 41       |
|    time_elapsed     | 4881     |
|    total_timesteps  | 200715   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0875   |
|    n_updates        | 25178    |
----------------------------------
Eval num_timesteps=201000, episode_reward=2.96 +/- 4.62
Episode length: 90.30 +/- 37.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.3     |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.909    |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 25249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.59     |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 41       |
|    time_elapsed     | 4892     |
|    total_timesteps  | 201219   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0227   |
|    n_updates        | 25304    |
----------------------------------
Eval num_timesteps=201500, episode_reward=2.74 +/- 3.83
Episode length: 100.60 +/- 44.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.909    |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00543  |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 41       |
|    time_elapsed     | 4903     |
|    total_timesteps  | 201527   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 25381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.59     |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 41       |
|    time_elapsed     | 4905     |
|    total_timesteps  | 201951   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 25487    |
----------------------------------
Eval num_timesteps=202000, episode_reward=1.98 +/- 2.88
Episode length: 90.48 +/- 38.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.5     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 0.908    |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.59     |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 41       |
|    time_elapsed     | 4915     |
|    total_timesteps  | 202433   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 25608    |
----------------------------------
Eval num_timesteps=202500, episode_reward=3.96 +/- 4.52
Episode length: 103.26 +/- 47.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.907    |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0509   |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.907    |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 41       |
|    time_elapsed     | 4928     |
|    total_timesteps  | 202791   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0947   |
|    n_updates        | 25697    |
----------------------------------
Eval num_timesteps=203000, episode_reward=2.24 +/- 3.28
Episode length: 95.82 +/- 33.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.8     |
|    mean_reward      | 2.24     |
| rollout/            |          |
|    exploration_rate | 0.907    |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.26     |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 41       |
|    time_elapsed     | 4939     |
|    total_timesteps  | 203160   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00998  |
|    n_updates        | 25789    |
----------------------------------
Eval num_timesteps=203500, episode_reward=2.42 +/- 4.50
Episode length: 87.06 +/- 31.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.1     |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.906    |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 25874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.38     |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 41       |
|    time_elapsed     | 4950     |
|    total_timesteps  | 203623   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 25905    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 41       |
|    time_elapsed     | 4951     |
|    total_timesteps  | 203974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00893  |
|    n_updates        | 25993    |
----------------------------------
Eval num_timesteps=204000, episode_reward=2.84 +/- 4.24
Episode length: 87.96 +/- 35.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.905    |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 41       |
|    time_elapsed     | 4962     |
|    total_timesteps  | 204407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.029    |
|    n_updates        | 26101    |
----------------------------------
Eval num_timesteps=204500, episode_reward=3.34 +/- 6.75
Episode length: 101.48 +/- 49.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.905    |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 41       |
|    time_elapsed     | 4974     |
|    total_timesteps  | 204639   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.223    |
|    n_updates        | 26159    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 41       |
|    time_elapsed     | 4975     |
|    total_timesteps  | 204943   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0388   |
|    n_updates        | 26235    |
----------------------------------
Eval num_timesteps=205000, episode_reward=2.30 +/- 2.87
Episode length: 98.36 +/- 35.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.904    |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00866  |
|    n_updates        | 26249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.27     |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 41       |
|    time_elapsed     | 4986     |
|    total_timesteps  | 205238   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0516   |
|    n_updates        | 26309    |
----------------------------------
Eval num_timesteps=205500, episode_reward=1.98 +/- 3.05
Episode length: 86.26 +/- 32.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.3     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 0.903    |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0511   |
|    n_updates        | 26374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.47     |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 41       |
|    time_elapsed     | 4997     |
|    total_timesteps  | 205775   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 26443    |
----------------------------------
Eval num_timesteps=206000, episode_reward=2.34 +/- 3.88
Episode length: 87.68 +/- 30.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.7     |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.902    |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0157   |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 41       |
|    time_elapsed     | 5007     |
|    total_timesteps  | 206110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 26527    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 41       |
|    time_elapsed     | 5008     |
|    total_timesteps  | 206438   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 26609    |
----------------------------------
Eval num_timesteps=206500, episode_reward=2.38 +/- 3.79
Episode length: 91.26 +/- 42.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.3     |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.902    |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00937  |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 41       |
|    time_elapsed     | 5021     |
|    total_timesteps  | 206870   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0716   |
|    n_updates        | 26717    |
----------------------------------
Eval num_timesteps=207000, episode_reward=2.32 +/- 3.97
Episode length: 92.22 +/- 42.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.2     |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.901    |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0943   |
|    n_updates        | 26749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 41       |
|    time_elapsed     | 5032     |
|    total_timesteps  | 207295   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0342   |
|    n_updates        | 26823    |
----------------------------------
Eval num_timesteps=207500, episode_reward=2.20 +/- 2.93
Episode length: 84.24 +/- 27.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.2     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.082    |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 41       |
|    time_elapsed     | 5043     |
|    total_timesteps  | 207632   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 26907    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.9      |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 41       |
|    time_elapsed     | 5044     |
|    total_timesteps  | 207963   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0636   |
|    n_updates        | 26990    |
----------------------------------
Eval num_timesteps=208000, episode_reward=2.50 +/- 3.44
Episode length: 86.20 +/- 34.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.2     |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0712   |
|    n_updates        | 26999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 3.06     |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 41       |
|    time_elapsed     | 5055     |
|    total_timesteps  | 208408   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 27101    |
----------------------------------
Eval num_timesteps=208500, episode_reward=2.08 +/- 2.94
Episode length: 84.76 +/- 30.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.8     |
|    mean_reward      | 2.08     |
| rollout/            |          |
|    exploration_rate | 0.899    |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0595   |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 41       |
|    time_elapsed     | 5065     |
|    total_timesteps  | 208766   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 27191    |
----------------------------------
Eval num_timesteps=209000, episode_reward=1.84 +/- 3.19
Episode length: 90.82 +/- 41.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 1.84     |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 3.11     |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 41       |
|    time_elapsed     | 5076     |
|    total_timesteps  | 209267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 27316    |
----------------------------------
Eval num_timesteps=209500, episode_reward=2.06 +/- 3.12
Episode length: 97.66 +/- 37.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.7     |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 27374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.897    |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 41       |
|    time_elapsed     | 5087     |
|    total_timesteps  | 209772   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.085    |
|    n_updates        | 27442    |
----------------------------------
Eval num_timesteps=210000, episode_reward=2.40 +/- 3.52
Episode length: 82.00 +/- 27.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82       |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.897    |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 27499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.897    |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 41       |
|    time_elapsed     | 5098     |
|    total_timesteps  | 210238   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 27559    |
----------------------------------
Eval num_timesteps=210500, episode_reward=3.00 +/- 5.50
Episode length: 89.68 +/- 41.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.7     |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0392   |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 41       |
|    time_elapsed     | 5110     |
|    total_timesteps  | 210621   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 27655    |
----------------------------------
Eval num_timesteps=211000, episode_reward=3.32 +/- 4.93
Episode length: 108.64 +/- 38.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0465   |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 41       |
|    time_elapsed     | 5123     |
|    total_timesteps  | 211038   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 27759    |
----------------------------------
Eval num_timesteps=211500, episode_reward=1.96 +/- 3.24
Episode length: 85.76 +/- 29.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.8     |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 0.895    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 27874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 2.74     |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 41       |
|    time_elapsed     | 5134     |
|    total_timesteps  | 211518   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 27879    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.4     |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 41       |
|    time_elapsed     | 5135     |
|    total_timesteps  | 211891   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 27972    |
----------------------------------
Eval num_timesteps=212000, episode_reward=1.80 +/- 2.42
Episode length: 78.62 +/- 26.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.6     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.894    |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 27999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 2.62     |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 41       |
|    time_elapsed     | 5145     |
|    total_timesteps  | 212237   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 28059    |
----------------------------------
Eval num_timesteps=212500, episode_reward=2.64 +/- 3.76
Episode length: 98.46 +/- 45.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.5     |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 28124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 41       |
|    time_elapsed     | 5157     |
|    total_timesteps  | 212633   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0342   |
|    n_updates        | 28158    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 41       |
|    time_elapsed     | 5157     |
|    total_timesteps  | 212899   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 28224    |
----------------------------------
Eval num_timesteps=213000, episode_reward=2.96 +/- 4.19
Episode length: 90.74 +/- 42.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.7     |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 28249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 2.67     |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 41       |
|    time_elapsed     | 5169     |
|    total_timesteps  | 213299   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.029    |
|    n_updates        | 28324    |
----------------------------------
Eval num_timesteps=213500, episode_reward=2.50 +/- 3.86
Episode length: 88.54 +/- 28.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.5     |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.892    |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0591   |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 2.68     |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 41       |
|    time_elapsed     | 5180     |
|    total_timesteps  | 213678   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0594   |
|    n_updates        | 28419    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 2.6      |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 41       |
|    time_elapsed     | 5180     |
|    total_timesteps  | 213994   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 28498    |
----------------------------------
Eval num_timesteps=214000, episode_reward=2.62 +/- 3.12
Episode length: 94.84 +/- 33.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.8     |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.032    |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 2.66     |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 41       |
|    time_elapsed     | 5192     |
|    total_timesteps  | 214340   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 28584    |
----------------------------------
Eval num_timesteps=214500, episode_reward=2.42 +/- 3.48
Episode length: 90.90 +/- 49.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.9     |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 28624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 41       |
|    time_elapsed     | 5203     |
|    total_timesteps  | 214812   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 28702    |
----------------------------------
Eval num_timesteps=215000, episode_reward=3.04 +/- 4.78
Episode length: 98.96 +/- 49.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99       |
|    mean_reward      | 3.04     |
| rollout/            |          |
|    exploration_rate | 0.89     |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 28749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 2.58     |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 41       |
|    time_elapsed     | 5215     |
|    total_timesteps  | 215105   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0443   |
|    n_updates        | 28776    |
----------------------------------
Eval num_timesteps=215500, episode_reward=1.78 +/- 2.83
Episode length: 85.50 +/- 30.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.5     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 0.889    |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 2.54     |
|    exploration_rate | 0.889    |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 41       |
|    time_elapsed     | 5226     |
|    total_timesteps  | 215590   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 28897    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 2.49     |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 41       |
|    time_elapsed     | 5227     |
|    total_timesteps  | 215956   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0627   |
|    n_updates        | 28988    |
----------------------------------
Eval num_timesteps=216000, episode_reward=2.42 +/- 4.19
Episode length: 97.58 +/- 43.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.888    |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 2.51     |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 41       |
|    time_elapsed     | 5239     |
|    total_timesteps  | 216319   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0543   |
|    n_updates        | 29079    |
----------------------------------
Eval num_timesteps=216500, episode_reward=1.56 +/- 2.20
Episode length: 84.30 +/- 32.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.3     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 0.888    |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.029    |
|    n_updates        | 29124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 2.34     |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 41       |
|    time_elapsed     | 5249     |
|    total_timesteps  | 216603   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0327   |
|    n_updates        | 29150    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 2.19     |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 41       |
|    time_elapsed     | 5249     |
|    total_timesteps  | 216805   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0162   |
|    n_updates        | 29201    |
----------------------------------
Eval num_timesteps=217000, episode_reward=3.16 +/- 4.59
Episode length: 101.88 +/- 38.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.887    |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0229   |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.26     |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 41       |
|    time_elapsed     | 5261     |
|    total_timesteps  | 217129   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0116   |
|    n_updates        | 29282    |
----------------------------------
Eval num_timesteps=217500, episode_reward=5.26 +/- 9.83
Episode length: 118.70 +/- 83.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 5.26     |
| rollout/            |          |
|    exploration_rate | 0.886    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 29374    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 2.26     |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 41       |
|    time_elapsed     | 5279     |
|    total_timesteps  | 217518   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0537   |
|    n_updates        | 29379    |
----------------------------------
Eval num_timesteps=218000, episode_reward=2.84 +/- 4.43
Episode length: 90.08 +/- 42.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.1     |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.885    |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.05     |
|    n_updates        | 29499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 2.46     |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 41       |
|    time_elapsed     | 5290     |
|    total_timesteps  | 218011   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 29502    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 2.45     |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 41       |
|    time_elapsed     | 5291     |
|    total_timesteps  | 218432   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 29607    |
----------------------------------
Eval num_timesteps=218500, episode_reward=4.66 +/- 6.71
Episode length: 107.64 +/- 47.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.66     |
| rollout/            |          |
|    exploration_rate | 0.885    |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00735  |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 2.44     |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 41       |
|    time_elapsed     | 5305     |
|    total_timesteps  | 218777   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00997  |
|    n_updates        | 29694    |
----------------------------------
Eval num_timesteps=219000, episode_reward=2.24 +/- 4.76
Episode length: 94.54 +/- 49.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.5     |
|    mean_reward      | 2.24     |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 29749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 2.32     |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 41       |
|    time_elapsed     | 5316     |
|    total_timesteps  | 219138   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00624  |
|    n_updates        | 29784    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.4     |
|    ep_rew_mean      | 2.34     |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 41       |
|    time_elapsed     | 5317     |
|    total_timesteps  | 219478   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0636   |
|    n_updates        | 29869    |
----------------------------------
Eval num_timesteps=219500, episode_reward=3.04 +/- 3.74
Episode length: 97.10 +/- 32.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.1     |
|    mean_reward      | 3.04     |
| rollout/            |          |
|    exploration_rate | 0.883    |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 29874    |
----------------------------------
Eval num_timesteps=220000, episode_reward=3.34 +/- 4.28
Episode length: 98.34 +/- 43.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.3     |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.883    |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 29999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 2.54     |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 41       |
|    time_elapsed     | 5341     |
|    total_timesteps  | 220172   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0711   |
|    n_updates        | 30042    |
----------------------------------
Eval num_timesteps=220500, episode_reward=2.36 +/- 3.89
Episode length: 98.64 +/- 36.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.882    |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0294   |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 2.79     |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 41       |
|    time_elapsed     | 5353     |
|    total_timesteps  | 220611   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0516   |
|    n_updates        | 30152    |
----------------------------------
Eval num_timesteps=221000, episode_reward=2.66 +/- 4.59
Episode length: 89.70 +/- 35.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.7     |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.881    |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0576   |
|    n_updates        | 30249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 2.86     |
|    exploration_rate | 0.881    |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 41       |
|    time_elapsed     | 5365     |
|    total_timesteps  | 221156   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0397   |
|    n_updates        | 30288    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 2.89     |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 41       |
|    time_elapsed     | 5366     |
|    total_timesteps  | 221486   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0324   |
|    n_updates        | 30371    |
----------------------------------
Eval num_timesteps=221500, episode_reward=2.04 +/- 2.60
Episode length: 94.98 +/- 37.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95       |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.88     |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 41       |
|    time_elapsed     | 5377     |
|    total_timesteps  | 221891   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 30472    |
----------------------------------
Eval num_timesteps=222000, episode_reward=3.80 +/- 3.45
Episode length: 93.90 +/- 29.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.9     |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.88     |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00556  |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 3.12     |
|    exploration_rate | 0.879    |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 41       |
|    time_elapsed     | 5389     |
|    total_timesteps  | 222405   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 30601    |
----------------------------------
Eval num_timesteps=222500, episode_reward=2.60 +/- 3.30
Episode length: 89.10 +/- 38.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.1     |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.879    |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0609   |
|    n_updates        | 30624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 41       |
|    time_elapsed     | 5402     |
|    total_timesteps  | 222804   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 30700    |
----------------------------------
Eval num_timesteps=223000, episode_reward=3.18 +/- 5.02
Episode length: 95.60 +/- 39.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.6     |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.878    |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 2288     |
|    fps              | 41       |
|    time_elapsed     | 5415     |
|    total_timesteps  | 223347   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0535   |
|    n_updates        | 30836    |
----------------------------------
Eval num_timesteps=223500, episode_reward=3.36 +/- 6.39
Episode length: 109.38 +/- 58.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 2292     |
|    fps              | 41       |
|    time_elapsed     | 5428     |
|    total_timesteps  | 223688   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0725   |
|    n_updates        | 30921    |
----------------------------------
Eval num_timesteps=224000, episode_reward=2.62 +/- 2.99
Episode length: 102.90 +/- 63.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0186   |
|    n_updates        | 30999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 41       |
|    time_elapsed     | 5441     |
|    total_timesteps  | 224191   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0555   |
|    n_updates        | 31047    |
----------------------------------
Eval num_timesteps=224500, episode_reward=3.70 +/- 10.01
Episode length: 100.82 +/- 52.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.876    |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0816   |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.51     |
|    exploration_rate | 0.876    |
| time/               |          |
|    episodes         | 2300     |
|    fps              | 41       |
|    time_elapsed     | 5453     |
|    total_timesteps  | 224641   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 31160    |
----------------------------------
Eval num_timesteps=225000, episode_reward=3.46 +/- 6.75
Episode length: 105.48 +/- 53.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.46     |
| rollout/            |          |
|    exploration_rate | 0.875    |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 2304     |
|    fps              | 41       |
|    time_elapsed     | 5466     |
|    total_timesteps  | 225126   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0604   |
|    n_updates        | 31281    |
----------------------------------
Eval num_timesteps=225500, episode_reward=2.70 +/- 3.67
Episode length: 98.78 +/- 35.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.8     |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.874    |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.55     |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 2308     |
|    fps              | 41       |
|    time_elapsed     | 5478     |
|    total_timesteps  | 225586   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0516   |
|    n_updates        | 31396    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.28     |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 2312     |
|    fps              | 41       |
|    time_elapsed     | 5479     |
|    total_timesteps  | 225893   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 31473    |
----------------------------------
Eval num_timesteps=226000, episode_reward=2.28 +/- 3.37
Episode length: 88.06 +/- 28.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.1     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.874    |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0395   |
|    n_updates        | 31499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.27     |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 2316     |
|    fps              | 41       |
|    time_elapsed     | 5489     |
|    total_timesteps  | 226195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 31548    |
----------------------------------
Eval num_timesteps=226500, episode_reward=4.50 +/- 8.75
Episode length: 106.08 +/- 51.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.5      |
| rollout/            |          |
|    exploration_rate | 0.873    |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.012    |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 2320     |
|    fps              | 41       |
|    time_elapsed     | 5502     |
|    total_timesteps  | 226567   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 31641    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 2324     |
|    fps              | 41       |
|    time_elapsed     | 5503     |
|    total_timesteps  | 226807   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 31701    |
----------------------------------
Eval num_timesteps=227000, episode_reward=2.06 +/- 3.07
Episode length: 83.42 +/- 37.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.4     |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.872    |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0498   |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 2328     |
|    fps              | 41       |
|    time_elapsed     | 5514     |
|    total_timesteps  | 227214   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 31803    |
----------------------------------
Eval num_timesteps=227500, episode_reward=1.36 +/- 2.10
Episode length: 90.16 +/- 30.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.2     |
|    mean_reward      | 1.36     |
| rollout/            |          |
|    exploration_rate | 0.871    |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 31874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.871    |
| time/               |          |
|    episodes         | 2332     |
|    fps              | 41       |
|    time_elapsed     | 5524     |
|    total_timesteps  | 227571   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 31892    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 0.871    |
| time/               |          |
|    episodes         | 2336     |
|    fps              | 41       |
|    time_elapsed     | 5525     |
|    total_timesteps  | 227916   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0638   |
|    n_updates        | 31978    |
----------------------------------
Eval num_timesteps=228000, episode_reward=2.22 +/- 3.53
Episode length: 101.12 +/- 52.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.871    |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0187   |
|    n_updates        | 31999    |
----------------------------------
Eval num_timesteps=228500, episode_reward=3.00 +/- 3.38
Episode length: 106.64 +/- 38.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.87     |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0862   |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.42     |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 2340     |
|    fps              | 41       |
|    time_elapsed     | 5549     |
|    total_timesteps  | 228511   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 32127    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.43     |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 2344     |
|    fps              | 41       |
|    time_elapsed     | 5550     |
|    total_timesteps  | 228843   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0437   |
|    n_updates        | 32210    |
----------------------------------
Eval num_timesteps=229000, episode_reward=4.22 +/- 7.31
Episode length: 113.30 +/- 72.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 4.22     |
| rollout/            |          |
|    exploration_rate | 0.869    |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0884   |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 2348     |
|    fps              | 41       |
|    time_elapsed     | 5564     |
|    total_timesteps  | 229376   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 32343    |
----------------------------------
Eval num_timesteps=229500, episode_reward=2.80 +/- 4.46
Episode length: 93.30 +/- 32.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.3     |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.868    |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 32374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.868    |
| time/               |          |
|    episodes         | 2352     |
|    fps              | 41       |
|    time_elapsed     | 5576     |
|    total_timesteps  | 229835   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0687   |
|    n_updates        | 32458    |
----------------------------------
Eval num_timesteps=230000, episode_reward=3.22 +/- 3.69
Episode length: 109.98 +/- 50.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.868    |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 32499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 2356     |
|    fps              | 41       |
|    time_elapsed     | 5589     |
|    total_timesteps  | 230169   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0456   |
|    n_updates        | 32542    |
----------------------------------
Eval num_timesteps=230500, episode_reward=2.64 +/- 5.07
Episode length: 106.08 +/- 62.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.867    |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0509   |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 2360     |
|    fps              | 41       |
|    time_elapsed     | 5602     |
|    total_timesteps  | 230594   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 32648    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 2364     |
|    fps              | 41       |
|    time_elapsed     | 5603     |
|    total_timesteps  | 230981   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 32745    |
----------------------------------
Eval num_timesteps=231000, episode_reward=2.20 +/- 3.69
Episode length: 100.78 +/- 51.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.866    |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 32749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.865    |
| time/               |          |
|    episodes         | 2368     |
|    fps              | 41       |
|    time_elapsed     | 5615     |
|    total_timesteps  | 231398   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 32849    |
----------------------------------
Eval num_timesteps=231500, episode_reward=1.96 +/- 3.96
Episode length: 91.02 +/- 29.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91       |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 0.865    |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.78     |
|    exploration_rate | 0.865    |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 41       |
|    time_elapsed     | 5627     |
|    total_timesteps  | 231882   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 32970    |
----------------------------------
Eval num_timesteps=232000, episode_reward=2.86 +/- 3.95
Episode length: 100.92 +/- 36.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.864    |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 2376     |
|    fps              | 41       |
|    time_elapsed     | 5639     |
|    total_timesteps  | 232368   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0745   |
|    n_updates        | 33091    |
----------------------------------
Eval num_timesteps=232500, episode_reward=3.24 +/- 5.04
Episode length: 117.32 +/- 60.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.864    |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.46     |
|    exploration_rate | 0.863    |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 41       |
|    time_elapsed     | 5656     |
|    total_timesteps  | 232817   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 33204    |
----------------------------------
Eval num_timesteps=233000, episode_reward=3.78 +/- 5.92
Episode length: 105.18 +/- 57.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.863    |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 33249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.56     |
|    exploration_rate | 0.863    |
| time/               |          |
|    episodes         | 2384     |
|    fps              | 41       |
|    time_elapsed     | 5669     |
|    total_timesteps  | 233237   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0444   |
|    n_updates        | 33309    |
----------------------------------
Eval num_timesteps=233500, episode_reward=2.54 +/- 3.41
Episode length: 100.94 +/- 42.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.862    |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0779   |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 2388     |
|    fps              | 41       |
|    time_elapsed     | 5681     |
|    total_timesteps  | 233841   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 33460    |
----------------------------------
Eval num_timesteps=234000, episode_reward=2.38 +/- 4.14
Episode length: 106.76 +/- 62.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.861    |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 2392     |
|    fps              | 41       |
|    time_elapsed     | 5693     |
|    total_timesteps  | 234312   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0528   |
|    n_updates        | 33577    |
----------------------------------
Eval num_timesteps=234500, episode_reward=2.72 +/- 5.84
Episode length: 106.34 +/- 52.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.861    |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0347   |
|    n_updates        | 33624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.71     |
|    exploration_rate | 0.86     |
| time/               |          |
|    episodes         | 2396     |
|    fps              | 41       |
|    time_elapsed     | 5706     |
|    total_timesteps  | 234718   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0358   |
|    n_updates        | 33679    |
----------------------------------
Eval num_timesteps=235000, episode_reward=2.66 +/- 4.72
Episode length: 105.52 +/- 48.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.86     |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.086    |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.86     |
| time/               |          |
|    episodes         | 2400     |
|    fps              | 41       |
|    time_elapsed     | 5718     |
|    total_timesteps  | 235091   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0797   |
|    n_updates        | 33772    |
----------------------------------
Eval num_timesteps=235500, episode_reward=2.64 +/- 4.22
Episode length: 110.36 +/- 53.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.859    |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0592   |
|    n_updates        | 33874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 2404     |
|    fps              | 41       |
|    time_elapsed     | 5732     |
|    total_timesteps  | 235648   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 33911    |
----------------------------------
Eval num_timesteps=236000, episode_reward=2.60 +/- 3.30
Episode length: 98.54 +/- 40.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.5     |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.858    |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0185   |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 2408     |
|    fps              | 41       |
|    time_elapsed     | 5744     |
|    total_timesteps  | 236162   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0509   |
|    n_updates        | 34040    |
----------------------------------
Eval num_timesteps=236500, episode_reward=2.78 +/- 4.95
Episode length: 94.66 +/- 37.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.858    |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 2412     |
|    fps              | 41       |
|    time_elapsed     | 5755     |
|    total_timesteps  | 236566   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 34141    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 2416     |
|    fps              | 41       |
|    time_elapsed     | 5756     |
|    total_timesteps  | 236994   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0693   |
|    n_updates        | 34248    |
----------------------------------
Eval num_timesteps=237000, episode_reward=2.92 +/- 5.12
Episode length: 91.80 +/- 36.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.8     |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.857    |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0344   |
|    n_updates        | 34249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.856    |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 41       |
|    time_elapsed     | 5767     |
|    total_timesteps  | 237373   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 34343    |
----------------------------------
Eval num_timesteps=237500, episode_reward=2.88 +/- 3.46
Episode length: 98.76 +/- 43.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.8     |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.856    |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 34374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.856    |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 41       |
|    time_elapsed     | 5780     |
|    total_timesteps  | 237743   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0223   |
|    n_updates        | 34435    |
----------------------------------
Eval num_timesteps=238000, episode_reward=3.36 +/- 4.71
Episode length: 106.04 +/- 45.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.855    |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0656   |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 2428     |
|    fps              | 41       |
|    time_elapsed     | 5794     |
|    total_timesteps  | 238155   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 34538    |
----------------------------------
Eval num_timesteps=238500, episode_reward=1.76 +/- 2.89
Episode length: 88.94 +/- 34.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.9     |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 0.854    |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 34624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 2432     |
|    fps              | 41       |
|    time_elapsed     | 5805     |
|    total_timesteps  | 238598   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 34649    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 2436     |
|    fps              | 41       |
|    time_elapsed     | 5806     |
|    total_timesteps  | 238974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 34743    |
----------------------------------
Eval num_timesteps=239000, episode_reward=1.44 +/- 2.26
Episode length: 91.56 +/- 35.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.6     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 0.854    |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.853    |
| time/               |          |
|    episodes         | 2440     |
|    fps              | 41       |
|    time_elapsed     | 5817     |
|    total_timesteps  | 239375   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 34843    |
----------------------------------
Eval num_timesteps=239500, episode_reward=2.62 +/- 5.12
Episode length: 101.68 +/- 48.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.853    |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0549   |
|    n_updates        | 34874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.84     |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 2444     |
|    fps              | 41       |
|    time_elapsed     | 5829     |
|    total_timesteps  | 239964   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 34990    |
----------------------------------
Eval num_timesteps=240000, episode_reward=3.18 +/- 4.65
Episode length: 98.78 +/- 45.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.8     |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.852    |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0744   |
|    n_updates        | 34999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 2448     |
|    fps              | 41       |
|    time_elapsed     | 5842     |
|    total_timesteps  | 240387   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0669   |
|    n_updates        | 35096    |
----------------------------------
Eval num_timesteps=240500, episode_reward=2.44 +/- 5.09
Episode length: 99.56 +/- 40.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.6     |
|    mean_reward      | 2.44     |
| rollout/            |          |
|    exploration_rate | 0.851    |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0316   |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.64     |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 2452     |
|    fps              | 41       |
|    time_elapsed     | 5855     |
|    total_timesteps  | 240941   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0779   |
|    n_updates        | 35235    |
----------------------------------
Eval num_timesteps=241000, episode_reward=3.28 +/- 5.20
Episode length: 97.48 +/- 41.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.5     |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.85     |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0922   |
|    n_updates        | 35249    |
----------------------------------
Eval num_timesteps=241500, episode_reward=2.14 +/- 4.08
Episode length: 88.66 +/- 42.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.7     |
|    mean_reward      | 2.14     |
| rollout/            |          |
|    exploration_rate | 0.85     |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0286   |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.56     |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 2456     |
|    fps              | 41       |
|    time_elapsed     | 5878     |
|    total_timesteps  | 241569   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0511   |
|    n_updates        | 35392    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.47     |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 2460     |
|    fps              | 41       |
|    time_elapsed     | 5879     |
|    total_timesteps  | 241902   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0288   |
|    n_updates        | 35475    |
----------------------------------
Eval num_timesteps=242000, episode_reward=2.86 +/- 5.10
Episode length: 105.92 +/- 54.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.849    |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0459   |
|    n_updates        | 35499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.49     |
|    exploration_rate | 0.848    |
| time/               |          |
|    episodes         | 2464     |
|    fps              | 41       |
|    time_elapsed     | 5892     |
|    total_timesteps  | 242365   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0622   |
|    n_updates        | 35591    |
----------------------------------
Eval num_timesteps=242500, episode_reward=3.60 +/- 4.40
Episode length: 111.52 +/- 64.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.848    |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0536   |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.51     |
|    exploration_rate | 0.848    |
| time/               |          |
|    episodes         | 2468     |
|    fps              | 41       |
|    time_elapsed     | 5907     |
|    total_timesteps  | 242743   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.084    |
|    n_updates        | 35685    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.847    |
| time/               |          |
|    episodes         | 2472     |
|    fps              | 41       |
|    time_elapsed     | 5908     |
|    total_timesteps  | 242977   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.042    |
|    n_updates        | 35744    |
----------------------------------
Eval num_timesteps=243000, episode_reward=2.06 +/- 2.82
Episode length: 99.32 +/- 44.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.847    |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 0.847    |
| time/               |          |
|    episodes         | 2476     |
|    fps              | 41       |
|    time_elapsed     | 5920     |
|    total_timesteps  | 243187   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 35796    |
----------------------------------
Eval num_timesteps=243500, episode_reward=3.94 +/- 6.06
Episode length: 126.02 +/- 84.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.05     |
|    n_updates        | 35874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 2480     |
|    fps              | 41       |
|    time_elapsed     | 5934     |
|    total_timesteps  | 243740   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0813   |
|    n_updates        | 35934    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.37     |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 2484     |
|    fps              | 41       |
|    time_elapsed     | 5935     |
|    total_timesteps  | 243970   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 35992    |
----------------------------------
Eval num_timesteps=244000, episode_reward=2.72 +/- 4.97
Episode length: 96.22 +/- 52.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 35999    |
----------------------------------
Eval num_timesteps=244500, episode_reward=2.78 +/- 4.81
Episode length: 103.00 +/- 56.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.845    |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0265   |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 2488     |
|    fps              | 41       |
|    time_elapsed     | 5958     |
|    total_timesteps  | 244533   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 36133    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 0.844    |
| time/               |          |
|    episodes         | 2492     |
|    fps              | 41       |
|    time_elapsed     | 5959     |
|    total_timesteps  | 244916   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 36228    |
----------------------------------
Eval num_timesteps=245000, episode_reward=2.40 +/- 2.79
Episode length: 89.44 +/- 29.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.4     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.844    |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 36249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.04     |
|    exploration_rate | 0.844    |
| time/               |          |
|    episodes         | 2496     |
|    fps              | 41       |
|    time_elapsed     | 5970     |
|    total_timesteps  | 245224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.026    |
|    n_updates        | 36305    |
----------------------------------
Eval num_timesteps=245500, episode_reward=2.50 +/- 5.94
Episode length: 123.14 +/- 109.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0861   |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 2500     |
|    fps              | 41       |
|    time_elapsed     | 5984     |
|    total_timesteps  | 245520   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.037    |
|    n_updates        | 36379    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.94     |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 2504     |
|    fps              | 41       |
|    time_elapsed     | 5985     |
|    total_timesteps  | 245881   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0369   |
|    n_updates        | 36470    |
----------------------------------
Eval num_timesteps=246000, episode_reward=2.86 +/- 3.73
Episode length: 90.54 +/- 32.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.5     |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.842    |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0503   |
|    n_updates        | 36499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.85     |
|    exploration_rate | 0.842    |
| time/               |          |
|    episodes         | 2508     |
|    fps              | 41       |
|    time_elapsed     | 6000     |
|    total_timesteps  | 246264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0702   |
|    n_updates        | 36565    |
----------------------------------
Eval num_timesteps=246500, episode_reward=2.56 +/- 4.67
Episode length: 97.48 +/- 39.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.5     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.842    |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 36624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 2.65     |
|    exploration_rate | 0.841    |
| time/               |          |
|    episodes         | 2512     |
|    fps              | 41       |
|    time_elapsed     | 6013     |
|    total_timesteps  | 246592   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 36647    |
----------------------------------
Eval num_timesteps=247000, episode_reward=2.72 +/- 4.55
Episode length: 94.10 +/- 53.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.1     |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.841    |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0764   |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 2516     |
|    fps              | 41       |
|    time_elapsed     | 6025     |
|    total_timesteps  | 247195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 36798    |
----------------------------------
Eval num_timesteps=247500, episode_reward=3.00 +/- 4.78
Episode length: 92.22 +/- 36.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.2     |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.84     |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 36874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 2520     |
|    fps              | 41       |
|    time_elapsed     | 6036     |
|    total_timesteps  | 247529   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0508   |
|    n_updates        | 36882    |
----------------------------------
Eval num_timesteps=248000, episode_reward=3.02 +/- 4.30
Episode length: 87.56 +/- 33.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.6     |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.839    |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0368   |
|    n_updates        | 36999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.37     |
|    exploration_rate | 0.839    |
| time/               |          |
|    episodes         | 2524     |
|    fps              | 40       |
|    time_elapsed     | 6051     |
|    total_timesteps  | 248069   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 37017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 2528     |
|    fps              | 41       |
|    time_elapsed     | 6053     |
|    total_timesteps  | 248437   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.053    |
|    n_updates        | 37109    |
----------------------------------
Eval num_timesteps=248500, episode_reward=2.32 +/- 3.46
Episode length: 92.32 +/- 31.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.3     |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.838    |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 2532     |
|    fps              | 41       |
|    time_elapsed     | 6064     |
|    total_timesteps  | 248717   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0745   |
|    n_updates        | 37179    |
----------------------------------
Eval num_timesteps=249000, episode_reward=1.98 +/- 3.34
Episode length: 91.14 +/- 29.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.1     |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 0.837    |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.066    |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 2536     |
|    fps              | 40       |
|    time_elapsed     | 6074     |
|    total_timesteps  | 249026   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 37256    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 2540     |
|    fps              | 41       |
|    time_elapsed     | 6075     |
|    total_timesteps  | 249410   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0326   |
|    n_updates        | 37352    |
----------------------------------
Eval num_timesteps=249500, episode_reward=2.04 +/- 2.54
Episode length: 90.00 +/- 30.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90       |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.837    |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 37374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.836    |
| time/               |          |
|    episodes         | 2544     |
|    fps              | 41       |
|    time_elapsed     | 6086     |
|    total_timesteps  | 249845   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0388   |
|    n_updates        | 37461    |
----------------------------------
Eval num_timesteps=250000, episode_reward=2.62 +/- 4.25
Episode length: 88.58 +/- 34.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.6     |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0665   |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 0.836    |
| time/               |          |
|    episodes         | 2548     |
|    fps              | 41       |
|    time_elapsed     | 6097     |
|    total_timesteps  | 250196   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0533   |
|    n_updates        | 37548    |
----------------------------------
Eval num_timesteps=250500, episode_reward=4.08 +/- 4.70
Episode length: 133.94 +/- 67.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 4.08     |
| rollout/            |          |
|    exploration_rate | 0.835    |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0617   |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 2.91     |
|    exploration_rate | 0.835    |
| time/               |          |
|    episodes         | 2552     |
|    fps              | 40       |
|    time_elapsed     | 6112     |
|    total_timesteps  | 250514   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 37628    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 2.74     |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 2556     |
|    fps              | 41       |
|    time_elapsed     | 6113     |
|    total_timesteps  | 250866   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0363   |
|    n_updates        | 37716    |
----------------------------------
Eval num_timesteps=251000, episode_reward=2.52 +/- 4.70
Episode length: 91.38 +/- 38.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.4     |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 0.834    |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 37749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 2.79     |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 2560     |
|    fps              | 41       |
|    time_elapsed     | 6124     |
|    total_timesteps  | 251176   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0772   |
|    n_updates        | 37793    |
----------------------------------
Eval num_timesteps=251500, episode_reward=2.76 +/- 5.26
Episode length: 95.90 +/- 66.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.9     |
|    mean_reward      | 2.76     |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0342   |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 2.89     |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 2564     |
|    fps              | 40       |
|    time_elapsed     | 6139     |
|    total_timesteps  | 251514   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0457   |
|    n_updates        | 37878    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 2568     |
|    fps              | 41       |
|    time_elapsed     | 6141     |
|    total_timesteps  | 251946   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0342   |
|    n_updates        | 37986    |
----------------------------------
Eval num_timesteps=252000, episode_reward=1.78 +/- 3.20
Episode length: 96.68 +/- 37.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.7     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0717   |
|    n_updates        | 37999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.2     |
|    ep_rew_mean      | 3.04     |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 2572     |
|    fps              | 40       |
|    time_elapsed     | 6156     |
|    total_timesteps  | 252198   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 38049    |
----------------------------------
Eval num_timesteps=252500, episode_reward=1.90 +/- 2.89
Episode length: 98.66 +/- 41.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 1.9      |
| rollout/            |          |
|    exploration_rate | 0.832    |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.6     |
|    ep_rew_mean      | 3.08     |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 2576     |
|    fps              | 40       |
|    time_elapsed     | 6172     |
|    total_timesteps  | 252546   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0957   |
|    n_updates        | 38136    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.8     |
|    ep_rew_mean      | 2.84     |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 2580     |
|    fps              | 40       |
|    time_elapsed     | 6173     |
|    total_timesteps  | 252823   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 38205    |
----------------------------------
Eval num_timesteps=253000, episode_reward=3.98 +/- 6.32
Episode length: 111.20 +/- 55.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.98     |
| rollout/            |          |
|    exploration_rate | 0.831    |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 38249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.2     |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 2584     |
|    fps              | 40       |
|    time_elapsed     | 6186     |
|    total_timesteps  | 253094   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0403   |
|    n_updates        | 38273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | 2.6      |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 2588     |
|    fps              | 40       |
|    time_elapsed     | 6187     |
|    total_timesteps  | 253361   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0778   |
|    n_updates        | 38340    |
----------------------------------
Eval num_timesteps=253500, episode_reward=3.66 +/- 5.45
Episode length: 116.22 +/- 91.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.83     |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0487   |
|    n_updates        | 38374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.8     |
|    ep_rew_mean      | 2.68     |
|    exploration_rate | 0.829    |
| time/               |          |
|    episodes         | 2592     |
|    fps              | 40       |
|    time_elapsed     | 6201     |
|    total_timesteps  | 253999   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0589   |
|    n_updates        | 38499    |
----------------------------------
Eval num_timesteps=254000, episode_reward=2.42 +/- 4.26
Episode length: 112.66 +/- 95.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 2.42     |
| time/              |          |
|    total_timesteps | 254000   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.6     |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.829    |
| time/               |          |
|    episodes         | 2596     |
|    fps              | 40       |
|    time_elapsed     | 6215     |
|    total_timesteps  | 254387   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0789   |
|    n_updates        | 38596    |
----------------------------------
Eval num_timesteps=254500, episode_reward=4.68 +/- 8.21
Episode length: 114.44 +/- 70.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0543   |
|    n_updates        | 38624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 2.64     |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 2600     |
|    fps              | 40       |
|    time_elapsed     | 6228     |
|    total_timesteps  | 254888   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0365   |
|    n_updates        | 38721    |
----------------------------------
Eval num_timesteps=255000, episode_reward=3.18 +/- 3.89
Episode length: 110.56 +/- 51.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0185   |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 2604     |
|    fps              | 40       |
|    time_elapsed     | 6242     |
|    total_timesteps  | 255285   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0923   |
|    n_updates        | 38821    |
----------------------------------
Eval num_timesteps=255500, episode_reward=2.38 +/- 2.86
Episode length: 85.90 +/- 34.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.9     |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.827    |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0937   |
|    n_updates        | 38874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.3     |
|    ep_rew_mean      | 2.93     |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 2608     |
|    fps              | 40       |
|    time_elapsed     | 6252     |
|    total_timesteps  | 255596   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0533   |
|    n_updates        | 38898    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 2.91     |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 2612     |
|    fps              | 40       |
|    time_elapsed     | 6253     |
|    total_timesteps  | 255867   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0209   |
|    n_updates        | 38966    |
----------------------------------
Eval num_timesteps=256000, episode_reward=3.70 +/- 7.01
Episode length: 98.12 +/- 39.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.1     |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.826    |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0576   |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.2     |
|    ep_rew_mean      | 2.64     |
|    exploration_rate | 0.825    |
| time/               |          |
|    episodes         | 2616     |
|    fps              | 40       |
|    time_elapsed     | 6265     |
|    total_timesteps  | 256316   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0675   |
|    n_updates        | 39078    |
----------------------------------
Eval num_timesteps=256500, episode_reward=1.86 +/- 2.74
Episode length: 83.68 +/- 26.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.7     |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 0.825    |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0836   |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 2.77     |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 2620     |
|    fps              | 40       |
|    time_elapsed     | 6276     |
|    total_timesteps  | 256905   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0793   |
|    n_updates        | 39226    |
----------------------------------
Eval num_timesteps=257000, episode_reward=3.02 +/- 4.19
Episode length: 108.22 +/- 39.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.824    |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0651   |
|    n_updates        | 39249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 2.41     |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 2624     |
|    fps              | 40       |
|    time_elapsed     | 6289     |
|    total_timesteps  | 257371   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0374   |
|    n_updates        | 39342    |
----------------------------------
Eval num_timesteps=257500, episode_reward=2.76 +/- 4.42
Episode length: 95.26 +/- 39.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.3     |
|    mean_reward      | 2.76     |
| rollout/            |          |
|    exploration_rate | 0.823    |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.2     |
|    ep_rew_mean      | 2.28     |
|    exploration_rate | 0.823    |
| time/               |          |
|    episodes         | 2628     |
|    fps              | 40       |
|    time_elapsed     | 6300     |
|    total_timesteps  | 257658   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0533   |
|    n_updates        | 39414    |
----------------------------------
Eval num_timesteps=258000, episode_reward=3.96 +/- 6.44
Episode length: 110.54 +/- 64.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.823    |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.097    |
|    n_updates        | 39499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.3     |
|    ep_rew_mean      | 2.32     |
|    exploration_rate | 0.822    |
| time/               |          |
|    episodes         | 2632     |
|    fps              | 40       |
|    time_elapsed     | 6313     |
|    total_timesteps  | 258050   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0262   |
|    n_updates        | 39512    |
----------------------------------
Eval num_timesteps=258500, episode_reward=2.66 +/- 4.19
Episode length: 90.76 +/- 48.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.822    |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0418   |
|    n_updates        | 39624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 2.46     |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 2636     |
|    fps              | 40       |
|    time_elapsed     | 6325     |
|    total_timesteps  | 258680   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.06     |
|    n_updates        | 39669    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 2640     |
|    fps              | 40       |
|    time_elapsed     | 6326     |
|    total_timesteps  | 258996   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0343   |
|    n_updates        | 39748    |
----------------------------------
Eval num_timesteps=259000, episode_reward=2.80 +/- 4.36
Episode length: 89.98 +/- 35.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90       |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.821    |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 39749    |
----------------------------------
Eval num_timesteps=259500, episode_reward=3.22 +/- 4.54
Episode length: 107.00 +/- 45.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.82     |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0733   |
|    n_updates        | 39874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 2.6      |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 2644     |
|    fps              | 40       |
|    time_elapsed     | 6349     |
|    total_timesteps  | 259544   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0392   |
|    n_updates        | 39885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | 2.68     |
|    exploration_rate | 0.819    |
| time/               |          |
|    episodes         | 2648     |
|    fps              | 40       |
|    time_elapsed     | 6350     |
|    total_timesteps  | 259855   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0329   |
|    n_updates        | 39963    |
----------------------------------
Eval num_timesteps=260000, episode_reward=2.92 +/- 4.06
Episode length: 96.10 +/- 47.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.1     |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.819    |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.076    |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 2.63     |
|    exploration_rate | 0.819    |
| time/               |          |
|    episodes         | 2652     |
|    fps              | 40       |
|    time_elapsed     | 6363     |
|    total_timesteps  | 260166   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0637   |
|    n_updates        | 40041    |
----------------------------------
Eval num_timesteps=260500, episode_reward=1.78 +/- 2.48
Episode length: 86.92 +/- 40.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.9     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 0.818    |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0746   |
|    n_updates        | 40124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | 2.72     |
|    exploration_rate | 0.818    |
| time/               |          |
|    episodes         | 2656     |
|    fps              | 40       |
|    time_elapsed     | 6373     |
|    total_timesteps  | 260523   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0367   |
|    n_updates        | 40130    |
----------------------------------
Eval num_timesteps=261000, episode_reward=2.74 +/- 3.61
Episode length: 106.12 +/- 53.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.817    |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0489   |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 2.72     |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 2660     |
|    fps              | 40       |
|    time_elapsed     | 6386     |
|    total_timesteps  | 261028   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0544   |
|    n_updates        | 40256    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 2.59     |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 2664     |
|    fps              | 40       |
|    time_elapsed     | 6387     |
|    total_timesteps  | 261389   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0689   |
|    n_updates        | 40347    |
----------------------------------
Eval num_timesteps=261500, episode_reward=1.44 +/- 2.29
Episode length: 94.34 +/- 44.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.3     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 0.817    |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0786   |
|    n_updates        | 40374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 2.38     |
|    exploration_rate | 0.816    |
| time/               |          |
|    episodes         | 2668     |
|    fps              | 40       |
|    time_elapsed     | 6399     |
|    total_timesteps  | 261804   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0616   |
|    n_updates        | 40450    |
----------------------------------
Eval num_timesteps=262000, episode_reward=2.18 +/- 3.28
Episode length: 97.66 +/- 58.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.7     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.816    |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0768   |
|    n_updates        | 40499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.45     |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 2672     |
|    fps              | 40       |
|    time_elapsed     | 6410     |
|    total_timesteps  | 262253   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0542   |
|    n_updates        | 40563    |
----------------------------------
Eval num_timesteps=262500, episode_reward=2.70 +/- 5.22
Episode length: 104.00 +/- 54.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0352   |
|    n_updates        | 40624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.55     |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 2676     |
|    fps              | 40       |
|    time_elapsed     | 6423     |
|    total_timesteps  | 262646   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0634   |
|    n_updates        | 40661    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.63     |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 2680     |
|    fps              | 40       |
|    time_elapsed     | 6424     |
|    total_timesteps  | 262980   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0255   |
|    n_updates        | 40744    |
----------------------------------
Eval num_timesteps=263000, episode_reward=2.18 +/- 3.40
Episode length: 92.56 +/- 34.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.814    |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0427   |
|    n_updates        | 40749    |
----------------------------------
Eval num_timesteps=263500, episode_reward=2.94 +/- 5.39
Episode length: 94.36 +/- 41.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.813    |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0856   |
|    n_updates        | 40874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 2.91     |
|    exploration_rate | 0.813    |
| time/               |          |
|    episodes         | 2684     |
|    fps              | 40       |
|    time_elapsed     | 6445     |
|    total_timesteps  | 263575   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 40893    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 2.96     |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 2688     |
|    fps              | 40       |
|    time_elapsed     | 6447     |
|    total_timesteps  | 263964   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0602   |
|    n_updates        | 40990    |
----------------------------------
Eval num_timesteps=264000, episode_reward=2.48 +/- 7.81
Episode length: 88.48 +/- 45.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.5     |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.812    |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3        |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 2692     |
|    fps              | 40       |
|    time_elapsed     | 6458     |
|    total_timesteps  | 264327   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 41081    |
----------------------------------
Eval num_timesteps=264500, episode_reward=3.74 +/- 7.98
Episode length: 122.08 +/- 68.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.811    |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0397   |
|    n_updates        | 41124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.811    |
| time/               |          |
|    episodes         | 2696     |
|    fps              | 40       |
|    time_elapsed     | 6472     |
|    total_timesteps  | 264937   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0658   |
|    n_updates        | 41234    |
----------------------------------
Eval num_timesteps=265000, episode_reward=1.84 +/- 3.34
Episode length: 87.12 +/- 33.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.1     |
|    mean_reward      | 1.84     |
| rollout/            |          |
|    exploration_rate | 0.811    |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.061    |
|    n_updates        | 41249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.81     |
| time/               |          |
|    episodes         | 2700     |
|    fps              | 40       |
|    time_elapsed     | 6484     |
|    total_timesteps  | 265241   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0326   |
|    n_updates        | 41310    |
----------------------------------
Eval num_timesteps=265500, episode_reward=3.06 +/- 5.37
Episode length: 102.68 +/- 62.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.81     |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0746   |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.19     |
|    exploration_rate | 0.809    |
| time/               |          |
|    episodes         | 2704     |
|    fps              | 40       |
|    time_elapsed     | 6497     |
|    total_timesteps  | 265810   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 41452    |
----------------------------------
Eval num_timesteps=266000, episode_reward=2.80 +/- 6.01
Episode length: 91.20 +/- 51.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.2     |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.809    |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.809    |
| time/               |          |
|    episodes         | 2708     |
|    fps              | 40       |
|    time_elapsed     | 6508     |
|    total_timesteps  | 266161   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0396   |
|    n_updates        | 41540    |
----------------------------------
Eval num_timesteps=266500, episode_reward=5.18 +/- 12.74
Episode length: 123.50 +/- 93.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.18     |
| rollout/            |          |
|    exploration_rate | 0.808    |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0332   |
|    n_updates        | 41624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 2712     |
|    fps              | 40       |
|    time_elapsed     | 6525     |
|    total_timesteps  | 266848   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0649   |
|    n_updates        | 41711    |
----------------------------------
Eval num_timesteps=267000, episode_reward=2.84 +/- 4.23
Episode length: 101.66 +/- 51.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.807    |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0264   |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 2716     |
|    fps              | 40       |
|    time_elapsed     | 6537     |
|    total_timesteps  | 267244   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 41810    |
----------------------------------
Eval num_timesteps=267500, episode_reward=3.74 +/- 4.42
Episode length: 100.96 +/- 41.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.806    |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 2720     |
|    fps              | 40       |
|    time_elapsed     | 6549     |
|    total_timesteps  | 267755   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 41938    |
----------------------------------
Eval num_timesteps=268000, episode_reward=2.30 +/- 4.41
Episode length: 93.90 +/- 37.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.9     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.805    |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 41999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.805    |
| time/               |          |
|    episodes         | 2724     |
|    fps              | 40       |
|    time_elapsed     | 6561     |
|    total_timesteps  | 268128   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 42031    |
----------------------------------
Eval num_timesteps=268500, episode_reward=3.56 +/- 7.64
Episode length: 118.28 +/- 72.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.805    |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 2728     |
|    fps              | 40       |
|    time_elapsed     | 6575     |
|    total_timesteps  | 268632   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0527   |
|    n_updates        | 42157    |
----------------------------------
Eval num_timesteps=269000, episode_reward=2.28 +/- 3.18
Episode length: 94.22 +/- 43.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.2     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.804    |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0353   |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 2732     |
|    fps              | 40       |
|    time_elapsed     | 6590     |
|    total_timesteps  | 269170   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0856   |
|    n_updates        | 42292    |
----------------------------------
Eval num_timesteps=269500, episode_reward=4.54 +/- 8.11
Episode length: 109.58 +/- 68.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 4.54     |
| rollout/            |          |
|    exploration_rate | 0.803    |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 42374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 2736     |
|    fps              | 40       |
|    time_elapsed     | 6604     |
|    total_timesteps  | 269600   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0528   |
|    n_updates        | 42399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.802    |
| time/               |          |
|    episodes         | 2740     |
|    fps              | 40       |
|    time_elapsed     | 6605     |
|    total_timesteps  | 269951   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 42487    |
----------------------------------
Eval num_timesteps=270000, episode_reward=2.70 +/- 4.18
Episode length: 96.86 +/- 37.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.9     |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.802    |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0522   |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.801    |
| time/               |          |
|    episodes         | 2744     |
|    fps              | 40       |
|    time_elapsed     | 6618     |
|    total_timesteps  | 270375   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0995   |
|    n_updates        | 42593    |
----------------------------------
Eval num_timesteps=270500, episode_reward=2.32 +/- 4.60
Episode length: 94.64 +/- 46.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.801    |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0562   |
|    n_updates        | 42624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 2748     |
|    fps              | 40       |
|    time_elapsed     | 6635     |
|    total_timesteps  | 270845   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 42711    |
----------------------------------
Eval num_timesteps=271000, episode_reward=1.84 +/- 3.65
Episode length: 85.34 +/- 35.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.3     |
|    mean_reward      | 1.84     |
| rollout/            |          |
|    exploration_rate | 0.8      |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0481   |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 2752     |
|    fps              | 40       |
|    time_elapsed     | 6648     |
|    total_timesteps  | 271181   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.027    |
|    n_updates        | 42795    |
----------------------------------
Eval num_timesteps=271500, episode_reward=2.28 +/- 4.32
Episode length: 88.64 +/- 42.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.6     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.799    |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0854   |
|    n_updates        | 42874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 2756     |
|    fps              | 40       |
|    time_elapsed     | 6659     |
|    total_timesteps  | 271551   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0785   |
|    n_updates        | 42887    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.62     |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 2760     |
|    fps              | 40       |
|    time_elapsed     | 6660     |
|    total_timesteps  | 271973   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0931   |
|    n_updates        | 42993    |
----------------------------------
Eval num_timesteps=272000, episode_reward=2.56 +/- 3.93
Episode length: 101.78 +/- 51.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.65     |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 2764     |
|    fps              | 40       |
|    time_elapsed     | 6672     |
|    total_timesteps  | 272397   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.118    |
|    n_updates        | 43099    |
----------------------------------
Eval num_timesteps=272500, episode_reward=1.68 +/- 3.32
Episode length: 77.36 +/- 31.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.4     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0809   |
|    n_updates        | 43124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.797    |
| time/               |          |
|    episodes         | 2768     |
|    fps              | 40       |
|    time_elapsed     | 6682     |
|    total_timesteps  | 272848   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0691   |
|    n_updates        | 43211    |
----------------------------------
Eval num_timesteps=273000, episode_reward=2.68 +/- 4.19
Episode length: 105.66 +/- 42.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.797    |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.159    |
|    n_updates        | 43249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.84     |
|    exploration_rate | 0.796    |
| time/               |          |
|    episodes         | 2772     |
|    fps              | 40       |
|    time_elapsed     | 6695     |
|    total_timesteps  | 273274   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0386   |
|    n_updates        | 43318    |
----------------------------------
Eval num_timesteps=273500, episode_reward=4.34 +/- 8.53
Episode length: 110.36 +/- 67.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.796    |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.795    |
| time/               |          |
|    episodes         | 2776     |
|    fps              | 40       |
|    time_elapsed     | 6708     |
|    total_timesteps  | 273695   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.084    |
|    n_updates        | 43423    |
----------------------------------
Eval num_timesteps=274000, episode_reward=2.30 +/- 4.06
Episode length: 85.72 +/- 31.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.7     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.795    |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0519   |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 2780     |
|    fps              | 40       |
|    time_elapsed     | 6720     |
|    total_timesteps  | 274313   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 43578    |
----------------------------------
Eval num_timesteps=274500, episode_reward=2.92 +/- 5.56
Episode length: 102.80 +/- 46.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.794    |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0738   |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 2784     |
|    fps              | 40       |
|    time_elapsed     | 6733     |
|    total_timesteps  | 274727   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0601   |
|    n_updates        | 43681    |
----------------------------------
Eval num_timesteps=275000, episode_reward=2.62 +/- 5.48
Episode length: 106.12 +/- 63.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.793    |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0376   |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.793    |
| time/               |          |
|    episodes         | 2788     |
|    fps              | 40       |
|    time_elapsed     | 6746     |
|    total_timesteps  | 275247   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0572   |
|    n_updates        | 43811    |
----------------------------------
Eval num_timesteps=275500, episode_reward=3.06 +/- 4.35
Episode length: 99.34 +/- 37.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.792    |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.064    |
|    n_updates        | 43874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.792    |
| time/               |          |
|    episodes         | 2792     |
|    fps              | 40       |
|    time_elapsed     | 6758     |
|    total_timesteps  | 275618   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0486   |
|    n_updates        | 43904    |
----------------------------------
Eval num_timesteps=276000, episode_reward=2.36 +/- 3.93
Episode length: 105.16 +/- 62.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.791    |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0635   |
|    n_updates        | 43999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.791    |
| time/               |          |
|    episodes         | 2796     |
|    fps              | 40       |
|    time_elapsed     | 6772     |
|    total_timesteps  | 276293   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 44073    |
----------------------------------
Eval num_timesteps=276500, episode_reward=1.54 +/- 2.54
Episode length: 91.12 +/- 37.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.1     |
|    mean_reward      | 1.54     |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0369   |
|    n_updates        | 44124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.79     |
| time/               |          |
|    episodes         | 2800     |
|    fps              | 40       |
|    time_elapsed     | 6783     |
|    total_timesteps  | 276675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.065    |
|    n_updates        | 44168    |
----------------------------------
Eval num_timesteps=277000, episode_reward=3.18 +/- 4.40
Episode length: 105.34 +/- 45.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0986   |
|    n_updates        | 44249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 2804     |
|    fps              | 40       |
|    time_elapsed     | 6796     |
|    total_timesteps  | 277067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 44266    |
----------------------------------
Eval num_timesteps=277500, episode_reward=3.12 +/- 3.80
Episode length: 105.50 +/- 38.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.789    |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0259   |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.71     |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 2808     |
|    fps              | 40       |
|    time_elapsed     | 6808     |
|    total_timesteps  | 277568   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0549   |
|    n_updates        | 44391    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.788    |
| time/               |          |
|    episodes         | 2812     |
|    fps              | 40       |
|    time_elapsed     | 6809     |
|    total_timesteps  | 277932   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 44482    |
----------------------------------
Eval num_timesteps=278000, episode_reward=2.46 +/- 3.98
Episode length: 96.44 +/- 43.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.4     |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.788    |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0468   |
|    n_updates        | 44499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 0.787    |
| time/               |          |
|    episodes         | 2816     |
|    fps              | 40       |
|    time_elapsed     | 6821     |
|    total_timesteps  | 278419   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0266   |
|    n_updates        | 44604    |
----------------------------------
Eval num_timesteps=278500, episode_reward=3.94 +/- 6.99
Episode length: 102.92 +/- 59.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.787    |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 44624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 2820     |
|    fps              | 40       |
|    time_elapsed     | 6834     |
|    total_timesteps  | 278869   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 44717    |
----------------------------------
Eval num_timesteps=279000, episode_reward=1.94 +/- 2.98
Episode length: 94.90 +/- 39.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.9     |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 0.786    |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0305   |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 2824     |
|    fps              | 40       |
|    time_elapsed     | 6847     |
|    total_timesteps  | 279230   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0645   |
|    n_updates        | 44807    |
----------------------------------
Eval num_timesteps=279500, episode_reward=2.54 +/- 6.05
Episode length: 118.50 +/- 113.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.785    |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.147    |
|    n_updates        | 44874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.17     |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 2828     |
|    fps              | 40       |
|    time_elapsed     | 6861     |
|    total_timesteps  | 279643   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0894   |
|    n_updates        | 44910    |
----------------------------------
Eval num_timesteps=280000, episode_reward=3.94 +/- 7.99
Episode length: 118.04 +/- 65.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.784    |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0649   |
|    n_updates        | 44999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 2.98     |
|    exploration_rate | 0.784    |
| time/               |          |
|    episodes         | 2832     |
|    fps              | 40       |
|    time_elapsed     | 6875     |
|    total_timesteps  | 280059   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0773   |
|    n_updates        | 45014    |
----------------------------------
Eval num_timesteps=280500, episode_reward=3.12 +/- 5.69
Episode length: 121.34 +/- 76.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.783    |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0784   |
|    n_updates        | 45124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.01     |
|    exploration_rate | 0.783    |
| time/               |          |
|    episodes         | 2836     |
|    fps              | 40       |
|    time_elapsed     | 6890     |
|    total_timesteps  | 280576   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 45143    |
----------------------------------
Eval num_timesteps=281000, episode_reward=3.02 +/- 5.85
Episode length: 111.42 +/- 60.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.782    |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0851   |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.11     |
|    exploration_rate | 0.782    |
| time/               |          |
|    episodes         | 2840     |
|    fps              | 40       |
|    time_elapsed     | 6905     |
|    total_timesteps  | 281080   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0611   |
|    n_updates        | 45269    |
----------------------------------
Eval num_timesteps=281500, episode_reward=5.54 +/- 7.44
Episode length: 140.90 +/- 68.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 5.54     |
| rollout/            |          |
|    exploration_rate | 0.782    |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0814   |
|    n_updates        | 45374    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.51     |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 2844     |
|    fps              | 40       |
|    time_elapsed     | 6923     |
|    total_timesteps  | 281637   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.169    |
|    n_updates        | 45409    |
----------------------------------
Eval num_timesteps=282000, episode_reward=2.40 +/- 5.25
Episode length: 103.62 +/- 83.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.781    |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 45499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 0.78     |
| time/               |          |
|    episodes         | 2848     |
|    fps              | 40       |
|    time_elapsed     | 6935     |
|    total_timesteps  | 282134   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 45533    |
----------------------------------
Eval num_timesteps=282500, episode_reward=3.78 +/- 5.70
Episode length: 120.28 +/- 75.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.78     |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.61     |
|    exploration_rate | 0.78     |
| time/               |          |
|    episodes         | 2852     |
|    fps              | 40       |
|    time_elapsed     | 6949     |
|    total_timesteps  | 282510   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0409   |
|    n_updates        | 45627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.779    |
| time/               |          |
|    episodes         | 2856     |
|    fps              | 40       |
|    time_elapsed     | 6950     |
|    total_timesteps  | 282875   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0591   |
|    n_updates        | 45718    |
----------------------------------
Eval num_timesteps=283000, episode_reward=4.18 +/- 6.18
Episode length: 113.68 +/- 47.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.068    |
|    n_updates        | 45749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 2860     |
|    fps              | 40       |
|    time_elapsed     | 6964     |
|    total_timesteps  | 283372   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0444   |
|    n_updates        | 45842    |
----------------------------------
Eval num_timesteps=283500, episode_reward=3.12 +/- 6.41
Episode length: 106.94 +/- 59.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.778    |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0797   |
|    n_updates        | 45874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 2864     |
|    fps              | 40       |
|    time_elapsed     | 6976     |
|    total_timesteps  | 283635   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 45908    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.777    |
| time/               |          |
|    episodes         | 2868     |
|    fps              | 40       |
|    time_elapsed     | 6977     |
|    total_timesteps  | 283989   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0424   |
|    n_updates        | 45997    |
----------------------------------
Eval num_timesteps=284000, episode_reward=2.96 +/- 4.28
Episode length: 101.36 +/- 39.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.777    |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0637   |
|    n_updates        | 45999    |
----------------------------------
Eval num_timesteps=284500, episode_reward=3.00 +/- 3.40
Episode length: 103.66 +/- 45.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.776    |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0743   |
|    n_updates        | 46124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.776    |
| time/               |          |
|    episodes         | 2872     |
|    fps              | 40       |
|    time_elapsed     | 7001     |
|    total_timesteps  | 284555   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0647   |
|    n_updates        | 46138    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 2876     |
|    fps              | 40       |
|    time_elapsed     | 7002     |
|    total_timesteps  | 284880   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 46219    |
----------------------------------
Eval num_timesteps=285000, episode_reward=3.04 +/- 5.17
Episode length: 99.86 +/- 38.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.9     |
|    mean_reward      | 3.04     |
| rollout/            |          |
|    exploration_rate | 0.775    |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.46     |
|    exploration_rate | 0.774    |
| time/               |          |
|    episodes         | 2880     |
|    fps              | 40       |
|    time_elapsed     | 7015     |
|    total_timesteps  | 285399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0488   |
|    n_updates        | 46349    |
----------------------------------
Eval num_timesteps=285500, episode_reward=3.16 +/- 6.16
Episode length: 108.52 +/- 46.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.774    |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0807   |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.52     |
|    exploration_rate | 0.774    |
| time/               |          |
|    episodes         | 2884     |
|    fps              | 40       |
|    time_elapsed     | 7028     |
|    total_timesteps  | 285746   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0591   |
|    n_updates        | 46436    |
----------------------------------
Eval num_timesteps=286000, episode_reward=4.54 +/- 6.39
Episode length: 118.88 +/- 71.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.54     |
| rollout/            |          |
|    exploration_rate | 0.773    |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0797   |
|    n_updates        | 46499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.49     |
|    exploration_rate | 0.773    |
| time/               |          |
|    episodes         | 2888     |
|    fps              | 40       |
|    time_elapsed     | 7042     |
|    total_timesteps  | 286088   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0487   |
|    n_updates        | 46521    |
----------------------------------
Eval num_timesteps=286500, episode_reward=2.90 +/- 4.15
Episode length: 109.74 +/- 51.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.772    |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0529   |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.56     |
|    exploration_rate | 0.772    |
| time/               |          |
|    episodes         | 2892     |
|    fps              | 40       |
|    time_elapsed     | 7056     |
|    total_timesteps  | 286599   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0958   |
|    n_updates        | 46649    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.772    |
| time/               |          |
|    episodes         | 2896     |
|    fps              | 40       |
|    time_elapsed     | 7057     |
|    total_timesteps  | 286904   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 46725    |
----------------------------------
Eval num_timesteps=287000, episode_reward=3.08 +/- 5.06
Episode length: 117.56 +/- 55.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.772    |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.031    |
|    n_updates        | 46749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 0.771    |
| time/               |          |
|    episodes         | 2900     |
|    fps              | 40       |
|    time_elapsed     | 7070     |
|    total_timesteps  | 287187   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0538   |
|    n_updates        | 46796    |
----------------------------------
Eval num_timesteps=287500, episode_reward=3.64 +/- 5.91
Episode length: 131.82 +/- 103.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.771    |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0484   |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 2904     |
|    fps              | 40       |
|    time_elapsed     | 7086     |
|    total_timesteps  | 287572   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.058    |
|    n_updates        | 46892    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 2908     |
|    fps              | 40       |
|    time_elapsed     | 7087     |
|    total_timesteps  | 287937   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0856   |
|    n_updates        | 46984    |
----------------------------------
Eval num_timesteps=288000, episode_reward=3.08 +/- 5.51
Episode length: 98.44 +/- 48.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.77     |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0942   |
|    n_updates        | 46999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.28     |
|    exploration_rate | 0.769    |
| time/               |          |
|    episodes         | 2912     |
|    fps              | 40       |
|    time_elapsed     | 7099     |
|    total_timesteps  | 288323   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 47080    |
----------------------------------
Eval num_timesteps=288500, episode_reward=1.66 +/- 2.60
Episode length: 84.42 +/- 26.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.4     |
|    mean_reward      | 1.66     |
| rollout/            |          |
|    exploration_rate | 0.769    |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0379   |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 0.768    |
| time/               |          |
|    episodes         | 2916     |
|    fps              | 40       |
|    time_elapsed     | 7110     |
|    total_timesteps  | 288844   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0698   |
|    n_updates        | 47210    |
----------------------------------
Eval num_timesteps=289000, episode_reward=2.78 +/- 4.70
Episode length: 95.86 +/- 45.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.9     |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.768    |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.43     |
|    exploration_rate | 0.767    |
| time/               |          |
|    episodes         | 2920     |
|    fps              | 40       |
|    time_elapsed     | 7122     |
|    total_timesteps  | 289228   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 47306    |
----------------------------------
Eval num_timesteps=289500, episode_reward=2.40 +/- 4.17
Episode length: 96.64 +/- 37.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.767    |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 47374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.59     |
|    exploration_rate | 0.766    |
| time/               |          |
|    episodes         | 2924     |
|    fps              | 40       |
|    time_elapsed     | 7136     |
|    total_timesteps  | 289833   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0418   |
|    n_updates        | 47458    |
----------------------------------
Eval num_timesteps=290000, episode_reward=4.20 +/- 5.55
Episode length: 119.36 +/- 73.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.766    |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.069    |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.55     |
|    exploration_rate | 0.765    |
| time/               |          |
|    episodes         | 2928     |
|    fps              | 40       |
|    time_elapsed     | 7150     |
|    total_timesteps  | 290281   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0828   |
|    n_updates        | 47570    |
----------------------------------
Eval num_timesteps=290500, episode_reward=3.34 +/- 3.91
Episode length: 103.84 +/- 48.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.765    |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0337   |
|    n_updates        | 47624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.765    |
| time/               |          |
|    episodes         | 2932     |
|    fps              | 40       |
|    time_elapsed     | 7163     |
|    total_timesteps  | 290686   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0862   |
|    n_updates        | 47671    |
----------------------------------
Eval num_timesteps=291000, episode_reward=2.64 +/- 3.80
Episode length: 102.48 +/- 78.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.764    |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0755   |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.764    |
| time/               |          |
|    episodes         | 2936     |
|    fps              | 40       |
|    time_elapsed     | 7176     |
|    total_timesteps  | 291054   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.141    |
|    n_updates        | 47763    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.43     |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 2940     |
|    fps              | 40       |
|    time_elapsed     | 7178     |
|    total_timesteps  | 291497   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 47874    |
----------------------------------
Eval num_timesteps=291500, episode_reward=4.52 +/- 6.59
Episode length: 119.68 +/- 63.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.763    |
| time/               |          |
|    total_timesteps  | 291500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 2.96     |
|    exploration_rate | 0.762    |
| time/               |          |
|    episodes         | 2944     |
|    fps              | 40       |
|    time_elapsed     | 7192     |
|    total_timesteps  | 291912   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0364   |
|    n_updates        | 47977    |
----------------------------------
Eval num_timesteps=292000, episode_reward=2.70 +/- 4.70
Episode length: 94.68 +/- 39.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.762    |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0347   |
|    n_updates        | 47999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.84     |
|    exploration_rate | 0.762    |
| time/               |          |
|    episodes         | 2948     |
|    fps              | 40       |
|    time_elapsed     | 7205     |
|    total_timesteps  | 292267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0661   |
|    n_updates        | 48066    |
----------------------------------
Eval num_timesteps=292500, episode_reward=3.56 +/- 5.40
Episode length: 125.12 +/- 66.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.761    |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.88     |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 2952     |
|    fps              | 40       |
|    time_elapsed     | 7220     |
|    total_timesteps  | 292644   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0576   |
|    n_updates        | 48160    |
----------------------------------
Eval num_timesteps=293000, episode_reward=3.28 +/- 5.20
Episode length: 106.20 +/- 48.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.76     |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.069    |
|    n_updates        | 48249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 2.85     |
|    exploration_rate | 0.76     |
| time/               |          |
|    episodes         | 2956     |
|    fps              | 40       |
|    time_elapsed     | 7233     |
|    total_timesteps  | 293188   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0954   |
|    n_updates        | 48296    |
----------------------------------
Eval num_timesteps=293500, episode_reward=4.72 +/- 11.70
Episode length: 122.26 +/- 88.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.759    |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0434   |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.74     |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 2960     |
|    fps              | 40       |
|    time_elapsed     | 7247     |
|    total_timesteps  | 293595   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 48398    |
----------------------------------
Eval num_timesteps=294000, episode_reward=3.86 +/- 7.82
Episode length: 116.62 +/- 82.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.759    |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0665   |
|    n_updates        | 48499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.01     |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 2964     |
|    fps              | 40       |
|    time_elapsed     | 7262     |
|    total_timesteps  | 294106   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0434   |
|    n_updates        | 48526    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 2.99     |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 2968     |
|    fps              | 40       |
|    time_elapsed     | 7264     |
|    total_timesteps  | 294429   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 48607    |
----------------------------------
Eval num_timesteps=294500, episode_reward=4.78 +/- 7.87
Episode length: 111.72 +/- 54.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.758    |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 48624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.757    |
| time/               |          |
|    episodes         | 2972     |
|    fps              | 40       |
|    time_elapsed     | 7278     |
|    total_timesteps  | 294851   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0611   |
|    n_updates        | 48712    |
----------------------------------
Eval num_timesteps=295000, episode_reward=3.00 +/- 5.12
Episode length: 96.12 +/- 55.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.1     |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.757    |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 48749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 2976     |
|    fps              | 40       |
|    time_elapsed     | 7291     |
|    total_timesteps  | 295335   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0377   |
|    n_updates        | 48833    |
----------------------------------
Eval num_timesteps=295500, episode_reward=1.62 +/- 2.97
Episode length: 85.64 +/- 30.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.6     |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 0.756    |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 48874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.06     |
|    exploration_rate | 0.755    |
| time/               |          |
|    episodes         | 2980     |
|    fps              | 40       |
|    time_elapsed     | 7302     |
|    total_timesteps  | 295682   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0585   |
|    n_updates        | 48920    |
----------------------------------
Eval num_timesteps=296000, episode_reward=2.56 +/- 3.84
Episode length: 103.66 +/- 50.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.755    |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0447   |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 0.754    |
| time/               |          |
|    episodes         | 2984     |
|    fps              | 40       |
|    time_elapsed     | 7315     |
|    total_timesteps  | 296239   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0956   |
|    n_updates        | 49059    |
----------------------------------
Eval num_timesteps=296500, episode_reward=1.92 +/- 2.96
Episode length: 94.60 +/- 36.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 1.92     |
| rollout/            |          |
|    exploration_rate | 0.754    |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0952   |
|    n_updates        | 49124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.01     |
|    exploration_rate | 0.754    |
| time/               |          |
|    episodes         | 2988     |
|    fps              | 40       |
|    time_elapsed     | 7327     |
|    total_timesteps  | 296688   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.038    |
|    n_updates        | 49171    |
----------------------------------
Eval num_timesteps=297000, episode_reward=4.72 +/- 6.46
Episode length: 124.76 +/- 64.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 49249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.753    |
| time/               |          |
|    episodes         | 2992     |
|    fps              | 40       |
|    time_elapsed     | 7342     |
|    total_timesteps  | 297025   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 49256    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.752    |
| time/               |          |
|    episodes         | 2996     |
|    fps              | 40       |
|    time_elapsed     | 7343     |
|    total_timesteps  | 297373   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 49343    |
----------------------------------
Eval num_timesteps=297500, episode_reward=4.00 +/- 6.36
Episode length: 117.82 +/- 59.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.752    |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.081    |
|    n_updates        | 49374    |
----------------------------------
Eval num_timesteps=298000, episode_reward=3.12 +/- 4.79
Episode length: 113.18 +/- 51.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.751    |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.092    |
|    n_updates        | 49499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 3000     |
|    fps              | 40       |
|    time_elapsed     | 7372     |
|    total_timesteps  | 298171   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0716   |
|    n_updates        | 49542    |
----------------------------------
Eval num_timesteps=298500, episode_reward=3.40 +/- 5.93
Episode length: 109.28 +/- 55.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.75     |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0279   |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.75     |
| time/               |          |
|    episodes         | 3004     |
|    fps              | 40       |
|    time_elapsed     | 7386     |
|    total_timesteps  | 298622   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0593   |
|    n_updates        | 49655    |
----------------------------------
Eval num_timesteps=299000, episode_reward=3.78 +/- 9.47
Episode length: 119.84 +/- 94.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.749    |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0978   |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.749    |
| time/               |          |
|    episodes         | 3008     |
|    fps              | 40       |
|    time_elapsed     | 7401     |
|    total_timesteps  | 299163   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0233   |
|    n_updates        | 49790    |
----------------------------------
Eval num_timesteps=299500, episode_reward=3.14 +/- 4.13
Episode length: 99.48 +/- 40.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.5     |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.748    |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.068    |
|    n_updates        | 49874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 3012     |
|    fps              | 40       |
|    time_elapsed     | 7413     |
|    total_timesteps  | 299599   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0724   |
|    n_updates        | 49899    |
----------------------------------
Eval num_timesteps=300000, episode_reward=2.24 +/- 3.90
Episode length: 116.56 +/- 88.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 2.24     |
| rollout/            |          |
|    exploration_rate | 0.747    |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.747    |
| time/               |          |
|    episodes         | 3016     |
|    fps              | 40       |
|    time_elapsed     | 7428     |
|    total_timesteps  | 300132   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.075    |
|    n_updates        | 50032    |
----------------------------------
Eval num_timesteps=300500, episode_reward=2.50 +/- 3.07
Episode length: 104.34 +/- 40.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.746    |
| time/               |          |
|    total_timesteps  | 300500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.163    |
|    n_updates        | 50124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.746    |
| time/               |          |
|    episodes         | 3020     |
|    fps              | 40       |
|    time_elapsed     | 7443     |
|    total_timesteps  | 300606   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 50151    |
----------------------------------
Eval num_timesteps=301000, episode_reward=3.00 +/- 3.97
Episode length: 107.50 +/- 85.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.745    |
| time/               |          |
|    total_timesteps  | 301000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.99     |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 3024     |
|    fps              | 40       |
|    time_elapsed     | 7457     |
|    total_timesteps  | 301073   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0825   |
|    n_updates        | 50268    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.07     |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 3028     |
|    fps              | 40       |
|    time_elapsed     | 7458     |
|    total_timesteps  | 301436   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0772   |
|    n_updates        | 50358    |
----------------------------------
Eval num_timesteps=301500, episode_reward=1.72 +/- 2.49
Episode length: 110.64 +/- 95.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 0.744    |
| time/               |          |
|    total_timesteps  | 301500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0628   |
|    n_updates        | 50374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 3032     |
|    fps              | 40       |
|    time_elapsed     | 7471     |
|    total_timesteps  | 301894   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0977   |
|    n_updates        | 50473    |
----------------------------------
Eval num_timesteps=302000, episode_reward=4.80 +/- 7.26
Episode length: 132.66 +/- 92.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.743    |
| time/               |          |
|    total_timesteps  | 302000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0594   |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.743    |
| time/               |          |
|    episodes         | 3036     |
|    fps              | 40       |
|    time_elapsed     | 7487     |
|    total_timesteps  | 302447   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0681   |
|    n_updates        | 50611    |
----------------------------------
Eval num_timesteps=302500, episode_reward=3.30 +/- 4.47
Episode length: 104.08 +/- 47.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.3      |
| rollout/            |          |
|    exploration_rate | 0.743    |
| time/               |          |
|    total_timesteps  | 302500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.039    |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.742    |
| time/               |          |
|    episodes         | 3040     |
|    fps              | 40       |
|    time_elapsed     | 7500     |
|    total_timesteps  | 302989   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.118    |
|    n_updates        | 50747    |
----------------------------------
Eval num_timesteps=303000, episode_reward=5.92 +/- 9.15
Episode length: 133.16 +/- 70.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 5.92     |
| rollout/            |          |
|    exploration_rate | 0.742    |
| time/               |          |
|    total_timesteps  | 303000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 50749    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.741    |
| time/               |          |
|    episodes         | 3044     |
|    fps              | 40       |
|    time_elapsed     | 7515     |
|    total_timesteps  | 303278   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0545   |
|    n_updates        | 50819    |
----------------------------------
Eval num_timesteps=303500, episode_reward=4.48 +/- 8.92
Episode length: 116.20 +/- 78.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.741    |
| time/               |          |
|    total_timesteps  | 303500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0921   |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.3      |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 3048     |
|    fps              | 40       |
|    time_elapsed     | 7530     |
|    total_timesteps  | 303898   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 50974    |
----------------------------------
Eval num_timesteps=304000, episode_reward=3.58 +/- 6.76
Episode length: 114.96 +/- 91.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.58     |
| rollout/            |          |
|    exploration_rate | 0.74     |
| time/               |          |
|    total_timesteps  | 304000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0442   |
|    n_updates        | 50999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.44     |
|    exploration_rate | 0.739    |
| time/               |          |
|    episodes         | 3052     |
|    fps              | 40       |
|    time_elapsed     | 7544     |
|    total_timesteps  | 304478   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0919   |
|    n_updates        | 51119    |
----------------------------------
Eval num_timesteps=304500, episode_reward=4.92 +/- 7.00
Episode length: 121.64 +/- 81.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.92     |
| rollout/            |          |
|    exploration_rate | 0.739    |
| time/               |          |
|    total_timesteps  | 304500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 51124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.44     |
|    exploration_rate | 0.738    |
| time/               |          |
|    episodes         | 3056     |
|    fps              | 40       |
|    time_elapsed     | 7558     |
|    total_timesteps  | 304966   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0603   |
|    n_updates        | 51241    |
----------------------------------
Eval num_timesteps=305000, episode_reward=1.72 +/- 2.51
Episode length: 93.76 +/- 45.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 1.72     |
| rollout/            |          |
|    exploration_rate | 0.738    |
| time/               |          |
|    total_timesteps  | 305000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0705   |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.737    |
| time/               |          |
|    episodes         | 3060     |
|    fps              | 40       |
|    time_elapsed     | 7570     |
|    total_timesteps  | 305475   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 51368    |
----------------------------------
Eval num_timesteps=305500, episode_reward=1.94 +/- 2.65
Episode length: 96.02 +/- 29.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 0.737    |
| time/               |          |
|    total_timesteps  | 305500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0399   |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 3064     |
|    fps              | 40       |
|    time_elapsed     | 7581     |
|    total_timesteps  | 305749   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0478   |
|    n_updates        | 51437    |
----------------------------------
Eval num_timesteps=306000, episode_reward=4.62 +/- 11.03
Episode length: 132.38 +/- 91.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 4.62     |
| rollout/            |          |
|    exploration_rate | 0.736    |
| time/               |          |
|    total_timesteps  | 306000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0809   |
|    n_updates        | 51499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 3068     |
|    fps              | 40       |
|    time_elapsed     | 7596     |
|    total_timesteps  | 306038   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0543   |
|    n_updates        | 51509    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 3072     |
|    fps              | 40       |
|    time_elapsed     | 7598     |
|    total_timesteps  | 306407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0444   |
|    n_updates        | 51601    |
----------------------------------
Eval num_timesteps=306500, episode_reward=2.36 +/- 3.24
Episode length: 87.30 +/- 30.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.3     |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.735    |
| time/               |          |
|    total_timesteps  | 306500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0621   |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.3      |
|    exploration_rate | 0.734    |
| time/               |          |
|    episodes         | 3076     |
|    fps              | 40       |
|    time_elapsed     | 7609     |
|    total_timesteps  | 306754   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 51688    |
----------------------------------
Eval num_timesteps=307000, episode_reward=4.80 +/- 7.88
Episode length: 112.96 +/- 47.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.734    |
| time/               |          |
|    total_timesteps  | 307000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 51749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.734    |
| time/               |          |
|    episodes         | 3080     |
|    fps              | 40       |
|    time_elapsed     | 7622     |
|    total_timesteps  | 307031   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0773   |
|    n_updates        | 51757    |
----------------------------------
Eval num_timesteps=307500, episode_reward=3.82 +/- 6.18
Episode length: 111.50 +/- 57.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.82     |
| rollout/            |          |
|    exploration_rate | 0.733    |
| time/               |          |
|    total_timesteps  | 307500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.19     |
|    exploration_rate | 0.733    |
| time/               |          |
|    episodes         | 3084     |
|    fps              | 40       |
|    time_elapsed     | 7636     |
|    total_timesteps  | 307501   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.057    |
|    n_updates        | 51875    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.29     |
|    exploration_rate | 0.732    |
| time/               |          |
|    episodes         | 3088     |
|    fps              | 40       |
|    time_elapsed     | 7638     |
|    total_timesteps  | 307953   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0531   |
|    n_updates        | 51988    |
----------------------------------
Eval num_timesteps=308000, episode_reward=2.16 +/- 3.04
Episode length: 113.80 +/- 91.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.16     |
| rollout/            |          |
|    exploration_rate | 0.732    |
| time/               |          |
|    total_timesteps  | 308000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0355   |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.731    |
| time/               |          |
|    episodes         | 3092     |
|    fps              | 40       |
|    time_elapsed     | 7651     |
|    total_timesteps  | 308252   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0336   |
|    n_updates        | 52062    |
----------------------------------
Eval num_timesteps=308500, episode_reward=2.02 +/- 3.34
Episode length: 94.42 +/- 37.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 2.02     |
| rollout/            |          |
|    exploration_rate | 0.731    |
| time/               |          |
|    total_timesteps  | 308500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 52124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.4      |
|    exploration_rate | 0.731    |
| time/               |          |
|    episodes         | 3096     |
|    fps              | 40       |
|    time_elapsed     | 7662     |
|    total_timesteps  | 308639   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 52159    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.73     |
| time/               |          |
|    episodes         | 3100     |
|    fps              | 40       |
|    time_elapsed     | 7663     |
|    total_timesteps  | 308945   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.034    |
|    n_updates        | 52236    |
----------------------------------
Eval num_timesteps=309000, episode_reward=3.08 +/- 4.91
Episode length: 104.08 +/- 48.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.73     |
| time/               |          |
|    total_timesteps  | 309000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0181   |
|    n_updates        | 52249    |
----------------------------------
Eval num_timesteps=309500, episode_reward=3.02 +/- 4.96
Episode length: 116.42 +/- 76.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.729    |
| time/               |          |
|    total_timesteps  | 309500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0925   |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.729    |
| time/               |          |
|    episodes         | 3104     |
|    fps              | 40       |
|    time_elapsed     | 7690     |
|    total_timesteps  | 309642   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 52410    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.59     |
|    exploration_rate | 0.728    |
| time/               |          |
|    episodes         | 3108     |
|    fps              | 40       |
|    time_elapsed     | 7691     |
|    total_timesteps  | 309992   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 52497    |
----------------------------------
Eval num_timesteps=310000, episode_reward=3.98 +/- 7.66
Episode length: 114.66 +/- 52.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.98     |
| rollout/            |          |
|    exploration_rate | 0.728    |
| time/               |          |
|    total_timesteps  | 310000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0873   |
|    n_updates        | 52499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 3112     |
|    fps              | 40       |
|    time_elapsed     | 7705     |
|    total_timesteps  | 310332   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 52582    |
----------------------------------
Eval num_timesteps=310500, episode_reward=2.18 +/- 3.15
Episode length: 97.46 +/- 34.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.5     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.727    |
| time/               |          |
|    total_timesteps  | 310500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 3116     |
|    fps              | 40       |
|    time_elapsed     | 7717     |
|    total_timesteps  | 310590   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 52647    |
----------------------------------
Eval num_timesteps=311000, episode_reward=3.20 +/- 4.94
Episode length: 122.54 +/- 77.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 3.2      |
| rollout/            |          |
|    exploration_rate | 0.726    |
| time/               |          |
|    total_timesteps  | 311000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0293   |
|    n_updates        | 52749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.726    |
| time/               |          |
|    episodes         | 3120     |
|    fps              | 40       |
|    time_elapsed     | 7731     |
|    total_timesteps  | 311015   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 52753    |
----------------------------------
Eval num_timesteps=311500, episode_reward=2.56 +/- 4.40
Episode length: 90.92 +/- 33.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.9     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.725    |
| time/               |          |
|    total_timesteps  | 311500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 52874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.725    |
| time/               |          |
|    episodes         | 3124     |
|    fps              | 40       |
|    time_elapsed     | 7743     |
|    total_timesteps  | 311570   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 52892    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.724    |
| time/               |          |
|    episodes         | 3128     |
|    fps              | 40       |
|    time_elapsed     | 7745     |
|    total_timesteps  | 311930   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 52982    |
----------------------------------
Eval num_timesteps=312000, episode_reward=3.50 +/- 6.19
Episode length: 117.70 +/- 85.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.724    |
| time/               |          |
|    total_timesteps  | 312000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.166    |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 0.723    |
| time/               |          |
|    episodes         | 3132     |
|    fps              | 40       |
|    time_elapsed     | 7759     |
|    total_timesteps  | 312471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0543   |
|    n_updates        | 53117    |
----------------------------------
Eval num_timesteps=312500, episode_reward=2.46 +/- 4.19
Episode length: 94.68 +/- 29.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.723    |
| time/               |          |
|    total_timesteps  | 312500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0451   |
|    n_updates        | 53124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.62     |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 3136     |
|    fps              | 40       |
|    time_elapsed     | 7771     |
|    total_timesteps  | 312969   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0179   |
|    n_updates        | 53242    |
----------------------------------
Eval num_timesteps=313000, episode_reward=3.10 +/- 4.54
Episode length: 97.26 +/- 44.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.722    |
| time/               |          |
|    total_timesteps  | 313000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0357   |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 3140     |
|    fps              | 40       |
|    time_elapsed     | 7786     |
|    total_timesteps  | 313330   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0756   |
|    n_updates        | 53332    |
----------------------------------
Eval num_timesteps=313500, episode_reward=3.16 +/- 5.86
Episode length: 100.48 +/- 52.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.721    |
| time/               |          |
|    total_timesteps  | 313500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0628   |
|    n_updates        | 53374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.71     |
|    exploration_rate | 0.721    |
| time/               |          |
|    episodes         | 3144     |
|    fps              | 40       |
|    time_elapsed     | 7798     |
|    total_timesteps  | 313668   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 53416    |
----------------------------------
Eval num_timesteps=314000, episode_reward=3.42 +/- 4.10
Episode length: 113.20 +/- 54.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.42     |
| rollout/            |          |
|    exploration_rate | 0.72     |
| time/               |          |
|    total_timesteps  | 314000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.048    |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.72     |
| time/               |          |
|    episodes         | 3148     |
|    fps              | 40       |
|    time_elapsed     | 7812     |
|    total_timesteps  | 314303   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0504   |
|    n_updates        | 53575    |
----------------------------------
Eval num_timesteps=314500, episode_reward=4.32 +/- 6.16
Episode length: 115.18 +/- 57.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.719    |
| time/               |          |
|    total_timesteps  | 314500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0808   |
|    n_updates        | 53624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.719    |
| time/               |          |
|    episodes         | 3152     |
|    fps              | 40       |
|    time_elapsed     | 7826     |
|    total_timesteps  | 314637   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0569   |
|    n_updates        | 53659    |
----------------------------------
Eval num_timesteps=315000, episode_reward=2.30 +/- 3.64
Episode length: 98.94 +/- 39.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.718    |
| time/               |          |
|    total_timesteps  | 315000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0671   |
|    n_updates        | 53749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.718    |
| time/               |          |
|    episodes         | 3156     |
|    fps              | 40       |
|    time_elapsed     | 7839     |
|    total_timesteps  | 315203   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0555   |
|    n_updates        | 53800    |
----------------------------------
Eval num_timesteps=315500, episode_reward=2.68 +/- 3.55
Episode length: 111.86 +/- 53.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.717    |
| time/               |          |
|    total_timesteps  | 315500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.089    |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 3160     |
|    fps              | 40       |
|    time_elapsed     | 7853     |
|    total_timesteps  | 315765   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0643   |
|    n_updates        | 53941    |
----------------------------------
Eval num_timesteps=316000, episode_reward=2.46 +/- 4.73
Episode length: 93.92 +/- 36.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.9     |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.716    |
| time/               |          |
|    total_timesteps  | 316000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.178    |
|    n_updates        | 53999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.716    |
| time/               |          |
|    episodes         | 3164     |
|    fps              | 40       |
|    time_elapsed     | 7865     |
|    total_timesteps  | 316174   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.1      |
|    n_updates        | 54043    |
----------------------------------
Eval num_timesteps=316500, episode_reward=2.38 +/- 5.61
Episode length: 93.96 +/- 48.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.715    |
| time/               |          |
|    total_timesteps  | 316500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0876   |
|    n_updates        | 54124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 3168     |
|    fps              | 40       |
|    time_elapsed     | 7876     |
|    total_timesteps  | 316570   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 54142    |
----------------------------------
Eval num_timesteps=317000, episode_reward=2.54 +/- 4.00
Episode length: 103.04 +/- 45.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.714    |
| time/               |          |
|    total_timesteps  | 317000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 54249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.714    |
| time/               |          |
|    episodes         | 3172     |
|    fps              | 40       |
|    time_elapsed     | 7890     |
|    total_timesteps  | 317224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.063    |
|    n_updates        | 54305    |
----------------------------------
Eval num_timesteps=317500, episode_reward=2.46 +/- 4.60
Episode length: 98.84 +/- 40.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.8     |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.713    |
| time/               |          |
|    total_timesteps  | 317500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.076    |
|    n_updates        | 54374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.1      |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 3176     |
|    fps              | 40       |
|    time_elapsed     | 7902     |
|    total_timesteps  | 317625   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 54406    |
----------------------------------
Eval num_timesteps=318000, episode_reward=2.94 +/- 3.78
Episode length: 123.98 +/- 57.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.712    |
| time/               |          |
|    total_timesteps  | 318000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0699   |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.712    |
| time/               |          |
|    episodes         | 3180     |
|    fps              | 40       |
|    time_elapsed     | 7917     |
|    total_timesteps  | 318010   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0432   |
|    n_updates        | 54502    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.712    |
| time/               |          |
|    episodes         | 3184     |
|    fps              | 40       |
|    time_elapsed     | 7918     |
|    total_timesteps  | 318357   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 54589    |
----------------------------------
Eval num_timesteps=318500, episode_reward=3.04 +/- 4.94
Episode length: 102.66 +/- 43.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.04     |
| rollout/            |          |
|    exploration_rate | 0.711    |
| time/               |          |
|    total_timesteps  | 318500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0723   |
|    n_updates        | 54624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.711    |
| time/               |          |
|    episodes         | 3188     |
|    fps              | 40       |
|    time_elapsed     | 7931     |
|    total_timesteps  | 318840   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0538   |
|    n_updates        | 54709    |
----------------------------------
Eval num_timesteps=319000, episode_reward=2.34 +/- 3.91
Episode length: 101.58 +/- 43.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.71     |
| time/               |          |
|    total_timesteps  | 319000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.052    |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.18     |
|    exploration_rate | 0.71     |
| time/               |          |
|    episodes         | 3192     |
|    fps              | 40       |
|    time_elapsed     | 7943     |
|    total_timesteps  | 319321   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 54830    |
----------------------------------
Eval num_timesteps=319500, episode_reward=3.34 +/- 4.59
Episode length: 126.10 +/- 100.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.709    |
| time/               |          |
|    total_timesteps  | 319500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.202    |
|    n_updates        | 54874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 3196     |
|    fps              | 40       |
|    time_elapsed     | 7958     |
|    total_timesteps  | 319661   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0986   |
|    n_updates        | 54915    |
----------------------------------
Eval num_timesteps=320000, episode_reward=2.46 +/- 3.41
Episode length: 141.46 +/- 99.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.708    |
| time/               |          |
|    total_timesteps  | 320000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.18     |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 3200     |
|    fps              | 40       |
|    time_elapsed     | 7974     |
|    total_timesteps  | 320018   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 55004    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 3204     |
|    fps              | 40       |
|    time_elapsed     | 7975     |
|    total_timesteps  | 320326   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0703   |
|    n_updates        | 55081    |
----------------------------------
Eval num_timesteps=320500, episode_reward=3.28 +/- 4.89
Episode length: 109.40 +/- 60.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.707    |
| time/               |          |
|    total_timesteps  | 320500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0348   |
|    n_updates        | 55124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.84     |
|    exploration_rate | 0.707    |
| time/               |          |
|    episodes         | 3208     |
|    fps              | 40       |
|    time_elapsed     | 7989     |
|    total_timesteps  | 320721   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.193    |
|    n_updates        | 55180    |
----------------------------------
Eval num_timesteps=321000, episode_reward=2.96 +/- 3.92
Episode length: 116.22 +/- 65.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.706    |
| time/               |          |
|    total_timesteps  | 321000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0709   |
|    n_updates        | 55249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.706    |
| time/               |          |
|    episodes         | 3212     |
|    fps              | 40       |
|    time_elapsed     | 8003     |
|    total_timesteps  | 321142   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 55285    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.706    |
| time/               |          |
|    episodes         | 3216     |
|    fps              | 40       |
|    time_elapsed     | 8004     |
|    total_timesteps  | 321455   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.04     |
|    n_updates        | 55363    |
----------------------------------
Eval num_timesteps=321500, episode_reward=4.56 +/- 6.74
Episode length: 102.92 +/- 66.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 4.56     |
| rollout/            |          |
|    exploration_rate | 0.705    |
| time/               |          |
|    total_timesteps  | 321500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0928   |
|    n_updates        | 55374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 3220     |
|    fps              | 40       |
|    time_elapsed     | 8017     |
|    total_timesteps  | 321876   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0375   |
|    n_updates        | 55468    |
----------------------------------
Eval num_timesteps=322000, episode_reward=4.00 +/- 6.24
Episode length: 115.24 +/- 61.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.704    |
| time/               |          |
|    total_timesteps  | 322000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.178    |
|    n_updates        | 55499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.704    |
| time/               |          |
|    episodes         | 3224     |
|    fps              | 40       |
|    time_elapsed     | 8031     |
|    total_timesteps  | 322374   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0293   |
|    n_updates        | 55593    |
----------------------------------
Eval num_timesteps=322500, episode_reward=3.16 +/- 4.25
Episode length: 97.58 +/- 41.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.703    |
| time/               |          |
|    total_timesteps  | 322500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.82     |
|    exploration_rate | 0.703    |
| time/               |          |
|    episodes         | 3228     |
|    fps              | 40       |
|    time_elapsed     | 8043     |
|    total_timesteps  | 322745   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0957   |
|    n_updates        | 55686    |
----------------------------------
Eval num_timesteps=323000, episode_reward=3.34 +/- 3.51
Episode length: 115.06 +/- 69.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.702    |
| time/               |          |
|    total_timesteps  | 323000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 55749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 3232     |
|    fps              | 40       |
|    time_elapsed     | 8058     |
|    total_timesteps  | 323335   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.093    |
|    n_updates        | 55833    |
----------------------------------
Eval num_timesteps=323500, episode_reward=5.68 +/- 9.74
Episode length: 128.34 +/- 93.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.68     |
| rollout/            |          |
|    exploration_rate | 0.701    |
| time/               |          |
|    total_timesteps  | 323500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 55874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.27     |
|    exploration_rate | 0.701    |
| time/               |          |
|    episodes         | 3236     |
|    fps              | 40       |
|    time_elapsed     | 8074     |
|    total_timesteps  | 323891   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 55972    |
----------------------------------
Eval num_timesteps=324000, episode_reward=3.02 +/- 4.79
Episode length: 96.78 +/- 40.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.8     |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.7      |
| time/               |          |
|    total_timesteps  | 324000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0835   |
|    n_updates        | 55999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.7      |
| time/               |          |
|    episodes         | 3240     |
|    fps              | 40       |
|    time_elapsed     | 8087     |
|    total_timesteps  | 324196   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.226    |
|    n_updates        | 56048    |
----------------------------------
Eval num_timesteps=324500, episode_reward=3.34 +/- 5.28
Episode length: 117.68 +/- 52.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.699    |
| time/               |          |
|    total_timesteps  | 324500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0842   |
|    n_updates        | 56124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.699    |
| time/               |          |
|    episodes         | 3244     |
|    fps              | 40       |
|    time_elapsed     | 8102     |
|    total_timesteps  | 324699   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0566   |
|    n_updates        | 56174    |
----------------------------------
Eval num_timesteps=325000, episode_reward=2.56 +/- 4.50
Episode length: 99.80 +/- 46.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.8     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.698    |
| time/               |          |
|    total_timesteps  | 325000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0988   |
|    n_updates        | 56249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.33     |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 3248     |
|    fps              | 40       |
|    time_elapsed     | 8116     |
|    total_timesteps  | 325237   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.188    |
|    n_updates        | 56309    |
----------------------------------
Eval num_timesteps=325500, episode_reward=2.90 +/- 4.01
Episode length: 90.50 +/- 38.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.5     |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.697    |
| time/               |          |
|    total_timesteps  | 325500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0613   |
|    n_updates        | 56374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.697    |
| time/               |          |
|    episodes         | 3252     |
|    fps              | 40       |
|    time_elapsed     | 8130     |
|    total_timesteps  | 325845   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0693   |
|    n_updates        | 56461    |
----------------------------------
Eval num_timesteps=326000, episode_reward=3.14 +/- 4.21
Episode length: 123.70 +/- 82.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.696    |
| time/               |          |
|    total_timesteps  | 326000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.195    |
|    n_updates        | 56499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.696    |
| time/               |          |
|    episodes         | 3256     |
|    fps              | 40       |
|    time_elapsed     | 8145     |
|    total_timesteps  | 326316   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.182    |
|    n_updates        | 56578    |
----------------------------------
Eval num_timesteps=326500, episode_reward=2.86 +/- 5.36
Episode length: 100.30 +/- 41.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.695    |
| time/               |          |
|    total_timesteps  | 326500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 56624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 3260     |
|    fps              | 40       |
|    time_elapsed     | 8157     |
|    total_timesteps  | 326756   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.238    |
|    n_updates        | 56688    |
----------------------------------
Eval num_timesteps=327000, episode_reward=3.88 +/- 5.35
Episode length: 102.22 +/- 36.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.88     |
| rollout/            |          |
|    exploration_rate | 0.694    |
| time/               |          |
|    total_timesteps  | 327000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0844   |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.694    |
| time/               |          |
|    episodes         | 3264     |
|    fps              | 40       |
|    time_elapsed     | 8171     |
|    total_timesteps  | 327150   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0924   |
|    n_updates        | 56787    |
----------------------------------
Eval num_timesteps=327500, episode_reward=3.32 +/- 4.77
Episode length: 102.36 +/- 42.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.693    |
| time/               |          |
|    total_timesteps  | 327500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0601   |
|    n_updates        | 56874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.693    |
| time/               |          |
|    episodes         | 3268     |
|    fps              | 40       |
|    time_elapsed     | 8185     |
|    total_timesteps  | 327562   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0484   |
|    n_updates        | 56890    |
----------------------------------
Eval num_timesteps=328000, episode_reward=2.56 +/- 3.80
Episode length: 92.08 +/- 44.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.1     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.692    |
| time/               |          |
|    total_timesteps  | 328000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 56999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 3272     |
|    fps              | 40       |
|    time_elapsed     | 8197     |
|    total_timesteps  | 328040   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0446   |
|    n_updates        | 57009    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 3276     |
|    fps              | 40       |
|    time_elapsed     | 8198     |
|    total_timesteps  | 328438   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0998   |
|    n_updates        | 57109    |
----------------------------------
Eval num_timesteps=328500, episode_reward=4.48 +/- 6.74
Episode length: 104.20 +/- 56.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.691    |
| time/               |          |
|    total_timesteps  | 328500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.056    |
|    n_updates        | 57124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 3280     |
|    fps              | 40       |
|    time_elapsed     | 8212     |
|    total_timesteps  | 328786   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 57196    |
----------------------------------
Eval num_timesteps=329000, episode_reward=2.58 +/- 4.06
Episode length: 110.42 +/- 76.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.58     |
| rollout/            |          |
|    exploration_rate | 0.69     |
| time/               |          |
|    total_timesteps  | 329000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0488   |
|    n_updates        | 57249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.69     |
| time/               |          |
|    episodes         | 3284     |
|    fps              | 40       |
|    time_elapsed     | 8225     |
|    total_timesteps  | 329045   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0564   |
|    n_updates        | 57261    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.69     |
| time/               |          |
|    episodes         | 3288     |
|    fps              | 40       |
|    time_elapsed     | 8226     |
|    total_timesteps  | 329394   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0852   |
|    n_updates        | 57348    |
----------------------------------
Eval num_timesteps=329500, episode_reward=2.44 +/- 3.87
Episode length: 96.52 +/- 48.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 2.44     |
| rollout/            |          |
|    exploration_rate | 0.689    |
| time/               |          |
|    total_timesteps  | 329500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.689    |
| time/               |          |
|    episodes         | 3292     |
|    fps              | 40       |
|    time_elapsed     | 8238     |
|    total_timesteps  | 329823   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 57455    |
----------------------------------
Eval num_timesteps=330000, episode_reward=3.18 +/- 5.11
Episode length: 102.66 +/- 53.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.688    |
| time/               |          |
|    total_timesteps  | 330000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 57499    |
----------------------------------
Eval num_timesteps=330500, episode_reward=2.62 +/- 3.36
Episode length: 93.88 +/- 40.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.9     |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.687    |
| time/               |          |
|    total_timesteps  | 330500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0891   |
|    n_updates        | 57624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.24     |
|    exploration_rate | 0.687    |
| time/               |          |
|    episodes         | 3296     |
|    fps              | 40       |
|    time_elapsed     | 8261     |
|    total_timesteps  | 330597   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 57649    |
----------------------------------
Eval num_timesteps=331000, episode_reward=3.26 +/- 4.27
Episode length: 106.28 +/- 35.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 331000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.67     |
|    exploration_rate | 0.686    |
| time/               |          |
|    episodes         | 3300     |
|    fps              | 40       |
|    time_elapsed     | 8275     |
|    total_timesteps  | 331188   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 57796    |
----------------------------------
Eval num_timesteps=331500, episode_reward=3.34 +/- 5.47
Episode length: 96.54 +/- 41.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.685    |
| time/               |          |
|    total_timesteps  | 331500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 57874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.685    |
| time/               |          |
|    episodes         | 3304     |
|    fps              | 40       |
|    time_elapsed     | 8287     |
|    total_timesteps  | 331657   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0907   |
|    n_updates        | 57914    |
----------------------------------
Eval num_timesteps=332000, episode_reward=2.88 +/- 4.40
Episode length: 103.52 +/- 44.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.684    |
| time/               |          |
|    total_timesteps  | 332000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.219    |
|    n_updates        | 57999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.684    |
| time/               |          |
|    episodes         | 3308     |
|    fps              | 40       |
|    time_elapsed     | 8300     |
|    total_timesteps  | 332250   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.088    |
|    n_updates        | 58062    |
----------------------------------
Eval num_timesteps=332500, episode_reward=1.66 +/- 3.98
Episode length: 92.78 +/- 47.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 1.66     |
| rollout/            |          |
|    exploration_rate | 0.683    |
| time/               |          |
|    total_timesteps  | 332500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 58124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.97     |
|    exploration_rate | 0.683    |
| time/               |          |
|    episodes         | 3312     |
|    fps              | 40       |
|    time_elapsed     | 8311     |
|    total_timesteps  | 332650   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0785   |
|    n_updates        | 58162    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.93     |
|    exploration_rate | 0.682    |
| time/               |          |
|    episodes         | 3316     |
|    fps              | 40       |
|    time_elapsed     | 8312     |
|    total_timesteps  | 332956   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.197    |
|    n_updates        | 58238    |
----------------------------------
Eval num_timesteps=333000, episode_reward=2.54 +/- 5.25
Episode length: 97.66 +/- 39.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.7     |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.682    |
| time/               |          |
|    total_timesteps  | 333000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 58249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.87     |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 3320     |
|    fps              | 40       |
|    time_elapsed     | 8324     |
|    total_timesteps  | 333353   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.1      |
|    n_updates        | 58338    |
----------------------------------
Eval num_timesteps=333500, episode_reward=2.62 +/- 3.75
Episode length: 104.68 +/- 43.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.62     |
| rollout/            |          |
|    exploration_rate | 0.681    |
| time/               |          |
|    total_timesteps  | 333500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 58374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.72     |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 3324     |
|    fps              | 40       |
|    time_elapsed     | 8337     |
|    total_timesteps  | 333720   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0818   |
|    n_updates        | 58429    |
----------------------------------
Eval num_timesteps=334000, episode_reward=3.18 +/- 3.84
Episode length: 108.80 +/- 44.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.68     |
| time/               |          |
|    total_timesteps  | 334000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0826   |
|    n_updates        | 58499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.71     |
|    exploration_rate | 0.68     |
| time/               |          |
|    episodes         | 3328     |
|    fps              | 39       |
|    time_elapsed     | 8354     |
|    total_timesteps  | 334023   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0356   |
|    n_updates        | 58505    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.44     |
|    exploration_rate | 0.679    |
| time/               |          |
|    episodes         | 3332     |
|    fps              | 40       |
|    time_elapsed     | 8356     |
|    total_timesteps  | 334421   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0894   |
|    n_updates        | 58605    |
----------------------------------
Eval num_timesteps=334500, episode_reward=3.40 +/- 4.81
Episode length: 105.54 +/- 49.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.679    |
| time/               |          |
|    total_timesteps  | 334500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0897   |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.679    |
| time/               |          |
|    episodes         | 3336     |
|    fps              | 39       |
|    time_elapsed     | 8371     |
|    total_timesteps  | 334782   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 58695    |
----------------------------------
Eval num_timesteps=335000, episode_reward=3.98 +/- 6.08
Episode length: 124.26 +/- 88.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.98     |
| rollout/            |          |
|    exploration_rate | 0.678    |
| time/               |          |
|    total_timesteps  | 335000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.22     |
|    exploration_rate | 0.678    |
| time/               |          |
|    episodes         | 3340     |
|    fps              | 39       |
|    time_elapsed     | 8387     |
|    total_timesteps  | 335172   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 58792    |
----------------------------------
Eval num_timesteps=335500, episode_reward=4.08 +/- 5.66
Episode length: 112.24 +/- 57.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.08     |
| rollout/            |          |
|    exploration_rate | 0.677    |
| time/               |          |
|    total_timesteps  | 335500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0719   |
|    n_updates        | 58874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.677    |
| time/               |          |
|    episodes         | 3344     |
|    fps              | 39       |
|    time_elapsed     | 8400     |
|    total_timesteps  | 335500   |
----------------------------------
Eval num_timesteps=336000, episode_reward=2.22 +/- 3.58
Episode length: 121.74 +/- 59.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.676    |
| time/               |          |
|    total_timesteps  | 336000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0996   |
|    n_updates        | 58999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.676    |
| time/               |          |
|    episodes         | 3348     |
|    fps              | 39       |
|    time_elapsed     | 8415     |
|    total_timesteps  | 336078   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0526   |
|    n_updates        | 59019    |
----------------------------------
Eval num_timesteps=336500, episode_reward=1.88 +/- 2.99
Episode length: 105.38 +/- 47.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 0.675    |
| time/               |          |
|    total_timesteps  | 336500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.034    |
|    n_updates        | 59124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.675    |
| time/               |          |
|    episodes         | 3352     |
|    fps              | 39       |
|    time_elapsed     | 8429     |
|    total_timesteps  | 336688   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0507   |
|    n_updates        | 59171    |
----------------------------------
Eval num_timesteps=337000, episode_reward=4.48 +/- 7.33
Episode length: 118.58 +/- 54.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.674    |
| time/               |          |
|    total_timesteps  | 337000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0534   |
|    n_updates        | 59249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 3356     |
|    fps              | 39       |
|    time_elapsed     | 8445     |
|    total_timesteps  | 337092   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 59272    |
----------------------------------
Eval num_timesteps=337500, episode_reward=2.96 +/- 4.80
Episode length: 92.60 +/- 43.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.673    |
| time/               |          |
|    total_timesteps  | 337500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 59374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.673    |
| time/               |          |
|    episodes         | 3360     |
|    fps              | 39       |
|    time_elapsed     | 8457     |
|    total_timesteps  | 337622   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0637   |
|    n_updates        | 59405    |
----------------------------------
Eval num_timesteps=338000, episode_reward=3.04 +/- 5.19
Episode length: 134.42 +/- 105.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 3.04     |
| rollout/            |          |
|    exploration_rate | 0.672    |
| time/               |          |
|    total_timesteps  | 338000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0616   |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.08     |
|    exploration_rate | 0.672    |
| time/               |          |
|    episodes         | 3364     |
|    fps              | 39       |
|    time_elapsed     | 8473     |
|    total_timesteps  | 338114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.147    |
|    n_updates        | 59528    |
----------------------------------
Eval num_timesteps=338500, episode_reward=2.92 +/- 4.18
Episode length: 97.20 +/- 42.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.2     |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.671    |
| time/               |          |
|    total_timesteps  | 338500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0833   |
|    n_updates        | 59624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.2      |
|    exploration_rate | 0.671    |
| time/               |          |
|    episodes         | 3368     |
|    fps              | 39       |
|    time_elapsed     | 8485     |
|    total_timesteps  | 338593   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.249    |
|    n_updates        | 59648    |
----------------------------------
Eval num_timesteps=339000, episode_reward=4.46 +/- 5.68
Episode length: 112.18 +/- 51.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.46     |
| rollout/            |          |
|    exploration_rate | 0.67     |
| time/               |          |
|    total_timesteps  | 339000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 3372     |
|    fps              | 39       |
|    time_elapsed     | 8504     |
|    total_timesteps  | 339251   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 59812    |
----------------------------------
Eval num_timesteps=339500, episode_reward=3.16 +/- 4.64
Episode length: 116.16 +/- 81.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.669    |
| time/               |          |
|    total_timesteps  | 339500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0503   |
|    n_updates        | 59874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.12     |
|    exploration_rate | 0.668    |
| time/               |          |
|    episodes         | 3376     |
|    fps              | 39       |
|    time_elapsed     | 8522     |
|    total_timesteps  | 339739   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.23     |
|    n_updates        | 59934    |
----------------------------------
Eval num_timesteps=340000, episode_reward=3.10 +/- 6.28
Episode length: 118.88 +/- 70.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.668    |
| time/               |          |
|    total_timesteps  | 340000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 59999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.18     |
|    exploration_rate | 0.668    |
| time/               |          |
|    episodes         | 3380     |
|    fps              | 39       |
|    time_elapsed     | 8538     |
|    total_timesteps  | 340122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 60030    |
----------------------------------
Eval num_timesteps=340500, episode_reward=4.24 +/- 5.85
Episode length: 139.02 +/- 59.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.667    |
| time/               |          |
|    total_timesteps  | 340500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.153    |
|    n_updates        | 60124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.56     |
|    exploration_rate | 0.666    |
| time/               |          |
|    episodes         | 3384     |
|    fps              | 39       |
|    time_elapsed     | 8554     |
|    total_timesteps  | 340632   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 60157    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.51     |
|    exploration_rate | 0.666    |
| time/               |          |
|    episodes         | 3388     |
|    fps              | 39       |
|    time_elapsed     | 8555     |
|    total_timesteps  | 340993   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 60248    |
----------------------------------
Eval num_timesteps=341000, episode_reward=6.36 +/- 9.57
Episode length: 149.60 +/- 98.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 6.36     |
| rollout/            |          |
|    exploration_rate | 0.666    |
| time/               |          |
|    total_timesteps  | 341000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 60249    |
----------------------------------
New best mean reward!
Eval num_timesteps=341500, episode_reward=3.50 +/- 5.13
Episode length: 105.80 +/- 56.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.665    |
| time/               |          |
|    total_timesteps  | 341500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0756   |
|    n_updates        | 60374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.665    |
| time/               |          |
|    episodes         | 3392     |
|    fps              | 39       |
|    time_elapsed     | 8586     |
|    total_timesteps  | 341558   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 60389    |
----------------------------------
Eval num_timesteps=342000, episode_reward=4.88 +/- 8.96
Episode length: 122.32 +/- 68.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.88     |
| rollout/            |          |
|    exploration_rate | 0.664    |
| time/               |          |
|    total_timesteps  | 342000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 60499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.664    |
| time/               |          |
|    episodes         | 3396     |
|    fps              | 39       |
|    time_elapsed     | 8601     |
|    total_timesteps  | 342026   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 60506    |
----------------------------------
Eval num_timesteps=342500, episode_reward=4.54 +/- 6.17
Episode length: 109.98 +/- 48.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 4.54     |
| rollout/            |          |
|    exploration_rate | 0.663    |
| time/               |          |
|    total_timesteps  | 342500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0797   |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.64     |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 3400     |
|    fps              | 39       |
|    time_elapsed     | 8614     |
|    total_timesteps  | 342564   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0519   |
|    n_updates        | 60640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.54     |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 3404     |
|    fps              | 39       |
|    time_elapsed     | 8615     |
|    total_timesteps  | 342917   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0843   |
|    n_updates        | 60729    |
----------------------------------
Eval num_timesteps=343000, episode_reward=3.60 +/- 4.71
Episode length: 117.20 +/- 51.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.662    |
| time/               |          |
|    total_timesteps  | 343000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 60749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.661    |
| time/               |          |
|    episodes         | 3408     |
|    fps              | 39       |
|    time_elapsed     | 8629     |
|    total_timesteps  | 343346   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 60836    |
----------------------------------
Eval num_timesteps=343500, episode_reward=2.04 +/- 3.20
Episode length: 103.80 +/- 42.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.66     |
| time/               |          |
|    total_timesteps  | 343500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 60874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.66     |
| time/               |          |
|    episodes         | 3412     |
|    fps              | 39       |
|    time_elapsed     | 8642     |
|    total_timesteps  | 343683   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0737   |
|    n_updates        | 60920    |
----------------------------------
Eval num_timesteps=344000, episode_reward=5.82 +/- 8.59
Episode length: 116.04 +/- 62.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 5.82     |
| rollout/            |          |
|    exploration_rate | 0.659    |
| time/               |          |
|    total_timesteps  | 344000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.168    |
|    n_updates        | 60999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.659    |
| time/               |          |
|    episodes         | 3416     |
|    fps              | 39       |
|    time_elapsed     | 8656     |
|    total_timesteps  | 344162   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0617   |
|    n_updates        | 61040    |
----------------------------------
Eval num_timesteps=344500, episode_reward=4.32 +/- 7.17
Episode length: 109.72 +/- 59.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.658    |
| time/               |          |
|    total_timesteps  | 344500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0292   |
|    n_updates        | 61124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.71     |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 3420     |
|    fps              | 39       |
|    time_elapsed     | 8669     |
|    total_timesteps  | 344504   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 61125    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 3424     |
|    fps              | 39       |
|    time_elapsed     | 8670     |
|    total_timesteps  | 344842   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0354   |
|    n_updates        | 61210    |
----------------------------------
Eval num_timesteps=345000, episode_reward=3.62 +/- 5.33
Episode length: 110.54 +/- 54.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.657    |
| time/               |          |
|    total_timesteps  | 345000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0843   |
|    n_updates        | 61249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.657    |
| time/               |          |
|    episodes         | 3428     |
|    fps              | 39       |
|    time_elapsed     | 8684     |
|    total_timesteps  | 345171   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0932   |
|    n_updates        | 61292    |
----------------------------------
Eval num_timesteps=345500, episode_reward=2.72 +/- 5.18
Episode length: 115.52 +/- 59.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.656    |
| time/               |          |
|    total_timesteps  | 345500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.067    |
|    n_updates        | 61374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.55     |
|    exploration_rate | 0.656    |
| time/               |          |
|    episodes         | 3432     |
|    fps              | 39       |
|    time_elapsed     | 8697     |
|    total_timesteps  | 345543   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.18     |
|    n_updates        | 61385    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.655    |
| time/               |          |
|    episodes         | 3436     |
|    fps              | 39       |
|    time_elapsed     | 8698     |
|    total_timesteps  | 345888   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0599   |
|    n_updates        | 61471    |
----------------------------------
Eval num_timesteps=346000, episode_reward=5.20 +/- 7.53
Episode length: 113.02 +/- 55.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 5.2      |
| rollout/            |          |
|    exploration_rate | 0.655    |
| time/               |          |
|    total_timesteps  | 346000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0863   |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 0.655    |
| time/               |          |
|    episodes         | 3440     |
|    fps              | 39       |
|    time_elapsed     | 8714     |
|    total_timesteps  | 346302   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0367   |
|    n_updates        | 61575    |
----------------------------------
Eval num_timesteps=346500, episode_reward=2.26 +/- 3.67
Episode length: 102.52 +/- 36.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.654    |
| time/               |          |
|    total_timesteps  | 346500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0912   |
|    n_updates        | 61624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.654    |
| time/               |          |
|    episodes         | 3444     |
|    fps              | 39       |
|    time_elapsed     | 8727     |
|    total_timesteps  | 346763   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0472   |
|    n_updates        | 61690    |
----------------------------------
Eval num_timesteps=347000, episode_reward=4.42 +/- 8.06
Episode length: 119.36 +/- 81.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.653    |
| time/               |          |
|    total_timesteps  | 347000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0737   |
|    n_updates        | 61749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.62     |
|    exploration_rate | 0.653    |
| time/               |          |
|    episodes         | 3448     |
|    fps              | 39       |
|    time_elapsed     | 8742     |
|    total_timesteps  | 347202   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 61800    |
----------------------------------
Eval num_timesteps=347500, episode_reward=3.84 +/- 6.38
Episode length: 124.00 +/- 66.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.652    |
| time/               |          |
|    total_timesteps  | 347500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0567   |
|    n_updates        | 61874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 0.652    |
| time/               |          |
|    episodes         | 3452     |
|    fps              | 39       |
|    time_elapsed     | 8757     |
|    total_timesteps  | 347544   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 61885    |
----------------------------------
Eval num_timesteps=348000, episode_reward=2.60 +/- 5.04
Episode length: 101.50 +/- 47.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.651    |
| time/               |          |
|    total_timesteps  | 348000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.177    |
|    n_updates        | 61999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.53     |
|    exploration_rate | 0.651    |
| time/               |          |
|    episodes         | 3456     |
|    fps              | 39       |
|    time_elapsed     | 8771     |
|    total_timesteps  | 348068   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0821   |
|    n_updates        | 62016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 0.65     |
| time/               |          |
|    episodes         | 3460     |
|    fps              | 39       |
|    time_elapsed     | 8772     |
|    total_timesteps  | 348409   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 62102    |
----------------------------------
Eval num_timesteps=348500, episode_reward=2.46 +/- 3.07
Episode length: 98.36 +/- 47.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.65     |
| time/               |          |
|    total_timesteps  | 348500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0265   |
|    n_updates        | 62124    |
----------------------------------
Eval num_timesteps=349000, episode_reward=2.82 +/- 3.89
Episode length: 108.72 +/- 54.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.649    |
| time/               |          |
|    total_timesteps  | 349000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.648    |
| time/               |          |
|    episodes         | 3464     |
|    fps              | 39       |
|    time_elapsed     | 8803     |
|    total_timesteps  | 349201   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.05     |
|    n_updates        | 62300    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.648    |
| time/               |          |
|    episodes         | 3468     |
|    fps              | 39       |
|    time_elapsed     | 8804     |
|    total_timesteps  | 349486   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.22     |
|    n_updates        | 62371    |
----------------------------------
Eval num_timesteps=349500, episode_reward=2.28 +/- 3.81
Episode length: 109.40 +/- 44.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.648    |
| time/               |          |
|    total_timesteps  | 349500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0807   |
|    n_updates        | 62374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.56     |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 3472     |
|    fps              | 39       |
|    time_elapsed     | 8817     |
|    total_timesteps  | 349849   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 62462    |
----------------------------------
Eval num_timesteps=350000, episode_reward=3.70 +/- 5.05
Episode length: 123.38 +/- 84.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.647    |
| time/               |          |
|    total_timesteps  | 350000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.18     |
|    n_updates        | 62499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.646    |
| time/               |          |
|    episodes         | 3476     |
|    fps              | 39       |
|    time_elapsed     | 8831     |
|    total_timesteps  | 350350   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0871   |
|    n_updates        | 62587    |
----------------------------------
Eval num_timesteps=350500, episode_reward=4.00 +/- 7.01
Episode length: 115.00 +/- 65.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.646    |
| time/               |          |
|    total_timesteps  | 350500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 62624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.645    |
| time/               |          |
|    episodes         | 3480     |
|    fps              | 39       |
|    time_elapsed     | 8845     |
|    total_timesteps  | 350974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0537   |
|    n_updates        | 62743    |
----------------------------------
Eval num_timesteps=351000, episode_reward=3.68 +/- 5.70
Episode length: 110.72 +/- 51.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.68     |
| rollout/            |          |
|    exploration_rate | 0.645    |
| time/               |          |
|    total_timesteps  | 351000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 62749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.644    |
| time/               |          |
|    episodes         | 3484     |
|    fps              | 39       |
|    time_elapsed     | 8858     |
|    total_timesteps  | 351405   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0537   |
|    n_updates        | 62851    |
----------------------------------
Eval num_timesteps=351500, episode_reward=3.18 +/- 6.49
Episode length: 117.94 +/- 94.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.644    |
| time/               |          |
|    total_timesteps  | 351500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0835   |
|    n_updates        | 62874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.643    |
| time/               |          |
|    episodes         | 3488     |
|    fps              | 39       |
|    time_elapsed     | 8872     |
|    total_timesteps  | 351769   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 62942    |
----------------------------------
Eval num_timesteps=352000, episode_reward=4.44 +/- 7.28
Episode length: 113.22 +/- 53.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 4.44     |
| rollout/            |          |
|    exploration_rate | 0.643    |
| time/               |          |
|    total_timesteps  | 352000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 62999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.642    |
| time/               |          |
|    episodes         | 3492     |
|    fps              | 39       |
|    time_elapsed     | 8885     |
|    total_timesteps  | 352065   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 63016    |
----------------------------------
Eval num_timesteps=352500, episode_reward=2.26 +/- 3.82
Episode length: 88.40 +/- 31.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.4     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.641    |
| time/               |          |
|    total_timesteps  | 352500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0551   |
|    n_updates        | 63124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.99     |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 3496     |
|    fps              | 39       |
|    time_elapsed     | 8897     |
|    total_timesteps  | 352641   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0705   |
|    n_updates        | 63160    |
----------------------------------
Eval num_timesteps=353000, episode_reward=4.70 +/- 10.19
Episode length: 110.82 +/- 69.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.7      |
| rollout/            |          |
|    exploration_rate | 0.64     |
| time/               |          |
|    total_timesteps  | 353000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.049    |
|    n_updates        | 63249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.69     |
|    exploration_rate | 0.64     |
| time/               |          |
|    episodes         | 3500     |
|    fps              | 39       |
|    time_elapsed     | 8912     |
|    total_timesteps  | 353404   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0751   |
|    n_updates        | 63350    |
----------------------------------
Eval num_timesteps=353500, episode_reward=6.02 +/- 11.63
Episode length: 141.14 +/- 109.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 6.02     |
| rollout/            |          |
|    exploration_rate | 0.639    |
| time/               |          |
|    total_timesteps  | 353500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0847   |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.639    |
| time/               |          |
|    episodes         | 3504     |
|    fps              | 39       |
|    time_elapsed     | 8929     |
|    total_timesteps  | 353870   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 63467    |
----------------------------------
Eval num_timesteps=354000, episode_reward=2.92 +/- 3.80
Episode length: 105.16 +/- 47.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.638    |
| time/               |          |
|    total_timesteps  | 354000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 63499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 3508     |
|    fps              | 39       |
|    time_elapsed     | 8942     |
|    total_timesteps  | 354305   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 63576    |
----------------------------------
Eval num_timesteps=354500, episode_reward=4.02 +/- 8.06
Episode length: 117.44 +/- 84.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.637    |
| time/               |          |
|    total_timesteps  | 354500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.2      |
|    n_updates        | 63624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.637    |
| time/               |          |
|    episodes         | 3512     |
|    fps              | 39       |
|    time_elapsed     | 8955     |
|    total_timesteps  | 354553   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 63638    |
----------------------------------
Eval num_timesteps=355000, episode_reward=3.24 +/- 5.31
Episode length: 94.40 +/- 44.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.636    |
| time/               |          |
|    total_timesteps  | 355000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0549   |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.51     |
|    exploration_rate | 0.636    |
| time/               |          |
|    episodes         | 3516     |
|    fps              | 39       |
|    time_elapsed     | 8967     |
|    total_timesteps  | 355009   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 63752    |
----------------------------------
Eval num_timesteps=355500, episode_reward=3.32 +/- 4.61
Episode length: 110.52 +/- 54.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.635    |
| time/               |          |
|    total_timesteps  | 355500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0885   |
|    n_updates        | 63874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.635    |
| time/               |          |
|    episodes         | 3520     |
|    fps              | 39       |
|    time_elapsed     | 8980     |
|    total_timesteps  | 355635   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0389   |
|    n_updates        | 63908    |
----------------------------------
Eval num_timesteps=356000, episode_reward=4.08 +/- 7.27
Episode length: 108.26 +/- 53.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.08     |
| rollout/            |          |
|    exploration_rate | 0.634    |
| time/               |          |
|    total_timesteps  | 356000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 63999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.71     |
|    exploration_rate | 0.634    |
| time/               |          |
|    episodes         | 3524     |
|    fps              | 39       |
|    time_elapsed     | 8994     |
|    total_timesteps  | 356127   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0782   |
|    n_updates        | 64031    |
----------------------------------
Eval num_timesteps=356500, episode_reward=3.00 +/- 5.67
Episode length: 117.60 +/- 56.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.633    |
| time/               |          |
|    total_timesteps  | 356500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0984   |
|    n_updates        | 64124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.73     |
|    exploration_rate | 0.633    |
| time/               |          |
|    episodes         | 3528     |
|    fps              | 39       |
|    time_elapsed     | 9010     |
|    total_timesteps  | 356622   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0872   |
|    n_updates        | 64155    |
----------------------------------
Eval num_timesteps=357000, episode_reward=2.20 +/- 5.39
Episode length: 97.60 +/- 58.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.632    |
| time/               |          |
|    total_timesteps  | 357000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.84     |
|    exploration_rate | 0.632    |
| time/               |          |
|    episodes         | 3532     |
|    fps              | 39       |
|    time_elapsed     | 9022     |
|    total_timesteps  | 357026   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.085    |
|    n_updates        | 64256    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.86     |
|    exploration_rate | 0.631    |
| time/               |          |
|    episodes         | 3536     |
|    fps              | 39       |
|    time_elapsed     | 9023     |
|    total_timesteps  | 357465   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 64366    |
----------------------------------
Eval num_timesteps=357500, episode_reward=3.48 +/- 4.86
Episode length: 114.24 +/- 48.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.48     |
| rollout/            |          |
|    exploration_rate | 0.631    |
| time/               |          |
|    total_timesteps  | 357500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 64374    |
----------------------------------
Eval num_timesteps=358000, episode_reward=4.06 +/- 6.78
Episode length: 103.62 +/- 62.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 4.06     |
| rollout/            |          |
|    exploration_rate | 0.63     |
| time/               |          |
|    total_timesteps  | 358000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0951   |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.24     |
|    exploration_rate | 0.629    |
| time/               |          |
|    episodes         | 3540     |
|    fps              | 39       |
|    time_elapsed     | 9049     |
|    total_timesteps  | 358150   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.045    |
|    n_updates        | 64537    |
----------------------------------
Eval num_timesteps=358500, episode_reward=4.48 +/- 7.47
Episode length: 103.18 +/- 49.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.629    |
| time/               |          |
|    total_timesteps  | 358500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0663   |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.25     |
|    exploration_rate | 0.629    |
| time/               |          |
|    episodes         | 3544     |
|    fps              | 39       |
|    time_elapsed     | 9061     |
|    total_timesteps  | 358559   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.085    |
|    n_updates        | 64639    |
----------------------------------
Eval num_timesteps=359000, episode_reward=1.76 +/- 2.53
Episode length: 110.72 +/- 82.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 0.628    |
| time/               |          |
|    total_timesteps  | 359000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0843   |
|    n_updates        | 64749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.33     |
|    exploration_rate | 0.627    |
| time/               |          |
|    episodes         | 3548     |
|    fps              | 39       |
|    time_elapsed     | 9074     |
|    total_timesteps  | 359141   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0514   |
|    n_updates        | 64785    |
----------------------------------
Eval num_timesteps=359500, episode_reward=2.30 +/- 3.07
Episode length: 111.44 +/- 80.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.626    |
| time/               |          |
|    total_timesteps  | 359500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 5.21     |
|    exploration_rate | 0.626    |
| time/               |          |
|    episodes         | 3552     |
|    fps              | 39       |
|    time_elapsed     | 9092     |
|    total_timesteps  | 359612   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.085    |
|    n_updates        | 64902    |
----------------------------------
Eval num_timesteps=360000, episode_reward=3.08 +/- 4.27
Episode length: 97.68 +/- 43.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.7     |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.625    |
| time/               |          |
|    total_timesteps  | 360000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0777   |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.27     |
|    exploration_rate | 0.625    |
| time/               |          |
|    episodes         | 3556     |
|    fps              | 39       |
|    time_elapsed     | 9104     |
|    total_timesteps  | 360099   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0422   |
|    n_updates        | 65024    |
----------------------------------
Eval num_timesteps=360500, episode_reward=3.36 +/- 4.98
Episode length: 116.34 +/- 55.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.624    |
| time/               |          |
|    total_timesteps  | 360500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.18     |
|    n_updates        | 65124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.49     |
|    exploration_rate | 0.624    |
| time/               |          |
|    episodes         | 3560     |
|    fps              | 39       |
|    time_elapsed     | 9118     |
|    total_timesteps  | 360663   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 65165    |
----------------------------------
Eval num_timesteps=361000, episode_reward=4.24 +/- 8.80
Episode length: 118.80 +/- 70.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.623    |
| time/               |          |
|    total_timesteps  | 361000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.07     |
|    exploration_rate | 0.623    |
| time/               |          |
|    episodes         | 3564     |
|    fps              | 39       |
|    time_elapsed     | 9132     |
|    total_timesteps  | 361140   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 65284    |
----------------------------------
Eval num_timesteps=361500, episode_reward=2.32 +/- 2.93
Episode length: 99.32 +/- 39.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.622    |
| time/               |          |
|    total_timesteps  | 361500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 65374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.14     |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 3568     |
|    fps              | 39       |
|    time_elapsed     | 9144     |
|    total_timesteps  | 361504   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0727   |
|    n_updates        | 65375    |
----------------------------------
Eval num_timesteps=362000, episode_reward=4.30 +/- 6.57
Episode length: 138.92 +/- 82.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.621    |
| time/               |          |
|    total_timesteps  | 362000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.27     |
|    exploration_rate | 0.621    |
| time/               |          |
|    episodes         | 3572     |
|    fps              | 39       |
|    time_elapsed     | 9161     |
|    total_timesteps  | 362113   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0721   |
|    n_updates        | 65528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.93     |
|    exploration_rate | 0.62     |
| time/               |          |
|    episodes         | 3576     |
|    fps              | 39       |
|    time_elapsed     | 9162     |
|    total_timesteps  | 362450   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.045    |
|    n_updates        | 65612    |
----------------------------------
Eval num_timesteps=362500, episode_reward=3.32 +/- 7.07
Episode length: 112.24 +/- 87.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 362500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0639   |
|    n_updates        | 65624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.619    |
| time/               |          |
|    episodes         | 3580     |
|    fps              | 39       |
|    time_elapsed     | 9175     |
|    total_timesteps  | 362785   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 65696    |
----------------------------------
Eval num_timesteps=363000, episode_reward=2.68 +/- 4.25
Episode length: 114.92 +/- 45.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.619    |
| time/               |          |
|    total_timesteps  | 363000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.175    |
|    n_updates        | 65749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.56     |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 3584     |
|    fps              | 39       |
|    time_elapsed     | 9189     |
|    total_timesteps  | 363288   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 65821    |
----------------------------------
Eval num_timesteps=363500, episode_reward=3.16 +/- 5.08
Episode length: 95.68 +/- 44.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.7     |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.618    |
| time/               |          |
|    total_timesteps  | 363500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.96     |
|    exploration_rate | 0.617    |
| time/               |          |
|    episodes         | 3588     |
|    fps              | 39       |
|    time_elapsed     | 9206     |
|    total_timesteps  | 363857   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0699   |
|    n_updates        | 65964    |
----------------------------------
Eval num_timesteps=364000, episode_reward=3.30 +/- 7.14
Episode length: 100.88 +/- 48.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.3      |
| rollout/            |          |
|    exploration_rate | 0.617    |
| time/               |          |
|    total_timesteps  | 364000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.161    |
|    n_updates        | 65999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.616    |
| time/               |          |
|    episodes         | 3592     |
|    fps              | 39       |
|    time_elapsed     | 9222     |
|    total_timesteps  | 364227   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 66056    |
----------------------------------
Eval num_timesteps=364500, episode_reward=2.98 +/- 5.31
Episode length: 109.74 +/- 64.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.616    |
| time/               |          |
|    total_timesteps  | 364500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.221    |
|    n_updates        | 66124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.92     |
|    exploration_rate | 0.615    |
| time/               |          |
|    episodes         | 3596     |
|    fps              | 39       |
|    time_elapsed     | 9236     |
|    total_timesteps  | 364718   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.163    |
|    n_updates        | 66179    |
----------------------------------
Eval num_timesteps=365000, episode_reward=3.84 +/- 5.38
Episode length: 110.46 +/- 58.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.615    |
| time/               |          |
|    total_timesteps  | 365000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0523   |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.18     |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 3600     |
|    fps              | 39       |
|    time_elapsed     | 9249     |
|    total_timesteps  | 365211   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0566   |
|    n_updates        | 66302    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.613    |
| time/               |          |
|    episodes         | 3604     |
|    fps              | 39       |
|    time_elapsed     | 9250     |
|    total_timesteps  | 365499   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0928   |
|    n_updates        | 66374    |
----------------------------------
Eval num_timesteps=365500, episode_reward=1.74 +/- 2.55
Episode length: 88.78 +/- 34.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.8     |
|    mean_reward     | 1.74     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=4.02 +/- 6.82
Episode length: 117.70 +/- 62.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.612    |
| time/               |          |
|    total_timesteps  | 366000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0503   |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.612    |
| time/               |          |
|    episodes         | 3608     |
|    fps              | 39       |
|    time_elapsed     | 9275     |
|    total_timesteps  | 366102   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 66525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.27     |
|    exploration_rate | 0.611    |
| time/               |          |
|    episodes         | 3612     |
|    fps              | 39       |
|    time_elapsed     | 9276     |
|    total_timesteps  | 366457   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 66614    |
----------------------------------
Eval num_timesteps=366500, episode_reward=2.68 +/- 3.41
Episode length: 95.34 +/- 36.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.3     |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.611    |
| time/               |          |
|    total_timesteps  | 366500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 66624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.61     |
| time/               |          |
|    episodes         | 3616     |
|    fps              | 39       |
|    time_elapsed     | 9288     |
|    total_timesteps  | 366994   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.174    |
|    n_updates        | 66748    |
----------------------------------
Eval num_timesteps=367000, episode_reward=4.00 +/- 5.71
Episode length: 123.10 +/- 68.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.61     |
| time/               |          |
|    total_timesteps  | 367000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0719   |
|    n_updates        | 66749    |
----------------------------------
Eval num_timesteps=367500, episode_reward=4.46 +/- 5.81
Episode length: 115.38 +/- 64.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.46     |
| rollout/            |          |
|    exploration_rate | 0.609    |
| time/               |          |
|    total_timesteps  | 367500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.173    |
|    n_updates        | 66874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.67     |
|    exploration_rate | 0.609    |
| time/               |          |
|    episodes         | 3620     |
|    fps              | 39       |
|    time_elapsed     | 9315     |
|    total_timesteps  | 367535   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0923   |
|    n_updates        | 66883    |
----------------------------------
Eval num_timesteps=368000, episode_reward=3.36 +/- 5.77
Episode length: 106.40 +/- 54.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.608    |
| time/               |          |
|    total_timesteps  | 368000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0698   |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 5.13     |
|    exploration_rate | 0.608    |
| time/               |          |
|    episodes         | 3624     |
|    fps              | 39       |
|    time_elapsed     | 9329     |
|    total_timesteps  | 368185   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.153    |
|    n_updates        | 67046    |
----------------------------------
Eval num_timesteps=368500, episode_reward=2.14 +/- 3.99
Episode length: 95.70 +/- 38.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.7     |
|    mean_reward      | 2.14     |
| rollout/            |          |
|    exploration_rate | 0.607    |
| time/               |          |
|    total_timesteps  | 368500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 67124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.15     |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 3628     |
|    fps              | 39       |
|    time_elapsed     | 9340     |
|    total_timesteps  | 368551   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0877   |
|    n_updates        | 67137    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5        |
|    exploration_rate | 0.606    |
| time/               |          |
|    episodes         | 3632     |
|    fps              | 39       |
|    time_elapsed     | 9341     |
|    total_timesteps  | 368929   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 67232    |
----------------------------------
Eval num_timesteps=369000, episode_reward=4.60 +/- 9.60
Episode length: 106.28 +/- 59.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.606    |
| time/               |          |
|    total_timesteps  | 369000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.195    |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.605    |
| time/               |          |
|    episodes         | 3636     |
|    fps              | 39       |
|    time_elapsed     | 9354     |
|    total_timesteps  | 369293   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 67323    |
----------------------------------
Eval num_timesteps=369500, episode_reward=2.22 +/- 4.18
Episode length: 96.40 +/- 64.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.4     |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.605    |
| time/               |          |
|    total_timesteps  | 369500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.169    |
|    n_updates        | 67374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.604    |
| time/               |          |
|    episodes         | 3640     |
|    fps              | 39       |
|    time_elapsed     | 9367     |
|    total_timesteps  | 369859   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.118    |
|    n_updates        | 67464    |
----------------------------------
Eval num_timesteps=370000, episode_reward=3.38 +/- 4.53
Episode length: 107.68 +/- 44.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.604    |
| time/               |          |
|    total_timesteps  | 370000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0513   |
|    n_updates        | 67499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.79     |
|    exploration_rate | 0.603    |
| time/               |          |
|    episodes         | 3644     |
|    fps              | 39       |
|    time_elapsed     | 9380     |
|    total_timesteps  | 370176   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 67543    |
----------------------------------
Eval num_timesteps=370500, episode_reward=2.48 +/- 3.60
Episode length: 89.68 +/- 36.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.7     |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.602    |
| time/               |          |
|    total_timesteps  | 370500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.175    |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.602    |
| time/               |          |
|    episodes         | 3648     |
|    fps              | 39       |
|    time_elapsed     | 9392     |
|    total_timesteps  | 370709   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 67677    |
----------------------------------
Eval num_timesteps=371000, episode_reward=5.02 +/- 7.52
Episode length: 113.04 +/- 55.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 5.02     |
| rollout/            |          |
|    exploration_rate | 0.601    |
| time/               |          |
|    total_timesteps  | 371000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 67749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.601    |
| time/               |          |
|    episodes         | 3652     |
|    fps              | 39       |
|    time_elapsed     | 9406     |
|    total_timesteps  | 371004   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0348   |
|    n_updates        | 67750    |
----------------------------------
Eval num_timesteps=371500, episode_reward=2.36 +/- 3.55
Episode length: 100.10 +/- 45.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.6      |
| time/               |          |
|    total_timesteps  | 371500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.91     |
|    exploration_rate | 0.6      |
| time/               |          |
|    episodes         | 3656     |
|    fps              | 39       |
|    time_elapsed     | 9419     |
|    total_timesteps  | 371531   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0681   |
|    n_updates        | 67882    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.83     |
|    exploration_rate | 0.599    |
| time/               |          |
|    episodes         | 3660     |
|    fps              | 39       |
|    time_elapsed     | 9421     |
|    total_timesteps  | 371943   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0546   |
|    n_updates        | 67985    |
----------------------------------
Eval num_timesteps=372000, episode_reward=6.12 +/- 8.53
Episode length: 123.00 +/- 78.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 6.12     |
| rollout/            |          |
|    exploration_rate | 0.599    |
| time/               |          |
|    total_timesteps  | 372000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0564   |
|    n_updates        | 67999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.63     |
|    exploration_rate | 0.599    |
| time/               |          |
|    episodes         | 3664     |
|    fps              | 39       |
|    time_elapsed     | 9435     |
|    total_timesteps  | 372236   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0832   |
|    n_updates        | 68058    |
----------------------------------
Eval num_timesteps=372500, episode_reward=5.14 +/- 7.19
Episode length: 122.40 +/- 52.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.598    |
| time/               |          |
|    total_timesteps  | 372500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 68124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.62     |
|    exploration_rate | 0.598    |
| time/               |          |
|    episodes         | 3668     |
|    fps              | 39       |
|    time_elapsed     | 9449     |
|    total_timesteps  | 372570   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.148    |
|    n_updates        | 68142    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.597    |
| time/               |          |
|    episodes         | 3672     |
|    fps              | 39       |
|    time_elapsed     | 9450     |
|    total_timesteps  | 372938   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0747   |
|    n_updates        | 68234    |
----------------------------------
Eval num_timesteps=373000, episode_reward=3.34 +/- 5.04
Episode length: 111.68 +/- 51.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.597    |
| time/               |          |
|    total_timesteps  | 373000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0825   |
|    n_updates        | 68249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.61     |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 3676     |
|    fps              | 39       |
|    time_elapsed     | 9463     |
|    total_timesteps  | 373207   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 68301    |
----------------------------------
Eval num_timesteps=373500, episode_reward=6.28 +/- 10.77
Episode length: 127.62 +/- 70.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 6.28     |
| rollout/            |          |
|    exploration_rate | 0.596    |
| time/               |          |
|    total_timesteps  | 373500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 68374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.61     |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 3680     |
|    fps              | 39       |
|    time_elapsed     | 9478     |
|    total_timesteps  | 373574   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 68393    |
----------------------------------
Eval num_timesteps=374000, episode_reward=3.56 +/- 7.16
Episode length: 104.54 +/- 64.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.595    |
| time/               |          |
|    total_timesteps  | 374000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 4.78     |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 3684     |
|    fps              | 39       |
|    time_elapsed     | 9493     |
|    total_timesteps  | 374029   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 68507    |
----------------------------------
Eval num_timesteps=374500, episode_reward=2.18 +/- 3.65
Episode length: 92.30 +/- 31.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.3     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.594    |
| time/               |          |
|    total_timesteps  | 374500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.08     |
|    n_updates        | 68624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.593    |
| time/               |          |
|    episodes         | 3688     |
|    fps              | 39       |
|    time_elapsed     | 9508     |
|    total_timesteps  | 374684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0338   |
|    n_updates        | 68670    |
----------------------------------
Eval num_timesteps=375000, episode_reward=2.46 +/- 4.31
Episode length: 92.00 +/- 41.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.593    |
| time/               |          |
|    total_timesteps  | 375000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.177    |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.592    |
| time/               |          |
|    episodes         | 3692     |
|    fps              | 39       |
|    time_elapsed     | 9520     |
|    total_timesteps  | 375103   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0483   |
|    n_updates        | 68775    |
----------------------------------
Eval num_timesteps=375500, episode_reward=2.50 +/- 3.95
Episode length: 98.20 +/- 42.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.2     |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.591    |
| time/               |          |
|    total_timesteps  | 375500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.591    |
| time/               |          |
|    episodes         | 3696     |
|    fps              | 39       |
|    time_elapsed     | 9533     |
|    total_timesteps  | 375738   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0572   |
|    n_updates        | 68934    |
----------------------------------
Eval num_timesteps=376000, episode_reward=3.56 +/- 5.89
Episode length: 104.14 +/- 54.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.59     |
| time/               |          |
|    total_timesteps  | 376000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0924   |
|    n_updates        | 68999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.6      |
|    exploration_rate | 0.59     |
| time/               |          |
|    episodes         | 3700     |
|    fps              | 39       |
|    time_elapsed     | 9546     |
|    total_timesteps  | 376216   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 69053    |
----------------------------------
Eval num_timesteps=376500, episode_reward=3.62 +/- 4.94
Episode length: 118.60 +/- 86.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.589    |
| time/               |          |
|    total_timesteps  | 376500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0752   |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.589    |
| time/               |          |
|    episodes         | 3704     |
|    fps              | 39       |
|    time_elapsed     | 9560     |
|    total_timesteps  | 376577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0674   |
|    n_updates        | 69144    |
----------------------------------
Eval num_timesteps=377000, episode_reward=2.96 +/- 4.60
Episode length: 124.68 +/- 90.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.588    |
| time/               |          |
|    total_timesteps  | 377000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0602   |
|    n_updates        | 69249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.588    |
| time/               |          |
|    episodes         | 3708     |
|    fps              | 39       |
|    time_elapsed     | 9575     |
|    total_timesteps  | 377020   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0666   |
|    n_updates        | 69254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.587    |
| time/               |          |
|    episodes         | 3712     |
|    fps              | 39       |
|    time_elapsed     | 9577     |
|    total_timesteps  | 377382   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0574   |
|    n_updates        | 69345    |
----------------------------------
Eval num_timesteps=377500, episode_reward=3.86 +/- 4.86
Episode length: 119.28 +/- 80.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.587    |
| time/               |          |
|    total_timesteps  | 377500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0478   |
|    n_updates        | 69374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.586    |
| time/               |          |
|    episodes         | 3716     |
|    fps              | 39       |
|    time_elapsed     | 9591     |
|    total_timesteps  | 377798   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.042    |
|    n_updates        | 69449    |
----------------------------------
Eval num_timesteps=378000, episode_reward=4.24 +/- 8.20
Episode length: 106.74 +/- 57.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.586    |
| time/               |          |
|    total_timesteps  | 378000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.22     |
|    exploration_rate | 0.585    |
| time/               |          |
|    episodes         | 3720     |
|    fps              | 39       |
|    time_elapsed     | 9604     |
|    total_timesteps  | 378404   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0824   |
|    n_updates        | 69600    |
----------------------------------
Eval num_timesteps=378500, episode_reward=2.66 +/- 5.58
Episode length: 100.58 +/- 51.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.585    |
| time/               |          |
|    total_timesteps  | 378500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 69624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.584    |
| time/               |          |
|    episodes         | 3724     |
|    fps              | 39       |
|    time_elapsed     | 9617     |
|    total_timesteps  | 378762   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.319    |
|    n_updates        | 69690    |
----------------------------------
Eval num_timesteps=379000, episode_reward=5.28 +/- 9.29
Episode length: 114.64 +/- 64.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 5.28     |
| rollout/            |          |
|    exploration_rate | 0.584    |
| time/               |          |
|    total_timesteps  | 379000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0749   |
|    n_updates        | 69749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 3728     |
|    fps              | 39       |
|    time_elapsed     | 9631     |
|    total_timesteps  | 379211   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 69802    |
----------------------------------
Eval num_timesteps=379500, episode_reward=3.76 +/- 7.61
Episode length: 101.96 +/- 49.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.582    |
| time/               |          |
|    total_timesteps  | 379500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.173    |
|    n_updates        | 69874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.582    |
| time/               |          |
|    episodes         | 3732     |
|    fps              | 39       |
|    time_elapsed     | 9643     |
|    total_timesteps  | 379609   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0514   |
|    n_updates        | 69902    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.581    |
| time/               |          |
|    episodes         | 3736     |
|    fps              | 39       |
|    time_elapsed     | 9644     |
|    total_timesteps  | 379968   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0457   |
|    n_updates        | 69991    |
----------------------------------
Eval num_timesteps=380000, episode_reward=2.40 +/- 3.68
Episode length: 95.30 +/- 35.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.3     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.581    |
| time/               |          |
|    total_timesteps  | 380000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.43     |
|    exploration_rate | 0.58     |
| time/               |          |
|    episodes         | 3740     |
|    fps              | 39       |
|    time_elapsed     | 9656     |
|    total_timesteps  | 380396   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 70098    |
----------------------------------
Eval num_timesteps=380500, episode_reward=2.30 +/- 4.35
Episode length: 96.74 +/- 39.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.7     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.58     |
| time/               |          |
|    total_timesteps  | 380500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0873   |
|    n_updates        | 70124    |
----------------------------------
Eval num_timesteps=381000, episode_reward=2.34 +/- 4.79
Episode length: 122.22 +/- 86.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.579    |
| time/               |          |
|    total_timesteps  | 381000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.179    |
|    n_updates        | 70249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.579    |
| time/               |          |
|    episodes         | 3744     |
|    fps              | 39       |
|    time_elapsed     | 9690     |
|    total_timesteps  | 381161   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0799   |
|    n_updates        | 70290    |
----------------------------------
Eval num_timesteps=381500, episode_reward=2.56 +/- 4.40
Episode length: 89.06 +/- 35.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.1     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.578    |
| time/               |          |
|    total_timesteps  | 381500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 70374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.78     |
|    exploration_rate | 0.578    |
| time/               |          |
|    episodes         | 3748     |
|    fps              | 39       |
|    time_elapsed     | 9705     |
|    total_timesteps  | 381563   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0833   |
|    n_updates        | 70390    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.9      |
|    exploration_rate | 0.577    |
| time/               |          |
|    episodes         | 3752     |
|    fps              | 39       |
|    time_elapsed     | 9706     |
|    total_timesteps  | 381938   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 70484    |
----------------------------------
Eval num_timesteps=382000, episode_reward=5.02 +/- 11.44
Episode length: 108.12 +/- 72.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 5.02     |
| rollout/            |          |
|    exploration_rate | 0.577    |
| time/               |          |
|    total_timesteps  | 382000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 70499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 3756     |
|    fps              | 39       |
|    time_elapsed     | 9720     |
|    total_timesteps  | 382284   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.201    |
|    n_updates        | 70570    |
----------------------------------
Eval num_timesteps=382500, episode_reward=3.56 +/- 5.57
Episode length: 99.80 +/- 52.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.8     |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.576    |
| time/               |          |
|    total_timesteps  | 382500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.173    |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.575    |
| time/               |          |
|    episodes         | 3760     |
|    fps              | 39       |
|    time_elapsed     | 9732     |
|    total_timesteps  | 382840   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 70709    |
----------------------------------
Eval num_timesteps=383000, episode_reward=3.24 +/- 7.36
Episode length: 114.88 +/- 83.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.575    |
| time/               |          |
|    total_timesteps  | 383000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.177    |
|    n_updates        | 70749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.574    |
| time/               |          |
|    episodes         | 3764     |
|    fps              | 39       |
|    time_elapsed     | 9746     |
|    total_timesteps  | 383242   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0485   |
|    n_updates        | 70810    |
----------------------------------
Eval num_timesteps=383500, episode_reward=4.22 +/- 7.82
Episode length: 107.64 +/- 68.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.22     |
| rollout/            |          |
|    exploration_rate | 0.573    |
| time/               |          |
|    total_timesteps  | 383500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 70874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.573    |
| time/               |          |
|    episodes         | 3768     |
|    fps              | 39       |
|    time_elapsed     | 9759     |
|    total_timesteps  | 383701   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 70925    |
----------------------------------
Eval num_timesteps=384000, episode_reward=4.04 +/- 7.58
Episode length: 101.12 +/- 45.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.572    |
| time/               |          |
|    total_timesteps  | 384000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.571    |
| time/               |          |
|    episodes         | 3772     |
|    fps              | 39       |
|    time_elapsed     | 9772     |
|    total_timesteps  | 384405   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0882   |
|    n_updates        | 71101    |
----------------------------------
Eval num_timesteps=384500, episode_reward=2.40 +/- 3.29
Episode length: 99.28 +/- 41.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.571    |
| time/               |          |
|    total_timesteps  | 384500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0567   |
|    n_updates        | 71124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.19     |
|    exploration_rate | 0.57     |
| time/               |          |
|    episodes         | 3776     |
|    fps              | 39       |
|    time_elapsed     | 9785     |
|    total_timesteps  | 384862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 71215    |
----------------------------------
Eval num_timesteps=385000, episode_reward=5.42 +/- 6.93
Episode length: 119.98 +/- 60.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 5.42     |
| rollout/            |          |
|    exploration_rate | 0.57     |
| time/               |          |
|    total_timesteps  | 385000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.57     |
| time/               |          |
|    episodes         | 3780     |
|    fps              | 39       |
|    time_elapsed     | 9798     |
|    total_timesteps  | 385201   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.141    |
|    n_updates        | 71300    |
----------------------------------
Eval num_timesteps=385500, episode_reward=4.18 +/- 6.01
Episode length: 104.42 +/- 46.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.569    |
| time/               |          |
|    total_timesteps  | 385500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0638   |
|    n_updates        | 71374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.569    |
| time/               |          |
|    episodes         | 3784     |
|    fps              | 39       |
|    time_elapsed     | 9811     |
|    total_timesteps  | 385570   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 71392    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.568    |
| time/               |          |
|    episodes         | 3788     |
|    fps              | 39       |
|    time_elapsed     | 9812     |
|    total_timesteps  | 385922   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.183    |
|    n_updates        | 71480    |
----------------------------------
Eval num_timesteps=386000, episode_reward=3.68 +/- 5.68
Episode length: 110.84 +/- 66.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.68     |
| rollout/            |          |
|    exploration_rate | 0.568    |
| time/               |          |
|    total_timesteps  | 386000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0986   |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.567    |
| time/               |          |
|    episodes         | 3792     |
|    fps              | 39       |
|    time_elapsed     | 9826     |
|    total_timesteps  | 386363   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0229   |
|    n_updates        | 71590    |
----------------------------------
Eval num_timesteps=386500, episode_reward=1.92 +/- 2.88
Episode length: 87.56 +/- 31.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.6     |
|    mean_reward      | 1.92     |
| rollout/            |          |
|    exploration_rate | 0.567    |
| time/               |          |
|    total_timesteps  | 386500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0944   |
|    n_updates        | 71624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.566    |
| time/               |          |
|    episodes         | 3796     |
|    fps              | 39       |
|    time_elapsed     | 9838     |
|    total_timesteps  | 386837   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 71709    |
----------------------------------
Eval num_timesteps=387000, episode_reward=2.32 +/- 4.32
Episode length: 89.70 +/- 36.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.7     |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.566    |
| time/               |          |
|    total_timesteps  | 387000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0494   |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.565    |
| time/               |          |
|    episodes         | 3800     |
|    fps              | 39       |
|    time_elapsed     | 9849     |
|    total_timesteps  | 387415   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 71853    |
----------------------------------
Eval num_timesteps=387500, episode_reward=2.50 +/- 4.26
Episode length: 88.28 +/- 42.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.3     |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.564    |
| time/               |          |
|    total_timesteps  | 387500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0498   |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.564    |
| time/               |          |
|    episodes         | 3804     |
|    fps              | 39       |
|    time_elapsed     | 9861     |
|    total_timesteps  | 387859   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.177    |
|    n_updates        | 71964    |
----------------------------------
Eval num_timesteps=388000, episode_reward=3.84 +/- 5.97
Episode length: 99.28 +/- 47.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.563    |
| time/               |          |
|    total_timesteps  | 388000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0459   |
|    n_updates        | 71999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.563    |
| time/               |          |
|    episodes         | 3808     |
|    fps              | 39       |
|    time_elapsed     | 9873     |
|    total_timesteps  | 388191   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0843   |
|    n_updates        | 72047    |
----------------------------------
Eval num_timesteps=388500, episode_reward=2.96 +/- 5.38
Episode length: 102.40 +/- 46.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.562    |
| time/               |          |
|    total_timesteps  | 388500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0855   |
|    n_updates        | 72124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.562    |
| time/               |          |
|    episodes         | 3812     |
|    fps              | 39       |
|    time_elapsed     | 9888     |
|    total_timesteps  | 388601   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.087    |
|    n_updates        | 72150    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.99     |
|    exploration_rate | 0.561    |
| time/               |          |
|    episodes         | 3816     |
|    fps              | 39       |
|    time_elapsed     | 9889     |
|    total_timesteps  | 388969   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 72242    |
----------------------------------
Eval num_timesteps=389000, episode_reward=3.26 +/- 5.20
Episode length: 103.00 +/- 51.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.561    |
| time/               |          |
|    total_timesteps  | 389000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 72249    |
----------------------------------
Eval num_timesteps=389500, episode_reward=4.50 +/- 6.40
Episode length: 111.44 +/- 59.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.5      |
| rollout/            |          |
|    exploration_rate | 0.56     |
| time/               |          |
|    total_timesteps  | 389500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.217    |
|    n_updates        | 72374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 3820     |
|    fps              | 39       |
|    time_elapsed     | 9914     |
|    total_timesteps  | 389536   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0979   |
|    n_updates        | 72383    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.26     |
|    exploration_rate | 0.559    |
| time/               |          |
|    episodes         | 3824     |
|    fps              | 39       |
|    time_elapsed     | 9915     |
|    total_timesteps  | 389988   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0506   |
|    n_updates        | 72496    |
----------------------------------
Eval num_timesteps=390000, episode_reward=2.98 +/- 4.17
Episode length: 99.26 +/- 47.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.559    |
| time/               |          |
|    total_timesteps  | 390000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0839   |
|    n_updates        | 72499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.558    |
| time/               |          |
|    episodes         | 3828     |
|    fps              | 39       |
|    time_elapsed     | 9927     |
|    total_timesteps  | 390324   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 72580    |
----------------------------------
Eval num_timesteps=390500, episode_reward=4.42 +/- 6.57
Episode length: 100.94 +/- 57.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.558    |
| time/               |          |
|    total_timesteps  | 390500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0946   |
|    n_updates        | 72624    |
----------------------------------
Eval num_timesteps=391000, episode_reward=2.82 +/- 5.34
Episode length: 96.44 +/- 46.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.4     |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.556    |
| time/               |          |
|    total_timesteps  | 391000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0877   |
|    n_updates        | 72749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.4      |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 3832     |
|    fps              | 39       |
|    time_elapsed     | 9951     |
|    total_timesteps  | 391012   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0676   |
|    n_updates        | 72752    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 3836     |
|    fps              | 39       |
|    time_elapsed     | 9953     |
|    total_timesteps  | 391420   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 72854    |
----------------------------------
Eval num_timesteps=391500, episode_reward=3.08 +/- 6.14
Episode length: 102.58 +/- 53.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.555    |
| time/               |          |
|    total_timesteps  | 391500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 72874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.6      |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 3840     |
|    fps              | 39       |
|    time_elapsed     | 9966     |
|    total_timesteps  | 391834   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.194    |
|    n_updates        | 72958    |
----------------------------------
Eval num_timesteps=392000, episode_reward=1.80 +/- 2.84
Episode length: 91.90 +/- 37.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.9     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.554    |
| time/               |          |
|    total_timesteps  | 392000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 72999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.553    |
| time/               |          |
|    episodes         | 3844     |
|    fps              | 39       |
|    time_elapsed     | 9978     |
|    total_timesteps  | 392378   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0479   |
|    n_updates        | 73094    |
----------------------------------
Eval num_timesteps=392500, episode_reward=2.42 +/- 3.66
Episode length: 97.98 +/- 41.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98       |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.553    |
| time/               |          |
|    total_timesteps  | 392500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0778   |
|    n_updates        | 73124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.552    |
| time/               |          |
|    episodes         | 3848     |
|    fps              | 39       |
|    time_elapsed     | 9991     |
|    total_timesteps  | 392954   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 73238    |
----------------------------------
Eval num_timesteps=393000, episode_reward=3.36 +/- 6.07
Episode length: 106.72 +/- 55.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.552    |
| time/               |          |
|    total_timesteps  | 393000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 73249    |
----------------------------------
Eval num_timesteps=393500, episode_reward=3.22 +/- 4.29
Episode length: 107.88 +/- 83.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.551    |
| time/               |          |
|    total_timesteps  | 393500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0578   |
|    n_updates        | 73374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 5.03     |
|    exploration_rate | 0.551    |
| time/               |          |
|    episodes         | 3852     |
|    fps              | 39       |
|    time_elapsed     | 10016    |
|    total_timesteps  | 393563   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0725   |
|    n_updates        | 73390    |
----------------------------------
Eval num_timesteps=394000, episode_reward=4.60 +/- 7.59
Episode length: 118.38 +/- 64.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.55     |
| time/               |          |
|    total_timesteps  | 394000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0947   |
|    n_updates        | 73499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.28     |
|    exploration_rate | 0.549    |
| time/               |          |
|    episodes         | 3856     |
|    fps              | 39       |
|    time_elapsed     | 10031    |
|    total_timesteps  | 394045   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.161    |
|    n_updates        | 73511    |
----------------------------------
Eval num_timesteps=394500, episode_reward=5.04 +/- 8.83
Episode length: 119.90 +/- 82.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 5.04     |
| rollout/            |          |
|    exploration_rate | 0.548    |
| time/               |          |
|    total_timesteps  | 394500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 73624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.2      |
|    exploration_rate | 0.548    |
| time/               |          |
|    episodes         | 3860     |
|    fps              | 39       |
|    time_elapsed     | 10046    |
|    total_timesteps  | 394504   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0799   |
|    n_updates        | 73625    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.25     |
|    exploration_rate | 0.547    |
| time/               |          |
|    episodes         | 3864     |
|    fps              | 39       |
|    time_elapsed     | 10048    |
|    total_timesteps  | 394934   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 73733    |
----------------------------------
Eval num_timesteps=395000, episode_reward=4.64 +/- 7.59
Episode length: 121.66 +/- 65.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.547    |
| time/               |          |
|    total_timesteps  | 395000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 73749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.34     |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 3868     |
|    fps              | 39       |
|    time_elapsed     | 10068    |
|    total_timesteps  | 395364   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0487   |
|    n_updates        | 73840    |
----------------------------------
Eval num_timesteps=395500, episode_reward=4.48 +/- 7.54
Episode length: 108.26 +/- 67.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.546    |
| time/               |          |
|    total_timesteps  | 395500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0722   |
|    n_updates        | 73874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.545    |
| time/               |          |
|    episodes         | 3872     |
|    fps              | 39       |
|    time_elapsed     | 10085    |
|    total_timesteps  | 395862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0727   |
|    n_updates        | 73965    |
----------------------------------
Eval num_timesteps=396000, episode_reward=4.16 +/- 7.37
Episode length: 108.34 +/- 59.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.16     |
| rollout/            |          |
|    exploration_rate | 0.545    |
| time/               |          |
|    total_timesteps  | 396000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0441   |
|    n_updates        | 73999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.91     |
|    exploration_rate | 0.544    |
| time/               |          |
|    episodes         | 3876     |
|    fps              | 39       |
|    time_elapsed     | 10099    |
|    total_timesteps  | 396264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.235    |
|    n_updates        | 74065    |
----------------------------------
Eval num_timesteps=396500, episode_reward=3.70 +/- 8.73
Episode length: 103.66 +/- 58.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.544    |
| time/               |          |
|    total_timesteps  | 396500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.336    |
|    n_updates        | 74124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.544    |
| time/               |          |
|    episodes         | 3880     |
|    fps              | 39       |
|    time_elapsed     | 10111    |
|    total_timesteps  | 396512   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 74127    |
----------------------------------
Eval num_timesteps=397000, episode_reward=5.30 +/- 7.73
Episode length: 125.22 +/- 73.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 5.3      |
| rollout/            |          |
|    exploration_rate | 0.543    |
| time/               |          |
|    total_timesteps  | 397000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0807   |
|    n_updates        | 74249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 5.19     |
|    exploration_rate | 0.542    |
| time/               |          |
|    episodes         | 3884     |
|    fps              | 39       |
|    time_elapsed     | 10126    |
|    total_timesteps  | 397093   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0837   |
|    n_updates        | 74273    |
----------------------------------
Eval num_timesteps=397500, episode_reward=3.06 +/- 5.01
Episode length: 110.84 +/- 61.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.542    |
| time/               |          |
|    total_timesteps  | 397500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0613   |
|    n_updates        | 74374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.64     |
|    exploration_rate | 0.541    |
| time/               |          |
|    episodes         | 3888     |
|    fps              | 39       |
|    time_elapsed     | 10140    |
|    total_timesteps  | 397788   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 74446    |
----------------------------------
Eval num_timesteps=398000, episode_reward=4.10 +/- 5.78
Episode length: 111.16 +/- 54.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.54     |
| time/               |          |
|    total_timesteps  | 398000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.24     |
|    n_updates        | 74499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.72     |
|    exploration_rate | 0.54     |
| time/               |          |
|    episodes         | 3892     |
|    fps              | 39       |
|    time_elapsed     | 10154    |
|    total_timesteps  | 398277   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0454   |
|    n_updates        | 74569    |
----------------------------------
Eval num_timesteps=398500, episode_reward=3.44 +/- 5.99
Episode length: 100.06 +/- 50.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.539    |
| time/               |          |
|    total_timesteps  | 398500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 74624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.84     |
|    exploration_rate | 0.539    |
| time/               |          |
|    episodes         | 3896     |
|    fps              | 39       |
|    time_elapsed     | 10167    |
|    total_timesteps  | 398793   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0358   |
|    n_updates        | 74698    |
----------------------------------
Eval num_timesteps=399000, episode_reward=2.24 +/- 4.19
Episode length: 99.10 +/- 53.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.1     |
|    mean_reward      | 2.24     |
| rollout/            |          |
|    exploration_rate | 0.538    |
| time/               |          |
|    total_timesteps  | 399000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 74749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.75     |
|    exploration_rate | 0.537    |
| time/               |          |
|    episodes         | 3900     |
|    fps              | 39       |
|    time_elapsed     | 10180    |
|    total_timesteps  | 399250   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 74812    |
----------------------------------
Eval num_timesteps=399500, episode_reward=4.28 +/- 6.78
Episode length: 102.18 +/- 56.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 4.28     |
| rollout/            |          |
|    exploration_rate | 0.537    |
| time/               |          |
|    total_timesteps  | 399500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0386   |
|    n_updates        | 74874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.71     |
|    exploration_rate | 0.537    |
| time/               |          |
|    episodes         | 3904     |
|    fps              | 39       |
|    time_elapsed     | 10193    |
|    total_timesteps  | 399607   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 74901    |
----------------------------------
Eval num_timesteps=400000, episode_reward=2.94 +/- 3.78
Episode length: 99.52 +/- 44.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.5     |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 400000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0321   |
|    n_updates        | 74999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 6.1      |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 3908     |
|    fps              | 39       |
|    time_elapsed     | 10206    |
|    total_timesteps  | 400144   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0723   |
|    n_updates        | 75035    |
----------------------------------
Eval num_timesteps=400500, episode_reward=2.66 +/- 6.45
Episode length: 113.74 +/- 92.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.535    |
| time/               |          |
|    total_timesteps  | 400500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0711   |
|    n_updates        | 75124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 6.04     |
|    exploration_rate | 0.534    |
| time/               |          |
|    episodes         | 3912     |
|    fps              | 39       |
|    time_elapsed     | 10219    |
|    total_timesteps  | 400537   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 75134    |
----------------------------------
Eval num_timesteps=401000, episode_reward=3.96 +/- 7.76
Episode length: 134.54 +/- 97.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.533    |
| time/               |          |
|    total_timesteps  | 401000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 75249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.95     |
|    exploration_rate | 0.533    |
| time/               |          |
|    episodes         | 3916     |
|    fps              | 39       |
|    time_elapsed     | 10235    |
|    total_timesteps  | 401006   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 75251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.63     |
|    exploration_rate | 0.532    |
| time/               |          |
|    episodes         | 3920     |
|    fps              | 39       |
|    time_elapsed     | 10236    |
|    total_timesteps  | 401396   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0828   |
|    n_updates        | 75348    |
----------------------------------
Eval num_timesteps=401500, episode_reward=3.90 +/- 5.58
Episode length: 110.22 +/- 58.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.9      |
| rollout/            |          |
|    exploration_rate | 0.532    |
| time/               |          |
|    total_timesteps  | 401500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0807   |
|    n_updates        | 75374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.61     |
|    exploration_rate | 0.532    |
| time/               |          |
|    episodes         | 3924     |
|    fps              | 39       |
|    time_elapsed     | 10250    |
|    total_timesteps  | 401803   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 75450    |
----------------------------------
Eval num_timesteps=402000, episode_reward=2.32 +/- 5.10
Episode length: 101.56 +/- 56.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.531    |
| time/               |          |
|    total_timesteps  | 402000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0475   |
|    n_updates        | 75499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.58     |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 3928     |
|    fps              | 39       |
|    time_elapsed     | 10262    |
|    total_timesteps  | 402156   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0841   |
|    n_updates        | 75538    |
----------------------------------
Eval num_timesteps=402500, episode_reward=4.72 +/- 9.06
Episode length: 110.66 +/- 53.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.53     |
| time/               |          |
|    total_timesteps  | 402500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0683   |
|    n_updates        | 75624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 5.3      |
|    exploration_rate | 0.53     |
| time/               |          |
|    episodes         | 3932     |
|    fps              | 39       |
|    time_elapsed     | 10276    |
|    total_timesteps  | 402562   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 75640    |
----------------------------------
Eval num_timesteps=403000, episode_reward=5.48 +/- 9.48
Episode length: 137.04 +/- 98.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.529    |
| time/               |          |
|    total_timesteps  | 403000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0952   |
|    n_updates        | 75749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.35     |
|    exploration_rate | 0.529    |
| time/               |          |
|    episodes         | 3936     |
|    fps              | 39       |
|    time_elapsed     | 10292    |
|    total_timesteps  | 403089   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 75772    |
----------------------------------
Eval num_timesteps=403500, episode_reward=1.80 +/- 2.69
Episode length: 103.42 +/- 40.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.528    |
| time/               |          |
|    total_timesteps  | 403500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 75874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.4      |
|    exploration_rate | 0.528    |
| time/               |          |
|    episodes         | 3940     |
|    fps              | 39       |
|    time_elapsed     | 10305    |
|    total_timesteps  | 403516   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.182    |
|    n_updates        | 75878    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 5.33     |
|    exploration_rate | 0.527    |
| time/               |          |
|    episodes         | 3944     |
|    fps              | 39       |
|    time_elapsed     | 10306    |
|    total_timesteps  | 403775   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.193    |
|    n_updates        | 75943    |
----------------------------------
Eval num_timesteps=404000, episode_reward=5.70 +/- 9.68
Episode length: 129.48 +/- 81.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.7      |
| rollout/            |          |
|    exploration_rate | 0.526    |
| time/               |          |
|    total_timesteps  | 404000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 75999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.526    |
| time/               |          |
|    episodes         | 3948     |
|    fps              | 39       |
|    time_elapsed     | 10322    |
|    total_timesteps  | 404285   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 76071    |
----------------------------------
Eval num_timesteps=404500, episode_reward=3.34 +/- 4.84
Episode length: 120.06 +/- 68.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.525    |
| time/               |          |
|    total_timesteps  | 404500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0989   |
|    n_updates        | 76124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.525    |
| time/               |          |
|    episodes         | 3952     |
|    fps              | 39       |
|    time_elapsed     | 10337    |
|    total_timesteps  | 404772   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 76192    |
----------------------------------
Eval num_timesteps=405000, episode_reward=5.88 +/- 9.59
Episode length: 125.64 +/- 72.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.88     |
| rollout/            |          |
|    exploration_rate | 0.524    |
| time/               |          |
|    total_timesteps  | 405000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0986   |
|    n_updates        | 76249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.4      |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 3956     |
|    fps              | 39       |
|    time_elapsed     | 10352    |
|    total_timesteps  | 405248   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 76311    |
----------------------------------
Eval num_timesteps=405500, episode_reward=4.40 +/- 6.64
Episode length: 127.72 +/- 100.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.523    |
| time/               |          |
|    total_timesteps  | 405500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 76374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 3960     |
|    fps              | 39       |
|    time_elapsed     | 10368    |
|    total_timesteps  | 405634   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0944   |
|    n_updates        | 76408    |
----------------------------------
Eval num_timesteps=406000, episode_reward=2.92 +/- 5.86
Episode length: 119.24 +/- 80.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.522    |
| time/               |          |
|    total_timesteps  | 406000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0584   |
|    n_updates        | 76499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.55     |
|    exploration_rate | 0.521    |
| time/               |          |
|    episodes         | 3964     |
|    fps              | 39       |
|    time_elapsed     | 10382    |
|    total_timesteps  | 406115   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 76528    |
----------------------------------
Eval num_timesteps=406500, episode_reward=3.32 +/- 4.06
Episode length: 125.76 +/- 88.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.521    |
| time/               |          |
|    total_timesteps  | 406500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0373   |
|    n_updates        | 76624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 3968     |
|    fps              | 39       |
|    time_elapsed     | 10397    |
|    total_timesteps  | 406619   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 76654    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.519    |
| time/               |          |
|    episodes         | 3972     |
|    fps              | 39       |
|    time_elapsed     | 10399    |
|    total_timesteps  | 406979   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.142    |
|    n_updates        | 76744    |
----------------------------------
Eval num_timesteps=407000, episode_reward=3.82 +/- 5.95
Episode length: 109.64 +/- 55.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.82     |
| rollout/            |          |
|    exploration_rate | 0.519    |
| time/               |          |
|    total_timesteps  | 407000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0802   |
|    n_updates        | 76749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.519    |
| time/               |          |
|    episodes         | 3976     |
|    fps              | 39       |
|    time_elapsed     | 10413    |
|    total_timesteps  | 407291   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 76822    |
----------------------------------
Eval num_timesteps=407500, episode_reward=4.70 +/- 7.79
Episode length: 133.04 +/- 78.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.7      |
| rollout/            |          |
|    exploration_rate | 0.518    |
| time/               |          |
|    total_timesteps  | 407500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0482   |
|    n_updates        | 76874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.518    |
| time/               |          |
|    episodes         | 3980     |
|    fps              | 39       |
|    time_elapsed     | 10429    |
|    total_timesteps  | 407768   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0622   |
|    n_updates        | 76941    |
----------------------------------
Eval num_timesteps=408000, episode_reward=4.18 +/- 8.36
Episode length: 112.24 +/- 75.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.517    |
| time/               |          |
|    total_timesteps  | 408000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0828   |
|    n_updates        | 76999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.22     |
|    exploration_rate | 0.516    |
| time/               |          |
|    episodes         | 3984     |
|    fps              | 39       |
|    time_elapsed     | 10443    |
|    total_timesteps  | 408314   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0758   |
|    n_updates        | 77078    |
----------------------------------
Eval num_timesteps=408500, episode_reward=4.30 +/- 6.55
Episode length: 107.28 +/- 57.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.516    |
| time/               |          |
|    total_timesteps  | 408500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 77124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.515    |
| time/               |          |
|    episodes         | 3988     |
|    fps              | 39       |
|    time_elapsed     | 10459    |
|    total_timesteps  | 408802   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 77200    |
----------------------------------
Eval num_timesteps=409000, episode_reward=4.64 +/- 8.58
Episode length: 125.44 +/- 62.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.515    |
| time/               |          |
|    total_timesteps  | 409000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.062    |
|    n_updates        | 77249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.61     |
|    exploration_rate | 0.514    |
| time/               |          |
|    episodes         | 3992     |
|    fps              | 39       |
|    time_elapsed     | 10474    |
|    total_timesteps  | 409144   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 77285    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.37     |
|    exploration_rate | 0.514    |
| time/               |          |
|    episodes         | 3996     |
|    fps              | 39       |
|    time_elapsed     | 10475    |
|    total_timesteps  | 409381   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0681   |
|    n_updates        | 77345    |
----------------------------------
Eval num_timesteps=409500, episode_reward=6.54 +/- 11.42
Episode length: 139.90 +/- 107.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 6.54     |
| rollout/            |          |
|    exploration_rate | 0.513    |
| time/               |          |
|    total_timesteps  | 409500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 77374    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.56     |
|    exploration_rate | 0.512    |
| time/               |          |
|    episodes         | 4000     |
|    fps              | 39       |
|    time_elapsed     | 10492    |
|    total_timesteps  | 409949   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0655   |
|    n_updates        | 77487    |
----------------------------------
Eval num_timesteps=410000, episode_reward=3.48 +/- 5.43
Episode length: 101.80 +/- 66.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.48     |
| rollout/            |          |
|    exploration_rate | 0.512    |
| time/               |          |
|    total_timesteps  | 410000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.055    |
|    n_updates        | 77499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.511    |
| time/               |          |
|    episodes         | 4004     |
|    fps              | 39       |
|    time_elapsed     | 10504    |
|    total_timesteps  | 410438   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0651   |
|    n_updates        | 77609    |
----------------------------------
Eval num_timesteps=410500, episode_reward=3.70 +/- 6.21
Episode length: 115.84 +/- 64.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.511    |
| time/               |          |
|    total_timesteps  | 410500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.058    |
|    n_updates        | 77624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.26     |
|    exploration_rate | 0.51     |
| time/               |          |
|    episodes         | 4008     |
|    fps              | 39       |
|    time_elapsed     | 10519    |
|    total_timesteps  | 410822   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0557   |
|    n_updates        | 77705    |
----------------------------------
Eval num_timesteps=411000, episode_reward=3.62 +/- 6.01
Episode length: 132.32 +/- 81.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.51     |
| time/               |          |
|    total_timesteps  | 411000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.198    |
|    n_updates        | 77749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.52     |
|    exploration_rate | 0.509    |
| time/               |          |
|    episodes         | 4012     |
|    fps              | 39       |
|    time_elapsed     | 10536    |
|    total_timesteps  | 411402   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0353   |
|    n_updates        | 77850    |
----------------------------------
Eval num_timesteps=411500, episode_reward=5.84 +/- 8.20
Episode length: 145.90 +/- 75.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 5.84     |
| rollout/            |          |
|    exploration_rate | 0.509    |
| time/               |          |
|    total_timesteps  | 411500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.164    |
|    n_updates        | 77874    |
----------------------------------
Eval num_timesteps=412000, episode_reward=3.02 +/- 4.06
Episode length: 94.44 +/- 45.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.508    |
| time/               |          |
|    total_timesteps  | 412000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 77999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.508    |
| time/               |          |
|    episodes         | 4016     |
|    fps              | 39       |
|    time_elapsed     | 10564    |
|    total_timesteps  | 412016   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0407   |
|    n_updates        | 78003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.507    |
| time/               |          |
|    episodes         | 4020     |
|    fps              | 39       |
|    time_elapsed     | 10565    |
|    total_timesteps  | 412402   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0929   |
|    n_updates        | 78100    |
----------------------------------
Eval num_timesteps=412500, episode_reward=2.36 +/- 3.07
Episode length: 91.40 +/- 29.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.4     |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.506    |
| time/               |          |
|    total_timesteps  | 412500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 78124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 4024     |
|    fps              | 39       |
|    time_elapsed     | 10577    |
|    total_timesteps  | 412893   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0551   |
|    n_updates        | 78223    |
----------------------------------
Eval num_timesteps=413000, episode_reward=5.70 +/- 12.97
Episode length: 123.28 +/- 85.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 5.7      |
| rollout/            |          |
|    exploration_rate | 0.505    |
| time/               |          |
|    total_timesteps  | 413000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.068    |
|    n_updates        | 78249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.504    |
| time/               |          |
|    episodes         | 4028     |
|    fps              | 39       |
|    time_elapsed     | 10592    |
|    total_timesteps  | 413407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0686   |
|    n_updates        | 78351    |
----------------------------------
Eval num_timesteps=413500, episode_reward=2.20 +/- 2.67
Episode length: 82.26 +/- 29.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.3     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.504    |
| time/               |          |
|    total_timesteps  | 413500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0826   |
|    n_updates        | 78374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.503    |
| time/               |          |
|    episodes         | 4032     |
|    fps              | 39       |
|    time_elapsed     | 10603    |
|    total_timesteps  | 413888   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.357    |
|    n_updates        | 78471    |
----------------------------------
Eval num_timesteps=414000, episode_reward=3.48 +/- 6.51
Episode length: 126.56 +/- 86.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 3.48     |
| rollout/            |          |
|    exploration_rate | 0.503    |
| time/               |          |
|    total_timesteps  | 414000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0649   |
|    n_updates        | 78499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.502    |
| time/               |          |
|    episodes         | 4036     |
|    fps              | 39       |
|    time_elapsed     | 10618    |
|    total_timesteps  | 414305   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 78576    |
----------------------------------
Eval num_timesteps=414500, episode_reward=7.08 +/- 13.20
Episode length: 136.86 +/- 76.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 7.08     |
| rollout/            |          |
|    exploration_rate | 0.502    |
| time/               |          |
|    total_timesteps  | 414500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 78624    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.501    |
| time/               |          |
|    episodes         | 4040     |
|    fps              | 39       |
|    time_elapsed     | 10634    |
|    total_timesteps  | 414763   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0839   |
|    n_updates        | 78690    |
----------------------------------
Eval num_timesteps=415000, episode_reward=1.88 +/- 2.98
Episode length: 89.52 +/- 35.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 0.5      |
| time/               |          |
|    total_timesteps  | 415000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 78749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.5      |
| time/               |          |
|    episodes         | 4044     |
|    fps              | 38       |
|    time_elapsed     | 10646    |
|    total_timesteps  | 415182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 78795    |
----------------------------------
Eval num_timesteps=415500, episode_reward=2.56 +/- 3.56
Episode length: 110.32 +/- 81.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.499    |
| time/               |          |
|    total_timesteps  | 415500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0557   |
|    n_updates        | 78874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.499    |
| time/               |          |
|    episodes         | 4048     |
|    fps              | 38       |
|    time_elapsed     | 10659    |
|    total_timesteps  | 415672   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0534   |
|    n_updates        | 78917    |
----------------------------------
Eval num_timesteps=416000, episode_reward=4.32 +/- 8.56
Episode length: 102.28 +/- 64.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.498    |
| time/               |          |
|    total_timesteps  | 416000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0558   |
|    n_updates        | 78999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.54     |
|    exploration_rate | 0.498    |
| time/               |          |
|    episodes         | 4052     |
|    fps              | 38       |
|    time_elapsed     | 10671    |
|    total_timesteps  | 416046   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 79011    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.53     |
|    exploration_rate | 0.497    |
| time/               |          |
|    episodes         | 4056     |
|    fps              | 39       |
|    time_elapsed     | 10673    |
|    total_timesteps  | 416449   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0941   |
|    n_updates        | 79112    |
----------------------------------
Eval num_timesteps=416500, episode_reward=2.38 +/- 3.92
Episode length: 102.56 +/- 72.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.497    |
| time/               |          |
|    total_timesteps  | 416500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 79124    |
----------------------------------
Eval num_timesteps=417000, episode_reward=3.24 +/- 3.80
Episode length: 110.64 +/- 63.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.496    |
| time/               |          |
|    total_timesteps  | 417000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0463   |
|    n_updates        | 79249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.495    |
| time/               |          |
|    episodes         | 4060     |
|    fps              | 38       |
|    time_elapsed     | 10698    |
|    total_timesteps  | 417102   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 79275    |
----------------------------------
Eval num_timesteps=417500, episode_reward=4.00 +/- 5.25
Episode length: 123.04 +/- 66.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.494    |
| time/               |          |
|    total_timesteps  | 417500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0629   |
|    n_updates        | 79374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.494    |
| time/               |          |
|    episodes         | 4064     |
|    fps              | 38       |
|    time_elapsed     | 10713    |
|    total_timesteps  | 417501   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 79375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.493    |
| time/               |          |
|    episodes         | 4068     |
|    fps              | 39       |
|    time_elapsed     | 10714    |
|    total_timesteps  | 417966   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0571   |
|    n_updates        | 79491    |
----------------------------------
Eval num_timesteps=418000, episode_reward=4.76 +/- 7.20
Episode length: 130.44 +/- 87.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.493    |
| time/               |          |
|    total_timesteps  | 418000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 79499    |
----------------------------------
Eval num_timesteps=418500, episode_reward=2.60 +/- 5.08
Episode length: 103.02 +/- 57.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.492    |
| time/               |          |
|    total_timesteps  | 418500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0479   |
|    n_updates        | 79624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.492    |
| time/               |          |
|    episodes         | 4072     |
|    fps              | 38       |
|    time_elapsed     | 10741    |
|    total_timesteps  | 418626   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.08     |
|    n_updates        | 79656    |
----------------------------------
Eval num_timesteps=419000, episode_reward=3.82 +/- 5.40
Episode length: 112.42 +/- 60.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.82     |
| rollout/            |          |
|    exploration_rate | 0.491    |
| time/               |          |
|    total_timesteps  | 419000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 79749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.491    |
| time/               |          |
|    episodes         | 4076     |
|    fps              | 38       |
|    time_elapsed     | 10755    |
|    total_timesteps  | 419061   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0599   |
|    n_updates        | 79765    |
----------------------------------
Eval num_timesteps=419500, episode_reward=2.70 +/- 5.24
Episode length: 89.00 +/- 40.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89       |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.49     |
| time/               |          |
|    total_timesteps  | 419500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0749   |
|    n_updates        | 79874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.07     |
|    exploration_rate | 0.489    |
| time/               |          |
|    episodes         | 4080     |
|    fps              | 38       |
|    time_elapsed     | 10766    |
|    total_timesteps  | 419619   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.154    |
|    n_updates        | 79904    |
----------------------------------
Eval num_timesteps=420000, episode_reward=4.62 +/- 6.35
Episode length: 131.08 +/- 77.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 4.62     |
| rollout/            |          |
|    exploration_rate | 0.489    |
| time/               |          |
|    total_timesteps  | 420000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0617   |
|    n_updates        | 79999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.488    |
| time/               |          |
|    episodes         | 4084     |
|    fps              | 38       |
|    time_elapsed     | 10781    |
|    total_timesteps  | 420016   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 80003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.487    |
| time/               |          |
|    episodes         | 4088     |
|    fps              | 38       |
|    time_elapsed     | 10783    |
|    total_timesteps  | 420486   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 80121    |
----------------------------------
Eval num_timesteps=420500, episode_reward=4.18 +/- 7.03
Episode length: 135.10 +/- 100.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.487    |
| time/               |          |
|    total_timesteps  | 420500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 80124    |
----------------------------------
Eval num_timesteps=421000, episode_reward=3.50 +/- 6.60
Episode length: 114.60 +/- 62.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.486    |
| time/               |          |
|    total_timesteps  | 421000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 80249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.486    |
| time/               |          |
|    episodes         | 4092     |
|    fps              | 38       |
|    time_elapsed     | 10812    |
|    total_timesteps  | 421082   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0815   |
|    n_updates        | 80270    |
----------------------------------
Eval num_timesteps=421500, episode_reward=2.40 +/- 3.10
Episode length: 97.46 +/- 57.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.5     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.485    |
| time/               |          |
|    total_timesteps  | 421500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 80374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.485    |
| time/               |          |
|    episodes         | 4096     |
|    fps              | 38       |
|    time_elapsed     | 10824    |
|    total_timesteps  | 421534   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0771   |
|    n_updates        | 80383    |
----------------------------------
Eval num_timesteps=422000, episode_reward=3.52 +/- 5.27
Episode length: 112.92 +/- 67.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.484    |
| time/               |          |
|    total_timesteps  | 422000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 80499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.51     |
|    exploration_rate | 0.484    |
| time/               |          |
|    episodes         | 4100     |
|    fps              | 38       |
|    time_elapsed     | 10838    |
|    total_timesteps  | 422064   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.147    |
|    n_updates        | 80515    |
----------------------------------
Eval num_timesteps=422500, episode_reward=3.34 +/- 6.14
Episode length: 103.16 +/- 61.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.483    |
| time/               |          |
|    total_timesteps  | 422500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0873   |
|    n_updates        | 80624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.482    |
| time/               |          |
|    episodes         | 4104     |
|    fps              | 38       |
|    time_elapsed     | 10850    |
|    total_timesteps  | 422507   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0699   |
|    n_updates        | 80626    |
----------------------------------
Eval num_timesteps=423000, episode_reward=1.78 +/- 3.23
Episode length: 89.78 +/- 32.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.8     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 0.481    |
| time/               |          |
|    total_timesteps  | 423000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.314    |
|    n_updates        | 80749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.481    |
| time/               |          |
|    episodes         | 4108     |
|    fps              | 38       |
|    time_elapsed     | 10862    |
|    total_timesteps  | 423024   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 80755    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.48     |
| time/               |          |
|    episodes         | 4112     |
|    fps              | 38       |
|    time_elapsed     | 10863    |
|    total_timesteps  | 423341   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.271    |
|    n_updates        | 80835    |
----------------------------------
Eval num_timesteps=423500, episode_reward=3.38 +/- 6.16
Episode length: 104.46 +/- 53.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.48     |
| time/               |          |
|    total_timesteps  | 423500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 80874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.479    |
| time/               |          |
|    episodes         | 4116     |
|    fps              | 38       |
|    time_elapsed     | 10876    |
|    total_timesteps  | 423830   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.285    |
|    n_updates        | 80957    |
----------------------------------
Eval num_timesteps=424000, episode_reward=3.06 +/- 6.85
Episode length: 102.02 +/- 55.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.479    |
| time/               |          |
|    total_timesteps  | 424000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.194    |
|    n_updates        | 80999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.18     |
|    exploration_rate | 0.478    |
| time/               |          |
|    episodes         | 4120     |
|    fps              | 38       |
|    time_elapsed     | 10889    |
|    total_timesteps  | 424243   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 81060    |
----------------------------------
Eval num_timesteps=424500, episode_reward=3.06 +/- 4.64
Episode length: 112.76 +/- 63.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.478    |
| time/               |          |
|    total_timesteps  | 424500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0795   |
|    n_updates        | 81124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.477    |
| time/               |          |
|    episodes         | 4124     |
|    fps              | 38       |
|    time_elapsed     | 10902    |
|    total_timesteps  | 424716   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.232    |
|    n_updates        | 81178    |
----------------------------------
Eval num_timesteps=425000, episode_reward=4.46 +/- 5.79
Episode length: 117.20 +/- 66.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.46     |
| rollout/            |          |
|    exploration_rate | 0.476    |
| time/               |          |
|    total_timesteps  | 425000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 81249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.476    |
| time/               |          |
|    episodes         | 4128     |
|    fps              | 38       |
|    time_elapsed     | 10916    |
|    total_timesteps  | 425087   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.157    |
|    n_updates        | 81271    |
----------------------------------
Eval num_timesteps=425500, episode_reward=4.00 +/- 5.63
Episode length: 118.50 +/- 57.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.475    |
| time/               |          |
|    total_timesteps  | 425500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 81374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.475    |
| time/               |          |
|    episodes         | 4132     |
|    fps              | 38       |
|    time_elapsed     | 10931    |
|    total_timesteps  | 425535   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0516   |
|    n_updates        | 81383    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.474    |
| time/               |          |
|    episodes         | 4136     |
|    fps              | 38       |
|    time_elapsed     | 10932    |
|    total_timesteps  | 425984   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0559   |
|    n_updates        | 81495    |
----------------------------------
Eval num_timesteps=426000, episode_reward=2.40 +/- 3.48
Episode length: 107.80 +/- 63.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.474    |
| time/               |          |
|    total_timesteps  | 426000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0917   |
|    n_updates        | 81499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.473    |
| time/               |          |
|    episodes         | 4140     |
|    fps              | 38       |
|    time_elapsed     | 10945    |
|    total_timesteps  | 426330   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0435   |
|    n_updates        | 81582    |
----------------------------------
Eval num_timesteps=426500, episode_reward=3.14 +/- 6.41
Episode length: 98.18 +/- 46.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.2     |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.473    |
| time/               |          |
|    total_timesteps  | 426500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 81624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.472    |
| time/               |          |
|    episodes         | 4144     |
|    fps              | 38       |
|    time_elapsed     | 10957    |
|    total_timesteps  | 426881   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 81720    |
----------------------------------
Eval num_timesteps=427000, episode_reward=3.46 +/- 5.51
Episode length: 101.22 +/- 44.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.46     |
| rollout/            |          |
|    exploration_rate | 0.472    |
| time/               |          |
|    total_timesteps  | 427000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.214    |
|    n_updates        | 81749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.471    |
| time/               |          |
|    episodes         | 4148     |
|    fps              | 38       |
|    time_elapsed     | 10970    |
|    total_timesteps  | 427311   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 81827    |
----------------------------------
Eval num_timesteps=427500, episode_reward=2.08 +/- 2.82
Episode length: 100.58 +/- 44.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.08     |
| rollout/            |          |
|    exploration_rate | 0.47     |
| time/               |          |
|    total_timesteps  | 427500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0574   |
|    n_updates        | 81874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.47     |
| time/               |          |
|    episodes         | 4152     |
|    fps              | 38       |
|    time_elapsed     | 10982    |
|    total_timesteps  | 427684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0784   |
|    n_updates        | 81920    |
----------------------------------
Eval num_timesteps=428000, episode_reward=3.52 +/- 5.53
Episode length: 104.16 +/- 48.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.469    |
| time/               |          |
|    total_timesteps  | 428000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 81999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.469    |
| time/               |          |
|    episodes         | 4156     |
|    fps              | 38       |
|    time_elapsed     | 10994    |
|    total_timesteps  | 428002   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0536   |
|    n_updates        | 82000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.53     |
|    exploration_rate | 0.469    |
| time/               |          |
|    episodes         | 4160     |
|    fps              | 38       |
|    time_elapsed     | 10995    |
|    total_timesteps  | 428292   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 82072    |
----------------------------------
Eval num_timesteps=428500, episode_reward=4.34 +/- 6.92
Episode length: 136.64 +/- 95.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.468    |
| time/               |          |
|    total_timesteps  | 428500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.161    |
|    n_updates        | 82124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.59     |
|    exploration_rate | 0.467    |
| time/               |          |
|    episodes         | 4164     |
|    fps              | 38       |
|    time_elapsed     | 11011    |
|    total_timesteps  | 428739   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 82184    |
----------------------------------
Eval num_timesteps=429000, episode_reward=4.78 +/- 6.96
Episode length: 103.52 +/- 54.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.467    |
| time/               |          |
|    total_timesteps  | 429000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0949   |
|    n_updates        | 82249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.467    |
| time/               |          |
|    episodes         | 4168     |
|    fps              | 38       |
|    time_elapsed     | 11024    |
|    total_timesteps  | 429087   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.084    |
|    n_updates        | 82271    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.466    |
| time/               |          |
|    episodes         | 4172     |
|    fps              | 38       |
|    time_elapsed     | 11026    |
|    total_timesteps  | 429454   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0661   |
|    n_updates        | 82363    |
----------------------------------
Eval num_timesteps=429500, episode_reward=2.06 +/- 3.35
Episode length: 95.86 +/- 45.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.9     |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.466    |
| time/               |          |
|    total_timesteps  | 429500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0519   |
|    n_updates        | 82374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.465    |
| time/               |          |
|    episodes         | 4176     |
|    fps              | 38       |
|    time_elapsed     | 11038    |
|    total_timesteps  | 429910   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 82477    |
----------------------------------
Eval num_timesteps=430000, episode_reward=5.44 +/- 7.00
Episode length: 156.34 +/- 124.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.464    |
| time/               |          |
|    total_timesteps  | 430000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0581   |
|    n_updates        | 82499    |
----------------------------------
Eval num_timesteps=430500, episode_reward=2.48 +/- 2.93
Episode length: 112.94 +/- 65.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.463    |
| time/               |          |
|    total_timesteps  | 430500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.171    |
|    n_updates        | 82624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.44     |
|    exploration_rate | 0.463    |
| time/               |          |
|    episodes         | 4180     |
|    fps              | 38       |
|    time_elapsed     | 11069    |
|    total_timesteps  | 430528   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.218    |
|    n_updates        | 82631    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 0.462    |
| time/               |          |
|    episodes         | 4184     |
|    fps              | 38       |
|    time_elapsed     | 11070    |
|    total_timesteps  | 430939   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0501   |
|    n_updates        | 82734    |
----------------------------------
Eval num_timesteps=431000, episode_reward=2.94 +/- 3.44
Episode length: 111.00 +/- 49.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.462    |
| time/               |          |
|    total_timesteps  | 431000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0721   |
|    n_updates        | 82749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.461    |
| time/               |          |
|    episodes         | 4188     |
|    fps              | 38       |
|    time_elapsed     | 11083    |
|    total_timesteps  | 431356   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 82838    |
----------------------------------
Eval num_timesteps=431500, episode_reward=4.78 +/- 10.89
Episode length: 128.82 +/- 65.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.461    |
| time/               |          |
|    total_timesteps  | 431500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0561   |
|    n_updates        | 82874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.46     |
| time/               |          |
|    episodes         | 4192     |
|    fps              | 38       |
|    time_elapsed     | 11103    |
|    total_timesteps  | 431870   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.229    |
|    n_updates        | 82967    |
----------------------------------
Eval num_timesteps=432000, episode_reward=4.46 +/- 7.29
Episode length: 128.28 +/- 66.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.46     |
| rollout/            |          |
|    exploration_rate | 0.459    |
| time/               |          |
|    total_timesteps  | 432000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0362   |
|    n_updates        | 82999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.459    |
| time/               |          |
|    episodes         | 4196     |
|    fps              | 38       |
|    time_elapsed     | 11122    |
|    total_timesteps  | 432276   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0618   |
|    n_updates        | 83068    |
----------------------------------
Eval num_timesteps=432500, episode_reward=3.46 +/- 5.81
Episode length: 117.02 +/- 67.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.46     |
| rollout/            |          |
|    exploration_rate | 0.458    |
| time/               |          |
|    total_timesteps  | 432500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0547   |
|    n_updates        | 83124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.23     |
|    exploration_rate | 0.458    |
| time/               |          |
|    episodes         | 4200     |
|    fps              | 38       |
|    time_elapsed     | 11136    |
|    total_timesteps  | 432773   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.207    |
|    n_updates        | 83193    |
----------------------------------
Eval num_timesteps=433000, episode_reward=3.08 +/- 6.12
Episode length: 109.74 +/- 54.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.457    |
| time/               |          |
|    total_timesteps  | 433000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0614   |
|    n_updates        | 83249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 0.456    |
| time/               |          |
|    episodes         | 4204     |
|    fps              | 38       |
|    time_elapsed     | 11150    |
|    total_timesteps  | 433224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 83305    |
----------------------------------
Eval num_timesteps=433500, episode_reward=2.22 +/- 4.01
Episode length: 113.20 +/- 88.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.456    |
| time/               |          |
|    total_timesteps  | 433500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0823   |
|    n_updates        | 83374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.455    |
| time/               |          |
|    episodes         | 4208     |
|    fps              | 38       |
|    time_elapsed     | 11164    |
|    total_timesteps  | 433697   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 83424    |
----------------------------------
Eval num_timesteps=434000, episode_reward=4.14 +/- 8.09
Episode length: 133.20 +/- 113.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.14     |
| rollout/            |          |
|    exploration_rate | 0.455    |
| time/               |          |
|    total_timesteps  | 434000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0775   |
|    n_updates        | 83499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 0.454    |
| time/               |          |
|    episodes         | 4212     |
|    fps              | 38       |
|    time_elapsed     | 11179    |
|    total_timesteps  | 434057   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.099    |
|    n_updates        | 83514    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.453    |
| time/               |          |
|    episodes         | 4216     |
|    fps              | 38       |
|    time_elapsed     | 11181    |
|    total_timesteps  | 434483   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.183    |
|    n_updates        | 83620    |
----------------------------------
Eval num_timesteps=434500, episode_reward=2.44 +/- 3.75
Episode length: 102.06 +/- 41.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.44     |
| rollout/            |          |
|    exploration_rate | 0.453    |
| time/               |          |
|    total_timesteps  | 434500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0873   |
|    n_updates        | 83624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 4220     |
|    fps              | 38       |
|    time_elapsed     | 11193    |
|    total_timesteps  | 434943   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0829   |
|    n_updates        | 83735    |
----------------------------------
Eval num_timesteps=435000, episode_reward=3.02 +/- 6.74
Episode length: 117.20 +/- 66.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.452    |
| time/               |          |
|    total_timesteps  | 435000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0885   |
|    n_updates        | 83749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.31     |
|    exploration_rate | 0.451    |
| time/               |          |
|    episodes         | 4224     |
|    fps              | 38       |
|    time_elapsed     | 11207    |
|    total_timesteps  | 435480   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 83869    |
----------------------------------
Eval num_timesteps=435500, episode_reward=3.26 +/- 6.23
Episode length: 133.64 +/- 105.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.451    |
| time/               |          |
|    total_timesteps  | 435500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0624   |
|    n_updates        | 83874    |
----------------------------------
Eval num_timesteps=436000, episode_reward=2.72 +/- 4.31
Episode length: 110.16 +/- 44.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.45     |
| time/               |          |
|    total_timesteps  | 436000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0785   |
|    n_updates        | 83999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.449    |
| time/               |          |
|    episodes         | 4228     |
|    fps              | 38       |
|    time_elapsed     | 11236    |
|    total_timesteps  | 436137   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 84034    |
----------------------------------
Eval num_timesteps=436500, episode_reward=2.86 +/- 4.25
Episode length: 117.52 +/- 84.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.448    |
| time/               |          |
|    total_timesteps  | 436500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 84124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.448    |
| time/               |          |
|    episodes         | 4232     |
|    fps              | 38       |
|    time_elapsed     | 11249    |
|    total_timesteps  | 436565   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.067    |
|    n_updates        | 84141    |
----------------------------------
Eval num_timesteps=437000, episode_reward=3.86 +/- 8.39
Episode length: 109.44 +/- 72.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.447    |
| time/               |          |
|    total_timesteps  | 437000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 84249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.447    |
| time/               |          |
|    episodes         | 4236     |
|    fps              | 38       |
|    time_elapsed     | 11263    |
|    total_timesteps  | 437108   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 84276    |
----------------------------------
Eval num_timesteps=437500, episode_reward=2.56 +/- 3.37
Episode length: 118.98 +/- 84.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.446    |
| time/               |          |
|    total_timesteps  | 437500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.17     |
|    n_updates        | 84374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.446    |
| time/               |          |
|    episodes         | 4240     |
|    fps              | 38       |
|    time_elapsed     | 11278    |
|    total_timesteps  | 437653   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0803   |
|    n_updates        | 84413    |
----------------------------------
Eval num_timesteps=438000, episode_reward=4.36 +/- 6.13
Episode length: 119.04 +/- 63.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.445    |
| time/               |          |
|    total_timesteps  | 438000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 84499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.445    |
| time/               |          |
|    episodes         | 4244     |
|    fps              | 38       |
|    time_elapsed     | 11297    |
|    total_timesteps  | 438046   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 84511    |
----------------------------------
Eval num_timesteps=438500, episode_reward=6.16 +/- 9.97
Episode length: 129.32 +/- 71.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.444    |
| time/               |          |
|    total_timesteps  | 438500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.172    |
|    n_updates        | 84624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.44     |
|    exploration_rate | 0.443    |
| time/               |          |
|    episodes         | 4248     |
|    fps              | 38       |
|    time_elapsed     | 11313    |
|    total_timesteps  | 438578   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.163    |
|    n_updates        | 84644    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.442    |
| time/               |          |
|    episodes         | 4252     |
|    fps              | 38       |
|    time_elapsed     | 11315    |
|    total_timesteps  | 438978   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0483   |
|    n_updates        | 84744    |
----------------------------------
Eval num_timesteps=439000, episode_reward=3.50 +/- 4.32
Episode length: 158.52 +/- 132.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.442    |
| time/               |          |
|    total_timesteps  | 439000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 84749    |
----------------------------------
Eval num_timesteps=439500, episode_reward=4.96 +/- 10.17
Episode length: 131.24 +/- 80.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 4.96     |
| rollout/            |          |
|    exploration_rate | 0.441    |
| time/               |          |
|    total_timesteps  | 439500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.182    |
|    n_updates        | 84874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.96     |
|    exploration_rate | 0.44     |
| time/               |          |
|    episodes         | 4256     |
|    fps              | 38       |
|    time_elapsed     | 11351    |
|    total_timesteps  | 439764   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.274    |
|    n_updates        | 84940    |
----------------------------------
Eval num_timesteps=440000, episode_reward=2.42 +/- 3.12
Episode length: 101.88 +/- 40.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.44     |
| time/               |          |
|    total_timesteps  | 440000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 84999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 5.31     |
|    exploration_rate | 0.439    |
| time/               |          |
|    episodes         | 4260     |
|    fps              | 38       |
|    time_elapsed     | 11363    |
|    total_timesteps  | 440394   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.291    |
|    n_updates        | 85098    |
----------------------------------
Eval num_timesteps=440500, episode_reward=2.84 +/- 5.08
Episode length: 101.02 +/- 49.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.439    |
| time/               |          |
|    total_timesteps  | 440500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 85124    |
----------------------------------
Eval num_timesteps=441000, episode_reward=4.42 +/- 6.99
Episode length: 127.02 +/- 81.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.437    |
| time/               |          |
|    total_timesteps  | 441000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.067    |
|    n_updates        | 85249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.44     |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 4264     |
|    fps              | 38       |
|    time_elapsed     | 11391    |
|    total_timesteps  | 441053   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0799   |
|    n_updates        | 85263    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.34     |
|    exploration_rate | 0.436    |
| time/               |          |
|    episodes         | 4268     |
|    fps              | 38       |
|    time_elapsed     | 11392    |
|    total_timesteps  | 441360   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.261    |
|    n_updates        | 85339    |
----------------------------------
Eval num_timesteps=441500, episode_reward=4.44 +/- 8.40
Episode length: 122.08 +/- 65.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.44     |
| rollout/            |          |
|    exploration_rate | 0.436    |
| time/               |          |
|    total_timesteps  | 441500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0637   |
|    n_updates        | 85374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.32     |
|    exploration_rate | 0.435    |
| time/               |          |
|    episodes         | 4272     |
|    fps              | 38       |
|    time_elapsed     | 11407    |
|    total_timesteps  | 441755   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0787   |
|    n_updates        | 85438    |
----------------------------------
Eval num_timesteps=442000, episode_reward=3.90 +/- 7.61
Episode length: 107.26 +/- 57.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.9      |
| rollout/            |          |
|    exploration_rate | 0.435    |
| time/               |          |
|    total_timesteps  | 442000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.233    |
|    n_updates        | 85499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 5.12     |
|    exploration_rate | 0.435    |
| time/               |          |
|    episodes         | 4276     |
|    fps              | 38       |
|    time_elapsed     | 11420    |
|    total_timesteps  | 442013   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 85503    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.02     |
|    exploration_rate | 0.434    |
| time/               |          |
|    episodes         | 4280     |
|    fps              | 38       |
|    time_elapsed     | 11421    |
|    total_timesteps  | 442496   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.259    |
|    n_updates        | 85623    |
----------------------------------
Eval num_timesteps=442500, episode_reward=1.86 +/- 3.42
Episode length: 84.24 +/- 28.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.2     |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 0.434    |
| time/               |          |
|    total_timesteps  | 442500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 85624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.433    |
| time/               |          |
|    episodes         | 4284     |
|    fps              | 38       |
|    time_elapsed     | 11432    |
|    total_timesteps  | 442833   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.154    |
|    n_updates        | 85708    |
----------------------------------
Eval num_timesteps=443000, episode_reward=2.06 +/- 2.78
Episode length: 141.90 +/- 129.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.432    |
| time/               |          |
|    total_timesteps  | 443000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0662   |
|    n_updates        | 85749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.82     |
|    exploration_rate | 0.432    |
| time/               |          |
|    episodes         | 4288     |
|    fps              | 38       |
|    time_elapsed     | 11448    |
|    total_timesteps  | 443279   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.191    |
|    n_updates        | 85819    |
----------------------------------
Eval num_timesteps=443500, episode_reward=2.56 +/- 4.33
Episode length: 91.28 +/- 41.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.3     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.431    |
| time/               |          |
|    total_timesteps  | 443500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0855   |
|    n_updates        | 85874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.63     |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 4292     |
|    fps              | 38       |
|    time_elapsed     | 11460    |
|    total_timesteps  | 443706   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 85926    |
----------------------------------
Eval num_timesteps=444000, episode_reward=4.66 +/- 6.96
Episode length: 126.94 +/- 103.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.66     |
| rollout/            |          |
|    exploration_rate | 0.43     |
| time/               |          |
|    total_timesteps  | 444000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.203    |
|    n_updates        | 85999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.73     |
|    exploration_rate | 0.43     |
| time/               |          |
|    episodes         | 4296     |
|    fps              | 38       |
|    time_elapsed     | 11475    |
|    total_timesteps  | 444086   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 86021    |
----------------------------------
Eval num_timesteps=444500, episode_reward=5.10 +/- 6.53
Episode length: 138.90 +/- 113.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 5.1      |
| rollout/            |          |
|    exploration_rate | 0.429    |
| time/               |          |
|    total_timesteps  | 444500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0367   |
|    n_updates        | 86124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.428    |
| time/               |          |
|    episodes         | 4300     |
|    fps              | 38       |
|    time_elapsed     | 11491    |
|    total_timesteps  | 444565   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.166    |
|    n_updates        | 86141    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.427    |
| time/               |          |
|    episodes         | 4304     |
|    fps              | 38       |
|    time_elapsed     | 11492    |
|    total_timesteps  | 444988   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0362   |
|    n_updates        | 86246    |
----------------------------------
Eval num_timesteps=445000, episode_reward=2.54 +/- 4.04
Episode length: 115.14 +/- 84.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.427    |
| time/               |          |
|    total_timesteps  | 445000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 86249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.87     |
|    exploration_rate | 0.426    |
| time/               |          |
|    episodes         | 4308     |
|    fps              | 38       |
|    time_elapsed     | 11505    |
|    total_timesteps  | 445426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0631   |
|    n_updates        | 86356    |
----------------------------------
Eval num_timesteps=445500, episode_reward=2.90 +/- 8.51
Episode length: 100.44 +/- 67.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.426    |
| time/               |          |
|    total_timesteps  | 445500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0483   |
|    n_updates        | 86374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.79     |
|    exploration_rate | 0.426    |
| time/               |          |
|    episodes         | 4312     |
|    fps              | 38       |
|    time_elapsed     | 11517    |
|    total_timesteps  | 445650   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 86412    |
----------------------------------
Eval num_timesteps=446000, episode_reward=2.10 +/- 2.68
Episode length: 93.16 +/- 35.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.2     |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 0.425    |
| time/               |          |
|    total_timesteps  | 446000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.04     |
|    n_updates        | 86499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.81     |
|    exploration_rate | 0.425    |
| time/               |          |
|    episodes         | 4316     |
|    fps              | 38       |
|    time_elapsed     | 11529    |
|    total_timesteps  | 446030   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.207    |
|    n_updates        | 86507    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.72     |
|    exploration_rate | 0.424    |
| time/               |          |
|    episodes         | 4320     |
|    fps              | 38       |
|    time_elapsed     | 11530    |
|    total_timesteps  | 446378   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.223    |
|    n_updates        | 86594    |
----------------------------------
Eval num_timesteps=446500, episode_reward=2.60 +/- 3.09
Episode length: 91.94 +/- 32.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.9     |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.424    |
| time/               |          |
|    total_timesteps  | 446500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0796   |
|    n_updates        | 86624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.68     |
|    exploration_rate | 0.423    |
| time/               |          |
|    episodes         | 4324     |
|    fps              | 38       |
|    time_elapsed     | 11541    |
|    total_timesteps  | 446724   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 86680    |
----------------------------------
Eval num_timesteps=447000, episode_reward=4.24 +/- 7.16
Episode length: 108.06 +/- 53.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.422    |
| time/               |          |
|    total_timesteps  | 447000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.249    |
|    n_updates        | 86749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.422    |
| time/               |          |
|    episodes         | 4328     |
|    fps              | 38       |
|    time_elapsed     | 11554    |
|    total_timesteps  | 447110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 86777    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.421    |
| time/               |          |
|    episodes         | 4332     |
|    fps              | 38       |
|    time_elapsed     | 11555    |
|    total_timesteps  | 447454   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.217    |
|    n_updates        | 86863    |
----------------------------------
Eval num_timesteps=447500, episode_reward=3.14 +/- 3.93
Episode length: 111.98 +/- 44.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.421    |
| time/               |          |
|    total_timesteps  | 447500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 86874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.42     |
| time/               |          |
|    episodes         | 4336     |
|    fps              | 38       |
|    time_elapsed     | 11569    |
|    total_timesteps  | 447977   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.186    |
|    n_updates        | 86994    |
----------------------------------
Eval num_timesteps=448000, episode_reward=2.44 +/- 3.67
Episode length: 103.16 +/- 45.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.44     |
| rollout/            |          |
|    exploration_rate | 0.42     |
| time/               |          |
|    total_timesteps  | 448000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 86999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.419    |
| time/               |          |
|    episodes         | 4340     |
|    fps              | 38       |
|    time_elapsed     | 11581    |
|    total_timesteps  | 448434   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.175    |
|    n_updates        | 87108    |
----------------------------------
Eval num_timesteps=448500, episode_reward=5.06 +/- 8.92
Episode length: 122.16 +/- 56.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.419    |
| time/               |          |
|    total_timesteps  | 448500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0885   |
|    n_updates        | 87124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.418    |
| time/               |          |
|    episodes         | 4344     |
|    fps              | 38       |
|    time_elapsed     | 11596    |
|    total_timesteps  | 448912   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0384   |
|    n_updates        | 87227    |
----------------------------------
Eval num_timesteps=449000, episode_reward=2.26 +/- 3.08
Episode length: 95.80 +/- 42.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.8     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.417    |
| time/               |          |
|    total_timesteps  | 449000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0983   |
|    n_updates        | 87249    |
----------------------------------
Eval num_timesteps=449500, episode_reward=2.56 +/- 3.63
Episode length: 103.82 +/- 37.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.416    |
| time/               |          |
|    total_timesteps  | 449500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.238    |
|    n_updates        | 87374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.416    |
| time/               |          |
|    episodes         | 4348     |
|    fps              | 38       |
|    time_elapsed     | 11619    |
|    total_timesteps  | 449540   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 87384    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.415    |
| time/               |          |
|    episodes         | 4352     |
|    fps              | 38       |
|    time_elapsed     | 11621    |
|    total_timesteps  | 449982   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0828   |
|    n_updates        | 87495    |
----------------------------------
Eval num_timesteps=450000, episode_reward=2.48 +/- 3.24
Episode length: 98.50 +/- 26.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.5     |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.415    |
| time/               |          |
|    total_timesteps  | 450000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 87499    |
----------------------------------
Eval num_timesteps=450500, episode_reward=3.22 +/- 6.72
Episode length: 94.52 +/- 45.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.5     |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.414    |
| time/               |          |
|    total_timesteps  | 450500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0689   |
|    n_updates        | 87624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.414    |
| time/               |          |
|    episodes         | 4356     |
|    fps              | 38       |
|    time_elapsed     | 11644    |
|    total_timesteps  | 450523   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 87630    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.413    |
| time/               |          |
|    episodes         | 4360     |
|    fps              | 38       |
|    time_elapsed     | 11645    |
|    total_timesteps  | 450878   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.34     |
|    n_updates        | 87719    |
----------------------------------
Eval num_timesteps=451000, episode_reward=5.28 +/- 8.14
Episode length: 114.98 +/- 63.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 5.28     |
| rollout/            |          |
|    exploration_rate | 0.412    |
| time/               |          |
|    total_timesteps  | 451000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.191    |
|    n_updates        | 87749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 3.31     |
|    exploration_rate | 0.412    |
| time/               |          |
|    episodes         | 4364     |
|    fps              | 38       |
|    time_elapsed     | 11659    |
|    total_timesteps  | 451248   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.083    |
|    n_updates        | 87811    |
----------------------------------
Eval num_timesteps=451500, episode_reward=2.98 +/- 4.95
Episode length: 101.64 +/- 49.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.411    |
| time/               |          |
|    total_timesteps  | 451500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.197    |
|    n_updates        | 87874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 4368     |
|    fps              | 38       |
|    time_elapsed     | 11672    |
|    total_timesteps  | 451872   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0562   |
|    n_updates        | 87967    |
----------------------------------
Eval num_timesteps=452000, episode_reward=3.34 +/- 5.49
Episode length: 112.88 +/- 49.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.41     |
| time/               |          |
|    total_timesteps  | 452000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0295   |
|    n_updates        | 87999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.78     |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 4372     |
|    fps              | 38       |
|    time_elapsed     | 11685    |
|    total_timesteps  | 452141   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 88035    |
----------------------------------
Eval num_timesteps=452500, episode_reward=2.56 +/- 4.81
Episode length: 95.76 +/- 48.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.8     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.409    |
| time/               |          |
|    total_timesteps  | 452500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 88124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.409    |
| time/               |          |
|    episodes         | 4376     |
|    fps              | 38       |
|    time_elapsed     | 11696    |
|    total_timesteps  | 452538   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 88134    |
----------------------------------
Eval num_timesteps=453000, episode_reward=2.94 +/- 3.97
Episode length: 113.86 +/- 84.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.407    |
| time/               |          |
|    total_timesteps  | 453000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 88249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.407    |
| time/               |          |
|    episodes         | 4380     |
|    fps              | 38       |
|    time_elapsed     | 11710    |
|    total_timesteps  | 453177   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 88294    |
----------------------------------
Eval num_timesteps=453500, episode_reward=1.94 +/- 2.14
Episode length: 93.84 +/- 31.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 0.406    |
| time/               |          |
|    total_timesteps  | 453500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.212    |
|    n_updates        | 88374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.406    |
| time/               |          |
|    episodes         | 4384     |
|    fps              | 38       |
|    time_elapsed     | 11722    |
|    total_timesteps  | 453518   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.142    |
|    n_updates        | 88379    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 4388     |
|    fps              | 38       |
|    time_elapsed     | 11723    |
|    total_timesteps  | 453904   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.197    |
|    n_updates        | 88475    |
----------------------------------
Eval num_timesteps=454000, episode_reward=3.10 +/- 7.59
Episode length: 105.40 +/- 48.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.405    |
| time/               |          |
|    total_timesteps  | 454000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 88499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.404    |
| time/               |          |
|    episodes         | 4392     |
|    fps              | 38       |
|    time_elapsed     | 11736    |
|    total_timesteps  | 454431   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 88607    |
----------------------------------
Eval num_timesteps=454500, episode_reward=1.46 +/- 2.41
Episode length: 88.44 +/- 30.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.4     |
|    mean_reward      | 1.46     |
| rollout/            |          |
|    exploration_rate | 0.404    |
| time/               |          |
|    total_timesteps  | 454500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.24     |
|    n_updates        | 88624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 4.44     |
|    exploration_rate | 0.403    |
| time/               |          |
|    episodes         | 4396     |
|    fps              | 38       |
|    time_elapsed     | 11747    |
|    total_timesteps  | 454670   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.216    |
|    n_updates        | 88667    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.402    |
| time/               |          |
|    episodes         | 4400     |
|    fps              | 38       |
|    time_elapsed     | 11748    |
|    total_timesteps  | 454955   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 88738    |
----------------------------------
Eval num_timesteps=455000, episode_reward=3.02 +/- 5.30
Episode length: 105.00 +/- 71.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.402    |
| time/               |          |
|    total_timesteps  | 455000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 88749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 4.51     |
|    exploration_rate | 0.401    |
| time/               |          |
|    episodes         | 4404     |
|    fps              | 38       |
|    time_elapsed     | 11761    |
|    total_timesteps  | 455429   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0662   |
|    n_updates        | 88857    |
----------------------------------
Eval num_timesteps=455500, episode_reward=2.48 +/- 5.19
Episode length: 109.44 +/- 58.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.401    |
| time/               |          |
|    total_timesteps  | 455500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0434   |
|    n_updates        | 88874    |
----------------------------------
Eval num_timesteps=456000, episode_reward=2.18 +/- 3.27
Episode length: 96.20 +/- 36.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.4      |
| time/               |          |
|    total_timesteps  | 456000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 88999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 4.4      |
|    exploration_rate | 0.4      |
| time/               |          |
|    episodes         | 4408     |
|    fps              | 38       |
|    time_elapsed     | 11785    |
|    total_timesteps  | 456050   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 89012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 4.34     |
|    exploration_rate | 0.399    |
| time/               |          |
|    episodes         | 4412     |
|    fps              | 38       |
|    time_elapsed     | 11786    |
|    total_timesteps  | 456387   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 89096    |
----------------------------------
Eval num_timesteps=456500, episode_reward=3.04 +/- 5.23
Episode length: 92.00 +/- 32.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | 3.04     |
| rollout/            |          |
|    exploration_rate | 0.399    |
| time/               |          |
|    total_timesteps  | 456500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 89124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 4416     |
|    fps              | 38       |
|    time_elapsed     | 11798    |
|    total_timesteps  | 456807   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0859   |
|    n_updates        | 89201    |
----------------------------------
Eval num_timesteps=457000, episode_reward=2.72 +/- 4.30
Episode length: 97.34 +/- 42.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.397    |
| time/               |          |
|    total_timesteps  | 457000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0525   |
|    n_updates        | 89249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.79     |
|    exploration_rate | 0.396    |
| time/               |          |
|    episodes         | 4420     |
|    fps              | 38       |
|    time_elapsed     | 11811    |
|    total_timesteps  | 457468   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 89366    |
----------------------------------
Eval num_timesteps=457500, episode_reward=4.68 +/- 6.46
Episode length: 116.92 +/- 49.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.396    |
| time/               |          |
|    total_timesteps  | 457500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 89374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.86     |
|    exploration_rate | 0.395    |
| time/               |          |
|    episodes         | 4424     |
|    fps              | 38       |
|    time_elapsed     | 11825    |
|    total_timesteps  | 457948   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.218    |
|    n_updates        | 89486    |
----------------------------------
Eval num_timesteps=458000, episode_reward=2.54 +/- 3.60
Episode length: 109.24 +/- 53.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.395    |
| time/               |          |
|    total_timesteps  | 458000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0652   |
|    n_updates        | 89499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.394    |
| time/               |          |
|    episodes         | 4428     |
|    fps              | 38       |
|    time_elapsed     | 11839    |
|    total_timesteps  | 458399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 89599    |
----------------------------------
Eval num_timesteps=458500, episode_reward=6.06 +/- 7.64
Episode length: 145.70 +/- 88.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 6.06     |
| rollout/            |          |
|    exploration_rate | 0.393    |
| time/               |          |
|    total_timesteps  | 458500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 89624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.91     |
|    exploration_rate | 0.393    |
| time/               |          |
|    episodes         | 4432     |
|    fps              | 38       |
|    time_elapsed     | 11855    |
|    total_timesteps  | 458766   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0768   |
|    n_updates        | 89691    |
----------------------------------
Eval num_timesteps=459000, episode_reward=3.80 +/- 5.55
Episode length: 127.18 +/- 85.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.392    |
| time/               |          |
|    total_timesteps  | 459000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0961   |
|    n_updates        | 89749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.79     |
|    exploration_rate | 0.392    |
| time/               |          |
|    episodes         | 4436     |
|    fps              | 38       |
|    time_elapsed     | 11874    |
|    total_timesteps  | 459110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 89777    |
----------------------------------
Eval num_timesteps=459500, episode_reward=2.86 +/- 6.43
Episode length: 95.62 +/- 62.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.6     |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.391    |
| time/               |          |
|    total_timesteps  | 459500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0582   |
|    n_updates        | 89874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.72     |
|    exploration_rate | 0.391    |
| time/               |          |
|    episodes         | 4440     |
|    fps              | 38       |
|    time_elapsed     | 11887    |
|    total_timesteps  | 459575   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0637   |
|    n_updates        | 89893    |
----------------------------------
Eval num_timesteps=460000, episode_reward=5.58 +/- 8.24
Episode length: 151.08 +/- 119.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 5.58     |
| rollout/            |          |
|    exploration_rate | 0.39     |
| time/               |          |
|    total_timesteps  | 460000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0528   |
|    n_updates        | 89999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.55     |
|    exploration_rate | 0.389    |
| time/               |          |
|    episodes         | 4444     |
|    fps              | 38       |
|    time_elapsed     | 11905    |
|    total_timesteps  | 460084   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 90020    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.389    |
| time/               |          |
|    episodes         | 4448     |
|    fps              | 38       |
|    time_elapsed     | 11906    |
|    total_timesteps  | 460434   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 90108    |
----------------------------------
Eval num_timesteps=460500, episode_reward=2.48 +/- 4.09
Episode length: 120.40 +/- 105.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.388    |
| time/               |          |
|    total_timesteps  | 460500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0951   |
|    n_updates        | 90124    |
----------------------------------
Eval num_timesteps=461000, episode_reward=1.48 +/- 2.12
Episode length: 94.26 +/- 31.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.3     |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 0.387    |
| time/               |          |
|    total_timesteps  | 461000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.201    |
|    n_updates        | 90249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.53     |
|    exploration_rate | 0.387    |
| time/               |          |
|    episodes         | 4452     |
|    fps              | 38       |
|    time_elapsed     | 11931    |
|    total_timesteps  | 461012   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 90252    |
----------------------------------
Eval num_timesteps=461500, episode_reward=4.90 +/- 6.86
Episode length: 120.88 +/- 81.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.9      |
| rollout/            |          |
|    exploration_rate | 0.386    |
| time/               |          |
|    total_timesteps  | 461500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0791   |
|    n_updates        | 90374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.386    |
| time/               |          |
|    episodes         | 4456     |
|    fps              | 38       |
|    time_elapsed     | 11946    |
|    total_timesteps  | 461585   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 90396    |
----------------------------------
Eval num_timesteps=462000, episode_reward=2.94 +/- 5.88
Episode length: 100.22 +/- 51.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.385    |
| time/               |          |
|    total_timesteps  | 462000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.171    |
|    n_updates        | 90499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 5.31     |
|    exploration_rate | 0.384    |
| time/               |          |
|    episodes         | 4460     |
|    fps              | 38       |
|    time_elapsed     | 11960    |
|    total_timesteps  | 462364   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0783   |
|    n_updates        | 90590    |
----------------------------------
Eval num_timesteps=462500, episode_reward=3.12 +/- 6.13
Episode length: 103.22 +/- 41.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.383    |
| time/               |          |
|    total_timesteps  | 462500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0689   |
|    n_updates        | 90624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.6      |
|    exploration_rate | 0.382    |
| time/               |          |
|    episodes         | 4464     |
|    fps              | 38       |
|    time_elapsed     | 11973    |
|    total_timesteps  | 462943   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 90735    |
----------------------------------
Eval num_timesteps=463000, episode_reward=2.36 +/- 4.11
Episode length: 109.10 +/- 79.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.382    |
| time/               |          |
|    total_timesteps  | 463000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.228    |
|    n_updates        | 90749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 5.22     |
|    exploration_rate | 0.381    |
| time/               |          |
|    episodes         | 4468     |
|    fps              | 38       |
|    time_elapsed     | 11986    |
|    total_timesteps  | 463223   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 90805    |
----------------------------------
Eval num_timesteps=463500, episode_reward=2.98 +/- 5.12
Episode length: 108.90 +/- 51.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.381    |
| time/               |          |
|    total_timesteps  | 463500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.17     |
|    n_updates        | 90874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 5.27     |
|    exploration_rate | 0.381    |
| time/               |          |
|    episodes         | 4472     |
|    fps              | 38       |
|    time_elapsed     | 11998    |
|    total_timesteps  | 463555   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 90888    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 5.17     |
|    exploration_rate | 0.38     |
| time/               |          |
|    episodes         | 4476     |
|    fps              | 38       |
|    time_elapsed     | 12000    |
|    total_timesteps  | 463858   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 90964    |
----------------------------------
Eval num_timesteps=464000, episode_reward=4.20 +/- 6.15
Episode length: 125.50 +/- 78.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.379    |
| time/               |          |
|    total_timesteps  | 464000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 90999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.379    |
| time/               |          |
|    episodes         | 4480     |
|    fps              | 38       |
|    time_elapsed     | 12018    |
|    total_timesteps  | 464195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.148    |
|    n_updates        | 91048    |
----------------------------------
Eval num_timesteps=464500, episode_reward=3.72 +/- 7.93
Episode length: 139.60 +/- 119.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 3.72     |
| rollout/            |          |
|    exploration_rate | 0.378    |
| time/               |          |
|    total_timesteps  | 464500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.189    |
|    n_updates        | 91124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 5.04     |
|    exploration_rate | 0.378    |
| time/               |          |
|    episodes         | 4484     |
|    fps              | 38       |
|    time_elapsed     | 12035    |
|    total_timesteps  | 464671   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 91167    |
----------------------------------
Eval num_timesteps=465000, episode_reward=2.28 +/- 3.78
Episode length: 90.36 +/- 33.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.4     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.377    |
| time/               |          |
|    total_timesteps  | 465000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 91249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 5.19     |
|    exploration_rate | 0.376    |
| time/               |          |
|    episodes         | 4488     |
|    fps              | 38       |
|    time_elapsed     | 12046    |
|    total_timesteps  | 465172   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 91292    |
----------------------------------
Eval num_timesteps=465500, episode_reward=3.80 +/- 5.59
Episode length: 111.04 +/- 50.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.376    |
| time/               |          |
|    total_timesteps  | 465500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 91374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.375    |
| time/               |          |
|    episodes         | 4492     |
|    fps              | 38       |
|    time_elapsed     | 12059    |
|    total_timesteps  | 465676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.196    |
|    n_updates        | 91418    |
----------------------------------
Eval num_timesteps=466000, episode_reward=4.64 +/- 6.94
Episode length: 115.66 +/- 65.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.374    |
| time/               |          |
|    total_timesteps  | 466000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0425   |
|    n_updates        | 91499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 5.02     |
|    exploration_rate | 0.374    |
| time/               |          |
|    episodes         | 4496     |
|    fps              | 38       |
|    time_elapsed     | 12075    |
|    total_timesteps  | 466239   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.229    |
|    n_updates        | 91559    |
----------------------------------
Eval num_timesteps=466500, episode_reward=5.14 +/- 8.41
Episode length: 113.80 +/- 57.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.373    |
| time/               |          |
|    total_timesteps  | 466500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.154    |
|    n_updates        | 91624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.14     |
|    exploration_rate | 0.372    |
| time/               |          |
|    episodes         | 4500     |
|    fps              | 38       |
|    time_elapsed     | 12089    |
|    total_timesteps  | 466850   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0894   |
|    n_updates        | 91712    |
----------------------------------
Eval num_timesteps=467000, episode_reward=2.36 +/- 3.22
Episode length: 114.28 +/- 51.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.372    |
| time/               |          |
|    total_timesteps  | 467000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0577   |
|    n_updates        | 91749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.93     |
|    exploration_rate | 0.371    |
| time/               |          |
|    episodes         | 4504     |
|    fps              | 38       |
|    time_elapsed     | 12103    |
|    total_timesteps  | 467224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.187    |
|    n_updates        | 91805    |
----------------------------------
Eval num_timesteps=467500, episode_reward=3.60 +/- 4.92
Episode length: 107.90 +/- 47.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.371    |
| time/               |          |
|    total_timesteps  | 467500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0927   |
|    n_updates        | 91874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.89     |
|    exploration_rate | 0.37     |
| time/               |          |
|    episodes         | 4508     |
|    fps              | 38       |
|    time_elapsed     | 12117    |
|    total_timesteps  | 467793   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 91948    |
----------------------------------
Eval num_timesteps=468000, episode_reward=3.26 +/- 4.21
Episode length: 91.30 +/- 34.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.3     |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.369    |
| time/               |          |
|    total_timesteps  | 468000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0816   |
|    n_updates        | 91999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.08     |
|    exploration_rate | 0.368    |
| time/               |          |
|    episodes         | 4512     |
|    fps              | 38       |
|    time_elapsed     | 12129    |
|    total_timesteps  | 468386   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0791   |
|    n_updates        | 92096    |
----------------------------------
Eval num_timesteps=468500, episode_reward=2.98 +/- 3.33
Episode length: 97.20 +/- 35.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.2     |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.368    |
| time/               |          |
|    total_timesteps  | 468500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0967   |
|    n_updates        | 92124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.11     |
|    exploration_rate | 0.367    |
| time/               |          |
|    episodes         | 4516     |
|    fps              | 38       |
|    time_elapsed     | 12140    |
|    total_timesteps  | 468813   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 92203    |
----------------------------------
Eval num_timesteps=469000, episode_reward=5.22 +/- 6.97
Episode length: 109.82 +/- 57.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 5.22     |
| rollout/            |          |
|    exploration_rate | 0.367    |
| time/               |          |
|    total_timesteps  | 469000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 92249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.366    |
| time/               |          |
|    episodes         | 4520     |
|    fps              | 38       |
|    time_elapsed     | 12153    |
|    total_timesteps  | 469103   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.227    |
|    n_updates        | 92275    |
----------------------------------
Eval num_timesteps=469500, episode_reward=2.78 +/- 4.74
Episode length: 124.48 +/- 96.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.365    |
| time/               |          |
|    total_timesteps  | 469500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0661   |
|    n_updates        | 92374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.365    |
| time/               |          |
|    episodes         | 4524     |
|    fps              | 38       |
|    time_elapsed     | 12168    |
|    total_timesteps  | 469540   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 92384    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.34     |
|    exploration_rate | 0.365    |
| time/               |          |
|    episodes         | 4528     |
|    fps              | 38       |
|    time_elapsed     | 12169    |
|    total_timesteps  | 469814   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 92453    |
----------------------------------
Eval num_timesteps=470000, episode_reward=4.90 +/- 9.91
Episode length: 107.76 +/- 61.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.9      |
| rollout/            |          |
|    exploration_rate | 0.364    |
| time/               |          |
|    total_timesteps  | 470000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0355   |
|    n_updates        | 92499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.39     |
|    exploration_rate | 0.364    |
| time/               |          |
|    episodes         | 4532     |
|    fps              | 38       |
|    time_elapsed     | 12182    |
|    total_timesteps  | 470151   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 92537    |
----------------------------------
Eval num_timesteps=470500, episode_reward=3.64 +/- 6.32
Episode length: 112.42 +/- 66.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.363    |
| time/               |          |
|    total_timesteps  | 470500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 92624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.362    |
| time/               |          |
|    episodes         | 4536     |
|    fps              | 38       |
|    time_elapsed     | 12196    |
|    total_timesteps  | 470641   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0619   |
|    n_updates        | 92660    |
----------------------------------
Eval num_timesteps=471000, episode_reward=3.24 +/- 5.08
Episode length: 117.80 +/- 88.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.361    |
| time/               |          |
|    total_timesteps  | 471000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.161    |
|    n_updates        | 92749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.74     |
|    exploration_rate | 0.361    |
| time/               |          |
|    episodes         | 4540     |
|    fps              | 38       |
|    time_elapsed     | 12214    |
|    total_timesteps  | 471139   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.157    |
|    n_updates        | 92784    |
----------------------------------
Eval num_timesteps=471500, episode_reward=5.00 +/- 6.97
Episode length: 134.36 +/- 69.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.36     |
| time/               |          |
|    total_timesteps  | 471500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 92874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.83     |
|    exploration_rate | 0.36     |
| time/               |          |
|    episodes         | 4544     |
|    fps              | 38       |
|    time_elapsed     | 12232    |
|    total_timesteps  | 471588   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 92896    |
----------------------------------
Eval num_timesteps=472000, episode_reward=2.42 +/- 3.78
Episode length: 92.88 +/- 37.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.9     |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.359    |
| time/               |          |
|    total_timesteps  | 472000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.203    |
|    n_updates        | 92999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.12     |
|    exploration_rate | 0.359    |
| time/               |          |
|    episodes         | 4548     |
|    fps              | 38       |
|    time_elapsed     | 12245    |
|    total_timesteps  | 472149   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.118    |
|    n_updates        | 93037    |
----------------------------------
Eval num_timesteps=472500, episode_reward=4.22 +/- 7.17
Episode length: 117.72 +/- 78.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.22     |
| rollout/            |          |
|    exploration_rate | 0.358    |
| time/               |          |
|    total_timesteps  | 472500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.163    |
|    n_updates        | 93124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.31     |
|    exploration_rate | 0.357    |
| time/               |          |
|    episodes         | 4552     |
|    fps              | 38       |
|    time_elapsed     | 12260    |
|    total_timesteps  | 472775   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0687   |
|    n_updates        | 93193    |
----------------------------------
Eval num_timesteps=473000, episode_reward=3.28 +/- 5.47
Episode length: 101.74 +/- 48.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.356    |
| time/               |          |
|    total_timesteps  | 473000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0903   |
|    n_updates        | 93249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 5.17     |
|    exploration_rate | 0.356    |
| time/               |          |
|    episodes         | 4556     |
|    fps              | 38       |
|    time_elapsed     | 12274    |
|    total_timesteps  | 473150   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 93287    |
----------------------------------
Eval num_timesteps=473500, episode_reward=3.46 +/- 5.01
Episode length: 96.48 +/- 49.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 3.46     |
| rollout/            |          |
|    exploration_rate | 0.355    |
| time/               |          |
|    total_timesteps  | 473500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.226    |
|    n_updates        | 93374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.355    |
| time/               |          |
|    episodes         | 4560     |
|    fps              | 38       |
|    time_elapsed     | 12286    |
|    total_timesteps  | 473503   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.18     |
|    n_updates        | 93375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.354    |
| time/               |          |
|    episodes         | 4564     |
|    fps              | 38       |
|    time_elapsed     | 12287    |
|    total_timesteps  | 473858   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 93464    |
----------------------------------
Eval num_timesteps=474000, episode_reward=8.90 +/- 12.94
Episode length: 155.44 +/- 103.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 155      |
|    mean_reward      | 8.9      |
| rollout/            |          |
|    exploration_rate | 0.354    |
| time/               |          |
|    total_timesteps  | 474000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 93499    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.353    |
| time/               |          |
|    episodes         | 4568     |
|    fps              | 38       |
|    time_elapsed     | 12306    |
|    total_timesteps  | 474219   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0851   |
|    n_updates        | 93554    |
----------------------------------
Eval num_timesteps=474500, episode_reward=3.16 +/- 4.51
Episode length: 103.08 +/- 43.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.352    |
| time/               |          |
|    total_timesteps  | 474500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 93624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.352    |
| time/               |          |
|    episodes         | 4572     |
|    fps              | 38       |
|    time_elapsed     | 12319    |
|    total_timesteps  | 474808   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.044    |
|    n_updates        | 93701    |
----------------------------------
Eval num_timesteps=475000, episode_reward=2.46 +/- 3.79
Episode length: 127.38 +/- 117.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.351    |
| time/               |          |
|    total_timesteps  | 475000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 93749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.12     |
|    exploration_rate | 0.35     |
| time/               |          |
|    episodes         | 4576     |
|    fps              | 38       |
|    time_elapsed     | 12335    |
|    total_timesteps  | 475267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0806   |
|    n_updates        | 93816    |
----------------------------------
Eval num_timesteps=475500, episode_reward=5.42 +/- 11.18
Episode length: 113.56 +/- 58.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 5.42     |
| rollout/            |          |
|    exploration_rate | 0.35     |
| time/               |          |
|    total_timesteps  | 475500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 93874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.349    |
| time/               |          |
|    episodes         | 4580     |
|    fps              | 38       |
|    time_elapsed     | 12350    |
|    total_timesteps  | 475676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.201    |
|    n_updates        | 93918    |
----------------------------------
Eval num_timesteps=476000, episode_reward=2.74 +/- 5.06
Episode length: 88.84 +/- 30.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.8     |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.349    |
| time/               |          |
|    total_timesteps  | 476000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 93999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.348    |
| time/               |          |
|    episodes         | 4584     |
|    fps              | 38       |
|    time_elapsed     | 12362    |
|    total_timesteps  | 476092   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.239    |
|    n_updates        | 94022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.347    |
| time/               |          |
|    episodes         | 4588     |
|    fps              | 38       |
|    time_elapsed     | 12363    |
|    total_timesteps  | 476406   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0337   |
|    n_updates        | 94101    |
----------------------------------
Eval num_timesteps=476500, episode_reward=1.76 +/- 2.66
Episode length: 89.12 +/- 37.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.1     |
|    mean_reward      | 1.76     |
| rollout/            |          |
|    exploration_rate | 0.347    |
| time/               |          |
|    total_timesteps  | 476500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.153    |
|    n_updates        | 94124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.71     |
|    exploration_rate | 0.346    |
| time/               |          |
|    episodes         | 4592     |
|    fps              | 38       |
|    time_elapsed     | 12374    |
|    total_timesteps  | 476878   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 94219    |
----------------------------------
Eval num_timesteps=477000, episode_reward=3.08 +/- 4.34
Episode length: 119.02 +/- 47.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.346    |
| time/               |          |
|    total_timesteps  | 477000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0469   |
|    n_updates        | 94249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.345    |
| time/               |          |
|    episodes         | 4596     |
|    fps              | 38       |
|    time_elapsed     | 12389    |
|    total_timesteps  | 477383   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0732   |
|    n_updates        | 94345    |
----------------------------------
Eval num_timesteps=477500, episode_reward=3.40 +/- 6.41
Episode length: 107.02 +/- 64.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.345    |
| time/               |          |
|    total_timesteps  | 477500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.067    |
|    n_updates        | 94374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.344    |
| time/               |          |
|    episodes         | 4600     |
|    fps              | 38       |
|    time_elapsed     | 12402    |
|    total_timesteps  | 477826   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0783   |
|    n_updates        | 94456    |
----------------------------------
Eval num_timesteps=478000, episode_reward=3.62 +/- 5.07
Episode length: 128.82 +/- 83.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.343    |
| time/               |          |
|    total_timesteps  | 478000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0988   |
|    n_updates        | 94499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.342    |
| time/               |          |
|    episodes         | 4604     |
|    fps              | 38       |
|    time_elapsed     | 12418    |
|    total_timesteps  | 478349   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0543   |
|    n_updates        | 94587    |
----------------------------------
Eval num_timesteps=478500, episode_reward=3.40 +/- 5.80
Episode length: 112.46 +/- 64.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.342    |
| time/               |          |
|    total_timesteps  | 478500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.047    |
|    n_updates        | 94624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.341    |
| time/               |          |
|    episodes         | 4608     |
|    fps              | 38       |
|    time_elapsed     | 12433    |
|    total_timesteps  | 478943   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 94735    |
----------------------------------
Eval num_timesteps=479000, episode_reward=4.20 +/- 6.35
Episode length: 111.74 +/- 46.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.341    |
| time/               |          |
|    total_timesteps  | 479000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 94749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.34     |
| time/               |          |
|    episodes         | 4612     |
|    fps              | 38       |
|    time_elapsed     | 12448    |
|    total_timesteps  | 479457   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0492   |
|    n_updates        | 94864    |
----------------------------------
Eval num_timesteps=479500, episode_reward=4.74 +/- 7.74
Episode length: 163.90 +/- 132.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 4.74     |
| rollout/            |          |
|    exploration_rate | 0.339    |
| time/               |          |
|    total_timesteps  | 479500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0936   |
|    n_updates        | 94874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.82     |
|    exploration_rate | 0.338    |
| time/               |          |
|    episodes         | 4616     |
|    fps              | 38       |
|    time_elapsed     | 12467    |
|    total_timesteps  | 479972   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0906   |
|    n_updates        | 94992    |
----------------------------------
Eval num_timesteps=480000, episode_reward=3.24 +/- 4.51
Episode length: 140.82 +/- 101.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.338    |
| time/               |          |
|    total_timesteps  | 480000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0846   |
|    n_updates        | 94999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.337    |
| time/               |          |
|    episodes         | 4620     |
|    fps              | 38       |
|    time_elapsed     | 12484    |
|    total_timesteps  | 480439   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.157    |
|    n_updates        | 95109    |
----------------------------------
Eval num_timesteps=480500, episode_reward=4.10 +/- 6.43
Episode length: 118.34 +/- 51.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.337    |
| time/               |          |
|    total_timesteps  | 480500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.069    |
|    n_updates        | 95124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.336    |
| time/               |          |
|    episodes         | 4624     |
|    fps              | 38       |
|    time_elapsed     | 12502    |
|    total_timesteps  | 480901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 95225    |
----------------------------------
Eval num_timesteps=481000, episode_reward=5.92 +/- 7.87
Episode length: 139.84 +/- 72.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 5.92     |
| rollout/            |          |
|    exploration_rate | 0.335    |
| time/               |          |
|    total_timesteps  | 481000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 95249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.335    |
| time/               |          |
|    episodes         | 4628     |
|    fps              | 38       |
|    time_elapsed     | 12519    |
|    total_timesteps  | 481280   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0845   |
|    n_updates        | 95319    |
----------------------------------
Eval num_timesteps=481500, episode_reward=2.34 +/- 3.83
Episode length: 109.92 +/- 52.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.334    |
| time/               |          |
|    total_timesteps  | 481500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0656   |
|    n_updates        | 95374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.334    |
| time/               |          |
|    episodes         | 4632     |
|    fps              | 38       |
|    time_elapsed     | 12537    |
|    total_timesteps  | 481729   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0783   |
|    n_updates        | 95432    |
----------------------------------
Eval num_timesteps=482000, episode_reward=3.74 +/- 5.88
Episode length: 135.54 +/- 86.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.333    |
| time/               |          |
|    total_timesteps  | 482000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.1      |
|    n_updates        | 95499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.18     |
|    exploration_rate | 0.332    |
| time/               |          |
|    episodes         | 4636     |
|    fps              | 38       |
|    time_elapsed     | 12554    |
|    total_timesteps  | 482242   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.084    |
|    n_updates        | 95560    |
----------------------------------
Eval num_timesteps=482500, episode_reward=2.84 +/- 4.93
Episode length: 104.72 +/- 39.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.332    |
| time/               |          |
|    total_timesteps  | 482500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0743   |
|    n_updates        | 95624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.331    |
| time/               |          |
|    episodes         | 4640     |
|    fps              | 38       |
|    time_elapsed     | 12567    |
|    total_timesteps  | 482880   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 95719    |
----------------------------------
Eval num_timesteps=483000, episode_reward=4.38 +/- 5.59
Episode length: 137.06 +/- 66.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 4.38     |
| rollout/            |          |
|    exploration_rate | 0.33     |
| time/               |          |
|    total_timesteps  | 483000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0706   |
|    n_updates        | 95749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.26     |
|    exploration_rate | 0.329    |
| time/               |          |
|    episodes         | 4644     |
|    fps              | 38       |
|    time_elapsed     | 12583    |
|    total_timesteps  | 483381   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0656   |
|    n_updates        | 95845    |
----------------------------------
Eval num_timesteps=483500, episode_reward=4.60 +/- 8.91
Episode length: 130.72 +/- 95.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.329    |
| time/               |          |
|    total_timesteps  | 483500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.04     |
|    n_updates        | 95874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.328    |
| time/               |          |
|    episodes         | 4648     |
|    fps              | 38       |
|    time_elapsed     | 12598    |
|    total_timesteps  | 483759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0604   |
|    n_updates        | 95939    |
----------------------------------
Eval num_timesteps=484000, episode_reward=3.70 +/- 6.96
Episode length: 113.46 +/- 56.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.328    |
| time/               |          |
|    total_timesteps  | 484000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0987   |
|    n_updates        | 95999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.65     |
|    exploration_rate | 0.327    |
| time/               |          |
|    episodes         | 4652     |
|    fps              | 38       |
|    time_elapsed     | 12612    |
|    total_timesteps  | 484171   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.093    |
|    n_updates        | 96042    |
----------------------------------
Eval num_timesteps=484500, episode_reward=3.22 +/- 5.09
Episode length: 116.40 +/- 47.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.326    |
| time/               |          |
|    total_timesteps  | 484500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0759   |
|    n_updates        | 96124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.326    |
| time/               |          |
|    episodes         | 4656     |
|    fps              | 38       |
|    time_elapsed     | 12626    |
|    total_timesteps  | 484603   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 96150    |
----------------------------------
Eval num_timesteps=485000, episode_reward=2.96 +/- 3.17
Episode length: 131.36 +/- 83.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.325    |
| time/               |          |
|    total_timesteps  | 485000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0823   |
|    n_updates        | 96249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.61     |
|    exploration_rate | 0.325    |
| time/               |          |
|    episodes         | 4660     |
|    fps              | 38       |
|    time_elapsed     | 12641    |
|    total_timesteps  | 485040   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.286    |
|    n_updates        | 96259    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.324    |
| time/               |          |
|    episodes         | 4664     |
|    fps              | 38       |
|    time_elapsed     | 12643    |
|    total_timesteps  | 485397   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.207    |
|    n_updates        | 96349    |
----------------------------------
Eval num_timesteps=485500, episode_reward=4.02 +/- 8.68
Episode length: 123.68 +/- 65.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.324    |
| time/               |          |
|    total_timesteps  | 485500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 96374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.64     |
|    exploration_rate | 0.323    |
| time/               |          |
|    episodes         | 4668     |
|    fps              | 38       |
|    time_elapsed     | 12658    |
|    total_timesteps  | 485918   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0604   |
|    n_updates        | 96479    |
----------------------------------
Eval num_timesteps=486000, episode_reward=2.94 +/- 5.00
Episode length: 96.08 +/- 46.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.1     |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.322    |
| time/               |          |
|    total_timesteps  | 486000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0978   |
|    n_updates        | 96499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 4672     |
|    fps              | 38       |
|    time_elapsed     | 12671    |
|    total_timesteps  | 486358   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.087    |
|    n_updates        | 96589    |
----------------------------------
Eval num_timesteps=486500, episode_reward=4.68 +/- 6.39
Episode length: 149.02 +/- 88.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.321    |
| time/               |          |
|    total_timesteps  | 486500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0311   |
|    n_updates        | 96624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.32     |
| time/               |          |
|    episodes         | 4676     |
|    fps              | 38       |
|    time_elapsed     | 12689    |
|    total_timesteps  | 486829   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 96707    |
----------------------------------
Eval num_timesteps=487000, episode_reward=3.04 +/- 5.07
Episode length: 121.82 +/- 60.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.04     |
| rollout/            |          |
|    exploration_rate | 0.32     |
| time/               |          |
|    total_timesteps  | 487000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.159    |
|    n_updates        | 96749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.61     |
|    exploration_rate | 0.319    |
| time/               |          |
|    episodes         | 4680     |
|    fps              | 38       |
|    time_elapsed     | 12704    |
|    total_timesteps  | 487234   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0728   |
|    n_updates        | 96808    |
----------------------------------
Eval num_timesteps=487500, episode_reward=4.30 +/- 6.30
Episode length: 118.92 +/- 65.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 487500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 96874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.318    |
| time/               |          |
|    episodes         | 4684     |
|    fps              | 38       |
|    time_elapsed     | 12719    |
|    total_timesteps  | 487601   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 96900    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.317    |
| time/               |          |
|    episodes         | 4688     |
|    fps              | 38       |
|    time_elapsed     | 12720    |
|    total_timesteps  | 487944   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.312    |
|    n_updates        | 96985    |
----------------------------------
Eval num_timesteps=488000, episode_reward=2.94 +/- 5.30
Episode length: 98.26 +/- 50.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.3     |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.317    |
| time/               |          |
|    total_timesteps  | 488000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 96999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.316    |
| time/               |          |
|    episodes         | 4692     |
|    fps              | 38       |
|    time_elapsed     | 12732    |
|    total_timesteps  | 488426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0445   |
|    n_updates        | 97106    |
----------------------------------
Eval num_timesteps=488500, episode_reward=3.84 +/- 5.18
Episode length: 124.38 +/- 69.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.316    |
| time/               |          |
|    total_timesteps  | 488500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0299   |
|    n_updates        | 97124    |
----------------------------------
Eval num_timesteps=489000, episode_reward=4.84 +/- 6.72
Episode length: 118.66 +/- 53.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.314    |
| time/               |          |
|    total_timesteps  | 489000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0667   |
|    n_updates        | 97249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.314    |
| time/               |          |
|    episodes         | 4696     |
|    fps              | 38       |
|    time_elapsed     | 12763    |
|    total_timesteps  | 489066   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.224    |
|    n_updates        | 97266    |
----------------------------------
Eval num_timesteps=489500, episode_reward=4.04 +/- 6.45
Episode length: 117.88 +/- 81.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.313    |
| time/               |          |
|    total_timesteps  | 489500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.057    |
|    n_updates        | 97374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.9      |
|    exploration_rate | 0.313    |
| time/               |          |
|    episodes         | 4700     |
|    fps              | 38       |
|    time_elapsed     | 12782    |
|    total_timesteps  | 489529   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 97382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.312    |
| time/               |          |
|    episodes         | 4704     |
|    fps              | 38       |
|    time_elapsed     | 12784    |
|    total_timesteps  | 489899   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0613   |
|    n_updates        | 97474    |
----------------------------------
Eval num_timesteps=490000, episode_reward=5.12 +/- 7.20
Episode length: 118.34 +/- 63.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 5.12     |
| rollout/            |          |
|    exploration_rate | 0.312    |
| time/               |          |
|    total_timesteps  | 490000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.26     |
|    n_updates        | 97499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.311    |
| time/               |          |
|    episodes         | 4708     |
|    fps              | 38       |
|    time_elapsed     | 12798    |
|    total_timesteps  | 490281   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 97570    |
----------------------------------
Eval num_timesteps=490500, episode_reward=5.80 +/- 8.11
Episode length: 128.52 +/- 83.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.8      |
| rollout/            |          |
|    exploration_rate | 0.31     |
| time/               |          |
|    total_timesteps  | 490500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.183    |
|    n_updates        | 97624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.31     |
| time/               |          |
|    episodes         | 4712     |
|    fps              | 38       |
|    time_elapsed     | 12819    |
|    total_timesteps  | 490860   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 97714    |
----------------------------------
Eval num_timesteps=491000, episode_reward=4.28 +/- 5.50
Episode length: 121.50 +/- 49.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.28     |
| rollout/            |          |
|    exploration_rate | 0.309    |
| time/               |          |
|    total_timesteps  | 491000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0412   |
|    n_updates        | 97749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.64     |
|    exploration_rate | 0.308    |
| time/               |          |
|    episodes         | 4716     |
|    fps              | 38       |
|    time_elapsed     | 12840    |
|    total_timesteps  | 491367   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 97841    |
----------------------------------
Eval num_timesteps=491500, episode_reward=4.18 +/- 6.05
Episode length: 129.22 +/- 89.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.308    |
| time/               |          |
|    total_timesteps  | 491500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0917   |
|    n_updates        | 97874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.307    |
| time/               |          |
|    episodes         | 4720     |
|    fps              | 38       |
|    time_elapsed     | 12860    |
|    total_timesteps  | 491909   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0941   |
|    n_updates        | 97977    |
----------------------------------
Eval num_timesteps=492000, episode_reward=2.78 +/- 3.69
Episode length: 112.16 +/- 51.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.307    |
| time/               |          |
|    total_timesteps  | 492000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 97999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.306    |
| time/               |          |
|    episodes         | 4724     |
|    fps              | 38       |
|    time_elapsed     | 12878    |
|    total_timesteps  | 492278   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 98069    |
----------------------------------
Eval num_timesteps=492500, episode_reward=2.78 +/- 5.13
Episode length: 104.40 +/- 50.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.305    |
| time/               |          |
|    total_timesteps  | 492500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 98124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.99     |
|    exploration_rate | 0.305    |
| time/               |          |
|    episodes         | 4728     |
|    fps              | 38       |
|    time_elapsed     | 12894    |
|    total_timesteps  | 492671   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0841   |
|    n_updates        | 98167    |
----------------------------------
Eval num_timesteps=493000, episode_reward=2.34 +/- 3.48
Episode length: 107.48 +/- 49.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.304    |
| time/               |          |
|    total_timesteps  | 493000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.191    |
|    n_updates        | 98249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.303    |
| time/               |          |
|    episodes         | 4732     |
|    fps              | 38       |
|    time_elapsed     | 12907    |
|    total_timesteps  | 493171   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0926   |
|    n_updates        | 98292    |
----------------------------------
Eval num_timesteps=493500, episode_reward=4.76 +/- 7.84
Episode length: 122.84 +/- 63.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.303    |
| time/               |          |
|    total_timesteps  | 493500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0919   |
|    n_updates        | 98374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.302    |
| time/               |          |
|    episodes         | 4736     |
|    fps              | 38       |
|    time_elapsed     | 12922    |
|    total_timesteps  | 493547   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.212    |
|    n_updates        | 98386    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.52     |
|    exploration_rate | 0.301    |
| time/               |          |
|    episodes         | 4740     |
|    fps              | 38       |
|    time_elapsed     | 12924    |
|    total_timesteps  | 493903   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.174    |
|    n_updates        | 98475    |
----------------------------------
Eval num_timesteps=494000, episode_reward=3.36 +/- 5.50
Episode length: 111.64 +/- 40.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.301    |
| time/               |          |
|    total_timesteps  | 494000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 98499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.44     |
|    exploration_rate | 0.301    |
| time/               |          |
|    episodes         | 4744     |
|    fps              | 38       |
|    time_elapsed     | 12937    |
|    total_timesteps  | 494212   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.147    |
|    n_updates        | 98552    |
----------------------------------
Eval num_timesteps=494500, episode_reward=3.80 +/- 5.56
Episode length: 108.96 +/- 46.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.3      |
| time/               |          |
|    total_timesteps  | 494500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.175    |
|    n_updates        | 98624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.299    |
| time/               |          |
|    episodes         | 4748     |
|    fps              | 38       |
|    time_elapsed     | 12951    |
|    total_timesteps  | 494771   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.188    |
|    n_updates        | 98692    |
----------------------------------
Eval num_timesteps=495000, episode_reward=3.76 +/- 6.73
Episode length: 116.06 +/- 68.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.299    |
| time/               |          |
|    total_timesteps  | 495000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.141    |
|    n_updates        | 98749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.298    |
| time/               |          |
|    episodes         | 4752     |
|    fps              | 38       |
|    time_elapsed     | 12965    |
|    total_timesteps  | 495305   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 98826    |
----------------------------------
Eval num_timesteps=495500, episode_reward=3.24 +/- 5.97
Episode length: 126.12 +/- 89.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.297    |
| time/               |          |
|    total_timesteps  | 495500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0739   |
|    n_updates        | 98874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 4756     |
|    fps              | 38       |
|    time_elapsed     | 12981    |
|    total_timesteps  | 495880   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.151    |
|    n_updates        | 98969    |
----------------------------------
Eval num_timesteps=496000, episode_reward=4.82 +/- 6.86
Episode length: 115.84 +/- 52.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.82     |
| rollout/            |          |
|    exploration_rate | 0.296    |
| time/               |          |
|    total_timesteps  | 496000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 98999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.295    |
| time/               |          |
|    episodes         | 4760     |
|    fps              | 38       |
|    time_elapsed     | 12995    |
|    total_timesteps  | 496417   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 99104    |
----------------------------------
Eval num_timesteps=496500, episode_reward=2.48 +/- 3.19
Episode length: 102.78 +/- 37.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.295    |
| time/               |          |
|    total_timesteps  | 496500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0366   |
|    n_updates        | 99124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.294    |
| time/               |          |
|    episodes         | 4764     |
|    fps              | 38       |
|    time_elapsed     | 13010    |
|    total_timesteps  | 496836   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0672   |
|    n_updates        | 99208    |
----------------------------------
Eval num_timesteps=497000, episode_reward=2.76 +/- 3.01
Episode length: 103.92 +/- 45.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.76     |
| rollout/            |          |
|    exploration_rate | 0.293    |
| time/               |          |
|    total_timesteps  | 497000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0506   |
|    n_updates        | 99249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.24     |
|    exploration_rate | 0.293    |
| time/               |          |
|    episodes         | 4768     |
|    fps              | 38       |
|    time_elapsed     | 13022    |
|    total_timesteps  | 497217   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0606   |
|    n_updates        | 99304    |
----------------------------------
Eval num_timesteps=497500, episode_reward=1.54 +/- 2.83
Episode length: 97.46 +/- 36.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.5     |
|    mean_reward      | 1.54     |
| rollout/            |          |
|    exploration_rate | 0.292    |
| time/               |          |
|    total_timesteps  | 497500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0959   |
|    n_updates        | 99374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.291    |
| time/               |          |
|    episodes         | 4772     |
|    fps              | 38       |
|    time_elapsed     | 13034    |
|    total_timesteps  | 497723   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0617   |
|    n_updates        | 99430    |
----------------------------------
Eval num_timesteps=498000, episode_reward=3.84 +/- 7.22
Episode length: 108.62 +/- 52.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.291    |
| time/               |          |
|    total_timesteps  | 498000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.141    |
|    n_updates        | 99499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.29     |
| time/               |          |
|    episodes         | 4776     |
|    fps              | 38       |
|    time_elapsed     | 13049    |
|    total_timesteps  | 498186   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 99546    |
----------------------------------
Eval num_timesteps=498500, episode_reward=4.52 +/- 7.50
Episode length: 151.68 +/- 120.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.289    |
| time/               |          |
|    total_timesteps  | 498500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0913   |
|    n_updates        | 99624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.39     |
|    exploration_rate | 0.289    |
| time/               |          |
|    episodes         | 4780     |
|    fps              | 38       |
|    time_elapsed     | 13066    |
|    total_timesteps  | 498560   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0884   |
|    n_updates        | 99639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.43     |
|    exploration_rate | 0.288    |
| time/               |          |
|    episodes         | 4784     |
|    fps              | 38       |
|    time_elapsed     | 13068    |
|    total_timesteps  | 498996   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.093    |
|    n_updates        | 99748    |
----------------------------------
Eval num_timesteps=499000, episode_reward=3.60 +/- 6.67
Episode length: 110.88 +/- 48.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.288    |
| time/               |          |
|    total_timesteps  | 499000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.211    |
|    n_updates        | 99749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.287    |
| time/               |          |
|    episodes         | 4788     |
|    fps              | 38       |
|    time_elapsed     | 13081    |
|    total_timesteps  | 499242   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.147    |
|    n_updates        | 99810    |
----------------------------------
Eval num_timesteps=499500, episode_reward=4.20 +/- 5.69
Episode length: 137.82 +/- 84.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.287    |
| time/               |          |
|    total_timesteps  | 499500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 99874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.286    |
| time/               |          |
|    episodes         | 4792     |
|    fps              | 38       |
|    time_elapsed     | 13097    |
|    total_timesteps  | 499580   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.195    |
|    n_updates        | 99894    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.285    |
| time/               |          |
|    episodes         | 4796     |
|    fps              | 38       |
|    time_elapsed     | 13098    |
|    total_timesteps  | 499934   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.186    |
|    n_updates        | 99983    |
----------------------------------
Eval num_timesteps=500000, episode_reward=6.70 +/- 9.76
Episode length: 156.14 +/- 86.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 6.7      |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 500000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0954   |
|    n_updates        | 99999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.284    |
| time/               |          |
|    episodes         | 4800     |
|    fps              | 38       |
|    time_elapsed     | 13116    |
|    total_timesteps  | 500416   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0666   |
|    n_updates        | 100103   |
----------------------------------
Eval num_timesteps=500500, episode_reward=4.10 +/- 4.46
Episode length: 148.02 +/- 107.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.284    |
| time/               |          |
|    total_timesteps  | 500500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.209    |
|    n_updates        | 100124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.283    |
| time/               |          |
|    episodes         | 4804     |
|    fps              | 38       |
|    time_elapsed     | 13134    |
|    total_timesteps  | 500983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0549   |
|    n_updates        | 100245   |
----------------------------------
Eval num_timesteps=501000, episode_reward=3.86 +/- 5.86
Episode length: 136.08 +/- 77.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.282    |
| time/               |          |
|    total_timesteps  | 501000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0868   |
|    n_updates        | 100249   |
----------------------------------
Eval num_timesteps=501500, episode_reward=6.24 +/- 10.29
Episode length: 154.20 +/- 94.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 6.24     |
| rollout/            |          |
|    exploration_rate | 0.281    |
| time/               |          |
|    total_timesteps  | 501500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.171    |
|    n_updates        | 100374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.281    |
| time/               |          |
|    episodes         | 4808     |
|    fps              | 38       |
|    time_elapsed     | 13166    |
|    total_timesteps  | 501522   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0765   |
|    n_updates        | 100380   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.28     |
| time/               |          |
|    episodes         | 4812     |
|    fps              | 38       |
|    time_elapsed     | 13168    |
|    total_timesteps  | 501923   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.162    |
|    n_updates        | 100480   |
----------------------------------
Eval num_timesteps=502000, episode_reward=3.44 +/- 6.37
Episode length: 144.38 +/- 94.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.28     |
| time/               |          |
|    total_timesteps  | 502000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0712   |
|    n_updates        | 100499   |
----------------------------------
Eval num_timesteps=502500, episode_reward=8.02 +/- 14.62
Episode length: 146.38 +/- 83.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 8.02     |
| rollout/            |          |
|    exploration_rate | 0.278    |
| time/               |          |
|    total_timesteps  | 502500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.219    |
|    n_updates        | 100624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.27     |
|    exploration_rate | 0.278    |
| time/               |          |
|    episodes         | 4816     |
|    fps              | 38       |
|    time_elapsed     | 13201    |
|    total_timesteps  | 502593   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.191    |
|    n_updates        | 100648   |
----------------------------------
Eval num_timesteps=503000, episode_reward=4.54 +/- 7.03
Episode length: 134.62 +/- 87.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 4.54     |
| rollout/            |          |
|    exploration_rate | 0.277    |
| time/               |          |
|    total_timesteps  | 503000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 100749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.276    |
| time/               |          |
|    episodes         | 4820     |
|    fps              | 38       |
|    time_elapsed     | 13218    |
|    total_timesteps  | 503245   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.17     |
|    n_updates        | 100811   |
----------------------------------
Eval num_timesteps=503500, episode_reward=3.06 +/- 5.34
Episode length: 108.58 +/- 55.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.276    |
| time/               |          |
|    total_timesteps  | 503500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.291    |
|    n_updates        | 100874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.275    |
| time/               |          |
|    episodes         | 4824     |
|    fps              | 38       |
|    time_elapsed     | 13230    |
|    total_timesteps  | 503635   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0754   |
|    n_updates        | 100908   |
----------------------------------
Eval num_timesteps=504000, episode_reward=6.84 +/- 11.16
Episode length: 157.62 +/- 100.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 158      |
|    mean_reward      | 6.84     |
| rollout/            |          |
|    exploration_rate | 0.274    |
| time/               |          |
|    total_timesteps  | 504000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 100999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.274    |
| time/               |          |
|    episodes         | 4828     |
|    fps              | 38       |
|    time_elapsed     | 13249    |
|    total_timesteps  | 504158   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0575   |
|    n_updates        | 101039   |
----------------------------------
Eval num_timesteps=504500, episode_reward=3.60 +/- 5.37
Episode length: 115.00 +/- 53.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.273    |
| time/               |          |
|    total_timesteps  | 504500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.199    |
|    n_updates        | 101124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.9      |
|    exploration_rate | 0.273    |
| time/               |          |
|    episodes         | 4832     |
|    fps              | 38       |
|    time_elapsed     | 13262    |
|    total_timesteps  | 504510   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0899   |
|    n_updates        | 101127   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.83     |
|    exploration_rate | 0.272    |
| time/               |          |
|    episodes         | 4836     |
|    fps              | 38       |
|    time_elapsed     | 13264    |
|    total_timesteps  | 504906   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0825   |
|    n_updates        | 101226   |
----------------------------------
Eval num_timesteps=505000, episode_reward=3.52 +/- 4.50
Episode length: 136.44 +/- 67.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.272    |
| time/               |          |
|    total_timesteps  | 505000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 101249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.26     |
|    exploration_rate | 0.271    |
| time/               |          |
|    episodes         | 4840     |
|    fps              | 38       |
|    time_elapsed     | 13280    |
|    total_timesteps  | 505434   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0769   |
|    n_updates        | 101358   |
----------------------------------
Eval num_timesteps=505500, episode_reward=4.22 +/- 6.35
Episode length: 144.52 +/- 112.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 4.22     |
| rollout/            |          |
|    exploration_rate | 0.27     |
| time/               |          |
|    total_timesteps  | 505500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.039    |
|    n_updates        | 101374   |
----------------------------------
Eval num_timesteps=506000, episode_reward=2.32 +/- 4.73
Episode length: 102.64 +/- 45.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.269    |
| time/               |          |
|    total_timesteps  | 506000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0649   |
|    n_updates        | 101499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.86     |
|    exploration_rate | 0.269    |
| time/               |          |
|    episodes         | 4844     |
|    fps              | 38       |
|    time_elapsed     | 13308    |
|    total_timesteps  | 506125   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0435   |
|    n_updates        | 101531   |
----------------------------------
Eval num_timesteps=506500, episode_reward=3.18 +/- 4.88
Episode length: 124.50 +/- 75.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.268    |
| time/               |          |
|    total_timesteps  | 506500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0417   |
|    n_updates        | 101624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.62     |
|    exploration_rate | 0.268    |
| time/               |          |
|    episodes         | 4848     |
|    fps              | 38       |
|    time_elapsed     | 13323    |
|    total_timesteps  | 506556   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 101638   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.38     |
|    exploration_rate | 0.267    |
| time/               |          |
|    episodes         | 4852     |
|    fps              | 38       |
|    time_elapsed     | 13324    |
|    total_timesteps  | 506877   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0949   |
|    n_updates        | 101719   |
----------------------------------
Eval num_timesteps=507000, episode_reward=3.14 +/- 8.79
Episode length: 142.12 +/- 99.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.266    |
| time/               |          |
|    total_timesteps  | 507000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0585   |
|    n_updates        | 101749   |
----------------------------------
Eval num_timesteps=507500, episode_reward=4.98 +/- 9.76
Episode length: 107.52 +/- 61.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.98     |
| rollout/            |          |
|    exploration_rate | 0.265    |
| time/               |          |
|    total_timesteps  | 507500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 101874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.265    |
| time/               |          |
|    episodes         | 4856     |
|    fps              | 38       |
|    time_elapsed     | 13353    |
|    total_timesteps  | 507610   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.313    |
|    n_updates        | 101902   |
----------------------------------
Eval num_timesteps=508000, episode_reward=3.28 +/- 5.48
Episode length: 137.46 +/- 76.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.264    |
| time/               |          |
|    total_timesteps  | 508000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 101999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.263    |
| time/               |          |
|    episodes         | 4860     |
|    fps              | 38       |
|    time_elapsed     | 13368    |
|    total_timesteps  | 508094   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0355   |
|    n_updates        | 102023   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.62     |
|    exploration_rate | 0.263    |
| time/               |          |
|    episodes         | 4864     |
|    fps              | 38       |
|    time_elapsed     | 13370    |
|    total_timesteps  | 508378   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 102094   |
----------------------------------
Eval num_timesteps=508500, episode_reward=4.48 +/- 6.26
Episode length: 132.22 +/- 51.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.262    |
| time/               |          |
|    total_timesteps  | 508500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0645   |
|    n_updates        | 102124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.52     |
|    exploration_rate | 0.262    |
| time/               |          |
|    episodes         | 4868     |
|    fps              | 38       |
|    time_elapsed     | 13385    |
|    total_timesteps  | 508716   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 102178   |
----------------------------------
Eval num_timesteps=509000, episode_reward=3.90 +/- 5.15
Episode length: 108.82 +/- 44.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.9      |
| rollout/            |          |
|    exploration_rate | 0.261    |
| time/               |          |
|    total_timesteps  | 509000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.256    |
|    n_updates        | 102249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.26     |
| time/               |          |
|    episodes         | 4872     |
|    fps              | 38       |
|    time_elapsed     | 13399    |
|    total_timesteps  | 509281   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 102320   |
----------------------------------
Eval num_timesteps=509500, episode_reward=3.56 +/- 5.06
Episode length: 137.92 +/- 89.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.26     |
| time/               |          |
|    total_timesteps  | 509500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0972   |
|    n_updates        | 102374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.38     |
|    exploration_rate | 0.259    |
| time/               |          |
|    episodes         | 4876     |
|    fps              | 37       |
|    time_elapsed     | 13415    |
|    total_timesteps  | 509716   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.198    |
|    n_updates        | 102428   |
----------------------------------
Eval num_timesteps=510000, episode_reward=3.90 +/- 5.25
Episode length: 143.16 +/- 93.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 3.9      |
| rollout/            |          |
|    exploration_rate | 0.258    |
| time/               |          |
|    total_timesteps  | 510000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.201    |
|    n_updates        | 102499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.51     |
|    exploration_rate | 0.257    |
| time/               |          |
|    episodes         | 4880     |
|    fps              | 37       |
|    time_elapsed     | 13438    |
|    total_timesteps  | 510282   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0265   |
|    n_updates        | 102570   |
----------------------------------
Eval num_timesteps=510500, episode_reward=4.48 +/- 6.25
Episode length: 135.00 +/- 69.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.257    |
| time/               |          |
|    total_timesteps  | 510500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 102624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.56     |
|    exploration_rate | 0.256    |
| time/               |          |
|    episodes         | 4884     |
|    fps              | 37       |
|    time_elapsed     | 13454    |
|    total_timesteps  | 510693   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 102673   |
----------------------------------
Eval num_timesteps=511000, episode_reward=4.52 +/- 6.25
Episode length: 116.60 +/- 52.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.255    |
| time/               |          |
|    total_timesteps  | 511000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0907   |
|    n_updates        | 102749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.71     |
|    exploration_rate | 0.255    |
| time/               |          |
|    episodes         | 4888     |
|    fps              | 37       |
|    time_elapsed     | 13469    |
|    total_timesteps  | 511206   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0774   |
|    n_updates        | 102801   |
----------------------------------
Eval num_timesteps=511500, episode_reward=2.48 +/- 3.00
Episode length: 105.16 +/- 35.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.254    |
| time/               |          |
|    total_timesteps  | 511500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.247    |
|    n_updates        | 102874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.254    |
| time/               |          |
|    episodes         | 4892     |
|    fps              | 37       |
|    time_elapsed     | 13481    |
|    total_timesteps  | 511650   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.073    |
|    n_updates        | 102912   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.253    |
| time/               |          |
|    episodes         | 4896     |
|    fps              | 37       |
|    time_elapsed     | 13482    |
|    total_timesteps  | 511965   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 102991   |
----------------------------------
Eval num_timesteps=512000, episode_reward=4.50 +/- 6.63
Episode length: 128.18 +/- 71.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.5      |
| rollout/            |          |
|    exploration_rate | 0.253    |
| time/               |          |
|    total_timesteps  | 512000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.076    |
|    n_updates        | 102999   |
----------------------------------
Eval num_timesteps=512500, episode_reward=5.80 +/- 9.30
Episode length: 139.92 +/- 89.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 5.8      |
| rollout/            |          |
|    exploration_rate | 0.251    |
| time/               |          |
|    total_timesteps  | 512500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.224    |
|    n_updates        | 103124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.16     |
|    exploration_rate | 0.251    |
| time/               |          |
|    episodes         | 4900     |
|    fps              | 37       |
|    time_elapsed     | 13513    |
|    total_timesteps  | 512729   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.324    |
|    n_updates        | 103182   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 4904     |
|    fps              | 37       |
|    time_elapsed     | 13514    |
|    total_timesteps  | 512979   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0852   |
|    n_updates        | 103244   |
----------------------------------
Eval num_timesteps=513000, episode_reward=5.12 +/- 8.06
Episode length: 132.86 +/- 78.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 5.12     |
| rollout/            |          |
|    exploration_rate | 0.25     |
| time/               |          |
|    total_timesteps  | 513000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 103249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.82     |
|    exploration_rate | 0.249    |
| time/               |          |
|    episodes         | 4908     |
|    fps              | 37       |
|    time_elapsed     | 13530    |
|    total_timesteps  | 513349   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 103337   |
----------------------------------
Eval num_timesteps=513500, episode_reward=3.30 +/- 7.99
Episode length: 119.58 +/- 63.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.3      |
| rollout/            |          |
|    exploration_rate | 0.249    |
| time/               |          |
|    total_timesteps  | 513500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0987   |
|    n_updates        | 103374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.74     |
|    exploration_rate | 0.248    |
| time/               |          |
|    episodes         | 4912     |
|    fps              | 37       |
|    time_elapsed     | 13544    |
|    total_timesteps  | 513712   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 103427   |
----------------------------------
Eval num_timesteps=514000, episode_reward=4.60 +/- 7.86
Episode length: 117.36 +/- 57.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.247    |
| time/               |          |
|    total_timesteps  | 514000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0933   |
|    n_updates        | 103499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.69     |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 4916     |
|    fps              | 37       |
|    time_elapsed     | 13558    |
|    total_timesteps  | 514324   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 103580   |
----------------------------------
Eval num_timesteps=514500, episode_reward=3.74 +/- 6.57
Episode length: 150.14 +/- 104.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.246    |
| time/               |          |
|    total_timesteps  | 514500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 103624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.61     |
|    exploration_rate | 0.245    |
| time/               |          |
|    episodes         | 4920     |
|    fps              | 37       |
|    time_elapsed     | 13576    |
|    total_timesteps  | 514795   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0585   |
|    n_updates        | 103698   |
----------------------------------
Eval num_timesteps=515000, episode_reward=3.92 +/- 5.04
Episode length: 130.88 +/- 86.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 3.92     |
| rollout/            |          |
|    exploration_rate | 0.245    |
| time/               |          |
|    total_timesteps  | 515000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 103749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.244    |
| time/               |          |
|    episodes         | 4924     |
|    fps              | 37       |
|    time_elapsed     | 13596    |
|    total_timesteps  | 515327   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 103831   |
----------------------------------
Eval num_timesteps=515500, episode_reward=3.16 +/- 4.45
Episode length: 103.76 +/- 47.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.243    |
| time/               |          |
|    total_timesteps  | 515500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.322    |
|    n_updates        | 103874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.79     |
|    exploration_rate | 0.243    |
| time/               |          |
|    episodes         | 4928     |
|    fps              | 37       |
|    time_elapsed     | 13612    |
|    total_timesteps  | 515705   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0792   |
|    n_updates        | 103926   |
----------------------------------
Eval num_timesteps=516000, episode_reward=2.60 +/- 4.14
Episode length: 93.62 +/- 46.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.6     |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.242    |
| time/               |          |
|    total_timesteps  | 516000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 103999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.8      |
|    exploration_rate | 0.242    |
| time/               |          |
|    episodes         | 4932     |
|    fps              | 37       |
|    time_elapsed     | 13624    |
|    total_timesteps  | 516022   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 104005   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.73     |
|    exploration_rate | 0.24     |
| time/               |          |
|    episodes         | 4936     |
|    fps              | 37       |
|    time_elapsed     | 13626    |
|    total_timesteps  | 516499   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.081    |
|    n_updates        | 104124   |
----------------------------------
Eval num_timesteps=516500, episode_reward=4.96 +/- 7.43
Episode length: 116.72 +/- 58.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 4.96     |
| time/              |          |
|    total_timesteps | 516500   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.51     |
|    exploration_rate | 0.239    |
| time/               |          |
|    episodes         | 4940     |
|    fps              | 37       |
|    time_elapsed     | 13640    |
|    total_timesteps  | 516960   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0531   |
|    n_updates        | 104239   |
----------------------------------
Eval num_timesteps=517000, episode_reward=2.68 +/- 3.23
Episode length: 104.90 +/- 44.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.239    |
| time/               |          |
|    total_timesteps  | 517000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0787   |
|    n_updates        | 104249   |
----------------------------------
Eval num_timesteps=517500, episode_reward=3.14 +/- 4.39
Episode length: 98.92 +/- 39.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.238    |
| time/               |          |
|    total_timesteps  | 517500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0504   |
|    n_updates        | 104374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.237    |
| time/               |          |
|    episodes         | 4944     |
|    fps              | 37       |
|    time_elapsed     | 13664    |
|    total_timesteps  | 517693   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0756   |
|    n_updates        | 104423   |
----------------------------------
Eval num_timesteps=518000, episode_reward=3.36 +/- 7.49
Episode length: 103.94 +/- 58.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.236    |
| time/               |          |
|    total_timesteps  | 518000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0888   |
|    n_updates        | 104499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.236    |
| time/               |          |
|    episodes         | 4948     |
|    fps              | 37       |
|    time_elapsed     | 13677    |
|    total_timesteps  | 518122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 104530   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.235    |
| time/               |          |
|    episodes         | 4952     |
|    fps              | 37       |
|    time_elapsed     | 13678    |
|    total_timesteps  | 518469   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 104617   |
----------------------------------
Eval num_timesteps=518500, episode_reward=4.34 +/- 7.92
Episode length: 121.64 +/- 70.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.235    |
| time/               |          |
|    total_timesteps  | 518500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.118    |
|    n_updates        | 104624   |
----------------------------------
Eval num_timesteps=519000, episode_reward=2.94 +/- 5.27
Episode length: 114.22 +/- 47.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.234    |
| time/               |          |
|    total_timesteps  | 519000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.172    |
|    n_updates        | 104749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.234    |
| time/               |          |
|    episodes         | 4956     |
|    fps              | 37       |
|    time_elapsed     | 13707    |
|    total_timesteps  | 519011   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.198    |
|    n_updates        | 104752   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.232    |
| time/               |          |
|    episodes         | 4960     |
|    fps              | 37       |
|    time_elapsed     | 13708    |
|    total_timesteps  | 519453   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 104863   |
----------------------------------
Eval num_timesteps=519500, episode_reward=3.58 +/- 6.81
Episode length: 111.74 +/- 55.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.58     |
| rollout/            |          |
|    exploration_rate | 0.232    |
| time/               |          |
|    total_timesteps  | 519500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 104874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.231    |
| time/               |          |
|    episodes         | 4964     |
|    fps              | 37       |
|    time_elapsed     | 13723    |
|    total_timesteps  | 519996   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 104998   |
----------------------------------
Eval num_timesteps=520000, episode_reward=3.90 +/- 6.21
Episode length: 108.42 +/- 48.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.9      |
| rollout/            |          |
|    exploration_rate | 0.231    |
| time/               |          |
|    total_timesteps  | 520000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0818   |
|    n_updates        | 104999   |
----------------------------------
Eval num_timesteps=520500, episode_reward=3.34 +/- 7.63
Episode length: 103.60 +/- 61.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.23     |
| time/               |          |
|    total_timesteps  | 520500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 105124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.74     |
|    exploration_rate | 0.229    |
| time/               |          |
|    episodes         | 4968     |
|    fps              | 37       |
|    time_elapsed     | 13748    |
|    total_timesteps  | 520544   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.148    |
|    n_updates        | 105135   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.228    |
| time/               |          |
|    episodes         | 4972     |
|    fps              | 37       |
|    time_elapsed     | 13750    |
|    total_timesteps  | 520957   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 105239   |
----------------------------------
Eval num_timesteps=521000, episode_reward=4.20 +/- 6.72
Episode length: 117.74 +/- 59.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.228    |
| time/               |          |
|    total_timesteps  | 521000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0617   |
|    n_updates        | 105249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.82     |
|    exploration_rate | 0.227    |
| time/               |          |
|    episodes         | 4976     |
|    fps              | 37       |
|    time_elapsed     | 13768    |
|    total_timesteps  | 521334   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 105333   |
----------------------------------
Eval num_timesteps=521500, episode_reward=3.30 +/- 4.23
Episode length: 109.78 +/- 47.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.3      |
| rollout/            |          |
|    exploration_rate | 0.227    |
| time/               |          |
|    total_timesteps  | 521500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.094    |
|    n_updates        | 105374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 5.08     |
|    exploration_rate | 0.226    |
| time/               |          |
|    episodes         | 4980     |
|    fps              | 37       |
|    time_elapsed     | 13783    |
|    total_timesteps  | 521859   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.195    |
|    n_updates        | 105464   |
----------------------------------
Eval num_timesteps=522000, episode_reward=3.68 +/- 5.64
Episode length: 124.02 +/- 61.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.68     |
| rollout/            |          |
|    exploration_rate | 0.225    |
| time/               |          |
|    total_timesteps  | 522000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.226    |
|    n_updates        | 105499   |
----------------------------------
Eval num_timesteps=522500, episode_reward=4.34 +/- 6.70
Episode length: 108.40 +/- 49.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.224    |
| time/               |          |
|    total_timesteps  | 522500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 105624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.17     |
|    exploration_rate | 0.224    |
| time/               |          |
|    episodes         | 4984     |
|    fps              | 37       |
|    time_elapsed     | 13817    |
|    total_timesteps  | 522503   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0537   |
|    n_updates        | 105625   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.01     |
|    exploration_rate | 0.223    |
| time/               |          |
|    episodes         | 4988     |
|    fps              | 37       |
|    time_elapsed     | 13818    |
|    total_timesteps  | 522875   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 105718   |
----------------------------------
Eval num_timesteps=523000, episode_reward=3.94 +/- 6.14
Episode length: 101.50 +/- 46.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.223    |
| time/               |          |
|    total_timesteps  | 523000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0994   |
|    n_updates        | 105749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 5.38     |
|    exploration_rate | 0.221    |
| time/               |          |
|    episodes         | 4992     |
|    fps              | 37       |
|    time_elapsed     | 13832    |
|    total_timesteps  | 523469   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.095    |
|    n_updates        | 105867   |
----------------------------------
Eval num_timesteps=523500, episode_reward=3.44 +/- 5.53
Episode length: 106.84 +/- 55.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.221    |
| time/               |          |
|    total_timesteps  | 523500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 105874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.41     |
|    exploration_rate | 0.22     |
| time/               |          |
|    episodes         | 4996     |
|    fps              | 37       |
|    time_elapsed     | 13847    |
|    total_timesteps  | 523855   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 105963   |
----------------------------------
Eval num_timesteps=524000, episode_reward=3.06 +/- 4.07
Episode length: 108.50 +/- 79.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.22     |
| time/               |          |
|    total_timesteps  | 524000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0865   |
|    n_updates        | 105999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 5.01     |
|    exploration_rate | 0.219    |
| time/               |          |
|    episodes         | 5000     |
|    fps              | 37       |
|    time_elapsed     | 13860    |
|    total_timesteps  | 524230   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 106057   |
----------------------------------
Eval num_timesteps=524500, episode_reward=4.38 +/- 6.09
Episode length: 119.26 +/- 63.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.38     |
| rollout/            |          |
|    exploration_rate | 0.219    |
| time/               |          |
|    total_timesteps  | 524500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 106124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.14     |
|    exploration_rate | 0.218    |
| time/               |          |
|    episodes         | 5004     |
|    fps              | 37       |
|    time_elapsed     | 13875    |
|    total_timesteps  | 524683   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.173    |
|    n_updates        | 106170   |
----------------------------------
Eval num_timesteps=525000, episode_reward=2.90 +/- 6.61
Episode length: 125.40 +/- 86.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.217    |
| time/               |          |
|    total_timesteps  | 525000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0638   |
|    n_updates        | 106249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.14     |
|    exploration_rate | 0.217    |
| time/               |          |
|    episodes         | 5008     |
|    fps              | 37       |
|    time_elapsed     | 13889    |
|    total_timesteps  | 525012   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.227    |
|    n_updates        | 106252   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 5.13     |
|    exploration_rate | 0.216    |
| time/               |          |
|    episodes         | 5012     |
|    fps              | 37       |
|    time_elapsed     | 13891    |
|    total_timesteps  | 525373   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.118    |
|    n_updates        | 106343   |
----------------------------------
Eval num_timesteps=525500, episode_reward=2.80 +/- 4.18
Episode length: 102.50 +/- 47.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.216    |
| time/               |          |
|    total_timesteps  | 525500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0745   |
|    n_updates        | 106374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.79     |
|    exploration_rate | 0.215    |
| time/               |          |
|    episodes         | 5016     |
|    fps              | 37       |
|    time_elapsed     | 13903    |
|    total_timesteps  | 525767   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 106441   |
----------------------------------
Eval num_timesteps=526000, episode_reward=3.50 +/- 5.03
Episode length: 109.40 +/- 49.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.214    |
| time/               |          |
|    total_timesteps  | 526000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.178    |
|    n_updates        | 106499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.213    |
| time/               |          |
|    episodes         | 5020     |
|    fps              | 37       |
|    time_elapsed     | 13917    |
|    total_timesteps  | 526330   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0446   |
|    n_updates        | 106582   |
----------------------------------
Eval num_timesteps=526500, episode_reward=4.82 +/- 5.68
Episode length: 133.20 +/- 69.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.82     |
| rollout/            |          |
|    exploration_rate | 0.213    |
| time/               |          |
|    total_timesteps  | 526500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0642   |
|    n_updates        | 106624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.68     |
|    exploration_rate | 0.212    |
| time/               |          |
|    episodes         | 5024     |
|    fps              | 37       |
|    time_elapsed     | 13934    |
|    total_timesteps  | 526781   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0617   |
|    n_updates        | 106695   |
----------------------------------
Eval num_timesteps=527000, episode_reward=4.02 +/- 8.13
Episode length: 104.02 +/- 53.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.212    |
| time/               |          |
|    total_timesteps  | 527000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 106749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.74     |
|    exploration_rate | 0.211    |
| time/               |          |
|    episodes         | 5028     |
|    fps              | 37       |
|    time_elapsed     | 13948    |
|    total_timesteps  | 527189   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0574   |
|    n_updates        | 106797   |
----------------------------------
Eval num_timesteps=527500, episode_reward=5.90 +/- 8.84
Episode length: 146.74 +/- 78.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 5.9      |
| rollout/            |          |
|    exploration_rate | 0.21     |
| time/               |          |
|    total_timesteps  | 527500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 106874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.69     |
|    exploration_rate | 0.21     |
| time/               |          |
|    episodes         | 5032     |
|    fps              | 37       |
|    time_elapsed     | 13967    |
|    total_timesteps  | 527697   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0506   |
|    n_updates        | 106924   |
----------------------------------
Eval num_timesteps=528000, episode_reward=3.34 +/- 3.95
Episode length: 131.42 +/- 60.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.209    |
| time/               |          |
|    total_timesteps  | 528000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.191    |
|    n_updates        | 106999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.71     |
|    exploration_rate | 0.209    |
| time/               |          |
|    episodes         | 5036     |
|    fps              | 37       |
|    time_elapsed     | 13985    |
|    total_timesteps  | 528078   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0422   |
|    n_updates        | 107019   |
----------------------------------
Eval num_timesteps=528500, episode_reward=5.52 +/- 7.48
Episode length: 133.70 +/- 85.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.52     |
| rollout/            |          |
|    exploration_rate | 0.207    |
| time/               |          |
|    total_timesteps  | 528500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 107124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.207    |
| time/               |          |
|    episodes         | 5040     |
|    fps              | 37       |
|    time_elapsed     | 14001    |
|    total_timesteps  | 528616   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0629   |
|    n_updates        | 107153   |
----------------------------------
Eval num_timesteps=529000, episode_reward=4.42 +/- 5.88
Episode length: 113.44 +/- 50.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.206    |
| time/               |          |
|    total_timesteps  | 529000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0372   |
|    n_updates        | 107249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.56     |
|    exploration_rate | 0.206    |
| time/               |          |
|    episodes         | 5044     |
|    fps              | 37       |
|    time_elapsed     | 14020    |
|    total_timesteps  | 529114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.191    |
|    n_updates        | 107278   |
----------------------------------
Eval num_timesteps=529500, episode_reward=3.70 +/- 5.43
Episode length: 120.00 +/- 57.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.205    |
| time/               |          |
|    total_timesteps  | 529500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 107374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.74     |
|    exploration_rate | 0.204    |
| time/               |          |
|    episodes         | 5048     |
|    fps              | 37       |
|    time_elapsed     | 14035    |
|    total_timesteps  | 529588   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0581   |
|    n_updates        | 107396   |
----------------------------------
Eval num_timesteps=530000, episode_reward=2.58 +/- 3.17
Episode length: 92.62 +/- 30.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 2.58     |
| rollout/            |          |
|    exploration_rate | 0.203    |
| time/               |          |
|    total_timesteps  | 530000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0979   |
|    n_updates        | 107499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.69     |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 5052     |
|    fps              | 37       |
|    time_elapsed     | 14048    |
|    total_timesteps  | 530010   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.262    |
|    n_updates        | 107502   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.202    |
| time/               |          |
|    episodes         | 5056     |
|    fps              | 37       |
|    time_elapsed     | 14049    |
|    total_timesteps  | 530386   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0425   |
|    n_updates        | 107596   |
----------------------------------
Eval num_timesteps=530500, episode_reward=4.02 +/- 6.64
Episode length: 126.64 +/- 62.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.202    |
| time/               |          |
|    total_timesteps  | 530500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.232    |
|    n_updates        | 107624   |
----------------------------------
Eval num_timesteps=531000, episode_reward=2.46 +/- 3.26
Episode length: 113.12 +/- 43.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.2      |
| time/               |          |
|    total_timesteps  | 531000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.147    |
|    n_updates        | 107749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.4      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 5060     |
|    fps              | 37       |
|    time_elapsed     | 14077    |
|    total_timesteps  | 531011   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 107752   |
----------------------------------
Eval num_timesteps=531500, episode_reward=5.44 +/- 7.25
Episode length: 168.76 +/- 120.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 169      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.199    |
| time/               |          |
|    total_timesteps  | 531500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 107874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.199    |
| time/               |          |
|    episodes         | 5064     |
|    fps              | 37       |
|    time_elapsed     | 14097    |
|    total_timesteps  | 531547   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0726   |
|    n_updates        | 107886   |
----------------------------------
Eval num_timesteps=532000, episode_reward=7.52 +/- 10.21
Episode length: 197.78 +/- 129.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | 7.52     |
| rollout/            |          |
|    exploration_rate | 0.198    |
| time/               |          |
|    total_timesteps  | 532000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0383   |
|    n_updates        | 107999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.197    |
| time/               |          |
|    episodes         | 5068     |
|    fps              | 37       |
|    time_elapsed     | 14122    |
|    total_timesteps  | 532205   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 108051   |
----------------------------------
Eval num_timesteps=532500, episode_reward=4.26 +/- 7.86
Episode length: 136.14 +/- 93.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 4.26     |
| rollout/            |          |
|    exploration_rate | 0.196    |
| time/               |          |
|    total_timesteps  | 532500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0292   |
|    n_updates        | 108124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.16     |
|    exploration_rate | 0.196    |
| time/               |          |
|    episodes         | 5072     |
|    fps              | 37       |
|    time_elapsed     | 14138    |
|    total_timesteps  | 532701   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.224    |
|    n_updates        | 108175   |
----------------------------------
Eval num_timesteps=533000, episode_reward=5.08 +/- 7.55
Episode length: 172.52 +/- 117.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 173      |
|    mean_reward      | 5.08     |
| rollout/            |          |
|    exploration_rate | 0.195    |
| time/               |          |
|    total_timesteps  | 533000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.218    |
|    n_updates        | 108249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.195    |
| time/               |          |
|    episodes         | 5076     |
|    fps              | 37       |
|    time_elapsed     | 14158    |
|    total_timesteps  | 533143   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.214    |
|    n_updates        | 108285   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.194    |
| time/               |          |
|    episodes         | 5080     |
|    fps              | 37       |
|    time_elapsed     | 14159    |
|    total_timesteps  | 533381   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.233    |
|    n_updates        | 108345   |
----------------------------------
Eval num_timesteps=533500, episode_reward=5.84 +/- 11.06
Episode length: 120.24 +/- 66.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 5.84     |
| rollout/            |          |
|    exploration_rate | 0.194    |
| time/               |          |
|    total_timesteps  | 533500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 108374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.193    |
| time/               |          |
|    episodes         | 5084     |
|    fps              | 37       |
|    time_elapsed     | 14174    |
|    total_timesteps  | 533852   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 108462   |
----------------------------------
Eval num_timesteps=534000, episode_reward=4.68 +/- 8.08
Episode length: 155.76 +/- 104.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.192    |
| time/               |          |
|    total_timesteps  | 534000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0493   |
|    n_updates        | 108499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.191    |
| time/               |          |
|    episodes         | 5088     |
|    fps              | 37       |
|    time_elapsed     | 14193    |
|    total_timesteps  | 534453   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 108613   |
----------------------------------
Eval num_timesteps=534500, episode_reward=2.30 +/- 3.23
Episode length: 94.50 +/- 32.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.5     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.191    |
| time/               |          |
|    total_timesteps  | 534500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0458   |
|    n_updates        | 108624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.19     |
| time/               |          |
|    episodes         | 5092     |
|    fps              | 37       |
|    time_elapsed     | 14204    |
|    total_timesteps  | 534809   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0639   |
|    n_updates        | 108702   |
----------------------------------
Eval num_timesteps=535000, episode_reward=5.68 +/- 11.04
Episode length: 140.92 +/- 80.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 5.68     |
| rollout/            |          |
|    exploration_rate | 0.189    |
| time/               |          |
|    total_timesteps  | 535000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.251    |
|    n_updates        | 108749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.62     |
|    exploration_rate | 0.189    |
| time/               |          |
|    episodes         | 5096     |
|    fps              | 37       |
|    time_elapsed     | 14225    |
|    total_timesteps  | 535213   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 108803   |
----------------------------------
Eval num_timesteps=535500, episode_reward=4.18 +/- 7.36
Episode length: 128.72 +/- 89.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.188    |
| time/               |          |
|    total_timesteps  | 535500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0712   |
|    n_updates        | 108874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.187    |
| time/               |          |
|    episodes         | 5100     |
|    fps              | 37       |
|    time_elapsed     | 14245    |
|    total_timesteps  | 535791   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 108947   |
----------------------------------
Eval num_timesteps=536000, episode_reward=4.24 +/- 5.72
Episode length: 122.90 +/- 59.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.187    |
| time/               |          |
|    total_timesteps  | 536000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 108999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.78     |
|    exploration_rate | 0.185    |
| time/               |          |
|    episodes         | 5104     |
|    fps              | 37       |
|    time_elapsed     | 14262    |
|    total_timesteps  | 536378   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 109094   |
----------------------------------
Eval num_timesteps=536500, episode_reward=4.82 +/- 6.73
Episode length: 143.54 +/- 109.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 4.82     |
| rollout/            |          |
|    exploration_rate | 0.185    |
| time/               |          |
|    total_timesteps  | 536500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0855   |
|    n_updates        | 109124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.185    |
| time/               |          |
|    episodes         | 5108     |
|    fps              | 37       |
|    time_elapsed     | 14284    |
|    total_timesteps  | 536668   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0611   |
|    n_updates        | 109166   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.184    |
| time/               |          |
|    episodes         | 5112     |
|    fps              | 37       |
|    time_elapsed     | 14286    |
|    total_timesteps  | 536965   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0603   |
|    n_updates        | 109241   |
----------------------------------
Eval num_timesteps=537000, episode_reward=3.16 +/- 5.07
Episode length: 99.46 +/- 37.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.5     |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.184    |
| time/               |          |
|    total_timesteps  | 537000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 109249   |
----------------------------------
Eval num_timesteps=537500, episode_reward=3.32 +/- 6.08
Episode length: 151.90 +/- 100.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.182    |
| time/               |          |
|    total_timesteps  | 537500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0321   |
|    n_updates        | 109374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.182    |
| time/               |          |
|    episodes         | 5116     |
|    fps              | 37       |
|    time_elapsed     | 14322    |
|    total_timesteps  | 537569   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.236    |
|    n_updates        | 109392   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.181    |
| time/               |          |
|    episodes         | 5120     |
|    fps              | 37       |
|    time_elapsed     | 14324    |
|    total_timesteps  | 537931   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0549   |
|    n_updates        | 109482   |
----------------------------------
Eval num_timesteps=538000, episode_reward=3.22 +/- 3.80
Episode length: 127.42 +/- 85.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.181    |
| time/               |          |
|    total_timesteps  | 538000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.049    |
|    n_updates        | 109499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.18     |
| time/               |          |
|    episodes         | 5124     |
|    fps              | 37       |
|    time_elapsed     | 14340    |
|    total_timesteps  | 538369   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 109592   |
----------------------------------
Eval num_timesteps=538500, episode_reward=7.34 +/- 11.44
Episode length: 162.80 +/- 128.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 163      |
|    mean_reward      | 7.34     |
| rollout/            |          |
|    exploration_rate | 0.18     |
| time/               |          |
|    total_timesteps  | 538500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 109624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.71     |
|    exploration_rate | 0.178    |
| time/               |          |
|    episodes         | 5128     |
|    fps              | 37       |
|    time_elapsed     | 14363    |
|    total_timesteps  | 538914   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 109728   |
----------------------------------
Eval num_timesteps=539000, episode_reward=3.70 +/- 8.66
Episode length: 129.64 +/- 117.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 539000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0475   |
|    n_updates        | 109749   |
----------------------------------
Eval num_timesteps=539500, episode_reward=5.28 +/- 8.22
Episode length: 120.76 +/- 79.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 5.28     |
| rollout/            |          |
|    exploration_rate | 0.177    |
| time/               |          |
|    total_timesteps  | 539500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.274    |
|    n_updates        | 109874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.177    |
| time/               |          |
|    episodes         | 5132     |
|    fps              | 37       |
|    time_elapsed     | 14392    |
|    total_timesteps  | 539540   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0497   |
|    n_updates        | 109884   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.175    |
| time/               |          |
|    episodes         | 5136     |
|    fps              | 37       |
|    time_elapsed     | 14394    |
|    total_timesteps  | 539942   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 109985   |
----------------------------------
Eval num_timesteps=540000, episode_reward=3.18 +/- 4.99
Episode length: 99.14 +/- 48.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.1     |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.175    |
| time/               |          |
|    total_timesteps  | 540000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.182    |
|    n_updates        | 109999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.174    |
| time/               |          |
|    episodes         | 5140     |
|    fps              | 37       |
|    time_elapsed     | 14406    |
|    total_timesteps  | 540325   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0772   |
|    n_updates        | 110081   |
----------------------------------
Eval num_timesteps=540500, episode_reward=3.60 +/- 5.71
Episode length: 114.30 +/- 87.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.174    |
| time/               |          |
|    total_timesteps  | 540500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 110124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.174    |
| time/               |          |
|    episodes         | 5144     |
|    fps              | 37       |
|    time_elapsed     | 14419    |
|    total_timesteps  | 540613   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 110153   |
----------------------------------
Eval num_timesteps=541000, episode_reward=3.94 +/- 6.45
Episode length: 128.74 +/- 91.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.173    |
| time/               |          |
|    total_timesteps  | 541000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0675   |
|    n_updates        | 110249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.172    |
| time/               |          |
|    episodes         | 5148     |
|    fps              | 37       |
|    time_elapsed     | 14435    |
|    total_timesteps  | 541302   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0833   |
|    n_updates        | 110325   |
----------------------------------
Eval num_timesteps=541500, episode_reward=2.58 +/- 5.32
Episode length: 103.80 +/- 74.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.58     |
| rollout/            |          |
|    exploration_rate | 0.171    |
| time/               |          |
|    total_timesteps  | 541500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0622   |
|    n_updates        | 110374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.83     |
|    exploration_rate | 0.171    |
| time/               |          |
|    episodes         | 5152     |
|    fps              | 37       |
|    time_elapsed     | 14447    |
|    total_timesteps  | 541661   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.172    |
|    n_updates        | 110415   |
----------------------------------
Eval num_timesteps=542000, episode_reward=4.92 +/- 8.88
Episode length: 166.00 +/- 127.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 4.92     |
| rollout/            |          |
|    exploration_rate | 0.17     |
| time/               |          |
|    total_timesteps  | 542000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0793   |
|    n_updates        | 110499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.169    |
| time/               |          |
|    episodes         | 5156     |
|    fps              | 37       |
|    time_elapsed     | 14467    |
|    total_timesteps  | 542223   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.194    |
|    n_updates        | 110555   |
----------------------------------
Eval num_timesteps=542500, episode_reward=2.84 +/- 4.85
Episode length: 127.62 +/- 72.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.168    |
| time/               |          |
|    total_timesteps  | 542500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0213   |
|    n_updates        | 110624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.78     |
|    exploration_rate | 0.168    |
| time/               |          |
|    episodes         | 5160     |
|    fps              | 37       |
|    time_elapsed     | 14483    |
|    total_timesteps  | 542654   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0997   |
|    n_updates        | 110663   |
----------------------------------
Eval num_timesteps=543000, episode_reward=4.32 +/- 8.92
Episode length: 145.54 +/- 102.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.167    |
| time/               |          |
|    total_timesteps  | 543000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0456   |
|    n_updates        | 110749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.166    |
| time/               |          |
|    episodes         | 5164     |
|    fps              | 37       |
|    time_elapsed     | 14500    |
|    total_timesteps  | 543309   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0785   |
|    n_updates        | 110827   |
----------------------------------
Eval num_timesteps=543500, episode_reward=4.04 +/- 5.22
Episode length: 171.42 +/- 122.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 171      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.165    |
| time/               |          |
|    total_timesteps  | 543500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.142    |
|    n_updates        | 110874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.165    |
| time/               |          |
|    episodes         | 5168     |
|    fps              | 37       |
|    time_elapsed     | 14520    |
|    total_timesteps  | 543661   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0881   |
|    n_updates        | 110915   |
----------------------------------
Eval num_timesteps=544000, episode_reward=3.34 +/- 5.35
Episode length: 108.82 +/- 75.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.164    |
| time/               |          |
|    total_timesteps  | 544000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0777   |
|    n_updates        | 110999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.164    |
| time/               |          |
|    episodes         | 5172     |
|    fps              | 37       |
|    time_elapsed     | 14534    |
|    total_timesteps  | 544186   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 111046   |
----------------------------------
Eval num_timesteps=544500, episode_reward=4.86 +/- 9.27
Episode length: 140.84 +/- 107.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 4.86     |
| rollout/            |          |
|    exploration_rate | 0.163    |
| time/               |          |
|    total_timesteps  | 544500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0668   |
|    n_updates        | 111124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.162    |
| time/               |          |
|    episodes         | 5176     |
|    fps              | 37       |
|    time_elapsed     | 14551    |
|    total_timesteps  | 544642   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 111160   |
----------------------------------
Eval num_timesteps=545000, episode_reward=4.36 +/- 7.63
Episode length: 130.24 +/- 101.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.161    |
| time/               |          |
|    total_timesteps  | 545000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 111249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.161    |
| time/               |          |
|    episodes         | 5180     |
|    fps              | 37       |
|    time_elapsed     | 14566    |
|    total_timesteps  | 545029   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0516   |
|    n_updates        | 111257   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.16     |
| time/               |          |
|    episodes         | 5184     |
|    fps              | 37       |
|    time_elapsed     | 14568    |
|    total_timesteps  | 545400   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 111349   |
----------------------------------
Eval num_timesteps=545500, episode_reward=5.16 +/- 8.44
Episode length: 156.94 +/- 116.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 157      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.16     |
| time/               |          |
|    total_timesteps  | 545500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 111374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.71     |
|    exploration_rate | 0.159    |
| time/               |          |
|    episodes         | 5188     |
|    fps              | 37       |
|    time_elapsed     | 14585    |
|    total_timesteps  | 545691   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0854   |
|    n_updates        | 111422   |
----------------------------------
Eval num_timesteps=546000, episode_reward=3.28 +/- 4.29
Episode length: 146.90 +/- 110.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.158    |
| time/               |          |
|    total_timesteps  | 546000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.209    |
|    n_updates        | 111499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.83     |
|    exploration_rate | 0.157    |
| time/               |          |
|    episodes         | 5192     |
|    fps              | 37       |
|    time_elapsed     | 14603    |
|    total_timesteps  | 546423   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0175   |
|    n_updates        | 111605   |
----------------------------------
Eval num_timesteps=546500, episode_reward=6.64 +/- 11.37
Episode length: 156.08 +/- 101.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 6.64     |
| rollout/            |          |
|    exploration_rate | 0.157    |
| time/               |          |
|    total_timesteps  | 546500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0639   |
|    n_updates        | 111624   |
----------------------------------
Eval num_timesteps=547000, episode_reward=3.74 +/- 4.84
Episode length: 136.16 +/- 77.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.156    |
| time/               |          |
|    total_timesteps  | 547000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.309    |
|    n_updates        | 111749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.155    |
| time/               |          |
|    episodes         | 5196     |
|    fps              | 37       |
|    time_elapsed     | 14638    |
|    total_timesteps  | 547190   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0513   |
|    n_updates        | 111797   |
----------------------------------
Eval num_timesteps=547500, episode_reward=3.52 +/- 6.08
Episode length: 104.40 +/- 55.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.154    |
| time/               |          |
|    total_timesteps  | 547500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 111874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.69     |
|    exploration_rate | 0.153    |
| time/               |          |
|    episodes         | 5200     |
|    fps              | 37       |
|    time_elapsed     | 14654    |
|    total_timesteps  | 547857   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0632   |
|    n_updates        | 111964   |
----------------------------------
Eval num_timesteps=548000, episode_reward=5.52 +/- 7.57
Episode length: 150.28 +/- 88.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 5.52     |
| rollout/            |          |
|    exploration_rate | 0.153    |
| time/               |          |
|    total_timesteps  | 548000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 111999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.152    |
| time/               |          |
|    episodes         | 5204     |
|    fps              | 37       |
|    time_elapsed     | 14672    |
|    total_timesteps  | 548266   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.074    |
|    n_updates        | 112066   |
----------------------------------
Eval num_timesteps=548500, episode_reward=4.80 +/- 9.90
Episode length: 186.86 +/- 138.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 187      |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.151    |
| time/               |          |
|    total_timesteps  | 548500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0935   |
|    n_updates        | 112124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.67     |
|    exploration_rate | 0.151    |
| time/               |          |
|    episodes         | 5208     |
|    fps              | 37       |
|    time_elapsed     | 14699    |
|    total_timesteps  | 548652   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0625   |
|    n_updates        | 112162   |
----------------------------------
Eval num_timesteps=549000, episode_reward=2.40 +/- 3.21
Episode length: 107.78 +/- 53.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.15     |
| time/               |          |
|    total_timesteps  | 549000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.222    |
|    n_updates        | 112249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.3      |
|    exploration_rate | 0.149    |
| time/               |          |
|    episodes         | 5212     |
|    fps              | 37       |
|    time_elapsed     | 14717    |
|    total_timesteps  | 549273   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0464   |
|    n_updates        | 112318   |
----------------------------------
Eval num_timesteps=549500, episode_reward=5.66 +/- 8.60
Episode length: 169.22 +/- 119.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 169      |
|    mean_reward      | 5.66     |
| rollout/            |          |
|    exploration_rate | 0.148    |
| time/               |          |
|    total_timesteps  | 549500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 112374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 5.18     |
|    exploration_rate | 0.148    |
| time/               |          |
|    episodes         | 5216     |
|    fps              | 37       |
|    time_elapsed     | 14740    |
|    total_timesteps  | 549712   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 112427   |
----------------------------------
Eval num_timesteps=550000, episode_reward=6.22 +/- 13.16
Episode length: 130.96 +/- 92.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 6.22     |
| rollout/            |          |
|    exploration_rate | 0.147    |
| time/               |          |
|    total_timesteps  | 550000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.147    |
|    n_updates        | 112499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.47     |
|    exploration_rate | 0.146    |
| time/               |          |
|    episodes         | 5220     |
|    fps              | 37       |
|    time_elapsed     | 14757    |
|    total_timesteps  | 550329   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 112582   |
----------------------------------
Eval num_timesteps=550500, episode_reward=4.32 +/- 7.30
Episode length: 127.16 +/- 87.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.146    |
| time/               |          |
|    total_timesteps  | 550500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0308   |
|    n_updates        | 112624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.42     |
|    exploration_rate | 0.145    |
| time/               |          |
|    episodes         | 5224     |
|    fps              | 37       |
|    time_elapsed     | 14772    |
|    total_timesteps  | 550802   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0777   |
|    n_updates        | 112700   |
----------------------------------
Eval num_timesteps=551000, episode_reward=3.72 +/- 6.56
Episode length: 120.18 +/- 65.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.72     |
| rollout/            |          |
|    exploration_rate | 0.144    |
| time/               |          |
|    total_timesteps  | 551000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0509   |
|    n_updates        | 112749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.61     |
|    exploration_rate | 0.143    |
| time/               |          |
|    episodes         | 5228     |
|    fps              | 37       |
|    time_elapsed     | 14787    |
|    total_timesteps  | 551306   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0884   |
|    n_updates        | 112826   |
----------------------------------
Eval num_timesteps=551500, episode_reward=2.88 +/- 4.09
Episode length: 103.72 +/- 50.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.143    |
| time/               |          |
|    total_timesteps  | 551500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0649   |
|    n_updates        | 112874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.56     |
|    exploration_rate | 0.142    |
| time/               |          |
|    episodes         | 5232     |
|    fps              | 37       |
|    time_elapsed     | 14801    |
|    total_timesteps  | 551862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 112965   |
----------------------------------
Eval num_timesteps=552000, episode_reward=4.04 +/- 6.32
Episode length: 132.44 +/- 93.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.141    |
| time/               |          |
|    total_timesteps  | 552000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0899   |
|    n_updates        | 112999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.68     |
|    exploration_rate | 0.14     |
| time/               |          |
|    episodes         | 5236     |
|    fps              | 37       |
|    time_elapsed     | 14817    |
|    total_timesteps  | 552492   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0944   |
|    n_updates        | 113122   |
----------------------------------
Eval num_timesteps=552500, episode_reward=2.64 +/- 3.63
Episode length: 91.10 +/- 37.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.1     |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.14     |
| time/               |          |
|    total_timesteps  | 552500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 113124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.67     |
|    exploration_rate | 0.139    |
| time/               |          |
|    episodes         | 5240     |
|    fps              | 37       |
|    time_elapsed     | 14828    |
|    total_timesteps  | 552798   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0453   |
|    n_updates        | 113199   |
----------------------------------
Eval num_timesteps=553000, episode_reward=4.32 +/- 7.04
Episode length: 124.88 +/- 58.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.138    |
| time/               |          |
|    total_timesteps  | 553000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0794   |
|    n_updates        | 113249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.68     |
|    exploration_rate | 0.138    |
| time/               |          |
|    episodes         | 5244     |
|    fps              | 37       |
|    time_elapsed     | 14843    |
|    total_timesteps  | 553286   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.283    |
|    n_updates        | 113321   |
----------------------------------
Eval num_timesteps=553500, episode_reward=6.22 +/- 10.50
Episode length: 134.76 +/- 90.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 6.22     |
| rollout/            |          |
|    exploration_rate | 0.137    |
| time/               |          |
|    total_timesteps  | 553500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0421   |
|    n_updates        | 113374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.16     |
|    exploration_rate | 0.137    |
| time/               |          |
|    episodes         | 5248     |
|    fps              | 37       |
|    time_elapsed     | 14859    |
|    total_timesteps  | 553556   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0744   |
|    n_updates        | 113388   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 5.22     |
|    exploration_rate | 0.136    |
| time/               |          |
|    episodes         | 5252     |
|    fps              | 37       |
|    time_elapsed     | 14860    |
|    total_timesteps  | 553879   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0829   |
|    n_updates        | 113469   |
----------------------------------
Eval num_timesteps=554000, episode_reward=2.72 +/- 5.54
Episode length: 119.76 +/- 85.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.136    |
| time/               |          |
|    total_timesteps  | 554000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 113499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.22     |
|    exploration_rate | 0.135    |
| time/               |          |
|    episodes         | 5256     |
|    fps              | 37       |
|    time_elapsed     | 14875    |
|    total_timesteps  | 554242   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 113560   |
----------------------------------
Eval num_timesteps=554500, episode_reward=4.22 +/- 7.01
Episode length: 124.14 +/- 90.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.22     |
| rollout/            |          |
|    exploration_rate | 0.134    |
| time/               |          |
|    total_timesteps  | 554500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 113624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 5.13     |
|    exploration_rate | 0.133    |
| time/               |          |
|    episodes         | 5260     |
|    fps              | 37       |
|    time_elapsed     | 14893    |
|    total_timesteps  | 554758   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.069    |
|    n_updates        | 113689   |
----------------------------------
Eval num_timesteps=555000, episode_reward=4.46 +/- 7.69
Episode length: 137.12 +/- 85.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 4.46     |
| rollout/            |          |
|    exploration_rate | 0.133    |
| time/               |          |
|    total_timesteps  | 555000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.237    |
|    n_updates        | 113749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 5.06     |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 5264     |
|    fps              | 37       |
|    time_elapsed     | 14910    |
|    total_timesteps  | 555252   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.215    |
|    n_updates        | 113812   |
----------------------------------
Eval num_timesteps=555500, episode_reward=5.40 +/- 9.33
Episode length: 136.16 +/- 96.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 5.4      |
| rollout/            |          |
|    exploration_rate | 0.131    |
| time/               |          |
|    total_timesteps  | 555500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0832   |
|    n_updates        | 113874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 5.22     |
|    exploration_rate | 0.13     |
| time/               |          |
|    episodes         | 5268     |
|    fps              | 37       |
|    time_elapsed     | 14928    |
|    total_timesteps  | 555883   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0939   |
|    n_updates        | 113970   |
----------------------------------
Eval num_timesteps=556000, episode_reward=3.80 +/- 6.80
Episode length: 140.40 +/- 113.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.13     |
| time/               |          |
|    total_timesteps  | 556000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0748   |
|    n_updates        | 113999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.129    |
| time/               |          |
|    episodes         | 5272     |
|    fps              | 37       |
|    time_elapsed     | 14947    |
|    total_timesteps  | 556244   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.073    |
|    n_updates        | 114060   |
----------------------------------
Eval num_timesteps=556500, episode_reward=2.46 +/- 3.67
Episode length: 106.48 +/- 44.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.128    |
| time/               |          |
|    total_timesteps  | 556500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 114124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.84     |
|    exploration_rate | 0.128    |
| time/               |          |
|    episodes         | 5276     |
|    fps              | 37       |
|    time_elapsed     | 14960    |
|    total_timesteps  | 556652   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.068    |
|    n_updates        | 114162   |
----------------------------------
Eval num_timesteps=557000, episode_reward=2.80 +/- 3.22
Episode length: 90.08 +/- 32.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.1     |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.127    |
| time/               |          |
|    total_timesteps  | 557000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 114249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.92     |
|    exploration_rate | 0.126    |
| time/               |          |
|    episodes         | 5280     |
|    fps              | 37       |
|    time_elapsed     | 14972    |
|    total_timesteps  | 557203   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 114300   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.126    |
| time/               |          |
|    episodes         | 5284     |
|    fps              | 37       |
|    time_elapsed     | 14973    |
|    total_timesteps  | 557498   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0378   |
|    n_updates        | 114374   |
----------------------------------
Eval num_timesteps=557500, episode_reward=4.06 +/- 7.09
Episode length: 104.72 +/- 63.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 4.06     |
| rollout/            |          |
|    exploration_rate | 0.126    |
| time/               |          |
|    total_timesteps  | 557500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 5.03     |
|    exploration_rate | 0.125    |
| time/               |          |
|    episodes         | 5288     |
|    fps              | 37       |
|    time_elapsed     | 14986    |
|    total_timesteps  | 557885   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0733   |
|    n_updates        | 114471   |
----------------------------------
Eval num_timesteps=558000, episode_reward=3.94 +/- 6.98
Episode length: 120.46 +/- 85.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.124    |
| time/               |          |
|    total_timesteps  | 558000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 114499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.07     |
|    exploration_rate | 0.123    |
| time/               |          |
|    episodes         | 5292     |
|    fps              | 37       |
|    time_elapsed     | 15001    |
|    total_timesteps  | 558452   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 114612   |
----------------------------------
Eval num_timesteps=558500, episode_reward=3.34 +/- 5.74
Episode length: 123.36 +/- 75.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.123    |
| time/               |          |
|    total_timesteps  | 558500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.2      |
|    n_updates        | 114624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.43     |
|    exploration_rate | 0.122    |
| time/               |          |
|    episodes         | 5296     |
|    fps              | 37       |
|    time_elapsed     | 15016    |
|    total_timesteps  | 558899   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0759   |
|    n_updates        | 114724   |
----------------------------------
Eval num_timesteps=559000, episode_reward=2.78 +/- 4.36
Episode length: 112.86 +/- 46.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.121    |
| time/               |          |
|    total_timesteps  | 559000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 114749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.121    |
| time/               |          |
|    episodes         | 5300     |
|    fps              | 37       |
|    time_elapsed     | 15030    |
|    total_timesteps  | 559261   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0893   |
|    n_updates        | 114815   |
----------------------------------
Eval num_timesteps=559500, episode_reward=5.34 +/- 10.44
Episode length: 136.68 +/- 79.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.12     |
| time/               |          |
|    total_timesteps  | 559500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0558   |
|    n_updates        | 114874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.82     |
|    exploration_rate | 0.119    |
| time/               |          |
|    episodes         | 5304     |
|    fps              | 37       |
|    time_elapsed     | 15046    |
|    total_timesteps  | 559649   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 114912   |
----------------------------------
Eval num_timesteps=560000, episode_reward=3.72 +/- 5.38
Episode length: 118.18 +/- 63.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.72     |
| rollout/            |          |
|    exploration_rate | 0.118    |
| time/               |          |
|    total_timesteps  | 560000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0596   |
|    n_updates        | 114999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.118    |
| time/               |          |
|    episodes         | 5308     |
|    fps              | 37       |
|    time_elapsed     | 15060    |
|    total_timesteps  | 560054   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0578   |
|    n_updates        | 115013   |
----------------------------------
Eval num_timesteps=560500, episode_reward=4.12 +/- 6.55
Episode length: 108.80 +/- 54.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 4.12     |
| rollout/            |          |
|    exploration_rate | 0.117    |
| time/               |          |
|    total_timesteps  | 560500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.174    |
|    n_updates        | 115124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.117    |
| time/               |          |
|    episodes         | 5312     |
|    fps              | 37       |
|    time_elapsed     | 15074    |
|    total_timesteps  | 560513   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0899   |
|    n_updates        | 115128   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.116    |
| time/               |          |
|    episodes         | 5316     |
|    fps              | 37       |
|    time_elapsed     | 15076    |
|    total_timesteps  | 560959   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.063    |
|    n_updates        | 115239   |
----------------------------------
Eval num_timesteps=561000, episode_reward=4.40 +/- 6.69
Episode length: 105.62 +/- 46.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.116    |
| time/               |          |
|    total_timesteps  | 561000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 115249   |
----------------------------------
Eval num_timesteps=561500, episode_reward=3.82 +/- 9.37
Episode length: 116.42 +/- 69.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.82     |
| rollout/            |          |
|    exploration_rate | 0.114    |
| time/               |          |
|    total_timesteps  | 561500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0978   |
|    n_updates        | 115374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 0.114    |
| time/               |          |
|    episodes         | 5320     |
|    fps              | 37       |
|    time_elapsed     | 15102    |
|    total_timesteps  | 561535   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 115383   |
----------------------------------
Eval num_timesteps=562000, episode_reward=4.18 +/- 7.53
Episode length: 122.84 +/- 62.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.113    |
| time/               |          |
|    total_timesteps  | 562000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0603   |
|    n_updates        | 115499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 0.113    |
| time/               |          |
|    episodes         | 5324     |
|    fps              | 37       |
|    time_elapsed     | 15117    |
|    total_timesteps  | 562009   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 115502   |
----------------------------------
Eval num_timesteps=562500, episode_reward=5.14 +/- 7.84
Episode length: 130.92 +/- 69.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.111    |
| time/               |          |
|    total_timesteps  | 562500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0886   |
|    n_updates        | 115624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.111    |
| time/               |          |
|    episodes         | 5328     |
|    fps              | 37       |
|    time_elapsed     | 15133    |
|    total_timesteps  | 562525   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.154    |
|    n_updates        | 115631   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 0.11     |
| time/               |          |
|    episodes         | 5332     |
|    fps              | 37       |
|    time_elapsed     | 15135    |
|    total_timesteps  | 562903   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 115725   |
----------------------------------
Eval num_timesteps=563000, episode_reward=4.36 +/- 7.55
Episode length: 139.30 +/- 76.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.11     |
| time/               |          |
|    total_timesteps  | 563000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0849   |
|    n_updates        | 115749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 2.68     |
|    exploration_rate | 0.109    |
| time/               |          |
|    episodes         | 5336     |
|    fps              | 37       |
|    time_elapsed     | 15152    |
|    total_timesteps  | 563257   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0494   |
|    n_updates        | 115814   |
----------------------------------
Eval num_timesteps=563500, episode_reward=3.08 +/- 6.62
Episode length: 106.62 +/- 44.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.108    |
| time/               |          |
|    total_timesteps  | 563500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.256    |
|    n_updates        | 115874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 5340     |
|    fps              | 37       |
|    time_elapsed     | 15165    |
|    total_timesteps  | 563727   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0919   |
|    n_updates        | 115931   |
----------------------------------
Eval num_timesteps=564000, episode_reward=6.16 +/- 10.80
Episode length: 132.44 +/- 84.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.107    |
| time/               |          |
|    total_timesteps  | 564000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0737   |
|    n_updates        | 115999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.106    |
| time/               |          |
|    episodes         | 5344     |
|    fps              | 37       |
|    time_elapsed     | 15182    |
|    total_timesteps  | 564278   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.056    |
|    n_updates        | 116069   |
----------------------------------
Eval num_timesteps=564500, episode_reward=4.50 +/- 8.15
Episode length: 137.76 +/- 83.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 4.5      |
| rollout/            |          |
|    exploration_rate | 0.105    |
| time/               |          |
|    total_timesteps  | 564500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0482   |
|    n_updates        | 116124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.105    |
| time/               |          |
|    episodes         | 5348     |
|    fps              | 37       |
|    time_elapsed     | 15200    |
|    total_timesteps  | 564784   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0393   |
|    n_updates        | 116195   |
----------------------------------
Eval num_timesteps=565000, episode_reward=3.96 +/- 6.27
Episode length: 116.28 +/- 53.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.104    |
| time/               |          |
|    total_timesteps  | 565000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 116249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.67     |
|    exploration_rate | 0.103    |
| time/               |          |
|    episodes         | 5352     |
|    fps              | 37       |
|    time_elapsed     | 15214    |
|    total_timesteps  | 565259   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.235    |
|    n_updates        | 116314   |
----------------------------------
Eval num_timesteps=565500, episode_reward=3.08 +/- 5.15
Episode length: 130.72 +/- 92.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.103    |
| time/               |          |
|    total_timesteps  | 565500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 116374   |
----------------------------------
Eval num_timesteps=566000, episode_reward=4.24 +/- 5.72
Episode length: 136.04 +/- 74.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.101    |
| time/               |          |
|    total_timesteps  | 566000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0518   |
|    n_updates        | 116499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.101    |
| time/               |          |
|    episodes         | 5356     |
|    fps              | 37       |
|    time_elapsed     | 15252    |
|    total_timesteps  | 566024   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.154    |
|    n_updates        | 116505   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.0999   |
| time/               |          |
|    episodes         | 5360     |
|    fps              | 37       |
|    time_elapsed     | 15254    |
|    total_timesteps  | 566433   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 116608   |
----------------------------------
Eval num_timesteps=566500, episode_reward=6.48 +/- 10.75
Episode length: 140.48 +/- 76.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 6.48     |
| rollout/            |          |
|    exploration_rate | 0.0997   |
| time/               |          |
|    total_timesteps  | 566500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 116624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.12     |
|    exploration_rate | 0.0983   |
| time/               |          |
|    episodes         | 5364     |
|    fps              | 37       |
|    time_elapsed     | 15274    |
|    total_timesteps  | 566975   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0982   |
|    n_updates        | 116743   |
----------------------------------
Eval num_timesteps=567000, episode_reward=4.14 +/- 5.91
Episode length: 137.46 +/- 82.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 4.14     |
| rollout/            |          |
|    exploration_rate | 0.0983   |
| time/               |          |
|    total_timesteps  | 567000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0692   |
|    n_updates        | 116749   |
----------------------------------
Eval num_timesteps=567500, episode_reward=4.18 +/- 7.64
Episode length: 121.26 +/- 62.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.0968   |
| time/               |          |
|    total_timesteps  | 567500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 116874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.24     |
|    exploration_rate | 0.0965   |
| time/               |          |
|    episodes         | 5368     |
|    fps              | 37       |
|    time_elapsed     | 15309    |
|    total_timesteps  | 567599   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 116899   |
----------------------------------
Eval num_timesteps=568000, episode_reward=3.24 +/- 4.50
Episode length: 133.78 +/- 88.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.0954   |
| time/               |          |
|    total_timesteps  | 568000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 116999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.29     |
|    exploration_rate | 0.0951   |
| time/               |          |
|    episodes         | 5372     |
|    fps              | 37       |
|    time_elapsed     | 15325    |
|    total_timesteps  | 568098   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 117024   |
----------------------------------
Eval num_timesteps=568500, episode_reward=6.94 +/- 10.40
Episode length: 151.76 +/- 83.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 6.94     |
| rollout/            |          |
|    exploration_rate | 0.0939   |
| time/               |          |
|    total_timesteps  | 568500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0778   |
|    n_updates        | 117124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.29     |
|    exploration_rate | 0.0939   |
| time/               |          |
|    episodes         | 5376     |
|    fps              | 37       |
|    time_elapsed     | 15343    |
|    total_timesteps  | 568510   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0625   |
|    n_updates        | 117127   |
----------------------------------
Eval num_timesteps=569000, episode_reward=3.26 +/- 6.32
Episode length: 125.82 +/- 66.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.0925   |
| time/               |          |
|    total_timesteps  | 569000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0711   |
|    n_updates        | 117249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.33     |
|    exploration_rate | 0.092    |
| time/               |          |
|    episodes         | 5380     |
|    fps              | 37       |
|    time_elapsed     | 15363    |
|    total_timesteps  | 569139   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.1      |
|    n_updates        | 117284   |
----------------------------------
Eval num_timesteps=569500, episode_reward=5.32 +/- 9.85
Episode length: 147.00 +/- 89.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 5.32     |
| rollout/            |          |
|    exploration_rate | 0.091    |
| time/               |          |
|    total_timesteps  | 569500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 117374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.09     |
| time/               |          |
|    episodes         | 5384     |
|    fps              | 37       |
|    time_elapsed     | 15382    |
|    total_timesteps  | 569850   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0878   |
|    n_updates        | 117462   |
----------------------------------
Eval num_timesteps=570000, episode_reward=5.10 +/- 8.30
Episode length: 120.88 +/- 68.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 5.1      |
| rollout/            |          |
|    exploration_rate | 0.0896   |
| time/               |          |
|    total_timesteps  | 570000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0461   |
|    n_updates        | 117499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.74     |
|    exploration_rate | 0.0886   |
| time/               |          |
|    episodes         | 5388     |
|    fps              | 37       |
|    time_elapsed     | 15397    |
|    total_timesteps  | 570330   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 117582   |
----------------------------------
Eval num_timesteps=570500, episode_reward=5.62 +/- 9.98
Episode length: 151.02 +/- 97.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 5.62     |
| rollout/            |          |
|    exploration_rate | 0.0881   |
| time/               |          |
|    total_timesteps  | 570500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 117624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.0874   |
| time/               |          |
|    episodes         | 5392     |
|    fps              | 37       |
|    time_elapsed     | 15414    |
|    total_timesteps  | 570731   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 117682   |
----------------------------------
Eval num_timesteps=571000, episode_reward=2.74 +/- 3.97
Episode length: 107.28 +/- 45.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.0866   |
| time/               |          |
|    total_timesteps  | 571000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0973   |
|    n_updates        | 117749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.0854   |
| time/               |          |
|    episodes         | 5396     |
|    fps              | 37       |
|    time_elapsed     | 15428    |
|    total_timesteps  | 571426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0451   |
|    n_updates        | 117856   |
----------------------------------
Eval num_timesteps=571500, episode_reward=2.96 +/- 4.37
Episode length: 126.20 +/- 62.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.0852   |
| time/               |          |
|    total_timesteps  | 571500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0869   |
|    n_updates        | 117874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.98     |
|    exploration_rate | 0.0838   |
| time/               |          |
|    episodes         | 5400     |
|    fps              | 37       |
|    time_elapsed     | 15444    |
|    total_timesteps  | 571974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.151    |
|    n_updates        | 117993   |
----------------------------------
Eval num_timesteps=572000, episode_reward=3.90 +/- 5.65
Episode length: 106.12 +/- 39.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.9      |
| rollout/            |          |
|    exploration_rate | 0.0837   |
| time/               |          |
|    total_timesteps  | 572000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 117999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.083    |
| time/               |          |
|    episodes         | 5404     |
|    fps              | 37       |
|    time_elapsed     | 15456    |
|    total_timesteps  | 572264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.228    |
|    n_updates        | 118065   |
----------------------------------
Eval num_timesteps=572500, episode_reward=3.64 +/- 4.99
Episode length: 122.48 +/- 79.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.0823   |
| time/               |          |
|    total_timesteps  | 572500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 118124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.96     |
|    exploration_rate | 0.0815   |
| time/               |          |
|    episodes         | 5408     |
|    fps              | 37       |
|    time_elapsed     | 15471    |
|    total_timesteps  | 572753   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.07     |
|    n_updates        | 118188   |
----------------------------------
Eval num_timesteps=573000, episode_reward=2.84 +/- 6.17
Episode length: 111.68 +/- 52.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.0808   |
| time/               |          |
|    total_timesteps  | 573000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0505   |
|    n_updates        | 118249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.97     |
|    exploration_rate | 0.0804   |
| time/               |          |
|    episodes         | 5412     |
|    fps              | 37       |
|    time_elapsed     | 15485    |
|    total_timesteps  | 573159   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 118289   |
----------------------------------
Eval num_timesteps=573500, episode_reward=3.78 +/- 6.92
Episode length: 133.90 +/- 95.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.0794   |
| time/               |          |
|    total_timesteps  | 573500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 118374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.0788   |
| time/               |          |
|    episodes         | 5416     |
|    fps              | 37       |
|    time_elapsed     | 15501    |
|    total_timesteps  | 573679   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.264    |
|    n_updates        | 118419   |
----------------------------------
Eval num_timesteps=574000, episode_reward=4.94 +/- 8.03
Episode length: 138.62 +/- 94.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 4.94     |
| rollout/            |          |
|    exploration_rate | 0.0779   |
| time/               |          |
|    total_timesteps  | 574000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0379   |
|    n_updates        | 118499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.87     |
|    exploration_rate | 0.0775   |
| time/               |          |
|    episodes         | 5420     |
|    fps              | 36       |
|    time_elapsed     | 15518    |
|    total_timesteps  | 574131   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 118532   |
----------------------------------
Eval num_timesteps=574500, episode_reward=5.96 +/- 9.08
Episode length: 161.98 +/- 96.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 162      |
|    mean_reward      | 5.96     |
| rollout/            |          |
|    exploration_rate | 0.0764   |
| time/               |          |
|    total_timesteps  | 574500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0477   |
|    n_updates        | 118624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.61     |
|    exploration_rate | 0.0762   |
| time/               |          |
|    episodes         | 5424     |
|    fps              | 36       |
|    time_elapsed     | 15537    |
|    total_timesteps  | 574596   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 118648   |
----------------------------------
Eval num_timesteps=575000, episode_reward=6.62 +/- 10.41
Episode length: 192.52 +/- 151.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 6.62     |
| rollout/            |          |
|    exploration_rate | 0.075    |
| time/               |          |
|    total_timesteps  | 575000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 118749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.0748   |
| time/               |          |
|    episodes         | 5428     |
|    fps              | 36       |
|    time_elapsed     | 15559    |
|    total_timesteps  | 575046   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 118761   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.0735   |
| time/               |          |
|    episodes         | 5432     |
|    fps              | 36       |
|    time_elapsed     | 15561    |
|    total_timesteps  | 575494   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.175    |
|    n_updates        | 118873   |
----------------------------------
Eval num_timesteps=575500, episode_reward=4.92 +/- 5.84
Episode length: 158.28 +/- 95.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 158      |
|    mean_reward      | 4.92     |
| rollout/            |          |
|    exploration_rate | 0.0735   |
| time/               |          |
|    total_timesteps  | 575500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 118874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.72     |
|    exploration_rate | 0.0722   |
| time/               |          |
|    episodes         | 5436     |
|    fps              | 36       |
|    time_elapsed     | 15579    |
|    total_timesteps  | 575966   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 118991   |
----------------------------------
Eval num_timesteps=576000, episode_reward=2.22 +/- 2.80
Episode length: 113.44 +/- 40.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.0721   |
| time/               |          |
|    total_timesteps  | 576000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 118999   |
----------------------------------
Eval num_timesteps=576500, episode_reward=3.92 +/- 5.87
Episode length: 141.42 +/- 80.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 3.92     |
| rollout/            |          |
|    exploration_rate | 0.0706   |
| time/               |          |
|    total_timesteps  | 576500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0402   |
|    n_updates        | 119124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.71     |
|    exploration_rate | 0.0705   |
| time/               |          |
|    episodes         | 5440     |
|    fps              | 36       |
|    time_elapsed     | 15610    |
|    total_timesteps  | 576519   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0486   |
|    n_updates        | 119129   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.0695   |
| time/               |          |
|    episodes         | 5444     |
|    fps              | 36       |
|    time_elapsed     | 15612    |
|    total_timesteps  | 576865   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 119216   |
----------------------------------
Eval num_timesteps=577000, episode_reward=4.00 +/- 8.46
Episode length: 134.62 +/- 75.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.0691   |
| time/               |          |
|    total_timesteps  | 577000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0396   |
|    n_updates        | 119249   |
----------------------------------
Eval num_timesteps=577500, episode_reward=4.00 +/- 7.12
Episode length: 126.20 +/- 66.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.0677   |
| time/               |          |
|    total_timesteps  | 577500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0413   |
|    n_updates        | 119374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.0676   |
| time/               |          |
|    episodes         | 5448     |
|    fps              | 36       |
|    time_elapsed     | 15648    |
|    total_timesteps  | 577514   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.173    |
|    n_updates        | 119378   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.24     |
|    exploration_rate | 0.0664   |
| time/               |          |
|    episodes         | 5452     |
|    fps              | 36       |
|    time_elapsed     | 15650    |
|    total_timesteps  | 577918   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 119479   |
----------------------------------
Eval num_timesteps=578000, episode_reward=3.26 +/- 5.27
Episode length: 117.10 +/- 78.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.0662   |
| time/               |          |
|    total_timesteps  | 578000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0301   |
|    n_updates        | 119499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 3.99     |
|    exploration_rate | 0.0651   |
| time/               |          |
|    episodes         | 5456     |
|    fps              | 36       |
|    time_elapsed     | 15665    |
|    total_timesteps  | 578362   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0764   |
|    n_updates        | 119590   |
----------------------------------
Eval num_timesteps=578500, episode_reward=5.16 +/- 6.51
Episode length: 152.52 +/- 110.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 153      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.0647   |
| time/               |          |
|    total_timesteps  | 578500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 119624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.0636   |
| time/               |          |
|    episodes         | 5460     |
|    fps              | 36       |
|    time_elapsed     | 15683    |
|    total_timesteps  | 578896   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0736   |
|    n_updates        | 119723   |
----------------------------------
Eval num_timesteps=579000, episode_reward=2.90 +/- 4.68
Episode length: 107.28 +/- 50.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.0633   |
| time/               |          |
|    total_timesteps  | 579000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0917   |
|    n_updates        | 119749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.0622   |
| time/               |          |
|    episodes         | 5464     |
|    fps              | 36       |
|    time_elapsed     | 15697    |
|    total_timesteps  | 579373   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0354   |
|    n_updates        | 119843   |
----------------------------------
Eval num_timesteps=579500, episode_reward=6.04 +/- 9.80
Episode length: 137.98 +/- 73.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 6.04     |
| rollout/            |          |
|    exploration_rate | 0.0618   |
| time/               |          |
|    total_timesteps  | 579500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 119874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.0605   |
| time/               |          |
|    episodes         | 5468     |
|    fps              | 36       |
|    time_elapsed     | 15714    |
|    total_timesteps  | 579936   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 119983   |
----------------------------------
Eval num_timesteps=580000, episode_reward=5.18 +/- 7.83
Episode length: 142.08 +/- 96.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 5.18     |
| rollout/            |          |
|    exploration_rate | 0.0603   |
| time/               |          |
|    total_timesteps  | 580000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 119999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 3.62     |
|    exploration_rate | 0.0593   |
| time/               |          |
|    episodes         | 5472     |
|    fps              | 36       |
|    time_elapsed     | 15730    |
|    total_timesteps  | 580340   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 120084   |
----------------------------------
Eval num_timesteps=580500, episode_reward=5.82 +/- 8.47
Episode length: 143.26 +/- 76.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 5.82     |
| rollout/            |          |
|    exploration_rate | 0.0589   |
| time/               |          |
|    total_timesteps  | 580500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.167    |
|    n_updates        | 120124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.0574   |
| time/               |          |
|    episodes         | 5476     |
|    fps              | 36       |
|    time_elapsed     | 15748    |
|    total_timesteps  | 580983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0879   |
|    n_updates        | 120245   |
----------------------------------
Eval num_timesteps=581000, episode_reward=5.34 +/- 10.52
Episode length: 122.62 +/- 61.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.0574   |
| time/               |          |
|    total_timesteps  | 581000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 120249   |
----------------------------------
Eval num_timesteps=581500, episode_reward=4.28 +/- 9.14
Episode length: 126.00 +/- 61.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.28     |
| rollout/            |          |
|    exploration_rate | 0.0559   |
| time/               |          |
|    total_timesteps  | 581500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.091    |
|    n_updates        | 120374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.0553   |
| time/               |          |
|    episodes         | 5480     |
|    fps              | 36       |
|    time_elapsed     | 15777    |
|    total_timesteps  | 581725   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 120431   |
----------------------------------
Eval num_timesteps=582000, episode_reward=1.86 +/- 3.52
Episode length: 94.36 +/- 50.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 0.0545   |
| time/               |          |
|    total_timesteps  | 582000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 120499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.0535   |
| time/               |          |
|    episodes         | 5484     |
|    fps              | 36       |
|    time_elapsed     | 15790    |
|    total_timesteps  | 582325   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0459   |
|    n_updates        | 120581   |
----------------------------------
Eval num_timesteps=582500, episode_reward=4.40 +/- 7.64
Episode length: 122.84 +/- 87.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.053    |
| time/               |          |
|    total_timesteps  | 582500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0611   |
|    n_updates        | 120624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.0519   |
| time/               |          |
|    episodes         | 5488     |
|    fps              | 36       |
|    time_elapsed     | 15805    |
|    total_timesteps  | 582878   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.086    |
|    n_updates        | 120719   |
----------------------------------
Eval num_timesteps=583000, episode_reward=4.78 +/- 8.29
Episode length: 126.94 +/- 79.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.0515   |
| time/               |          |
|    total_timesteps  | 583000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 120749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.99     |
|    exploration_rate | 0.0506   |
| time/               |          |
|    episodes         | 5492     |
|    fps              | 36       |
|    time_elapsed     | 15821    |
|    total_timesteps  | 583326   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0548   |
|    n_updates        | 120831   |
----------------------------------
Eval num_timesteps=583500, episode_reward=4.36 +/- 7.72
Episode length: 134.72 +/- 79.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.05     |
| time/               |          |
|    total_timesteps  | 583500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 120874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.0492   |
| time/               |          |
|    episodes         | 5496     |
|    fps              | 36       |
|    time_elapsed     | 15837    |
|    total_timesteps  | 583781   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0712   |
|    n_updates        | 120945   |
----------------------------------
Eval num_timesteps=584000, episode_reward=3.94 +/- 8.01
Episode length: 121.92 +/- 91.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.0486   |
| time/               |          |
|    total_timesteps  | 584000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0827   |
|    n_updates        | 120999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 3.64     |
|    exploration_rate | 0.0482   |
| time/               |          |
|    episodes         | 5500     |
|    fps              | 36       |
|    time_elapsed     | 15856    |
|    total_timesteps  | 584131   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.166    |
|    n_updates        | 121032   |
----------------------------------
Eval num_timesteps=584500, episode_reward=3.36 +/- 4.96
Episode length: 127.08 +/- 89.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.0471   |
| time/               |          |
|    total_timesteps  | 584500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0933   |
|    n_updates        | 121124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.0467   |
| time/               |          |
|    episodes         | 5504     |
|    fps              | 36       |
|    time_elapsed     | 15876    |
|    total_timesteps  | 584643   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 121160   |
----------------------------------
Eval num_timesteps=585000, episode_reward=4.60 +/- 8.78
Episode length: 141.70 +/- 73.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.0456   |
| time/               |          |
|    total_timesteps  | 585000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 121249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.0454   |
| time/               |          |
|    episodes         | 5508     |
|    fps              | 36       |
|    time_elapsed     | 15894    |
|    total_timesteps  | 585061   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0495   |
|    n_updates        | 121265   |
----------------------------------
Eval num_timesteps=585500, episode_reward=3.80 +/- 6.51
Episode length: 125.80 +/- 91.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.0441   |
| time/               |          |
|    total_timesteps  | 585500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0566   |
|    n_updates        | 121374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.0439   |
| time/               |          |
|    episodes         | 5512     |
|    fps              | 36       |
|    time_elapsed     | 15909    |
|    total_timesteps  | 585570   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 121392   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.0427   |
| time/               |          |
|    episodes         | 5516     |
|    fps              | 36       |
|    time_elapsed     | 15911    |
|    total_timesteps  | 585984   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0584   |
|    n_updates        | 121495   |
----------------------------------
Eval num_timesteps=586000, episode_reward=4.30 +/- 6.69
Episode length: 134.82 +/- 103.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.0427   |
| time/               |          |
|    total_timesteps  | 586000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0371   |
|    n_updates        | 121499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.0415   |
| time/               |          |
|    episodes         | 5520     |
|    fps              | 36       |
|    time_elapsed     | 15932    |
|    total_timesteps  | 586388   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.18     |
|    n_updates        | 121596   |
----------------------------------
Eval num_timesteps=586500, episode_reward=7.28 +/- 14.72
Episode length: 146.96 +/- 128.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 7.28     |
| rollout/            |          |
|    exploration_rate | 0.0412   |
| time/               |          |
|    total_timesteps  | 586500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0209   |
|    n_updates        | 121624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.39     |
|    exploration_rate | 0.04     |
| time/               |          |
|    episodes         | 5524     |
|    fps              | 36       |
|    time_elapsed     | 15953    |
|    total_timesteps  | 586905   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 121726   |
----------------------------------
Eval num_timesteps=587000, episode_reward=2.92 +/- 4.72
Episode length: 106.10 +/- 54.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.0397   |
| time/               |          |
|    total_timesteps  | 587000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0474   |
|    n_updates        | 121749   |
----------------------------------
Eval num_timesteps=587500, episode_reward=2.98 +/- 4.51
Episode length: 118.78 +/- 85.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.0382   |
| time/               |          |
|    total_timesteps  | 587500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0694   |
|    n_updates        | 121874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.0382   |
| time/               |          |
|    episodes         | 5528     |
|    fps              | 36       |
|    time_elapsed     | 15989    |
|    total_timesteps  | 587513   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 121878   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.43     |
|    exploration_rate | 0.0368   |
| time/               |          |
|    episodes         | 5532     |
|    fps              | 36       |
|    time_elapsed     | 15992    |
|    total_timesteps  | 587981   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0857   |
|    n_updates        | 121995   |
----------------------------------
Eval num_timesteps=588000, episode_reward=3.74 +/- 4.64
Episode length: 118.54 +/- 53.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.0368   |
| time/               |          |
|    total_timesteps  | 588000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0372   |
|    n_updates        | 121999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.39     |
|    exploration_rate | 0.0356   |
| time/               |          |
|    episodes         | 5536     |
|    fps              | 36       |
|    time_elapsed     | 16009    |
|    total_timesteps  | 588398   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0984   |
|    n_updates        | 122099   |
----------------------------------
Eval num_timesteps=588500, episode_reward=5.60 +/- 9.40
Episode length: 147.74 +/- 96.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 5.6      |
| rollout/            |          |
|    exploration_rate | 0.0353   |
| time/               |          |
|    total_timesteps  | 588500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0972   |
|    n_updates        | 122124   |
----------------------------------
Eval num_timesteps=589000, episode_reward=11.22 +/- 15.68
Episode length: 171.74 +/- 121.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 11.2     |
| rollout/            |          |
|    exploration_rate | 0.0338   |
| time/               |          |
|    total_timesteps  | 589000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0392   |
|    n_updates        | 122249   |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.0336   |
| time/               |          |
|    episodes         | 5540     |
|    fps              | 36       |
|    time_elapsed     | 16053    |
|    total_timesteps  | 589067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 122266   |
----------------------------------
Eval num_timesteps=589500, episode_reward=3.68 +/- 5.75
Episode length: 139.82 +/- 101.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 3.68     |
| rollout/            |          |
|    exploration_rate | 0.0323   |
| time/               |          |
|    total_timesteps  | 589500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0371   |
|    n_updates        | 122374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.0321   |
| time/               |          |
|    episodes         | 5544     |
|    fps              | 36       |
|    time_elapsed     | 16070    |
|    total_timesteps  | 589563   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0711   |
|    n_updates        | 122390   |
----------------------------------
Eval num_timesteps=590000, episode_reward=7.28 +/- 14.18
Episode length: 154.06 +/- 106.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 7.28     |
| rollout/            |          |
|    exploration_rate | 0.0308   |
| time/               |          |
|    total_timesteps  | 590000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.16     |
|    n_updates        | 122499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.0306   |
| time/               |          |
|    episodes         | 5548     |
|    fps              | 36       |
|    time_elapsed     | 16089    |
|    total_timesteps  | 590090   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0761   |
|    n_updates        | 122522   |
----------------------------------
Eval num_timesteps=590500, episode_reward=5.14 +/- 7.36
Episode length: 128.60 +/- 61.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.0293   |
| time/               |          |
|    total_timesteps  | 590500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.145    |
|    n_updates        | 122624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.029    |
| time/               |          |
|    episodes         | 5552     |
|    fps              | 36       |
|    time_elapsed     | 16104    |
|    total_timesteps  | 590602   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0832   |
|    n_updates        | 122650   |
----------------------------------
Eval num_timesteps=591000, episode_reward=5.16 +/- 9.50
Episode length: 189.06 +/- 143.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.0279   |
| time/               |          |
|    total_timesteps  | 591000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 122749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.0277   |
| time/               |          |
|    episodes         | 5556     |
|    fps              | 36       |
|    time_elapsed     | 16126    |
|    total_timesteps  | 591063   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0565   |
|    n_updates        | 122765   |
----------------------------------
Eval num_timesteps=591500, episode_reward=5.16 +/- 10.12
Episode length: 136.54 +/- 92.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.0264   |
| time/               |          |
|    total_timesteps  | 591500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 122874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.59     |
|    exploration_rate | 0.0261   |
| time/               |          |
|    episodes         | 5560     |
|    fps              | 36       |
|    time_elapsed     | 16142    |
|    total_timesteps  | 591577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0653   |
|    n_updates        | 122894   |
----------------------------------
Eval num_timesteps=592000, episode_reward=3.40 +/- 6.68
Episode length: 132.60 +/- 74.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.0249   |
| time/               |          |
|    total_timesteps  | 592000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 122999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.52     |
|    exploration_rate | 0.0245   |
| time/               |          |
|    episodes         | 5564     |
|    fps              | 36       |
|    time_elapsed     | 16159    |
|    total_timesteps  | 592120   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.17     |
|    n_updates        | 123029   |
----------------------------------
Eval num_timesteps=592500, episode_reward=7.58 +/- 11.41
Episode length: 154.78 +/- 121.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 155      |
|    mean_reward      | 7.58     |
| rollout/            |          |
|    exploration_rate | 0.0234   |
| time/               |          |
|    total_timesteps  | 592500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 123124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.0233   |
| time/               |          |
|    episodes         | 5568     |
|    fps              | 36       |
|    time_elapsed     | 16176    |
|    total_timesteps  | 592524   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0522   |
|    n_updates        | 123130   |
----------------------------------
Eval num_timesteps=593000, episode_reward=4.40 +/- 8.67
Episode length: 133.64 +/- 69.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.0219   |
| time/               |          |
|    total_timesteps  | 593000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0465   |
|    n_updates        | 123249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.61     |
|    exploration_rate | 0.0216   |
| time/               |          |
|    episodes         | 5572     |
|    fps              | 36       |
|    time_elapsed     | 16193    |
|    total_timesteps  | 593094   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.181    |
|    n_updates        | 123273   |
----------------------------------
Eval num_timesteps=593500, episode_reward=4.40 +/- 7.56
Episode length: 142.92 +/- 91.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.0204   |
| time/               |          |
|    total_timesteps  | 593500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 123374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.0201   |
| time/               |          |
|    episodes         | 5576     |
|    fps              | 36       |
|    time_elapsed     | 16211    |
|    total_timesteps  | 593621   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0756   |
|    n_updates        | 123405   |
----------------------------------
Eval num_timesteps=594000, episode_reward=6.28 +/- 10.37
Episode length: 138.84 +/- 76.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 6.28     |
| rollout/            |          |
|    exploration_rate | 0.0189   |
| time/               |          |
|    total_timesteps  | 594000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 123499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.0184   |
| time/               |          |
|    episodes         | 5580     |
|    fps              | 36       |
|    time_elapsed     | 16229    |
|    total_timesteps  | 594175   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.141    |
|    n_updates        | 123543   |
----------------------------------
Eval num_timesteps=594500, episode_reward=4.26 +/- 9.82
Episode length: 123.06 +/- 72.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.26     |
| rollout/            |          |
|    exploration_rate | 0.0174   |
| time/               |          |
|    total_timesteps  | 594500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 123624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.0171   |
| time/               |          |
|    episodes         | 5584     |
|    fps              | 36       |
|    time_elapsed     | 16244    |
|    total_timesteps  | 594628   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 123656   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 5588     |
|    fps              | 36       |
|    time_elapsed     | 16246    |
|    total_timesteps  | 594983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0925   |
|    n_updates        | 123745   |
----------------------------------
Eval num_timesteps=595000, episode_reward=3.40 +/- 6.09
Episode length: 131.38 +/- 90.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 595000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0476   |
|    n_updates        | 123749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.82     |
|    exploration_rate | 0.0149   |
| time/               |          |
|    episodes         | 5592     |
|    fps              | 36       |
|    time_elapsed     | 16262    |
|    total_timesteps  | 595355   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 123838   |
----------------------------------
Eval num_timesteps=595500, episode_reward=5.20 +/- 9.54
Episode length: 119.12 +/- 67.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 5.2      |
| rollout/            |          |
|    exploration_rate | 0.0145   |
| time/               |          |
|    total_timesteps  | 595500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0499   |
|    n_updates        | 123874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.0134   |
| time/               |          |
|    episodes         | 5596     |
|    fps              | 36       |
|    time_elapsed     | 16276    |
|    total_timesteps  | 595858   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 123964   |
----------------------------------
Eval num_timesteps=596000, episode_reward=6.36 +/- 9.83
Episode length: 138.84 +/- 75.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 6.36     |
| rollout/            |          |
|    exploration_rate | 0.013    |
| time/               |          |
|    total_timesteps  | 596000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0595   |
|    n_updates        | 123999   |
----------------------------------
Eval num_timesteps=596500, episode_reward=4.58 +/- 8.16
Episode length: 122.60 +/- 64.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.0115   |
| time/               |          |
|    total_timesteps  | 596500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0381   |
|    n_updates        | 124124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.0105   |
| time/               |          |
|    episodes         | 5600     |
|    fps              | 36       |
|    time_elapsed     | 16308    |
|    total_timesteps  | 596814   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0732   |
|    n_updates        | 124203   |
----------------------------------
Eval num_timesteps=597000, episode_reward=5.16 +/- 10.29
Episode length: 153.16 +/- 129.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 153      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.00998  |
| time/               |          |
|    total_timesteps  | 597000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0635   |
|    n_updates        | 124249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.00919  |
| time/               |          |
|    episodes         | 5604     |
|    fps              | 36       |
|    time_elapsed     | 16327    |
|    total_timesteps  | 597264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0442   |
|    n_updates        | 124315   |
----------------------------------
Eval num_timesteps=597500, episode_reward=5.24 +/- 8.89
Episode length: 134.92 +/- 103.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.24     |
| rollout/            |          |
|    exploration_rate | 0.00849  |
| time/               |          |
|    total_timesteps  | 597500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0949   |
|    n_updates        | 124374   |
----------------------------------
Eval num_timesteps=598000, episode_reward=3.28 +/- 4.17
Episode length: 113.44 +/- 48.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.00699  |
| time/               |          |
|    total_timesteps  | 598000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.182    |
|    n_updates        | 124499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.00665  |
| time/               |          |
|    episodes         | 5608     |
|    fps              | 36       |
|    time_elapsed     | 16357    |
|    total_timesteps  | 598114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0875   |
|    n_updates        | 124528   |
----------------------------------
Eval num_timesteps=598500, episode_reward=5.10 +/- 10.37
Episode length: 135.38 +/- 79.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.1      |
| rollout/            |          |
|    exploration_rate | 0.0055   |
| time/               |          |
|    total_timesteps  | 598500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0393   |
|    n_updates        | 124624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.00514  |
| time/               |          |
|    episodes         | 5612     |
|    fps              | 36       |
|    time_elapsed     | 16374    |
|    total_timesteps  | 598617   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 124654   |
----------------------------------
Eval num_timesteps=599000, episode_reward=5.06 +/- 11.11
Episode length: 140.74 +/- 100.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.004    |
| time/               |          |
|    total_timesteps  | 599000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 124749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.07     |
|    exploration_rate | 0.00381  |
| time/               |          |
|    episodes         | 5616     |
|    fps              | 36       |
|    time_elapsed     | 16391    |
|    total_timesteps  | 599062   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0847   |
|    n_updates        | 124765   |
----------------------------------
Eval num_timesteps=599500, episode_reward=3.52 +/- 4.92
Episode length: 119.54 +/- 86.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.0025   |
| time/               |          |
|    total_timesteps  | 599500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 124874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.00241  |
| time/               |          |
|    episodes         | 5620     |
|    fps              | 36       |
|    time_elapsed     | 16408    |
|    total_timesteps  | 599528   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0781   |
|    n_updates        | 124881   |
----------------------------------
Eval num_timesteps=600000, episode_reward=4.00 +/- 6.29
Episode length: 126.38 +/- 86.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 600000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 124999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5624     |
|    fps              | 36       |
|    time_elapsed     | 16428    |
|    total_timesteps  | 600156   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0864   |
|    n_updates        | 125038   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5628     |
|    fps              | 36       |
|    time_elapsed     | 16430    |
|    total_timesteps  | 600474   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 125118   |
----------------------------------
Eval num_timesteps=600500, episode_reward=5.46 +/- 7.89
Episode length: 151.10 +/- 92.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 5.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 600500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 125124   |
----------------------------------
Eval num_timesteps=601000, episode_reward=5.66 +/- 7.69
Episode length: 155.44 +/- 118.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 155      |
|    mean_reward      | 5.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 601000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.178    |
|    n_updates        | 125249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5632     |
|    fps              | 36       |
|    time_elapsed     | 16466    |
|    total_timesteps  | 601041   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 125260   |
----------------------------------
Eval num_timesteps=601500, episode_reward=3.90 +/- 6.43
Episode length: 173.28 +/- 108.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 173      |
|    mean_reward      | 3.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 601500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0377   |
|    n_updates        | 125374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5636     |
|    fps              | 36       |
|    time_elapsed     | 16486    |
|    total_timesteps  | 601529   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 125382   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5640     |
|    fps              | 36       |
|    time_elapsed     | 16488    |
|    total_timesteps  | 601940   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 125484   |
----------------------------------
Eval num_timesteps=602000, episode_reward=5.16 +/- 8.24
Episode length: 164.16 +/- 134.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 602000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 125499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5644     |
|    fps              | 36       |
|    time_elapsed     | 16507    |
|    total_timesteps  | 602309   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0839   |
|    n_updates        | 125577   |
----------------------------------
Eval num_timesteps=602500, episode_reward=4.48 +/- 5.72
Episode length: 127.64 +/- 54.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 602500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.204    |
|    n_updates        | 125624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5648     |
|    fps              | 36       |
|    time_elapsed     | 16523    |
|    total_timesteps  | 602941   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0185   |
|    n_updates        | 125735   |
----------------------------------
Eval num_timesteps=603000, episode_reward=4.52 +/- 8.86
Episode length: 127.44 +/- 80.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 603000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0811   |
|    n_updates        | 125749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5652     |
|    fps              | 36       |
|    time_elapsed     | 16539    |
|    total_timesteps  | 603319   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.078    |
|    n_updates        | 125829   |
----------------------------------
Eval num_timesteps=603500, episode_reward=6.76 +/- 10.07
Episode length: 171.54 +/- 128.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 6.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 603500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 125874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5656     |
|    fps              | 36       |
|    time_elapsed     | 16560    |
|    total_timesteps  | 603862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 125965   |
----------------------------------
Eval num_timesteps=604000, episode_reward=4.56 +/- 6.55
Episode length: 127.16 +/- 66.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 604000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0929   |
|    n_updates        | 125999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5660     |
|    fps              | 36       |
|    time_elapsed     | 16576    |
|    total_timesteps  | 604441   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0846   |
|    n_updates        | 126110   |
----------------------------------
Eval num_timesteps=604500, episode_reward=3.98 +/- 5.73
Episode length: 110.14 +/- 56.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 604500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0782   |
|    n_updates        | 126124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5664     |
|    fps              | 36       |
|    time_elapsed     | 16590    |
|    total_timesteps  | 604814   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0611   |
|    n_updates        | 126203   |
----------------------------------
Eval num_timesteps=605000, episode_reward=2.22 +/- 3.22
Episode length: 95.20 +/- 28.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.2     |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 605000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0213   |
|    n_updates        | 126249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.15     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5668     |
|    fps              | 36       |
|    time_elapsed     | 16608    |
|    total_timesteps  | 605449   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.091    |
|    n_updates        | 126362   |
----------------------------------
Eval num_timesteps=605500, episode_reward=6.36 +/- 13.74
Episode length: 151.10 +/- 94.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 6.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 605500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.216    |
|    n_updates        | 126374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5672     |
|    fps              | 36       |
|    time_elapsed     | 16629    |
|    total_timesteps  | 605938   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0953   |
|    n_updates        | 126484   |
----------------------------------
Eval num_timesteps=606000, episode_reward=6.64 +/- 8.67
Episode length: 171.36 +/- 110.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 171      |
|    mean_reward      | 6.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 606000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0739   |
|    n_updates        | 126499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5676     |
|    fps              | 36       |
|    time_elapsed     | 16649    |
|    total_timesteps  | 606243   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.188    |
|    n_updates        | 126560   |
----------------------------------
Eval num_timesteps=606500, episode_reward=5.30 +/- 7.18
Episode length: 125.52 +/- 64.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 606500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 126624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5680     |
|    fps              | 36       |
|    time_elapsed     | 16665    |
|    total_timesteps  | 606736   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 126683   |
----------------------------------
Eval num_timesteps=607000, episode_reward=5.08 +/- 7.52
Episode length: 129.58 +/- 71.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 607000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 126749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5684     |
|    fps              | 36       |
|    time_elapsed     | 16682    |
|    total_timesteps  | 607234   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.223    |
|    n_updates        | 126808   |
----------------------------------
Eval num_timesteps=607500, episode_reward=4.60 +/- 7.26
Episode length: 119.00 +/- 60.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 607500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 126874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5688     |
|    fps              | 36       |
|    time_elapsed     | 16699    |
|    total_timesteps  | 607581   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0723   |
|    n_updates        | 126895   |
----------------------------------
Eval num_timesteps=608000, episode_reward=3.76 +/- 8.40
Episode length: 114.32 +/- 44.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 608000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0617   |
|    n_updates        | 126999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5692     |
|    fps              | 36       |
|    time_elapsed     | 16719    |
|    total_timesteps  | 608092   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 127022   |
----------------------------------
Eval num_timesteps=608500, episode_reward=5.56 +/- 7.75
Episode length: 133.58 +/- 60.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 608500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 127124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5696     |
|    fps              | 36       |
|    time_elapsed     | 16741    |
|    total_timesteps  | 608711   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0386   |
|    n_updates        | 127177   |
----------------------------------
Eval num_timesteps=609000, episode_reward=5.86 +/- 9.02
Episode length: 162.86 +/- 116.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 163      |
|    mean_reward      | 5.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 609000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 127249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5700     |
|    fps              | 36       |
|    time_elapsed     | 16761    |
|    total_timesteps  | 609104   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0492   |
|    n_updates        | 127275   |
----------------------------------
Eval num_timesteps=609500, episode_reward=3.36 +/- 4.99
Episode length: 109.90 +/- 49.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 609500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 127374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5704     |
|    fps              | 36       |
|    time_elapsed     | 16779    |
|    total_timesteps  | 609516   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0756   |
|    n_updates        | 127378   |
----------------------------------
Eval num_timesteps=610000, episode_reward=5.06 +/- 9.70
Episode length: 126.90 +/- 90.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 610000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0531   |
|    n_updates        | 127499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5708     |
|    fps              | 36       |
|    time_elapsed     | 16801    |
|    total_timesteps  | 610147   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 127536   |
----------------------------------
Eval num_timesteps=610500, episode_reward=4.18 +/- 5.06
Episode length: 116.52 +/- 68.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 610500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 127624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5712     |
|    fps              | 36       |
|    time_elapsed     | 16822    |
|    total_timesteps  | 610834   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.177    |
|    n_updates        | 127708   |
----------------------------------
Eval num_timesteps=611000, episode_reward=8.36 +/- 13.04
Episode length: 153.32 +/- 82.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 153      |
|    mean_reward      | 8.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 611000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 127749   |
----------------------------------
Eval num_timesteps=611500, episode_reward=7.14 +/- 11.97
Episode length: 147.16 +/- 70.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 7.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 611500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.083    |
|    n_updates        | 127874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5716     |
|    fps              | 36       |
|    time_elapsed     | 16864    |
|    total_timesteps  | 611515   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0438   |
|    n_updates        | 127878   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.56     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5720     |
|    fps              | 36       |
|    time_elapsed     | 16866    |
|    total_timesteps  | 611849   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.067    |
|    n_updates        | 127962   |
----------------------------------
Eval num_timesteps=612000, episode_reward=3.62 +/- 6.01
Episode length: 116.76 +/- 67.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 612000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0864   |
|    n_updates        | 127999   |
----------------------------------
Eval num_timesteps=612500, episode_reward=7.02 +/- 13.03
Episode length: 139.32 +/- 106.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 7.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 612500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 128124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5724     |
|    fps              | 36       |
|    time_elapsed     | 16897    |
|    total_timesteps  | 612738   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0502   |
|    n_updates        | 128184   |
----------------------------------
Eval num_timesteps=613000, episode_reward=4.56 +/- 8.00
Episode length: 111.24 +/- 58.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 613000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0642   |
|    n_updates        | 128249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5728     |
|    fps              | 36       |
|    time_elapsed     | 16911    |
|    total_timesteps  | 613105   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0862   |
|    n_updates        | 128276   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5732     |
|    fps              | 36       |
|    time_elapsed     | 16912    |
|    total_timesteps  | 613487   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0599   |
|    n_updates        | 128371   |
----------------------------------
Eval num_timesteps=613500, episode_reward=2.98 +/- 5.15
Episode length: 99.78 +/- 54.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.8     |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 613500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0846   |
|    n_updates        | 128374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5736     |
|    fps              | 36       |
|    time_elapsed     | 16924    |
|    total_timesteps  | 613827   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0423   |
|    n_updates        | 128456   |
----------------------------------
Eval num_timesteps=614000, episode_reward=4.10 +/- 6.51
Episode length: 125.22 +/- 86.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 614000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.196    |
|    n_updates        | 128499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5740     |
|    fps              | 36       |
|    time_elapsed     | 16940    |
|    total_timesteps  | 614484   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 128620   |
----------------------------------
Eval num_timesteps=614500, episode_reward=3.56 +/- 5.66
Episode length: 109.56 +/- 53.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 614500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 128624   |
----------------------------------
Eval num_timesteps=615000, episode_reward=4.68 +/- 6.40
Episode length: 131.72 +/- 65.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 615000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 128749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.65     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5744     |
|    fps              | 36       |
|    time_elapsed     | 16969    |
|    total_timesteps  | 615136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 128783   |
----------------------------------
Eval num_timesteps=615500, episode_reward=5.38 +/- 6.87
Episode length: 137.86 +/- 94.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 5.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 615500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0677   |
|    n_updates        | 128874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5748     |
|    fps              | 36       |
|    time_elapsed     | 16986    |
|    total_timesteps  | 615724   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 128930   |
----------------------------------
Eval num_timesteps=616000, episode_reward=7.28 +/- 14.23
Episode length: 149.90 +/- 103.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 7.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 616000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.062    |
|    n_updates        | 128999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5752     |
|    fps              | 36       |
|    time_elapsed     | 17003    |
|    total_timesteps  | 616159   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0803   |
|    n_updates        | 129039   |
----------------------------------
Eval num_timesteps=616500, episode_reward=4.54 +/- 10.49
Episode length: 125.92 +/- 77.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 616500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.034    |
|    n_updates        | 129124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5756     |
|    fps              | 36       |
|    time_elapsed     | 17018    |
|    total_timesteps  | 616558   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0272   |
|    n_updates        | 129139   |
----------------------------------
Eval num_timesteps=617000, episode_reward=5.44 +/- 9.80
Episode length: 119.20 +/- 67.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 617000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0937   |
|    n_updates        | 129249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.69     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5760     |
|    fps              | 36       |
|    time_elapsed     | 17033    |
|    total_timesteps  | 617173   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 129293   |
----------------------------------
Eval num_timesteps=617500, episode_reward=4.12 +/- 6.13
Episode length: 110.64 +/- 44.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 617500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 129374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 5.82     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5764     |
|    fps              | 36       |
|    time_elapsed     | 17047    |
|    total_timesteps  | 617709   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0717   |
|    n_updates        | 129427   |
----------------------------------
Eval num_timesteps=618000, episode_reward=3.66 +/- 6.74
Episode length: 100.18 +/- 46.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 618000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.045    |
|    n_updates        | 129499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5768     |
|    fps              | 36       |
|    time_elapsed     | 17060    |
|    total_timesteps  | 618130   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0835   |
|    n_updates        | 129532   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5772     |
|    fps              | 36       |
|    time_elapsed     | 17062    |
|    total_timesteps  | 618477   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 129619   |
----------------------------------
Eval num_timesteps=618500, episode_reward=4.28 +/- 5.97
Episode length: 143.36 +/- 96.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 4.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 618500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0423   |
|    n_updates        | 129624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.69     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5776     |
|    fps              | 36       |
|    time_elapsed     | 17081    |
|    total_timesteps  | 618842   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0806   |
|    n_updates        | 129710   |
----------------------------------
Eval num_timesteps=619000, episode_reward=4.24 +/- 6.58
Episode length: 123.52 +/- 60.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 619000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.324    |
|    n_updates        | 129749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 6.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5780     |
|    fps              | 36       |
|    time_elapsed     | 17100    |
|    total_timesteps  | 619384   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0298   |
|    n_updates        | 129845   |
----------------------------------
Eval num_timesteps=619500, episode_reward=3.18 +/- 4.81
Episode length: 91.90 +/- 34.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.9     |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 619500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0792   |
|    n_updates        | 129874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 6.13     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5784     |
|    fps              | 36       |
|    time_elapsed     | 17112    |
|    total_timesteps  | 619902   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0737   |
|    n_updates        | 129975   |
----------------------------------
Eval num_timesteps=620000, episode_reward=6.28 +/- 11.99
Episode length: 133.10 +/- 92.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 6.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 620000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 129999   |
----------------------------------
Eval num_timesteps=620500, episode_reward=2.90 +/- 4.65
Episode length: 145.94 +/- 127.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 620500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0523   |
|    n_updates        | 130124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 6.16     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5788     |
|    fps              | 36       |
|    time_elapsed     | 17144    |
|    total_timesteps  | 620609   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0437   |
|    n_updates        | 130152   |
----------------------------------
Eval num_timesteps=621000, episode_reward=5.88 +/- 8.39
Episode length: 140.90 +/- 94.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 5.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 621000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0656   |
|    n_updates        | 130249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 6.3      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5792     |
|    fps              | 36       |
|    time_elapsed     | 17162    |
|    total_timesteps  | 621049   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 130262   |
----------------------------------
Eval num_timesteps=621500, episode_reward=3.94 +/- 6.74
Episode length: 110.76 +/- 58.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 621500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.142    |
|    n_updates        | 130374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 6.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5796     |
|    fps              | 36       |
|    time_elapsed     | 17176    |
|    total_timesteps  | 621673   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0775   |
|    n_updates        | 130418   |
----------------------------------
Eval num_timesteps=622000, episode_reward=2.66 +/- 4.57
Episode length: 107.06 +/- 46.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 622000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.084    |
|    n_updates        | 130499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 6.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5800     |
|    fps              | 36       |
|    time_elapsed     | 17189    |
|    total_timesteps  | 622122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 130530   |
----------------------------------
Eval num_timesteps=622500, episode_reward=5.90 +/- 10.90
Episode length: 144.88 +/- 108.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 5.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 622500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0607   |
|    n_updates        | 130624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 6.49     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5804     |
|    fps              | 36       |
|    time_elapsed     | 17207    |
|    total_timesteps  | 622625   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0433   |
|    n_updates        | 130656   |
----------------------------------
Eval num_timesteps=623000, episode_reward=5.86 +/- 9.44
Episode length: 151.40 +/- 107.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 5.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 623000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0371   |
|    n_updates        | 130749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 6.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5808     |
|    fps              | 36       |
|    time_elapsed     | 17226    |
|    total_timesteps  | 623061   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 130765   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 6.11     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5812     |
|    fps              | 36       |
|    time_elapsed     | 17227    |
|    total_timesteps  | 623387   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 130846   |
----------------------------------
Eval num_timesteps=623500, episode_reward=4.16 +/- 5.30
Episode length: 106.32 +/- 53.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 623500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0518   |
|    n_updates        | 130874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.71     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5816     |
|    fps              | 36       |
|    time_elapsed     | 17241    |
|    total_timesteps  | 623884   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 130970   |
----------------------------------
Eval num_timesteps=624000, episode_reward=3.86 +/- 7.64
Episode length: 121.16 +/- 68.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 624000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0433   |
|    n_updates        | 130999   |
----------------------------------
Eval num_timesteps=624500, episode_reward=2.10 +/- 3.40
Episode length: 110.72 +/- 81.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 624500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0976   |
|    n_updates        | 131124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 6.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5820     |
|    fps              | 36       |
|    time_elapsed     | 17269    |
|    total_timesteps  | 624789   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 131197   |
----------------------------------
Eval num_timesteps=625000, episode_reward=3.34 +/- 4.79
Episode length: 132.46 +/- 110.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 625000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0709   |
|    n_updates        | 131249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.41     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5824     |
|    fps              | 36       |
|    time_elapsed     | 17284    |
|    total_timesteps  | 625151   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 131287   |
----------------------------------
Eval num_timesteps=625500, episode_reward=4.20 +/- 6.31
Episode length: 125.30 +/- 51.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 625500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 131374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.28     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5828     |
|    fps              | 36       |
|    time_elapsed     | 17299    |
|    total_timesteps  | 625610   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.195    |
|    n_updates        | 131402   |
----------------------------------
Eval num_timesteps=626000, episode_reward=2.74 +/- 4.50
Episode length: 107.84 +/- 47.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 626000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0719   |
|    n_updates        | 131499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5832     |
|    fps              | 36       |
|    time_elapsed     | 17313    |
|    total_timesteps  | 626173   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0641   |
|    n_updates        | 131543   |
----------------------------------
Eval num_timesteps=626500, episode_reward=4.36 +/- 8.35
Episode length: 143.50 +/- 100.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 626500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0402   |
|    n_updates        | 131624   |
----------------------------------
Eval num_timesteps=627000, episode_reward=4.90 +/- 7.16
Episode length: 146.36 +/- 92.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 4.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 627000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0284   |
|    n_updates        | 131749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.53     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5836     |
|    fps              | 36       |
|    time_elapsed     | 17347    |
|    total_timesteps  | 627093   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0467   |
|    n_updates        | 131773   |
----------------------------------
Eval num_timesteps=627500, episode_reward=5.40 +/- 9.84
Episode length: 143.40 +/- 85.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 5.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 627500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0968   |
|    n_updates        | 131874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.5      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5840     |
|    fps              | 36       |
|    time_elapsed     | 17365    |
|    total_timesteps  | 627741   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 131935   |
----------------------------------
Eval num_timesteps=628000, episode_reward=6.02 +/- 9.19
Episode length: 126.56 +/- 74.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 6.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 628000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 131999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5844     |
|    fps              | 36       |
|    time_elapsed     | 17380    |
|    total_timesteps  | 628089   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0468   |
|    n_updates        | 132022   |
----------------------------------
Eval num_timesteps=628500, episode_reward=5.56 +/- 7.97
Episode length: 113.00 +/- 67.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 5.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 628500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 132124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 5.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5848     |
|    fps              | 36       |
|    time_elapsed     | 17395    |
|    total_timesteps  | 628657   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.186    |
|    n_updates        | 132164   |
----------------------------------
Eval num_timesteps=629000, episode_reward=3.92 +/- 6.30
Episode length: 129.66 +/- 92.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 3.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 629000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 132249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5852     |
|    fps              | 36       |
|    time_elapsed     | 17411    |
|    total_timesteps  | 629368   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0879   |
|    n_updates        | 132341   |
----------------------------------
Eval num_timesteps=629500, episode_reward=3.02 +/- 4.47
Episode length: 104.82 +/- 40.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 629500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 132374   |
----------------------------------
Eval num_timesteps=630000, episode_reward=4.82 +/- 7.56
Episode length: 137.14 +/- 73.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 4.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 630000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.132    |
|    n_updates        | 132499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 5.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5856     |
|    fps              | 36       |
|    time_elapsed     | 17439    |
|    total_timesteps  | 630051   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.272    |
|    n_updates        | 132512   |
----------------------------------
Eval num_timesteps=630500, episode_reward=5.02 +/- 8.91
Episode length: 135.78 +/- 93.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 5.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 630500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 132624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5860     |
|    fps              | 36       |
|    time_elapsed     | 17459    |
|    total_timesteps  | 630618   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.345    |
|    n_updates        | 132654   |
----------------------------------
Eval num_timesteps=631000, episode_reward=5.20 +/- 7.58
Episode length: 133.50 +/- 71.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 631000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.211    |
|    n_updates        | 132749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 6.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5864     |
|    fps              | 36       |
|    time_elapsed     | 17475    |
|    total_timesteps  | 631055   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 132763   |
----------------------------------
Eval num_timesteps=631500, episode_reward=5.34 +/- 10.32
Episode length: 122.80 +/- 79.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 631500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.037    |
|    n_updates        | 132874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 6.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5868     |
|    fps              | 36       |
|    time_elapsed     | 17493    |
|    total_timesteps  | 631757   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 132939   |
----------------------------------
Eval num_timesteps=632000, episode_reward=5.00 +/- 8.98
Episode length: 127.88 +/- 74.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 632000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0626   |
|    n_updates        | 132999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 6.52     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5872     |
|    fps              | 36       |
|    time_elapsed     | 17510    |
|    total_timesteps  | 632454   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.206    |
|    n_updates        | 133113   |
----------------------------------
Eval num_timesteps=632500, episode_reward=2.72 +/- 4.68
Episode length: 111.26 +/- 83.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 632500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0266   |
|    n_updates        | 133124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 6.38     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5876     |
|    fps              | 36       |
|    time_elapsed     | 17524    |
|    total_timesteps  | 632901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 133225   |
----------------------------------
Eval num_timesteps=633000, episode_reward=3.06 +/- 5.09
Episode length: 94.38 +/- 42.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 633000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0657   |
|    n_updates        | 133249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5880     |
|    fps              | 36       |
|    time_elapsed     | 17536    |
|    total_timesteps  | 633113   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0785   |
|    n_updates        | 133278   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 5.81     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5884     |
|    fps              | 36       |
|    time_elapsed     | 17537    |
|    total_timesteps  | 633418   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.077    |
|    n_updates        | 133354   |
----------------------------------
Eval num_timesteps=633500, episode_reward=6.56 +/- 12.07
Episode length: 159.06 +/- 123.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 6.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 633500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.187    |
|    n_updates        | 133374   |
----------------------------------
Eval num_timesteps=634000, episode_reward=6.70 +/- 13.24
Episode length: 156.30 +/- 120.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 6.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 634000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 133499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 6.02     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5888     |
|    fps              | 36       |
|    time_elapsed     | 17576    |
|    total_timesteps  | 634499   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 133624   |
----------------------------------
Eval num_timesteps=634500, episode_reward=2.88 +/- 4.07
Episode length: 101.66 +/- 39.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 2.88     |
| time/              |          |
|    total_timesteps | 634500   |
---------------------------------
Eval num_timesteps=635000, episode_reward=1.94 +/- 3.85
Episode length: 101.58 +/- 83.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 1.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 635000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0916   |
|    n_updates        | 133749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 6.13     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5892     |
|    fps              | 36       |
|    time_elapsed     | 17601    |
|    total_timesteps  | 635313   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0858   |
|    n_updates        | 133828   |
----------------------------------
Eval num_timesteps=635500, episode_reward=6.34 +/- 10.41
Episode length: 124.54 +/- 72.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 6.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 635500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0602   |
|    n_updates        | 133874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 5.87     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5896     |
|    fps              | 36       |
|    time_elapsed     | 17616    |
|    total_timesteps  | 635679   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0389   |
|    n_updates        | 133919   |
----------------------------------
Eval num_timesteps=636000, episode_reward=3.22 +/- 5.51
Episode length: 108.64 +/- 57.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 636000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 133999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 6.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5900     |
|    fps              | 36       |
|    time_elapsed     | 17630    |
|    total_timesteps  | 636245   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0796   |
|    n_updates        | 134061   |
----------------------------------
Eval num_timesteps=636500, episode_reward=5.22 +/- 8.48
Episode length: 159.76 +/- 111.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 160      |
|    mean_reward      | 5.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 636500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 134124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 6.2      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5904     |
|    fps              | 36       |
|    time_elapsed     | 17650    |
|    total_timesteps  | 636793   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0343   |
|    n_updates        | 134198   |
----------------------------------
Eval num_timesteps=637000, episode_reward=3.50 +/- 5.14
Episode length: 109.60 +/- 84.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 637000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.128    |
|    n_updates        | 134249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 6.51     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5908     |
|    fps              | 36       |
|    time_elapsed     | 17666    |
|    total_timesteps  | 637366   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0305   |
|    n_updates        | 134341   |
----------------------------------
Eval num_timesteps=637500, episode_reward=3.28 +/- 4.84
Episode length: 107.50 +/- 60.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 637500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 134374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 6.75     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5912     |
|    fps              | 36       |
|    time_elapsed     | 17682    |
|    total_timesteps  | 637967   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 134491   |
----------------------------------
Eval num_timesteps=638000, episode_reward=3.98 +/- 6.24
Episode length: 110.84 +/- 51.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 638000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0816   |
|    n_updates        | 134499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 6.82     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5916     |
|    fps              | 36       |
|    time_elapsed     | 17697    |
|    total_timesteps  | 638490   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.165    |
|    n_updates        | 134622   |
----------------------------------
Eval num_timesteps=638500, episode_reward=5.70 +/- 8.29
Episode length: 125.84 +/- 74.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 638500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0417   |
|    n_updates        | 134624   |
----------------------------------
Eval num_timesteps=639000, episode_reward=5.90 +/- 12.18
Episode length: 117.68 +/- 89.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 5.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 639000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 134749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 6.61     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5920     |
|    fps              | 36       |
|    time_elapsed     | 17726    |
|    total_timesteps  | 639155   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 134788   |
----------------------------------
Eval num_timesteps=639500, episode_reward=3.56 +/- 5.06
Episode length: 111.00 +/- 59.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 639500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0614   |
|    n_updates        | 134874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 6.74     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5924     |
|    fps              | 36       |
|    time_elapsed     | 17740    |
|    total_timesteps  | 639594   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 134898   |
----------------------------------
Eval num_timesteps=640000, episode_reward=4.18 +/- 5.81
Episode length: 114.60 +/- 45.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 640000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0668   |
|    n_updates        | 134999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 6.97     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5928     |
|    fps              | 36       |
|    time_elapsed     | 17754    |
|    total_timesteps  | 640231   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 135057   |
----------------------------------
Eval num_timesteps=640500, episode_reward=3.54 +/- 5.41
Episode length: 108.12 +/- 46.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 640500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.076    |
|    n_updates        | 135124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 6.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5932     |
|    fps              | 36       |
|    time_elapsed     | 17767    |
|    total_timesteps  | 640634   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0302   |
|    n_updates        | 135158   |
----------------------------------
Eval num_timesteps=641000, episode_reward=4.36 +/- 6.15
Episode length: 122.16 +/- 60.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 641000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.041    |
|    n_updates        | 135249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 6.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5936     |
|    fps              | 36       |
|    time_elapsed     | 17783    |
|    total_timesteps  | 641178   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0519   |
|    n_updates        | 135294   |
----------------------------------
Eval num_timesteps=641500, episode_reward=2.34 +/- 3.58
Episode length: 113.90 +/- 86.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 641500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0625   |
|    n_updates        | 135374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 6.46     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5940     |
|    fps              | 36       |
|    time_elapsed     | 17798    |
|    total_timesteps  | 641552   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0312   |
|    n_updates        | 135387   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 6.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5944     |
|    fps              | 36       |
|    time_elapsed     | 17801    |
|    total_timesteps  | 641974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0643   |
|    n_updates        | 135493   |
----------------------------------
Eval num_timesteps=642000, episode_reward=6.28 +/- 8.19
Episode length: 143.76 +/- 65.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 6.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 642000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.17     |
|    n_updates        | 135499   |
----------------------------------
Eval num_timesteps=642500, episode_reward=4.10 +/- 6.08
Episode length: 105.24 +/- 47.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 642500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0657   |
|    n_updates        | 135624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 6.81     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5948     |
|    fps              | 36       |
|    time_elapsed     | 17832    |
|    total_timesteps  | 642731   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0474   |
|    n_updates        | 135682   |
----------------------------------
Eval num_timesteps=643000, episode_reward=4.60 +/- 10.34
Episode length: 115.20 +/- 76.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 643000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.2      |
|    n_updates        | 135749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 6.59     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5952     |
|    fps              | 36       |
|    time_elapsed     | 17846    |
|    total_timesteps  | 643184   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0298   |
|    n_updates        | 135795   |
----------------------------------
Eval num_timesteps=643500, episode_reward=2.84 +/- 6.66
Episode length: 108.14 +/- 53.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 643500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 135874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 6.26     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5956     |
|    fps              | 36       |
|    time_elapsed     | 17860    |
|    total_timesteps  | 643682   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0795   |
|    n_updates        | 135920   |
----------------------------------
Eval num_timesteps=644000, episode_reward=5.44 +/- 7.28
Episode length: 113.94 +/- 45.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 644000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0995   |
|    n_updates        | 135999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 6        |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5960     |
|    fps              | 36       |
|    time_elapsed     | 17874    |
|    total_timesteps  | 644125   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 136031   |
----------------------------------
Eval num_timesteps=644500, episode_reward=3.62 +/- 5.25
Episode length: 130.42 +/- 75.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 644500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0717   |
|    n_updates        | 136124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5964     |
|    fps              | 36       |
|    time_elapsed     | 17890    |
|    total_timesteps  | 644787   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0394   |
|    n_updates        | 136196   |
----------------------------------
Eval num_timesteps=645000, episode_reward=4.42 +/- 8.86
Episode length: 115.80 +/- 66.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 645000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 136249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 5.59     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5968     |
|    fps              | 36       |
|    time_elapsed     | 17904    |
|    total_timesteps  | 645266   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0525   |
|    n_updates        | 136316   |
----------------------------------
Eval num_timesteps=645500, episode_reward=3.20 +/- 5.16
Episode length: 109.04 +/- 41.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 645500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 136374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5972     |
|    fps              | 36       |
|    time_elapsed     | 17917    |
|    total_timesteps  | 645791   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 136447   |
----------------------------------
Eval num_timesteps=646000, episode_reward=2.06 +/- 3.71
Episode length: 118.48 +/- 82.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 646000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 136499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 5.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5976     |
|    fps              | 36       |
|    time_elapsed     | 17933    |
|    total_timesteps  | 646424   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 136605   |
----------------------------------
Eval num_timesteps=646500, episode_reward=4.82 +/- 8.60
Episode length: 134.88 +/- 76.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 4.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 646500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 136624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.64     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5980     |
|    fps              | 36       |
|    time_elapsed     | 17949    |
|    total_timesteps  | 646828   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.073    |
|    n_updates        | 136706   |
----------------------------------
Eval num_timesteps=647000, episode_reward=3.46 +/- 7.37
Episode length: 105.54 +/- 55.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 647000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.159    |
|    n_updates        | 136749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 5.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5984     |
|    fps              | 36       |
|    time_elapsed     | 17964    |
|    total_timesteps  | 647265   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0265   |
|    n_updates        | 136816   |
----------------------------------
Eval num_timesteps=647500, episode_reward=4.58 +/- 8.06
Episode length: 133.30 +/- 63.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 647500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0483   |
|    n_updates        | 136874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5988     |
|    fps              | 36       |
|    time_elapsed     | 17981    |
|    total_timesteps  | 647558   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0429   |
|    n_updates        | 136889   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.16     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5992     |
|    fps              | 36       |
|    time_elapsed     | 17983    |
|    total_timesteps  | 647940   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 136984   |
----------------------------------
Eval num_timesteps=648000, episode_reward=5.84 +/- 11.02
Episode length: 150.84 +/- 101.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 5.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 648000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0531   |
|    n_updates        | 136999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.37     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5996     |
|    fps              | 36       |
|    time_elapsed     | 18001    |
|    total_timesteps  | 648420   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0792   |
|    n_updates        | 137104   |
----------------------------------
Eval num_timesteps=648500, episode_reward=5.74 +/- 7.34
Episode length: 139.16 +/- 68.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 5.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 648500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0223   |
|    n_updates        | 137124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.09     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6000     |
|    fps              | 36       |
|    time_elapsed     | 18018    |
|    total_timesteps  | 648848   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0956   |
|    n_updates        | 137211   |
----------------------------------
Eval num_timesteps=649000, episode_reward=5.32 +/- 7.02
Episode length: 144.60 +/- 72.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 5.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 649000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 137249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6004     |
|    fps              | 36       |
|    time_elapsed     | 18035    |
|    total_timesteps  | 649388   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0328   |
|    n_updates        | 137346   |
----------------------------------
Eval num_timesteps=649500, episode_reward=5.48 +/- 13.17
Episode length: 143.10 +/- 116.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 649500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0665   |
|    n_updates        | 137374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.26     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6008     |
|    fps              | 36       |
|    time_elapsed     | 18052    |
|    total_timesteps  | 649992   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 137497   |
----------------------------------
Eval num_timesteps=650000, episode_reward=3.94 +/- 6.24
Episode length: 140.70 +/- 92.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 650000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0749   |
|    n_updates        | 137499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6012     |
|    fps              | 35       |
|    time_elapsed     | 18068    |
|    total_timesteps  | 650407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.244    |
|    n_updates        | 137601   |
----------------------------------
Eval num_timesteps=650500, episode_reward=4.62 +/- 5.89
Episode length: 142.58 +/- 89.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 4.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 650500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.216    |
|    n_updates        | 137624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6016     |
|    fps              | 35       |
|    time_elapsed     | 18085    |
|    total_timesteps  | 650911   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 137727   |
----------------------------------
Eval num_timesteps=651000, episode_reward=3.52 +/- 4.56
Episode length: 112.52 +/- 85.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 651000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 137749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.81     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6020     |
|    fps              | 35       |
|    time_elapsed     | 18099    |
|    total_timesteps  | 651402   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0692   |
|    n_updates        | 137850   |
----------------------------------
Eval num_timesteps=651500, episode_reward=4.04 +/- 5.96
Episode length: 122.56 +/- 59.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 651500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0713   |
|    n_updates        | 137874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6024     |
|    fps              | 35       |
|    time_elapsed     | 18114    |
|    total_timesteps  | 651805   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0551   |
|    n_updates        | 137951   |
----------------------------------
Eval num_timesteps=652000, episode_reward=5.66 +/- 8.98
Episode length: 152.78 +/- 99.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 153      |
|    mean_reward      | 5.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 652000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 137999   |
----------------------------------
Eval num_timesteps=652500, episode_reward=4.80 +/- 5.48
Episode length: 117.62 +/- 47.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 652500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.204    |
|    n_updates        | 138124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6028     |
|    fps              | 35       |
|    time_elapsed     | 18146    |
|    total_timesteps  | 652806   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0351   |
|    n_updates        | 138201   |
----------------------------------
Eval num_timesteps=653000, episode_reward=3.74 +/- 8.03
Episode length: 124.36 +/- 59.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 653000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0541   |
|    n_updates        | 138249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6032     |
|    fps              | 35       |
|    time_elapsed     | 18160    |
|    total_timesteps  | 653143   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0594   |
|    n_updates        | 138285   |
----------------------------------
Eval num_timesteps=653500, episode_reward=4.58 +/- 6.19
Episode length: 127.48 +/- 62.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 653500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0996   |
|    n_updates        | 138374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.59     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6036     |
|    fps              | 35       |
|    time_elapsed     | 18176    |
|    total_timesteps  | 653946   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 138486   |
----------------------------------
Eval num_timesteps=654000, episode_reward=2.84 +/- 3.77
Episode length: 109.90 +/- 41.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 654000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.287    |
|    n_updates        | 138499   |
----------------------------------
Eval num_timesteps=654500, episode_reward=4.84 +/- 6.17
Episode length: 129.86 +/- 89.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 654500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.083    |
|    n_updates        | 138624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6040     |
|    fps              | 35       |
|    time_elapsed     | 18204    |
|    total_timesteps  | 654559   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 138639   |
----------------------------------
Eval num_timesteps=655000, episode_reward=4.20 +/- 7.68
Episode length: 126.22 +/- 86.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 655000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0286   |
|    n_updates        | 138749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5        |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6044     |
|    fps              | 35       |
|    time_elapsed     | 18219    |
|    total_timesteps  | 655044   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 138760   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6048     |
|    fps              | 35       |
|    time_elapsed     | 18221    |
|    total_timesteps  | 655422   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 138855   |
----------------------------------
Eval num_timesteps=655500, episode_reward=4.60 +/- 7.39
Episode length: 125.74 +/- 63.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 655500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.287    |
|    n_updates        | 138874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6052     |
|    fps              | 35       |
|    time_elapsed     | 18237    |
|    total_timesteps  | 655990   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 138997   |
----------------------------------
Eval num_timesteps=656000, episode_reward=2.52 +/- 3.75
Episode length: 110.96 +/- 52.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 656000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0474   |
|    n_updates        | 138999   |
----------------------------------
Eval num_timesteps=656500, episode_reward=3.74 +/- 6.28
Episode length: 121.60 +/- 73.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 656500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0618   |
|    n_updates        | 139124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6056     |
|    fps              | 35       |
|    time_elapsed     | 18266    |
|    total_timesteps  | 656575   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 139143   |
----------------------------------
Eval num_timesteps=657000, episode_reward=3.26 +/- 4.24
Episode length: 133.60 +/- 94.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 657000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 139249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6060     |
|    fps              | 35       |
|    time_elapsed     | 18285    |
|    total_timesteps  | 657097   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 139274   |
----------------------------------
Eval num_timesteps=657500, episode_reward=5.38 +/- 7.77
Episode length: 128.48 +/- 54.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 657500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0321   |
|    n_updates        | 139374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6064     |
|    fps              | 35       |
|    time_elapsed     | 18302    |
|    total_timesteps  | 657548   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0775   |
|    n_updates        | 139386   |
----------------------------------
Eval num_timesteps=658000, episode_reward=4.76 +/- 8.32
Episode length: 134.64 +/- 94.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 658000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 139499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6068     |
|    fps              | 35       |
|    time_elapsed     | 18320    |
|    total_timesteps  | 658304   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0522   |
|    n_updates        | 139575   |
----------------------------------
Eval num_timesteps=658500, episode_reward=3.88 +/- 5.39
Episode length: 139.98 +/- 90.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 3.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 658500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0699   |
|    n_updates        | 139624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.18     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6072     |
|    fps              | 35       |
|    time_elapsed     | 18336    |
|    total_timesteps  | 658631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0778   |
|    n_updates        | 139657   |
----------------------------------
Eval num_timesteps=659000, episode_reward=2.50 +/- 3.87
Episode length: 110.62 +/- 49.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 659000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0508   |
|    n_updates        | 139749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6076     |
|    fps              | 35       |
|    time_elapsed     | 18351    |
|    total_timesteps  | 659075   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 139768   |
----------------------------------
Eval num_timesteps=659500, episode_reward=4.24 +/- 6.97
Episode length: 122.46 +/- 63.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 659500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0254   |
|    n_updates        | 139874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 3.84     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6080     |
|    fps              | 35       |
|    time_elapsed     | 18366    |
|    total_timesteps  | 659545   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0844   |
|    n_updates        | 139886   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6084     |
|    fps              | 35       |
|    time_elapsed     | 18367    |
|    total_timesteps  | 659950   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0608   |
|    n_updates        | 139987   |
----------------------------------
Eval num_timesteps=660000, episode_reward=3.54 +/- 7.65
Episode length: 118.18 +/- 91.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 660000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0329   |
|    n_updates        | 139999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6088     |
|    fps              | 35       |
|    time_elapsed     | 18381    |
|    total_timesteps  | 660271   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.184    |
|    n_updates        | 140067   |
----------------------------------
Eval num_timesteps=660500, episode_reward=4.18 +/- 7.70
Episode length: 111.24 +/- 65.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 660500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0419   |
|    n_updates        | 140124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6092     |
|    fps              | 35       |
|    time_elapsed     | 18396    |
|    total_timesteps  | 660933   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0428   |
|    n_updates        | 140233   |
----------------------------------
Eval num_timesteps=661000, episode_reward=3.74 +/- 5.86
Episode length: 130.64 +/- 56.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 661000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0519   |
|    n_updates        | 140249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6096     |
|    fps              | 35       |
|    time_elapsed     | 18412    |
|    total_timesteps  | 661312   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0655   |
|    n_updates        | 140327   |
----------------------------------
Eval num_timesteps=661500, episode_reward=3.96 +/- 4.77
Episode length: 137.76 +/- 95.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 661500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0419   |
|    n_updates        | 140374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6100     |
|    fps              | 35       |
|    time_elapsed     | 18430    |
|    total_timesteps  | 661704   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0276   |
|    n_updates        | 140425   |
----------------------------------
Eval num_timesteps=662000, episode_reward=3.98 +/- 7.39
Episode length: 123.02 +/- 58.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 3.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 662000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0707   |
|    n_updates        | 140499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6104     |
|    fps              | 35       |
|    time_elapsed     | 18445    |
|    total_timesteps  | 662182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.085    |
|    n_updates        | 140545   |
----------------------------------
Eval num_timesteps=662500, episode_reward=4.84 +/- 9.21
Episode length: 124.90 +/- 86.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 662500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.066    |
|    n_updates        | 140624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6108     |
|    fps              | 35       |
|    time_elapsed     | 18460    |
|    total_timesteps  | 662602   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0252   |
|    n_updates        | 140650   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6112     |
|    fps              | 35       |
|    time_elapsed     | 18461    |
|    total_timesteps  | 662994   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.05     |
|    n_updates        | 140748   |
----------------------------------
Eval num_timesteps=663000, episode_reward=4.10 +/- 8.24
Episode length: 138.08 +/- 112.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 663000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.074    |
|    n_updates        | 140749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6116     |
|    fps              | 35       |
|    time_elapsed     | 18478    |
|    total_timesteps  | 663442   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0546   |
|    n_updates        | 140860   |
----------------------------------
Eval num_timesteps=663500, episode_reward=2.46 +/- 5.61
Episode length: 109.96 +/- 59.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 663500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00756  |
|    n_updates        | 140874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.42     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6120     |
|    fps              | 35       |
|    time_elapsed     | 18492    |
|    total_timesteps  | 663924   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 140980   |
----------------------------------
Eval num_timesteps=664000, episode_reward=3.06 +/- 3.28
Episode length: 110.12 +/- 40.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 664000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.026    |
|    n_updates        | 140999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6124     |
|    fps              | 35       |
|    time_elapsed     | 18505    |
|    total_timesteps  | 664437   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0833   |
|    n_updates        | 141109   |
----------------------------------
Eval num_timesteps=664500, episode_reward=3.12 +/- 6.23
Episode length: 122.20 +/- 86.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 664500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0544   |
|    n_updates        | 141124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6128     |
|    fps              | 35       |
|    time_elapsed     | 18519    |
|    total_timesteps  | 664819   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 141204   |
----------------------------------
Eval num_timesteps=665000, episode_reward=4.14 +/- 7.25
Episode length: 124.68 +/- 63.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 665000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0392   |
|    n_updates        | 141249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6132     |
|    fps              | 35       |
|    time_elapsed     | 18535    |
|    total_timesteps  | 665169   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 141292   |
----------------------------------
Eval num_timesteps=665500, episode_reward=2.46 +/- 4.34
Episode length: 104.18 +/- 43.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 665500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0541   |
|    n_updates        | 141374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6136     |
|    fps              | 35       |
|    time_elapsed     | 18547    |
|    total_timesteps  | 665599   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0356   |
|    n_updates        | 141399   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6140     |
|    fps              | 35       |
|    time_elapsed     | 18549    |
|    total_timesteps  | 665975   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0689   |
|    n_updates        | 141493   |
----------------------------------
Eval num_timesteps=666000, episode_reward=2.60 +/- 3.30
Episode length: 100.56 +/- 29.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 666000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0859   |
|    n_updates        | 141499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6144     |
|    fps              | 35       |
|    time_elapsed     | 18562    |
|    total_timesteps  | 666398   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 141599   |
----------------------------------
Eval num_timesteps=666500, episode_reward=3.32 +/- 6.09
Episode length: 110.82 +/- 38.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 666500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0912   |
|    n_updates        | 141624   |
----------------------------------
Eval num_timesteps=667000, episode_reward=3.00 +/- 6.08
Episode length: 126.42 +/- 89.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 667000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0559   |
|    n_updates        | 141749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6148     |
|    fps              | 35       |
|    time_elapsed     | 18591    |
|    total_timesteps  | 667099   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 141774   |
----------------------------------
Eval num_timesteps=667500, episode_reward=4.42 +/- 7.74
Episode length: 110.72 +/- 53.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 667500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 141874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6152     |
|    fps              | 35       |
|    time_elapsed     | 18608    |
|    total_timesteps  | 667658   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.256    |
|    n_updates        | 141914   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.15     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6156     |
|    fps              | 35       |
|    time_elapsed     | 18610    |
|    total_timesteps  | 667965   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 141991   |
----------------------------------
Eval num_timesteps=668000, episode_reward=3.10 +/- 4.73
Episode length: 122.34 +/- 86.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 668000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.193    |
|    n_updates        | 141999   |
----------------------------------
Eval num_timesteps=668500, episode_reward=3.16 +/- 4.30
Episode length: 114.22 +/- 45.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 668500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 142124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6160     |
|    fps              | 35       |
|    time_elapsed     | 18637    |
|    total_timesteps  | 668556   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 142138   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6164     |
|    fps              | 35       |
|    time_elapsed     | 18638    |
|    total_timesteps  | 668826   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 142206   |
----------------------------------
Eval num_timesteps=669000, episode_reward=2.92 +/- 5.16
Episode length: 104.56 +/- 44.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 669000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 142249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6168     |
|    fps              | 35       |
|    time_elapsed     | 18651    |
|    total_timesteps  | 669237   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0784   |
|    n_updates        | 142309   |
----------------------------------
Eval num_timesteps=669500, episode_reward=2.30 +/- 3.80
Episode length: 97.84 +/- 43.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.8     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 669500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0492   |
|    n_updates        | 142374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6172     |
|    fps              | 35       |
|    time_elapsed     | 18663    |
|    total_timesteps  | 669676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 142418   |
----------------------------------
Eval num_timesteps=670000, episode_reward=4.00 +/- 8.11
Episode length: 109.48 +/- 59.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 670000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0954   |
|    n_updates        | 142499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6176     |
|    fps              | 35       |
|    time_elapsed     | 18677    |
|    total_timesteps  | 670066   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0276   |
|    n_updates        | 142516   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6180     |
|    fps              | 35       |
|    time_elapsed     | 18678    |
|    total_timesteps  | 670434   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.205    |
|    n_updates        | 142608   |
----------------------------------
Eval num_timesteps=670500, episode_reward=3.64 +/- 4.78
Episode length: 118.44 +/- 48.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 670500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0976   |
|    n_updates        | 142624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.27     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6184     |
|    fps              | 35       |
|    time_elapsed     | 18693    |
|    total_timesteps  | 670868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0614   |
|    n_updates        | 142716   |
----------------------------------
Eval num_timesteps=671000, episode_reward=5.72 +/- 9.55
Episode length: 126.24 +/- 59.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 671000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0913   |
|    n_updates        | 142749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6188     |
|    fps              | 35       |
|    time_elapsed     | 18709    |
|    total_timesteps  | 671276   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00749  |
|    n_updates        | 142818   |
----------------------------------
Eval num_timesteps=671500, episode_reward=5.06 +/- 6.82
Episode length: 108.28 +/- 45.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 671500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 142874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6192     |
|    fps              | 35       |
|    time_elapsed     | 18723    |
|    total_timesteps  | 671665   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.035    |
|    n_updates        | 142916   |
----------------------------------
Eval num_timesteps=672000, episode_reward=3.38 +/- 4.60
Episode length: 117.30 +/- 53.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 672000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 142999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6196     |
|    fps              | 35       |
|    time_elapsed     | 18742    |
|    total_timesteps  | 672049   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 143012   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6200     |
|    fps              | 35       |
|    time_elapsed     | 18744    |
|    total_timesteps  | 672427   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0495   |
|    n_updates        | 143106   |
----------------------------------
Eval num_timesteps=672500, episode_reward=1.58 +/- 2.89
Episode length: 99.90 +/- 43.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.9     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 672500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0291   |
|    n_updates        | 143124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6204     |
|    fps              | 35       |
|    time_elapsed     | 18761    |
|    total_timesteps  | 672905   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.082    |
|    n_updates        | 143226   |
----------------------------------
Eval num_timesteps=673000, episode_reward=4.88 +/- 8.43
Episode length: 126.66 +/- 65.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 673000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 143249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6208     |
|    fps              | 35       |
|    time_elapsed     | 18780    |
|    total_timesteps  | 673377   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0622   |
|    n_updates        | 143344   |
----------------------------------
Eval num_timesteps=673500, episode_reward=2.74 +/- 4.13
Episode length: 111.24 +/- 48.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 673500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 143374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6212     |
|    fps              | 35       |
|    time_elapsed     | 18798    |
|    total_timesteps  | 673824   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 143455   |
----------------------------------
Eval num_timesteps=674000, episode_reward=2.20 +/- 3.71
Episode length: 99.16 +/- 38.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.2     |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 674000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 143499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6216     |
|    fps              | 35       |
|    time_elapsed     | 18813    |
|    total_timesteps  | 674266   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 143566   |
----------------------------------
Eval num_timesteps=674500, episode_reward=3.16 +/- 6.77
Episode length: 109.02 +/- 53.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 674500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00916  |
|    n_updates        | 143624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6220     |
|    fps              | 35       |
|    time_elapsed     | 18827    |
|    total_timesteps  | 674718   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0404   |
|    n_updates        | 143679   |
----------------------------------
Eval num_timesteps=675000, episode_reward=3.22 +/- 4.36
Episode length: 104.56 +/- 46.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 675000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 143749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6224     |
|    fps              | 35       |
|    time_elapsed     | 18839    |
|    total_timesteps  | 675168   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 143791   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6228     |
|    fps              | 35       |
|    time_elapsed     | 18840    |
|    total_timesteps  | 675457   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0762   |
|    n_updates        | 143864   |
----------------------------------
Eval num_timesteps=675500, episode_reward=3.74 +/- 5.21
Episode length: 119.70 +/- 57.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 675500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0625   |
|    n_updates        | 143874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6232     |
|    fps              | 35       |
|    time_elapsed     | 18855    |
|    total_timesteps  | 675847   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0845   |
|    n_updates        | 143961   |
----------------------------------
Eval num_timesteps=676000, episode_reward=1.68 +/- 2.94
Episode length: 98.64 +/- 36.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 1.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 676000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0498   |
|    n_updates        | 143999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6236     |
|    fps              | 35       |
|    time_elapsed     | 18866    |
|    total_timesteps  | 676175   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 144043   |
----------------------------------
Eval num_timesteps=676500, episode_reward=3.20 +/- 5.08
Episode length: 110.22 +/- 47.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 676500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0579   |
|    n_updates        | 144124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6240     |
|    fps              | 35       |
|    time_elapsed     | 18880    |
|    total_timesteps  | 676738   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 144184   |
----------------------------------
Eval num_timesteps=677000, episode_reward=2.42 +/- 3.32
Episode length: 93.76 +/- 33.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 677000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 144249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6244     |
|    fps              | 35       |
|    time_elapsed     | 18892    |
|    total_timesteps  | 677096   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0496   |
|    n_updates        | 144273   |
----------------------------------
Eval num_timesteps=677500, episode_reward=2.86 +/- 3.76
Episode length: 103.78 +/- 40.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 677500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 144374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6248     |
|    fps              | 35       |
|    time_elapsed     | 18904    |
|    total_timesteps  | 677526   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0351   |
|    n_updates        | 144381   |
----------------------------------
Eval num_timesteps=678000, episode_reward=4.30 +/- 6.25
Episode length: 128.42 +/- 86.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 678000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00774  |
|    n_updates        | 144499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6252     |
|    fps              | 35       |
|    time_elapsed     | 18920    |
|    total_timesteps  | 678110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 144527   |
----------------------------------
Eval num_timesteps=678500, episode_reward=2.76 +/- 4.39
Episode length: 108.22 +/- 43.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 678500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.054    |
|    n_updates        | 144624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6256     |
|    fps              | 35       |
|    time_elapsed     | 18933    |
|    total_timesteps  | 678609   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0319   |
|    n_updates        | 144652   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6260     |
|    fps              | 35       |
|    time_elapsed     | 18935    |
|    total_timesteps  | 678921   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 144730   |
----------------------------------
Eval num_timesteps=679000, episode_reward=3.16 +/- 4.62
Episode length: 99.44 +/- 45.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 3.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 679000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0186   |
|    n_updates        | 144749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.62     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6264     |
|    fps              | 35       |
|    time_elapsed     | 18947    |
|    total_timesteps  | 679498   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 144874   |
----------------------------------
Eval num_timesteps=679500, episode_reward=3.84 +/- 5.14
Episode length: 132.66 +/- 75.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 679500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6268     |
|    fps              | 35       |
|    time_elapsed     | 18963    |
|    total_timesteps  | 679995   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 144998   |
----------------------------------
Eval num_timesteps=680000, episode_reward=3.74 +/- 6.30
Episode length: 117.04 +/- 55.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 680000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0286   |
|    n_updates        | 144999   |
----------------------------------
Eval num_timesteps=680500, episode_reward=4.20 +/- 7.85
Episode length: 115.46 +/- 49.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 680500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0774   |
|    n_updates        | 145124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.64     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6272     |
|    fps              | 35       |
|    time_elapsed     | 18990    |
|    total_timesteps  | 680716   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 145178   |
----------------------------------
Eval num_timesteps=681000, episode_reward=2.08 +/- 3.24
Episode length: 92.30 +/- 36.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.3     |
|    mean_reward      | 2.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 681000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.184    |
|    n_updates        | 145249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6276     |
|    fps              | 35       |
|    time_elapsed     | 19002    |
|    total_timesteps  | 681355   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0564   |
|    n_updates        | 145338   |
----------------------------------
Eval num_timesteps=681500, episode_reward=3.00 +/- 4.39
Episode length: 104.58 +/- 39.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 681500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0939   |
|    n_updates        | 145374   |
----------------------------------
Eval num_timesteps=682000, episode_reward=4.42 +/- 7.72
Episode length: 120.32 +/- 58.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 682000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 145499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6280     |
|    fps              | 35       |
|    time_elapsed     | 19028    |
|    total_timesteps  | 682051   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 145512   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6284     |
|    fps              | 35       |
|    time_elapsed     | 19030    |
|    total_timesteps  | 682450   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 145612   |
----------------------------------
Eval num_timesteps=682500, episode_reward=2.30 +/- 4.08
Episode length: 101.74 +/- 43.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 682500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0466   |
|    n_updates        | 145624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6288     |
|    fps              | 35       |
|    time_elapsed     | 19043    |
|    total_timesteps  | 682920   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.048    |
|    n_updates        | 145729   |
----------------------------------
Eval num_timesteps=683000, episode_reward=4.94 +/- 9.14
Episode length: 120.84 +/- 59.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 683000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0486   |
|    n_updates        | 145749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6292     |
|    fps              | 35       |
|    time_elapsed     | 19058    |
|    total_timesteps  | 683247   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0655   |
|    n_updates        | 145811   |
----------------------------------
Eval num_timesteps=683500, episode_reward=3.18 +/- 6.00
Episode length: 102.94 +/- 50.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 683500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 145874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6296     |
|    fps              | 35       |
|    time_elapsed     | 19072    |
|    total_timesteps  | 683811   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00643  |
|    n_updates        | 145952   |
----------------------------------
Eval num_timesteps=684000, episode_reward=2.94 +/- 5.12
Episode length: 104.12 +/- 47.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 684000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 145999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6300     |
|    fps              | 35       |
|    time_elapsed     | 19085    |
|    total_timesteps  | 684350   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 146087   |
----------------------------------
Eval num_timesteps=684500, episode_reward=3.60 +/- 4.66
Episode length: 103.98 +/- 43.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 684500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 146124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6304     |
|    fps              | 35       |
|    time_elapsed     | 19098    |
|    total_timesteps  | 684848   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0855   |
|    n_updates        | 146211   |
----------------------------------
Eval num_timesteps=685000, episode_reward=2.74 +/- 4.42
Episode length: 99.32 +/- 37.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 685000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 146249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.47     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6308     |
|    fps              | 35       |
|    time_elapsed     | 19110    |
|    total_timesteps  | 685261   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00513  |
|    n_updates        | 146315   |
----------------------------------
Eval num_timesteps=685500, episode_reward=4.88 +/- 7.36
Episode length: 129.78 +/- 64.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 685500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 146374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6312     |
|    fps              | 35       |
|    time_elapsed     | 19126    |
|    total_timesteps  | 685799   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.061    |
|    n_updates        | 146449   |
----------------------------------
Eval num_timesteps=686000, episode_reward=3.76 +/- 6.98
Episode length: 112.98 +/- 47.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 686000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 146499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6316     |
|    fps              | 35       |
|    time_elapsed     | 19142    |
|    total_timesteps  | 686188   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 146546   |
----------------------------------
Eval num_timesteps=686500, episode_reward=2.14 +/- 3.38
Episode length: 92.06 +/- 28.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.1     |
|    mean_reward      | 2.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 686500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0802   |
|    n_updates        | 146624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6320     |
|    fps              | 35       |
|    time_elapsed     | 19154    |
|    total_timesteps  | 686660   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0467   |
|    n_updates        | 146664   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.9      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6324     |
|    fps              | 35       |
|    time_elapsed     | 19156    |
|    total_timesteps  | 686974   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0593   |
|    n_updates        | 146743   |
----------------------------------
Eval num_timesteps=687000, episode_reward=5.90 +/- 8.91
Episode length: 126.36 +/- 64.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 687000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 146749   |
----------------------------------
Eval num_timesteps=687500, episode_reward=3.86 +/- 6.38
Episode length: 119.04 +/- 52.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 687500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 146874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6328     |
|    fps              | 35       |
|    time_elapsed     | 19184    |
|    total_timesteps  | 687585   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 146896   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.15     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6332     |
|    fps              | 35       |
|    time_elapsed     | 19186    |
|    total_timesteps  | 687931   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0294   |
|    n_updates        | 146982   |
----------------------------------
Eval num_timesteps=688000, episode_reward=3.26 +/- 4.12
Episode length: 101.26 +/- 41.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 688000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.166    |
|    n_updates        | 146999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6336     |
|    fps              | 35       |
|    time_elapsed     | 19200    |
|    total_timesteps  | 688479   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0687   |
|    n_updates        | 147119   |
----------------------------------
Eval num_timesteps=688500, episode_reward=4.04 +/- 5.65
Episode length: 120.38 +/- 49.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 688500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0598   |
|    n_updates        | 147124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6340     |
|    fps              | 35       |
|    time_elapsed     | 19214    |
|    total_timesteps  | 688913   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0708   |
|    n_updates        | 147228   |
----------------------------------
Eval num_timesteps=689000, episode_reward=2.68 +/- 4.65
Episode length: 116.54 +/- 56.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 689000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0509   |
|    n_updates        | 147249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.29     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6344     |
|    fps              | 35       |
|    time_elapsed     | 19229    |
|    total_timesteps  | 689406   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 147351   |
----------------------------------
Eval num_timesteps=689500, episode_reward=4.18 +/- 6.06
Episode length: 111.76 +/- 42.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 689500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 147374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6348     |
|    fps              | 35       |
|    time_elapsed     | 19242    |
|    total_timesteps  | 689812   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.093    |
|    n_updates        | 147452   |
----------------------------------
Eval num_timesteps=690000, episode_reward=5.64 +/- 9.46
Episode length: 122.14 +/- 57.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 5.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 690000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0987   |
|    n_updates        | 147499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6352     |
|    fps              | 35       |
|    time_elapsed     | 19259    |
|    total_timesteps  | 690213   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 147553   |
----------------------------------
Eval num_timesteps=690500, episode_reward=2.68 +/- 3.65
Episode length: 99.42 +/- 44.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 690500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 147624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6356     |
|    fps              | 35       |
|    time_elapsed     | 19272    |
|    total_timesteps  | 690770   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0399   |
|    n_updates        | 147692   |
----------------------------------
Eval num_timesteps=691000, episode_reward=2.12 +/- 4.90
Episode length: 99.06 +/- 44.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.1     |
|    mean_reward      | 2.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 691000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 147749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6360     |
|    fps              | 35       |
|    time_elapsed     | 19285    |
|    total_timesteps  | 691270   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0898   |
|    n_updates        | 147817   |
----------------------------------
Eval num_timesteps=691500, episode_reward=2.78 +/- 3.73
Episode length: 108.20 +/- 32.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 691500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 147874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.56     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6364     |
|    fps              | 35       |
|    time_elapsed     | 19300    |
|    total_timesteps  | 691743   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0532   |
|    n_updates        | 147935   |
----------------------------------
Eval num_timesteps=692000, episode_reward=3.46 +/- 4.92
Episode length: 104.80 +/- 49.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 692000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 147999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6368     |
|    fps              | 35       |
|    time_elapsed     | 19312    |
|    total_timesteps  | 692058   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 148014   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6372     |
|    fps              | 35       |
|    time_elapsed     | 19313    |
|    total_timesteps  | 692397   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 148099   |
----------------------------------
Eval num_timesteps=692500, episode_reward=4.78 +/- 8.03
Episode length: 117.74 +/- 56.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 692500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0599   |
|    n_updates        | 148124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6376     |
|    fps              | 35       |
|    time_elapsed     | 19331    |
|    total_timesteps  | 692872   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0347   |
|    n_updates        | 148217   |
----------------------------------
Eval num_timesteps=693000, episode_reward=2.60 +/- 4.40
Episode length: 89.58 +/- 32.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.6     |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 693000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 148249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6380     |
|    fps              | 35       |
|    time_elapsed     | 19342    |
|    total_timesteps  | 693187   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.085    |
|    n_updates        | 148296   |
----------------------------------
Eval num_timesteps=693500, episode_reward=3.02 +/- 4.55
Episode length: 113.46 +/- 52.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 693500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.066    |
|    n_updates        | 148374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6384     |
|    fps              | 35       |
|    time_elapsed     | 19356    |
|    total_timesteps  | 693588   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0636   |
|    n_updates        | 148396   |
----------------------------------
Eval num_timesteps=694000, episode_reward=2.32 +/- 3.12
Episode length: 99.64 +/- 37.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.6     |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 694000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0609   |
|    n_updates        | 148499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6388     |
|    fps              | 35       |
|    time_elapsed     | 19368    |
|    total_timesteps  | 694040   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.141    |
|    n_updates        | 148509   |
----------------------------------
Eval num_timesteps=694500, episode_reward=2.72 +/- 3.27
Episode length: 110.18 +/- 52.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 694500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 148624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6392     |
|    fps              | 35       |
|    time_elapsed     | 19381    |
|    total_timesteps  | 694591   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0516   |
|    n_updates        | 148647   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6396     |
|    fps              | 35       |
|    time_elapsed     | 19383    |
|    total_timesteps  | 694991   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 148747   |
----------------------------------
Eval num_timesteps=695000, episode_reward=3.56 +/- 4.73
Episode length: 111.96 +/- 43.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 695000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 148749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6400     |
|    fps              | 35       |
|    time_elapsed     | 19397    |
|    total_timesteps  | 695438   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0602   |
|    n_updates        | 148859   |
----------------------------------
Eval num_timesteps=695500, episode_reward=4.20 +/- 6.54
Episode length: 114.96 +/- 62.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 695500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.158    |
|    n_updates        | 148874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6404     |
|    fps              | 35       |
|    time_elapsed     | 19410    |
|    total_timesteps  | 695750   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 148937   |
----------------------------------
Eval num_timesteps=696000, episode_reward=3.56 +/- 6.57
Episode length: 112.82 +/- 47.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 696000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0317   |
|    n_updates        | 148999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6408     |
|    fps              | 35       |
|    time_elapsed     | 19424    |
|    total_timesteps  | 696035   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 149008   |
----------------------------------
Eval num_timesteps=696500, episode_reward=4.48 +/- 10.97
Episode length: 108.86 +/- 55.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 696500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0223   |
|    n_updates        | 149124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6412     |
|    fps              | 35       |
|    time_elapsed     | 19438    |
|    total_timesteps  | 696515   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0623   |
|    n_updates        | 149128   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6416     |
|    fps              | 35       |
|    time_elapsed     | 19439    |
|    total_timesteps  | 696879   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.179    |
|    n_updates        | 149219   |
----------------------------------
Eval num_timesteps=697000, episode_reward=4.04 +/- 5.81
Episode length: 110.66 +/- 56.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 697000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.031    |
|    n_updates        | 149249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6420     |
|    fps              | 35       |
|    time_elapsed     | 19453    |
|    total_timesteps  | 697300   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.115    |
|    n_updates        | 149324   |
----------------------------------
Eval num_timesteps=697500, episode_reward=2.42 +/- 3.64
Episode length: 101.90 +/- 55.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 697500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0661   |
|    n_updates        | 149374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6424     |
|    fps              | 35       |
|    time_elapsed     | 19466    |
|    total_timesteps  | 697807   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0443   |
|    n_updates        | 149451   |
----------------------------------
Eval num_timesteps=698000, episode_reward=3.06 +/- 3.82
Episode length: 93.50 +/- 32.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.5     |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 698000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 149499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.9      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6428     |
|    fps              | 35       |
|    time_elapsed     | 19481    |
|    total_timesteps  | 698252   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0718   |
|    n_updates        | 149562   |
----------------------------------
Eval num_timesteps=698500, episode_reward=2.60 +/- 3.50
Episode length: 105.34 +/- 41.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 698500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.061    |
|    n_updates        | 149624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6432     |
|    fps              | 35       |
|    time_elapsed     | 19499    |
|    total_timesteps  | 698766   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 149691   |
----------------------------------
Eval num_timesteps=699000, episode_reward=4.06 +/- 7.31
Episode length: 114.24 +/- 62.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 699000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.082    |
|    n_updates        | 149749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6436     |
|    fps              | 35       |
|    time_elapsed     | 19513    |
|    total_timesteps  | 699159   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0276   |
|    n_updates        | 149789   |
----------------------------------
Eval num_timesteps=699500, episode_reward=4.18 +/- 5.25
Episode length: 115.36 +/- 65.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 699500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.088    |
|    n_updates        | 149874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.38     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6440     |
|    fps              | 35       |
|    time_elapsed     | 19527    |
|    total_timesteps  | 699778   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0683   |
|    n_updates        | 149944   |
----------------------------------
Eval num_timesteps=700000, episode_reward=3.36 +/- 4.13
Episode length: 99.12 +/- 43.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.1     |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 700000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 149999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6444     |
|    fps              | 35       |
|    time_elapsed     | 19539    |
|    total_timesteps  | 700119   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0364   |
|    n_updates        | 150029   |
----------------------------------
Eval num_timesteps=700500, episode_reward=3.74 +/- 5.05
Episode length: 112.56 +/- 46.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 700500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 150124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6448     |
|    fps              | 35       |
|    time_elapsed     | 19555    |
|    total_timesteps  | 700595   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0733   |
|    n_updates        | 150148   |
----------------------------------
Eval num_timesteps=701000, episode_reward=4.26 +/- 6.81
Episode length: 111.86 +/- 55.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 701000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0436   |
|    n_updates        | 150249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6452     |
|    fps              | 35       |
|    time_elapsed     | 19570    |
|    total_timesteps  | 701067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0894   |
|    n_updates        | 150266   |
----------------------------------
Eval num_timesteps=701500, episode_reward=3.18 +/- 4.20
Episode length: 100.42 +/- 33.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 701500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0264   |
|    n_updates        | 150374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6456     |
|    fps              | 35       |
|    time_elapsed     | 19583    |
|    total_timesteps  | 701635   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0938   |
|    n_updates        | 150408   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 4.07     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6460     |
|    fps              | 35       |
|    time_elapsed     | 19584    |
|    total_timesteps  | 701936   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.017    |
|    n_updates        | 150483   |
----------------------------------
Eval num_timesteps=702000, episode_reward=4.60 +/- 10.45
Episode length: 113.30 +/- 58.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 702000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0751   |
|    n_updates        | 150499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6464     |
|    fps              | 35       |
|    time_elapsed     | 19598    |
|    total_timesteps  | 702429   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.127    |
|    n_updates        | 150607   |
----------------------------------
Eval num_timesteps=702500, episode_reward=2.92 +/- 4.89
Episode length: 108.44 +/- 43.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 702500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0764   |
|    n_updates        | 150624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6468     |
|    fps              | 35       |
|    time_elapsed     | 19611    |
|    total_timesteps  | 702843   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 150710   |
----------------------------------
Eval num_timesteps=703000, episode_reward=1.88 +/- 2.72
Episode length: 102.02 +/- 31.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 703000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.152    |
|    n_updates        | 150749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6472     |
|    fps              | 35       |
|    time_elapsed     | 19624    |
|    total_timesteps  | 703373   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.228    |
|    n_updates        | 150843   |
----------------------------------
Eval num_timesteps=703500, episode_reward=3.48 +/- 7.61
Episode length: 108.98 +/- 59.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 703500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0899   |
|    n_updates        | 150874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6476     |
|    fps              | 35       |
|    time_elapsed     | 19636    |
|    total_timesteps  | 703607   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 150901   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6480     |
|    fps              | 35       |
|    time_elapsed     | 19637    |
|    total_timesteps  | 703929   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0404   |
|    n_updates        | 150982   |
----------------------------------
Eval num_timesteps=704000, episode_reward=6.04 +/- 8.55
Episode length: 127.64 +/- 78.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 6.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 704000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0814   |
|    n_updates        | 150999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.82     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6484     |
|    fps              | 35       |
|    time_elapsed     | 19652    |
|    total_timesteps  | 704331   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0727   |
|    n_updates        | 151082   |
----------------------------------
Eval num_timesteps=704500, episode_reward=4.62 +/- 6.98
Episode length: 124.38 +/- 71.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 704500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 151124   |
----------------------------------
Eval num_timesteps=705000, episode_reward=3.30 +/- 7.68
Episode length: 111.22 +/- 56.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 705000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 151249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.18     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6488     |
|    fps              | 35       |
|    time_elapsed     | 19684    |
|    total_timesteps  | 705122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0358   |
|    n_updates        | 151280   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6492     |
|    fps              | 35       |
|    time_elapsed     | 19685    |
|    total_timesteps  | 705377   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0674   |
|    n_updates        | 151344   |
----------------------------------
Eval num_timesteps=705500, episode_reward=3.28 +/- 4.27
Episode length: 106.82 +/- 48.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 705500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0884   |
|    n_updates        | 151374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6496     |
|    fps              | 35       |
|    time_elapsed     | 19698    |
|    total_timesteps  | 705805   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.176    |
|    n_updates        | 151451   |
----------------------------------
Eval num_timesteps=706000, episode_reward=2.82 +/- 3.30
Episode length: 105.10 +/- 33.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 706000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.151    |
|    n_updates        | 151499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6500     |
|    fps              | 35       |
|    time_elapsed     | 19711    |
|    total_timesteps  | 706240   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0895   |
|    n_updates        | 151559   |
----------------------------------
Eval num_timesteps=706500, episode_reward=3.76 +/- 5.83
Episode length: 112.20 +/- 50.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 706500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0836   |
|    n_updates        | 151624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 4        |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6504     |
|    fps              | 35       |
|    time_elapsed     | 19724    |
|    total_timesteps  | 706674   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0272   |
|    n_updates        | 151668   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6508     |
|    fps              | 35       |
|    time_elapsed     | 19725    |
|    total_timesteps  | 706959   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0526   |
|    n_updates        | 151739   |
----------------------------------
Eval num_timesteps=707000, episode_reward=3.80 +/- 6.16
Episode length: 113.78 +/- 56.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 707000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 151749   |
----------------------------------
Eval num_timesteps=707500, episode_reward=2.50 +/- 4.58
Episode length: 105.90 +/- 47.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 707500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0963   |
|    n_updates        | 151874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.66     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6512     |
|    fps              | 35       |
|    time_elapsed     | 19751    |
|    total_timesteps  | 707557   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0981   |
|    n_updates        | 151889   |
----------------------------------
Eval num_timesteps=708000, episode_reward=4.38 +/- 8.11
Episode length: 114.42 +/- 55.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 708000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 151999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6516     |
|    fps              | 35       |
|    time_elapsed     | 19765    |
|    total_timesteps  | 708041   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0324   |
|    n_updates        | 152010   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6520     |
|    fps              | 35       |
|    time_elapsed     | 19767    |
|    total_timesteps  | 708460   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.063    |
|    n_updates        | 152114   |
----------------------------------
Eval num_timesteps=708500, episode_reward=3.44 +/- 5.72
Episode length: 110.52 +/- 58.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 708500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.148    |
|    n_updates        | 152124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6524     |
|    fps              | 35       |
|    time_elapsed     | 19780    |
|    total_timesteps  | 708786   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 152196   |
----------------------------------
Eval num_timesteps=709000, episode_reward=4.58 +/- 6.33
Episode length: 120.70 +/- 56.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 709000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0536   |
|    n_updates        | 152249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.52     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6528     |
|    fps              | 35       |
|    time_elapsed     | 19795    |
|    total_timesteps  | 709178   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 152294   |
----------------------------------
Eval num_timesteps=709500, episode_reward=3.24 +/- 4.62
Episode length: 119.52 +/- 52.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 709500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 152374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6532     |
|    fps              | 35       |
|    time_elapsed     | 19810    |
|    total_timesteps  | 709647   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 152411   |
----------------------------------
Eval num_timesteps=710000, episode_reward=3.02 +/- 4.05
Episode length: 109.56 +/- 51.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 710000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0428   |
|    n_updates        | 152499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6536     |
|    fps              | 35       |
|    time_elapsed     | 19823    |
|    total_timesteps  | 710055   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0439   |
|    n_updates        | 152513   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 2.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6540     |
|    fps              | 35       |
|    time_elapsed     | 19824    |
|    total_timesteps  | 710305   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.168    |
|    n_updates        | 152576   |
----------------------------------
Eval num_timesteps=710500, episode_reward=4.30 +/- 6.98
Episode length: 125.28 +/- 56.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 710500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0204   |
|    n_updates        | 152624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6544     |
|    fps              | 35       |
|    time_elapsed     | 19840    |
|    total_timesteps  | 710907   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0905   |
|    n_updates        | 152726   |
----------------------------------
Eval num_timesteps=711000, episode_reward=3.08 +/- 4.45
Episode length: 107.76 +/- 53.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 711000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0824   |
|    n_updates        | 152749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6548     |
|    fps              | 35       |
|    time_elapsed     | 19853    |
|    total_timesteps  | 711475   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0481   |
|    n_updates        | 152868   |
----------------------------------
Eval num_timesteps=711500, episode_reward=5.46 +/- 8.50
Episode length: 139.06 +/- 72.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 5.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 711500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 152874   |
----------------------------------
Eval num_timesteps=712000, episode_reward=4.64 +/- 6.70
Episode length: 148.02 +/- 97.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 712000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 152999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.51     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6552     |
|    fps              | 35       |
|    time_elapsed     | 19886    |
|    total_timesteps  | 712014   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.183    |
|    n_updates        | 153003   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6556     |
|    fps              | 35       |
|    time_elapsed     | 19888    |
|    total_timesteps  | 712322   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 153080   |
----------------------------------
Eval num_timesteps=712500, episode_reward=4.10 +/- 7.69
Episode length: 114.28 +/- 64.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 712500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0541   |
|    n_updates        | 153124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6560     |
|    fps              | 35       |
|    time_elapsed     | 19901    |
|    total_timesteps  | 712726   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.109    |
|    n_updates        | 153181   |
----------------------------------
Eval num_timesteps=713000, episode_reward=3.70 +/- 6.32
Episode length: 118.70 +/- 59.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 713000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0294   |
|    n_updates        | 153249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6564     |
|    fps              | 35       |
|    time_elapsed     | 19915    |
|    total_timesteps  | 713215   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.196    |
|    n_updates        | 153303   |
----------------------------------
Eval num_timesteps=713500, episode_reward=4.60 +/- 7.35
Episode length: 119.52 +/- 66.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 713500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0337   |
|    n_updates        | 153374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6568     |
|    fps              | 35       |
|    time_elapsed     | 19931    |
|    total_timesteps  | 713705   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0579   |
|    n_updates        | 153426   |
----------------------------------
Eval num_timesteps=714000, episode_reward=3.14 +/- 4.64
Episode length: 110.72 +/- 49.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 714000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0191   |
|    n_updates        | 153499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6572     |
|    fps              | 35       |
|    time_elapsed     | 19945    |
|    total_timesteps  | 714287   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0335   |
|    n_updates        | 153571   |
----------------------------------
Eval num_timesteps=714500, episode_reward=2.86 +/- 3.51
Episode length: 110.90 +/- 46.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 714500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0522   |
|    n_updates        | 153624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6576     |
|    fps              | 35       |
|    time_elapsed     | 19958    |
|    total_timesteps  | 714704   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0632   |
|    n_updates        | 153675   |
----------------------------------
Eval num_timesteps=715000, episode_reward=5.66 +/- 8.62
Episode length: 145.20 +/- 91.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 5.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 715000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.112    |
|    n_updates        | 153749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.46     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6580     |
|    fps              | 35       |
|    time_elapsed     | 19974    |
|    total_timesteps  | 715001   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0233   |
|    n_updates        | 153750   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.37     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6584     |
|    fps              | 35       |
|    time_elapsed     | 19975    |
|    total_timesteps  | 715358   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 153839   |
----------------------------------
Eval num_timesteps=715500, episode_reward=3.86 +/- 5.83
Episode length: 126.84 +/- 88.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 715500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0597   |
|    n_updates        | 153874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 2.87     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6588     |
|    fps              | 35       |
|    time_elapsed     | 19990    |
|    total_timesteps  | 715677   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0306   |
|    n_updates        | 153919   |
----------------------------------
Eval num_timesteps=716000, episode_reward=5.86 +/- 9.40
Episode length: 138.56 +/- 64.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 5.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 716000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0364   |
|    n_updates        | 153999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 2.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6592     |
|    fps              | 35       |
|    time_elapsed     | 20006    |
|    total_timesteps  | 716146   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 154036   |
----------------------------------
Eval num_timesteps=716500, episode_reward=4.72 +/- 5.64
Episode length: 144.06 +/- 78.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 716500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0696   |
|    n_updates        | 154124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 2.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6596     |
|    fps              | 35       |
|    time_elapsed     | 20024    |
|    total_timesteps  | 716649   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0811   |
|    n_updates        | 154162   |
----------------------------------
Eval num_timesteps=717000, episode_reward=4.04 +/- 6.65
Episode length: 120.30 +/- 59.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 717000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0642   |
|    n_updates        | 154249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.18     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6600     |
|    fps              | 35       |
|    time_elapsed     | 20038    |
|    total_timesteps  | 717149   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0744   |
|    n_updates        | 154287   |
----------------------------------
Eval num_timesteps=717500, episode_reward=5.92 +/- 10.84
Episode length: 130.70 +/- 70.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 5.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 717500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 154374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6604     |
|    fps              | 35       |
|    time_elapsed     | 20054    |
|    total_timesteps  | 717672   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 154417   |
----------------------------------
Eval num_timesteps=718000, episode_reward=6.16 +/- 9.05
Episode length: 148.00 +/- 86.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 718000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0744   |
|    n_updates        | 154499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6608     |
|    fps              | 35       |
|    time_elapsed     | 20071    |
|    total_timesteps  | 718114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0495   |
|    n_updates        | 154528   |
----------------------------------
Eval num_timesteps=718500, episode_reward=4.66 +/- 8.48
Episode length: 117.38 +/- 58.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 718500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0562   |
|    n_updates        | 154624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6612     |
|    fps              | 35       |
|    time_elapsed     | 20085    |
|    total_timesteps  | 718659   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 154664   |
----------------------------------
Eval num_timesteps=719000, episode_reward=2.04 +/- 4.44
Episode length: 105.12 +/- 44.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 719000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 154749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.42     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6616     |
|    fps              | 35       |
|    time_elapsed     | 20098    |
|    total_timesteps  | 719195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 154798   |
----------------------------------
Eval num_timesteps=719500, episode_reward=3.66 +/- 6.18
Episode length: 116.12 +/- 52.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 719500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0514   |
|    n_updates        | 154874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6620     |
|    fps              | 35       |
|    time_elapsed     | 20112    |
|    total_timesteps  | 719672   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0763   |
|    n_updates        | 154917   |
----------------------------------
Eval num_timesteps=720000, episode_reward=1.62 +/- 2.55
Episode length: 110.42 +/- 53.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 720000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0733   |
|    n_updates        | 154999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6624     |
|    fps              | 35       |
|    time_elapsed     | 20126    |
|    total_timesteps  | 720360   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0363   |
|    n_updates        | 155089   |
----------------------------------
Eval num_timesteps=720500, episode_reward=3.36 +/- 6.36
Episode length: 124.48 +/- 89.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 720500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0594   |
|    n_updates        | 155124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6628     |
|    fps              | 35       |
|    time_elapsed     | 20140    |
|    total_timesteps  | 720850   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0665   |
|    n_updates        | 155212   |
----------------------------------
Eval num_timesteps=721000, episode_reward=5.22 +/- 7.65
Episode length: 127.62 +/- 63.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 721000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0372   |
|    n_updates        | 155249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6632     |
|    fps              | 35       |
|    time_elapsed     | 20155    |
|    total_timesteps  | 721217   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0604   |
|    n_updates        | 155304   |
----------------------------------
Eval num_timesteps=721500, episode_reward=4.76 +/- 6.72
Episode length: 124.72 +/- 54.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 721500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0308   |
|    n_updates        | 155374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6636     |
|    fps              | 35       |
|    time_elapsed     | 20169    |
|    total_timesteps  | 721684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 155420   |
----------------------------------
Eval num_timesteps=722000, episode_reward=4.42 +/- 7.43
Episode length: 141.20 +/- 97.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 722000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.148    |
|    n_updates        | 155499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6640     |
|    fps              | 35       |
|    time_elapsed     | 20185    |
|    total_timesteps  | 722042   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0711   |
|    n_updates        | 155510   |
----------------------------------
Eval num_timesteps=722500, episode_reward=4.02 +/- 6.76
Episode length: 120.36 +/- 71.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 722500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 155624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.15     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6644     |
|    fps              | 35       |
|    time_elapsed     | 20200    |
|    total_timesteps  | 722704   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.06     |
|    n_updates        | 155675   |
----------------------------------
Eval num_timesteps=723000, episode_reward=3.54 +/- 5.82
Episode length: 117.00 +/- 56.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 723000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0185   |
|    n_updates        | 155749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.75     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6648     |
|    fps              | 35       |
|    time_elapsed     | 20214    |
|    total_timesteps  | 723297   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0795   |
|    n_updates        | 155824   |
----------------------------------
Eval num_timesteps=723500, episode_reward=5.44 +/- 10.44
Episode length: 118.14 +/- 73.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 723500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 155874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.65     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6652     |
|    fps              | 35       |
|    time_elapsed     | 20229    |
|    total_timesteps  | 723868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 155966   |
----------------------------------
Eval num_timesteps=724000, episode_reward=7.02 +/- 7.83
Episode length: 153.86 +/- 106.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 7.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 724000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0359   |
|    n_updates        | 155999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6656     |
|    fps              | 35       |
|    time_elapsed     | 20247    |
|    total_timesteps  | 724434   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0538   |
|    n_updates        | 156108   |
----------------------------------
Eval num_timesteps=724500, episode_reward=5.34 +/- 6.72
Episode length: 125.80 +/- 72.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 724500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0845   |
|    n_updates        | 156124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6660     |
|    fps              | 35       |
|    time_elapsed     | 20262    |
|    total_timesteps  | 724885   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 156221   |
----------------------------------
Eval num_timesteps=725000, episode_reward=6.08 +/- 10.68
Episode length: 123.14 +/- 70.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 6.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 725000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0598   |
|    n_updates        | 156249   |
----------------------------------
Eval num_timesteps=725500, episode_reward=3.12 +/- 3.91
Episode length: 113.74 +/- 42.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 725500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.053    |
|    n_updates        | 156374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6664     |
|    fps              | 35       |
|    time_elapsed     | 20290    |
|    total_timesteps  | 725635   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 156408   |
----------------------------------
Eval num_timesteps=726000, episode_reward=5.24 +/- 7.77
Episode length: 133.50 +/- 58.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 726000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0782   |
|    n_updates        | 156499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6668     |
|    fps              | 35       |
|    time_elapsed     | 20305    |
|    total_timesteps  | 726071   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0364   |
|    n_updates        | 156517   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6672     |
|    fps              | 35       |
|    time_elapsed     | 20307    |
|    total_timesteps  | 726460   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 156614   |
----------------------------------
Eval num_timesteps=726500, episode_reward=3.70 +/- 6.12
Episode length: 126.62 +/- 61.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 726500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 156624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6676     |
|    fps              | 35       |
|    time_elapsed     | 20322    |
|    total_timesteps  | 726896   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0357   |
|    n_updates        | 156723   |
----------------------------------
Eval num_timesteps=727000, episode_reward=2.34 +/- 4.13
Episode length: 100.30 +/- 40.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 727000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0946   |
|    n_updates        | 156749   |
----------------------------------
Eval num_timesteps=727500, episode_reward=3.56 +/- 6.00
Episode length: 127.54 +/- 82.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 727500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0651   |
|    n_updates        | 156874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6680     |
|    fps              | 35       |
|    time_elapsed     | 20349    |
|    total_timesteps  | 727517   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0514   |
|    n_updates        | 156879   |
----------------------------------
Eval num_timesteps=728000, episode_reward=3.66 +/- 5.70
Episode length: 123.90 +/- 65.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 728000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0375   |
|    n_updates        | 156999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.97     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6684     |
|    fps              | 35       |
|    time_elapsed     | 20367    |
|    total_timesteps  | 728159   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0537   |
|    n_updates        | 157039   |
----------------------------------
Eval num_timesteps=728500, episode_reward=2.98 +/- 6.05
Episode length: 103.66 +/- 45.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 728500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0415   |
|    n_updates        | 157124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 5.17     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6688     |
|    fps              | 35       |
|    time_elapsed     | 20379    |
|    total_timesteps  | 728593   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0551   |
|    n_updates        | 157148   |
----------------------------------
Eval num_timesteps=729000, episode_reward=3.86 +/- 6.19
Episode length: 118.06 +/- 51.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 729000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0317   |
|    n_updates        | 157249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6692     |
|    fps              | 35       |
|    time_elapsed     | 20394    |
|    total_timesteps  | 729161   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0545   |
|    n_updates        | 157290   |
----------------------------------
Eval num_timesteps=729500, episode_reward=4.76 +/- 7.79
Episode length: 121.36 +/- 55.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 729500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.019    |
|    n_updates        | 157374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.13     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6696     |
|    fps              | 35       |
|    time_elapsed     | 20409    |
|    total_timesteps  | 729686   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0243   |
|    n_updates        | 157421   |
----------------------------------
Eval num_timesteps=730000, episode_reward=5.14 +/- 8.02
Episode length: 116.04 +/- 57.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 730000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.17     |
|    n_updates        | 157499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.16     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6700     |
|    fps              | 35       |
|    time_elapsed     | 20424    |
|    total_timesteps  | 730424   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 157605   |
----------------------------------
Eval num_timesteps=730500, episode_reward=4.44 +/- 5.26
Episode length: 138.96 +/- 78.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 4.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 730500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 157624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6704     |
|    fps              | 35       |
|    time_elapsed     | 20440    |
|    total_timesteps  | 730887   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0796   |
|    n_updates        | 157721   |
----------------------------------
Eval num_timesteps=731000, episode_reward=3.26 +/- 6.05
Episode length: 109.88 +/- 50.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 731000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0356   |
|    n_updates        | 157749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.15     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6708     |
|    fps              | 35       |
|    time_elapsed     | 20453    |
|    total_timesteps  | 731365   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0232   |
|    n_updates        | 157841   |
----------------------------------
Eval num_timesteps=731500, episode_reward=5.08 +/- 6.85
Episode length: 142.06 +/- 65.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 5.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 731500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0655   |
|    n_updates        | 157874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.1      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6712     |
|    fps              | 35       |
|    time_elapsed     | 20470    |
|    total_timesteps  | 731776   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0878   |
|    n_updates        | 157943   |
----------------------------------
Eval num_timesteps=732000, episode_reward=5.56 +/- 8.61
Episode length: 121.20 +/- 67.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 5.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 732000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0791   |
|    n_updates        | 157999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5        |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6716     |
|    fps              | 35       |
|    time_elapsed     | 20484    |
|    total_timesteps  | 732226   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0767   |
|    n_updates        | 158056   |
----------------------------------
Eval num_timesteps=732500, episode_reward=5.68 +/- 8.00
Episode length: 127.68 +/- 56.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 732500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0745   |
|    n_updates        | 158124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6720     |
|    fps              | 35       |
|    time_elapsed     | 20499    |
|    total_timesteps  | 732710   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 158177   |
----------------------------------
Eval num_timesteps=733000, episode_reward=6.24 +/- 9.85
Episode length: 140.62 +/- 76.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 6.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 733000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0351   |
|    n_updates        | 158249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6724     |
|    fps              | 35       |
|    time_elapsed     | 20517    |
|    total_timesteps  | 733339   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.12     |
|    n_updates        | 158334   |
----------------------------------
Eval num_timesteps=733500, episode_reward=4.36 +/- 5.83
Episode length: 120.42 +/- 48.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 733500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0419   |
|    n_updates        | 158374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6728     |
|    fps              | 35       |
|    time_elapsed     | 20531    |
|    total_timesteps  | 733788   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.133    |
|    n_updates        | 158446   |
----------------------------------
Eval num_timesteps=734000, episode_reward=5.74 +/- 9.75
Episode length: 123.78 +/- 66.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 734000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.046    |
|    n_updates        | 158499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.17     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6732     |
|    fps              | 35       |
|    time_elapsed     | 20546    |
|    total_timesteps  | 734399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 158599   |
----------------------------------
Eval num_timesteps=734500, episode_reward=8.90 +/- 9.83
Episode length: 157.74 +/- 77.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 158      |
|    mean_reward      | 8.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 734500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 158624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.1      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6736     |
|    fps              | 35       |
|    time_elapsed     | 20564    |
|    total_timesteps  | 734893   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.05     |
|    n_updates        | 158723   |
----------------------------------
Eval num_timesteps=735000, episode_reward=2.94 +/- 4.38
Episode length: 130.20 +/- 84.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 735000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 158749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.49     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6740     |
|    fps              | 35       |
|    time_elapsed     | 20580    |
|    total_timesteps  | 735430   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 158857   |
----------------------------------
Eval num_timesteps=735500, episode_reward=4.54 +/- 8.29
Episode length: 123.90 +/- 64.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 735500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0278   |
|    n_updates        | 158874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.3      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6744     |
|    fps              | 35       |
|    time_elapsed     | 20594    |
|    total_timesteps  | 735855   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0653   |
|    n_updates        | 158963   |
----------------------------------
Eval num_timesteps=736000, episode_reward=2.68 +/- 4.14
Episode length: 107.88 +/- 46.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 736000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0647   |
|    n_updates        | 158999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.26     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6748     |
|    fps              | 35       |
|    time_elapsed     | 20608    |
|    total_timesteps  | 736393   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0252   |
|    n_updates        | 159098   |
----------------------------------
Eval num_timesteps=736500, episode_reward=4.36 +/- 5.71
Episode length: 136.20 +/- 64.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 736500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0485   |
|    n_updates        | 159124   |
----------------------------------
Eval num_timesteps=737000, episode_reward=2.84 +/- 3.94
Episode length: 113.80 +/- 42.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 737000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0616   |
|    n_updates        | 159249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6752     |
|    fps              | 35       |
|    time_elapsed     | 20638    |
|    total_timesteps  | 737159   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0309   |
|    n_updates        | 159289   |
----------------------------------
Eval num_timesteps=737500, episode_reward=5.32 +/- 7.70
Episode length: 143.78 +/- 87.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 5.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 737500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 159374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.47     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6756     |
|    fps              | 35       |
|    time_elapsed     | 20654    |
|    total_timesteps  | 737694   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0414   |
|    n_updates        | 159423   |
----------------------------------
Eval num_timesteps=738000, episode_reward=3.96 +/- 5.59
Episode length: 115.42 +/- 54.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 738000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.186    |
|    n_updates        | 159499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 6.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6760     |
|    fps              | 35       |
|    time_elapsed     | 20670    |
|    total_timesteps  | 738366   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0805   |
|    n_updates        | 159591   |
----------------------------------
Eval num_timesteps=738500, episode_reward=5.42 +/- 8.22
Episode length: 123.52 +/- 57.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 738500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0456   |
|    n_updates        | 159624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.8      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6764     |
|    fps              | 35       |
|    time_elapsed     | 20684    |
|    total_timesteps  | 738750   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0768   |
|    n_updates        | 159687   |
----------------------------------
Eval num_timesteps=739000, episode_reward=4.70 +/- 7.76
Episode length: 114.92 +/- 60.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 739000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.08     |
|    n_updates        | 159749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.59     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6768     |
|    fps              | 35       |
|    time_elapsed     | 20698    |
|    total_timesteps  | 739277   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 159819   |
----------------------------------
Eval num_timesteps=739500, episode_reward=3.42 +/- 5.01
Episode length: 115.34 +/- 55.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 739500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.181    |
|    n_updates        | 159874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6772     |
|    fps              | 35       |
|    time_elapsed     | 20711    |
|    total_timesteps  | 739589   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.188    |
|    n_updates        | 159897   |
----------------------------------
Eval num_timesteps=740000, episode_reward=4.18 +/- 7.09
Episode length: 132.54 +/- 70.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 740000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.088    |
|    n_updates        | 159999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6776     |
|    fps              | 35       |
|    time_elapsed     | 20727    |
|    total_timesteps  | 740160   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.028    |
|    n_updates        | 160039   |
----------------------------------
Eval num_timesteps=740500, episode_reward=4.72 +/- 8.66
Episode length: 126.86 +/- 65.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 740500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.126    |
|    n_updates        | 160124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.56     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6780     |
|    fps              | 35       |
|    time_elapsed     | 20742    |
|    total_timesteps  | 740596   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 160148   |
----------------------------------
Eval num_timesteps=741000, episode_reward=2.76 +/- 4.90
Episode length: 118.98 +/- 45.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 2.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 741000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0688   |
|    n_updates        | 160249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 5.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6784     |
|    fps              | 35       |
|    time_elapsed     | 20756    |
|    total_timesteps  | 741041   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 160260   |
----------------------------------
Eval num_timesteps=741500, episode_reward=3.40 +/- 5.28
Episode length: 108.90 +/- 47.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 741500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0403   |
|    n_updates        | 160374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.42     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6788     |
|    fps              | 35       |
|    time_elapsed     | 20770    |
|    total_timesteps  | 741725   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0534   |
|    n_updates        | 160431   |
----------------------------------
Eval num_timesteps=742000, episode_reward=2.42 +/- 3.27
Episode length: 115.58 +/- 46.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 742000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0573   |
|    n_updates        | 160499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.46     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6792     |
|    fps              | 35       |
|    time_elapsed     | 20784    |
|    total_timesteps  | 742191   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0694   |
|    n_updates        | 160547   |
----------------------------------
Eval num_timesteps=742500, episode_reward=3.76 +/- 5.03
Episode length: 110.50 +/- 59.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 742500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0602   |
|    n_updates        | 160624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 5.44     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6796     |
|    fps              | 35       |
|    time_elapsed     | 20798    |
|    total_timesteps  | 742611   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.05     |
|    n_updates        | 160652   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.07     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6800     |
|    fps              | 35       |
|    time_elapsed     | 20799    |
|    total_timesteps  | 742883   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0653   |
|    n_updates        | 160720   |
----------------------------------
Eval num_timesteps=743000, episode_reward=4.52 +/- 7.70
Episode length: 117.34 +/- 68.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 743000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0887   |
|    n_updates        | 160749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.18     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6804     |
|    fps              | 35       |
|    time_elapsed     | 20813    |
|    total_timesteps  | 743392   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 160847   |
----------------------------------
Eval num_timesteps=743500, episode_reward=3.34 +/- 5.70
Episode length: 110.12 +/- 52.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 743500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 160874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.18     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6808     |
|    fps              | 35       |
|    time_elapsed     | 20827    |
|    total_timesteps  | 743869   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 160967   |
----------------------------------
Eval num_timesteps=744000, episode_reward=2.44 +/- 3.49
Episode length: 105.14 +/- 38.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 744000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0565   |
|    n_updates        | 160999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6812     |
|    fps              | 35       |
|    time_elapsed     | 20840    |
|    total_timesteps  | 744307   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0937   |
|    n_updates        | 161076   |
----------------------------------
Eval num_timesteps=744500, episode_reward=3.64 +/- 5.88
Episode length: 119.54 +/- 62.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 744500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0793   |
|    n_updates        | 161124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.41     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6816     |
|    fps              | 35       |
|    time_elapsed     | 20855    |
|    total_timesteps  | 744830   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0489   |
|    n_updates        | 161207   |
----------------------------------
Eval num_timesteps=745000, episode_reward=3.74 +/- 5.11
Episode length: 117.56 +/- 50.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 745000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 161249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.28     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6820     |
|    fps              | 35       |
|    time_elapsed     | 20869    |
|    total_timesteps  | 745203   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0372   |
|    n_updates        | 161300   |
----------------------------------
Eval num_timesteps=745500, episode_reward=4.04 +/- 6.60
Episode length: 121.02 +/- 65.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 745500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0113   |
|    n_updates        | 161374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.37     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6824     |
|    fps              | 35       |
|    time_elapsed     | 20886    |
|    total_timesteps  | 745880   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0505   |
|    n_updates        | 161469   |
----------------------------------
Eval num_timesteps=746000, episode_reward=7.48 +/- 10.22
Episode length: 141.02 +/- 69.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 7.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 746000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0461   |
|    n_updates        | 161499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6828     |
|    fps              | 35       |
|    time_elapsed     | 20903    |
|    total_timesteps  | 746212   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 161552   |
----------------------------------
Eval num_timesteps=746500, episode_reward=3.66 +/- 5.52
Episode length: 109.68 +/- 53.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 746500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0799   |
|    n_updates        | 161624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.27     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6832     |
|    fps              | 35       |
|    time_elapsed     | 20916    |
|    total_timesteps  | 746681   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0426   |
|    n_updates        | 161670   |
----------------------------------
Eval num_timesteps=747000, episode_reward=3.48 +/- 6.89
Episode length: 116.68 +/- 51.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 747000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0415   |
|    n_updates        | 161749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6836     |
|    fps              | 35       |
|    time_elapsed     | 20933    |
|    total_timesteps  | 747324   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0989   |
|    n_updates        | 161830   |
----------------------------------
Eval num_timesteps=747500, episode_reward=5.52 +/- 8.49
Episode length: 133.14 +/- 66.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 5.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 747500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0727   |
|    n_updates        | 161874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 5.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6840     |
|    fps              | 35       |
|    time_elapsed     | 20949    |
|    total_timesteps  | 747651   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.123    |
|    n_updates        | 161912   |
----------------------------------
Eval num_timesteps=748000, episode_reward=4.56 +/- 5.98
Episode length: 113.92 +/- 50.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 748000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0443   |
|    n_updates        | 161999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.45     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6844     |
|    fps              | 35       |
|    time_elapsed     | 20963    |
|    total_timesteps  | 748212   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.039    |
|    n_updates        | 162052   |
----------------------------------
Eval num_timesteps=748500, episode_reward=4.00 +/- 5.83
Episode length: 107.34 +/- 56.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 748500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0536   |
|    n_updates        | 162124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.4      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6848     |
|    fps              | 35       |
|    time_elapsed     | 20977    |
|    total_timesteps  | 748649   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0256   |
|    n_updates        | 162162   |
----------------------------------
Eval num_timesteps=749000, episode_reward=2.98 +/- 4.93
Episode length: 114.50 +/- 58.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 749000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0394   |
|    n_updates        | 162249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6852     |
|    fps              | 35       |
|    time_elapsed     | 20991    |
|    total_timesteps  | 749184   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 162295   |
----------------------------------
Eval num_timesteps=749500, episode_reward=3.22 +/- 4.37
Episode length: 101.34 +/- 53.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 749500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 162374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 5.1      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6856     |
|    fps              | 35       |
|    time_elapsed     | 21005    |
|    total_timesteps  | 749697   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0658   |
|    n_updates        | 162424   |
----------------------------------
Eval num_timesteps=750000, episode_reward=2.88 +/- 5.08
Episode length: 101.98 +/- 46.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 750000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0476   |
|    n_updates        | 162499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.83     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6860     |
|    fps              | 35       |
|    time_elapsed     | 21017    |
|    total_timesteps  | 750232   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0645   |
|    n_updates        | 162557   |
----------------------------------
Eval num_timesteps=750500, episode_reward=2.92 +/- 6.37
Episode length: 98.32 +/- 49.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.3     |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 750500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 162624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 5.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6864     |
|    fps              | 35       |
|    time_elapsed     | 21030    |
|    total_timesteps  | 750917   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 162729   |
----------------------------------
Eval num_timesteps=751000, episode_reward=2.96 +/- 4.26
Episode length: 103.38 +/- 39.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 751000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0924   |
|    n_updates        | 162749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 5.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6868     |
|    fps              | 35       |
|    time_elapsed     | 21043    |
|    total_timesteps  | 751391   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0373   |
|    n_updates        | 162847   |
----------------------------------
Eval num_timesteps=751500, episode_reward=5.08 +/- 9.31
Episode length: 115.02 +/- 61.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 5.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 751500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.047    |
|    n_updates        | 162874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 5.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6872     |
|    fps              | 35       |
|    time_elapsed     | 21057    |
|    total_timesteps  | 751827   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 162956   |
----------------------------------
Eval num_timesteps=752000, episode_reward=2.96 +/- 4.90
Episode length: 101.18 +/- 48.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 752000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0481   |
|    n_updates        | 162999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6876     |
|    fps              | 35       |
|    time_elapsed     | 21070    |
|    total_timesteps  | 752444   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 163110   |
----------------------------------
Eval num_timesteps=752500, episode_reward=1.58 +/- 2.98
Episode length: 92.70 +/- 42.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.7     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 752500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 163124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 5.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6880     |
|    fps              | 35       |
|    time_elapsed     | 21081    |
|    total_timesteps  | 752834   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0793   |
|    n_updates        | 163208   |
----------------------------------
Eval num_timesteps=753000, episode_reward=3.10 +/- 6.41
Episode length: 101.00 +/- 54.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 753000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0263   |
|    n_updates        | 163249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 5.28     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6884     |
|    fps              | 35       |
|    time_elapsed     | 21094    |
|    total_timesteps  | 753235   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0473   |
|    n_updates        | 163308   |
----------------------------------
Eval num_timesteps=753500, episode_reward=6.28 +/- 9.93
Episode length: 133.30 +/- 78.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 6.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 753500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.153    |
|    n_updates        | 163374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6888     |
|    fps              | 35       |
|    time_elapsed     | 21113    |
|    total_timesteps  | 753769   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.094    |
|    n_updates        | 163442   |
----------------------------------
Eval num_timesteps=754000, episode_reward=4.06 +/- 5.21
Episode length: 121.08 +/- 56.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 754000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 163499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6892     |
|    fps              | 35       |
|    time_elapsed     | 21130    |
|    total_timesteps  | 754182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0408   |
|    n_updates        | 163545   |
----------------------------------
Eval num_timesteps=754500, episode_reward=2.54 +/- 3.83
Episode length: 105.32 +/- 53.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 754500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.185    |
|    n_updates        | 163624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6896     |
|    fps              | 35       |
|    time_elapsed     | 21144    |
|    total_timesteps  | 754623   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0598   |
|    n_updates        | 163655   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6900     |
|    fps              | 35       |
|    time_elapsed     | 21145    |
|    total_timesteps  | 754955   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0376   |
|    n_updates        | 163738   |
----------------------------------
Eval num_timesteps=755000, episode_reward=3.88 +/- 8.90
Episode length: 101.66 +/- 53.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 755000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0578   |
|    n_updates        | 163749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.77     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6904     |
|    fps              | 35       |
|    time_elapsed     | 21158    |
|    total_timesteps  | 755314   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0373   |
|    n_updates        | 163828   |
----------------------------------
Eval num_timesteps=755500, episode_reward=5.16 +/- 9.75
Episode length: 114.76 +/- 64.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 755500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 163874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6908     |
|    fps              | 35       |
|    time_elapsed     | 21173    |
|    total_timesteps  | 755731   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0316   |
|    n_updates        | 163932   |
----------------------------------
Eval num_timesteps=756000, episode_reward=2.18 +/- 2.87
Episode length: 100.90 +/- 34.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 756000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0276   |
|    n_updates        | 163999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.74     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6912     |
|    fps              | 35       |
|    time_elapsed     | 21186    |
|    total_timesteps  | 756323   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 164080   |
----------------------------------
Eval num_timesteps=756500, episode_reward=3.36 +/- 7.33
Episode length: 108.42 +/- 48.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 756500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0288   |
|    n_updates        | 164124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.52     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6916     |
|    fps              | 35       |
|    time_elapsed     | 21199    |
|    total_timesteps  | 756666   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 164166   |
----------------------------------
Eval num_timesteps=757000, episode_reward=2.36 +/- 2.73
Episode length: 98.26 +/- 37.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.3     |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 757000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0673   |
|    n_updates        | 164249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6920     |
|    fps              | 35       |
|    time_elapsed     | 21211    |
|    total_timesteps  | 757128   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0586   |
|    n_updates        | 164281   |
----------------------------------
Eval num_timesteps=757500, episode_reward=2.36 +/- 4.04
Episode length: 102.14 +/- 43.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 757500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.038    |
|    n_updates        | 164374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.26     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6924     |
|    fps              | 35       |
|    time_elapsed     | 21225    |
|    total_timesteps  | 757699   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0585   |
|    n_updates        | 164424   |
----------------------------------
Eval num_timesteps=758000, episode_reward=2.88 +/- 6.22
Episode length: 106.84 +/- 50.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 758000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0647   |
|    n_updates        | 164499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.29     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6928     |
|    fps              | 35       |
|    time_elapsed     | 21238    |
|    total_timesteps  | 758193   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0866   |
|    n_updates        | 164548   |
----------------------------------
Eval num_timesteps=758500, episode_reward=1.74 +/- 2.54
Episode length: 95.04 +/- 37.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95       |
|    mean_reward      | 1.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 758500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 164624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.2      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6932     |
|    fps              | 35       |
|    time_elapsed     | 21249    |
|    total_timesteps  | 758530   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 164632   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6936     |
|    fps              | 35       |
|    time_elapsed     | 21251    |
|    total_timesteps  | 758909   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 164727   |
----------------------------------
Eval num_timesteps=759000, episode_reward=4.72 +/- 6.68
Episode length: 116.56 +/- 58.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 759000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 164749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6940     |
|    fps              | 35       |
|    time_elapsed     | 21268    |
|    total_timesteps  | 759425   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 164856   |
----------------------------------
Eval num_timesteps=759500, episode_reward=2.66 +/- 3.33
Episode length: 96.22 +/- 37.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 759500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 164874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6944     |
|    fps              | 35       |
|    time_elapsed     | 21281    |
|    total_timesteps  | 759871   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 164967   |
----------------------------------
Eval num_timesteps=760000, episode_reward=2.88 +/- 3.77
Episode length: 110.64 +/- 43.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 760000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0848   |
|    n_updates        | 164999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6948     |
|    fps              | 35       |
|    time_elapsed     | 21294    |
|    total_timesteps  | 760306   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 165076   |
----------------------------------
Eval num_timesteps=760500, episode_reward=3.02 +/- 3.84
Episode length: 124.00 +/- 75.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 760500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0658   |
|    n_updates        | 165124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.54     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6952     |
|    fps              | 35       |
|    time_elapsed     | 21309    |
|    total_timesteps  | 760684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0306   |
|    n_updates        | 165170   |
----------------------------------
Eval num_timesteps=761000, episode_reward=3.20 +/- 7.15
Episode length: 98.60 +/- 55.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 3.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 761000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 165249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6956     |
|    fps              | 35       |
|    time_elapsed     | 21322    |
|    total_timesteps  | 761234   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0341   |
|    n_updates        | 165308   |
----------------------------------
Eval num_timesteps=761500, episode_reward=3.38 +/- 6.96
Episode length: 108.02 +/- 55.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 761500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.083    |
|    n_updates        | 165374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6960     |
|    fps              | 35       |
|    time_elapsed     | 21335    |
|    total_timesteps  | 761530   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 165382   |
----------------------------------
Eval num_timesteps=762000, episode_reward=3.42 +/- 5.35
Episode length: 105.22 +/- 51.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 762000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 165499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6964     |
|    fps              | 35       |
|    time_elapsed     | 21350    |
|    total_timesteps  | 762213   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0527   |
|    n_updates        | 165553   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6968     |
|    fps              | 35       |
|    time_elapsed     | 21351    |
|    total_timesteps  | 762495   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 165623   |
----------------------------------
Eval num_timesteps=762500, episode_reward=3.82 +/- 6.05
Episode length: 104.58 +/- 51.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 762500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 165624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 2.84     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6972     |
|    fps              | 35       |
|    time_elapsed     | 21364    |
|    total_timesteps  | 762931   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0766   |
|    n_updates        | 165732   |
----------------------------------
Eval num_timesteps=763000, episode_reward=4.58 +/- 7.44
Episode length: 116.40 +/- 55.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 763000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.172    |
|    n_updates        | 165749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 2.56     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6976     |
|    fps              | 35       |
|    time_elapsed     | 21378    |
|    total_timesteps  | 763339   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 165834   |
----------------------------------
Eval num_timesteps=763500, episode_reward=2.56 +/- 3.76
Episode length: 101.22 +/- 41.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 763500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0472   |
|    n_updates        | 165874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 2.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6980     |
|    fps              | 35       |
|    time_elapsed     | 21390    |
|    total_timesteps  | 763732   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0458   |
|    n_updates        | 165932   |
----------------------------------
Eval num_timesteps=764000, episode_reward=3.12 +/- 4.66
Episode length: 102.12 +/- 44.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 764000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0658   |
|    n_updates        | 165999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 2.59     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6984     |
|    fps              | 35       |
|    time_elapsed     | 21403    |
|    total_timesteps  | 764097   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0423   |
|    n_updates        | 166024   |
----------------------------------
Eval num_timesteps=764500, episode_reward=3.30 +/- 6.06
Episode length: 104.40 +/- 46.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 764500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0956   |
|    n_updates        | 166124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 2.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6988     |
|    fps              | 35       |
|    time_elapsed     | 21417    |
|    total_timesteps  | 764935   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0723   |
|    n_updates        | 166233   |
----------------------------------
Eval num_timesteps=765000, episode_reward=3.86 +/- 5.22
Episode length: 119.74 +/- 56.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 765000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0205   |
|    n_updates        | 166249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 2.89     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6992     |
|    fps              | 35       |
|    time_elapsed     | 21432    |
|    total_timesteps  | 765424   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.169    |
|    n_updates        | 166355   |
----------------------------------
Eval num_timesteps=765500, episode_reward=3.24 +/- 5.56
Episode length: 98.54 +/- 42.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.5     |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 765500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 166374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3        |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 6996     |
|    fps              | 35       |
|    time_elapsed     | 21444    |
|    total_timesteps  | 765710   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0932   |
|    n_updates        | 166427   |
----------------------------------
Eval num_timesteps=766000, episode_reward=4.64 +/- 6.32
Episode length: 119.86 +/- 61.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 766000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.069    |
|    n_updates        | 166499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 2.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7000     |
|    fps              | 35       |
|    time_elapsed     | 21458    |
|    total_timesteps  | 766093   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0358   |
|    n_updates        | 166523   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 2.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7004     |
|    fps              | 35       |
|    time_elapsed     | 21460    |
|    total_timesteps  | 766462   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0167   |
|    n_updates        | 166615   |
----------------------------------
Eval num_timesteps=766500, episode_reward=3.72 +/- 5.78
Episode length: 104.08 +/- 50.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 766500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0505   |
|    n_updates        | 166624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 2.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7008     |
|    fps              | 35       |
|    time_elapsed     | 21473    |
|    total_timesteps  | 766840   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0338   |
|    n_updates        | 166709   |
----------------------------------
Eval num_timesteps=767000, episode_reward=3.82 +/- 5.35
Episode length: 113.50 +/- 51.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 767000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0745   |
|    n_updates        | 166749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 2.83     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7012     |
|    fps              | 35       |
|    time_elapsed     | 21487    |
|    total_timesteps  | 767311   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 166827   |
----------------------------------
Eval num_timesteps=767500, episode_reward=4.84 +/- 9.72
Episode length: 119.66 +/- 70.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 767500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0368   |
|    n_updates        | 166874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7016     |
|    fps              | 35       |
|    time_elapsed     | 21503    |
|    total_timesteps  | 767940   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 166984   |
----------------------------------
Eval num_timesteps=768000, episode_reward=3.12 +/- 4.55
Episode length: 95.02 +/- 47.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95       |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 768000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 166999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7020     |
|    fps              | 35       |
|    time_elapsed     | 21515    |
|    total_timesteps  | 768293   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 167073   |
----------------------------------
Eval num_timesteps=768500, episode_reward=6.06 +/- 10.42
Episode length: 139.14 +/- 98.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 6.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 768500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0577   |
|    n_updates        | 167124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7024     |
|    fps              | 35       |
|    time_elapsed     | 21531    |
|    total_timesteps  | 768645   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 167161   |
----------------------------------
Eval num_timesteps=769000, episode_reward=1.92 +/- 3.14
Episode length: 104.62 +/- 44.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 1.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 769000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 167249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7028     |
|    fps              | 35       |
|    time_elapsed     | 21544    |
|    total_timesteps  | 769127   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0395   |
|    n_updates        | 167281   |
----------------------------------
Eval num_timesteps=769500, episode_reward=3.30 +/- 5.64
Episode length: 108.14 +/- 46.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 769500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0624   |
|    n_updates        | 167374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7032     |
|    fps              | 35       |
|    time_elapsed     | 21558    |
|    total_timesteps  | 769623   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 167405   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7036     |
|    fps              | 35       |
|    time_elapsed     | 21559    |
|    total_timesteps  | 769893   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0363   |
|    n_updates        | 167473   |
----------------------------------
Eval num_timesteps=770000, episode_reward=5.48 +/- 7.34
Episode length: 129.52 +/- 57.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 770000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0359   |
|    n_updates        | 167499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7040     |
|    fps              | 35       |
|    time_elapsed     | 21574    |
|    total_timesteps  | 770143   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 167535   |
----------------------------------
Eval num_timesteps=770500, episode_reward=5.16 +/- 6.51
Episode length: 139.04 +/- 62.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 770500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0986   |
|    n_updates        | 167624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7044     |
|    fps              | 35       |
|    time_elapsed     | 21590    |
|    total_timesteps  | 770544   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0527   |
|    n_updates        | 167635   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 2.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7048     |
|    fps              | 35       |
|    time_elapsed     | 21591    |
|    total_timesteps  | 770834   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 167708   |
----------------------------------
Eval num_timesteps=771000, episode_reward=4.52 +/- 8.80
Episode length: 131.86 +/- 64.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 771000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0405   |
|    n_updates        | 167749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7052     |
|    fps              | 35       |
|    time_elapsed     | 21607    |
|    total_timesteps  | 771279   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0905   |
|    n_updates        | 167819   |
----------------------------------
Eval num_timesteps=771500, episode_reward=2.90 +/- 6.34
Episode length: 115.96 +/- 82.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 771500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 167874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 2.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7056     |
|    fps              | 35       |
|    time_elapsed     | 21622    |
|    total_timesteps  | 771867   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0227   |
|    n_updates        | 167966   |
----------------------------------
Eval num_timesteps=772000, episode_reward=6.72 +/- 8.52
Episode length: 152.50 +/- 90.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 6.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 772000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 167999   |
----------------------------------
Eval num_timesteps=772500, episode_reward=3.78 +/- 6.22
Episode length: 120.68 +/- 62.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 772500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 168124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7060     |
|    fps              | 35       |
|    time_elapsed     | 21655    |
|    total_timesteps  | 772597   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0734   |
|    n_updates        | 168149   |
----------------------------------
Eval num_timesteps=773000, episode_reward=4.40 +/- 5.47
Episode length: 127.56 +/- 68.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 773000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0441   |
|    n_updates        | 168249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7064     |
|    fps              | 35       |
|    time_elapsed     | 21672    |
|    total_timesteps  | 773220   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.149    |
|    n_updates        | 168304   |
----------------------------------
Eval num_timesteps=773500, episode_reward=3.00 +/- 4.31
Episode length: 119.34 +/- 59.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 773500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0983   |
|    n_updates        | 168374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7068     |
|    fps              | 35       |
|    time_elapsed     | 21686    |
|    total_timesteps  | 773642   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0407   |
|    n_updates        | 168410   |
----------------------------------
Eval num_timesteps=774000, episode_reward=2.98 +/- 4.17
Episode length: 104.90 +/- 44.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 774000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 168499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7072     |
|    fps              | 35       |
|    time_elapsed     | 21699    |
|    total_timesteps  | 774104   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.166    |
|    n_updates        | 168525   |
----------------------------------
Eval num_timesteps=774500, episode_reward=2.82 +/- 5.45
Episode length: 106.82 +/- 51.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 774500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 168624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 2.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7076     |
|    fps              | 35       |
|    time_elapsed     | 21712    |
|    total_timesteps  | 774571   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 168642   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 2.97     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7080     |
|    fps              | 35       |
|    time_elapsed     | 21714    |
|    total_timesteps  | 774950   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 168737   |
----------------------------------
Eval num_timesteps=775000, episode_reward=6.42 +/- 13.12
Episode length: 146.22 +/- 94.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 6.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 775000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0191   |
|    n_updates        | 168749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7084     |
|    fps              | 35       |
|    time_elapsed     | 21731    |
|    total_timesteps  | 775437   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 168859   |
----------------------------------
Eval num_timesteps=775500, episode_reward=3.32 +/- 5.38
Episode length: 115.94 +/- 64.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 775500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0493   |
|    n_updates        | 168874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 2.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7088     |
|    fps              | 35       |
|    time_elapsed     | 21744    |
|    total_timesteps  | 775901   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 168975   |
----------------------------------
Eval num_timesteps=776000, episode_reward=3.66 +/- 7.04
Episode length: 118.76 +/- 55.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 776000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 168999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 2.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7092     |
|    fps              | 35       |
|    time_elapsed     | 21759    |
|    total_timesteps  | 776472   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.124    |
|    n_updates        | 169117   |
----------------------------------
Eval num_timesteps=776500, episode_reward=5.38 +/- 7.84
Episode length: 139.38 +/- 64.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 5.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 776500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.064    |
|    n_updates        | 169124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 2.9      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7096     |
|    fps              | 35       |
|    time_elapsed     | 21776    |
|    total_timesteps  | 776996   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.027    |
|    n_updates        | 169248   |
----------------------------------
Eval num_timesteps=777000, episode_reward=5.46 +/- 8.81
Episode length: 133.70 +/- 73.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 777000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0538   |
|    n_updates        | 169249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 2.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7100     |
|    fps              | 35       |
|    time_elapsed     | 21792    |
|    total_timesteps  | 777375   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0404   |
|    n_updates        | 169343   |
----------------------------------
Eval num_timesteps=777500, episode_reward=4.08 +/- 6.87
Episode length: 127.76 +/- 65.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 777500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0416   |
|    n_updates        | 169374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7104     |
|    fps              | 35       |
|    time_elapsed     | 21807    |
|    total_timesteps  | 777882   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0455   |
|    n_updates        | 169470   |
----------------------------------
Eval num_timesteps=778000, episode_reward=2.28 +/- 5.29
Episode length: 100.92 +/- 52.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 778000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 169499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7108     |
|    fps              | 35       |
|    time_elapsed     | 21820    |
|    total_timesteps  | 778390   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 169597   |
----------------------------------
Eval num_timesteps=778500, episode_reward=2.42 +/- 4.06
Episode length: 103.08 +/- 41.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 778500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0726   |
|    n_updates        | 169624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7112     |
|    fps              | 35       |
|    time_elapsed     | 21833    |
|    total_timesteps  | 778936   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 169733   |
----------------------------------
Eval num_timesteps=779000, episode_reward=4.34 +/- 6.53
Episode length: 121.22 +/- 56.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 779000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0346   |
|    n_updates        | 169749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 2.77     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7116     |
|    fps              | 35       |
|    time_elapsed     | 21848    |
|    total_timesteps  | 779485   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0905   |
|    n_updates        | 169871   |
----------------------------------
Eval num_timesteps=779500, episode_reward=3.52 +/- 5.28
Episode length: 121.84 +/- 70.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 779500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0798   |
|    n_updates        | 169874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7120     |
|    fps              | 35       |
|    time_elapsed     | 21863    |
|    total_timesteps  | 779868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0299   |
|    n_updates        | 169966   |
----------------------------------
Eval num_timesteps=780000, episode_reward=1.48 +/- 2.71
Episode length: 103.84 +/- 46.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 1.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 780000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 169999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 2.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7124     |
|    fps              | 35       |
|    time_elapsed     | 21876    |
|    total_timesteps  | 780311   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.051    |
|    n_updates        | 170077   |
----------------------------------
Eval num_timesteps=780500, episode_reward=3.74 +/- 6.87
Episode length: 114.40 +/- 51.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 780500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0279   |
|    n_updates        | 170124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 2.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7128     |
|    fps              | 35       |
|    time_elapsed     | 21893    |
|    total_timesteps  | 780833   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0722   |
|    n_updates        | 170208   |
----------------------------------
Eval num_timesteps=781000, episode_reward=4.84 +/- 10.26
Episode length: 125.74 +/- 69.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 781000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0575   |
|    n_updates        | 170249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 2.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7132     |
|    fps              | 35       |
|    time_elapsed     | 21913    |
|    total_timesteps  | 781299   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 170324   |
----------------------------------
Eval num_timesteps=781500, episode_reward=2.88 +/- 4.71
Episode length: 104.20 +/- 51.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 781500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0113   |
|    n_updates        | 170374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 2.71     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7136     |
|    fps              | 35       |
|    time_elapsed     | 21928    |
|    total_timesteps  | 781724   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 170430   |
----------------------------------
Eval num_timesteps=782000, episode_reward=2.18 +/- 4.02
Episode length: 93.32 +/- 37.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.3     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 782000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0858   |
|    n_updates        | 170499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 2.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7140     |
|    fps              | 35       |
|    time_elapsed     | 21940    |
|    total_timesteps  | 782116   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0463   |
|    n_updates        | 170528   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 2.75     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7144     |
|    fps              | 35       |
|    time_elapsed     | 21942    |
|    total_timesteps  | 782446   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0321   |
|    n_updates        | 170611   |
----------------------------------
Eval num_timesteps=782500, episode_reward=3.22 +/- 4.31
Episode length: 113.36 +/- 49.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 782500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0792   |
|    n_updates        | 170624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 2.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7148     |
|    fps              | 35       |
|    time_elapsed     | 21955    |
|    total_timesteps  | 782863   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 170715   |
----------------------------------
Eval num_timesteps=783000, episode_reward=3.38 +/- 5.22
Episode length: 114.94 +/- 47.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 783000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0591   |
|    n_updates        | 170749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 2.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7152     |
|    fps              | 35       |
|    time_elapsed     | 21970    |
|    total_timesteps  | 783363   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 170840   |
----------------------------------
Eval num_timesteps=783500, episode_reward=3.56 +/- 4.94
Episode length: 106.54 +/- 47.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 783500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 170874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 2.85     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7156     |
|    fps              | 35       |
|    time_elapsed     | 21983    |
|    total_timesteps  | 783940   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0341   |
|    n_updates        | 170984   |
----------------------------------
Eval num_timesteps=784000, episode_reward=3.00 +/- 5.21
Episode length: 110.66 +/- 57.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 784000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 170999   |
----------------------------------
Eval num_timesteps=784500, episode_reward=2.74 +/- 4.96
Episode length: 110.98 +/- 69.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 784500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0711   |
|    n_updates        | 171124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 2.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7160     |
|    fps              | 35       |
|    time_elapsed     | 22009    |
|    total_timesteps  | 784621   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 171155   |
----------------------------------
Eval num_timesteps=785000, episode_reward=2.60 +/- 5.67
Episode length: 108.90 +/- 58.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 2.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 785000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0567   |
|    n_updates        | 171249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 2.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7164     |
|    fps              | 35       |
|    time_elapsed     | 22023    |
|    total_timesteps  | 785056   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 171263   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 2.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7168     |
|    fps              | 35       |
|    time_elapsed     | 22025    |
|    total_timesteps  | 785402   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0967   |
|    n_updates        | 171350   |
----------------------------------
Eval num_timesteps=785500, episode_reward=4.38 +/- 7.98
Episode length: 125.54 +/- 64.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 785500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0775   |
|    n_updates        | 171374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7172     |
|    fps              | 35       |
|    time_elapsed     | 22041    |
|    total_timesteps  | 785877   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0258   |
|    n_updates        | 171469   |
----------------------------------
Eval num_timesteps=786000, episode_reward=3.76 +/- 4.71
Episode length: 109.62 +/- 68.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 786000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 171499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7176     |
|    fps              | 35       |
|    time_elapsed     | 22054    |
|    total_timesteps  | 786290   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0503   |
|    n_updates        | 171572   |
----------------------------------
Eval num_timesteps=786500, episode_reward=2.52 +/- 4.19
Episode length: 100.62 +/- 51.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 786500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0723   |
|    n_updates        | 171624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7180     |
|    fps              | 35       |
|    time_elapsed     | 22067    |
|    total_timesteps  | 786828   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0534   |
|    n_updates        | 171706   |
----------------------------------
Eval num_timesteps=787000, episode_reward=3.96 +/- 4.88
Episode length: 122.48 +/- 63.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 787000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.121    |
|    n_updates        | 171749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7184     |
|    fps              | 35       |
|    time_elapsed     | 22082    |
|    total_timesteps  | 787207   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0587   |
|    n_updates        | 171801   |
----------------------------------
Eval num_timesteps=787500, episode_reward=3.28 +/- 4.18
Episode length: 98.58 +/- 38.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 787500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.138    |
|    n_updates        | 171874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7188     |
|    fps              | 35       |
|    time_elapsed     | 22096    |
|    total_timesteps  | 787905   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0378   |
|    n_updates        | 171976   |
----------------------------------
Eval num_timesteps=788000, episode_reward=5.18 +/- 8.25
Episode length: 123.40 +/- 52.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 5.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 788000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0455   |
|    n_updates        | 171999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.38     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7192     |
|    fps              | 35       |
|    time_elapsed     | 22115    |
|    total_timesteps  | 788362   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0229   |
|    n_updates        | 172090   |
----------------------------------
Eval num_timesteps=788500, episode_reward=4.48 +/- 5.43
Episode length: 120.14 +/- 57.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 788500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.027    |
|    n_updates        | 172124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7196     |
|    fps              | 35       |
|    time_elapsed     | 22130    |
|    total_timesteps  | 788672   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0699   |
|    n_updates        | 172167   |
----------------------------------
Eval num_timesteps=789000, episode_reward=2.88 +/- 4.09
Episode length: 104.70 +/- 39.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 789000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0375   |
|    n_updates        | 172249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7200     |
|    fps              | 35       |
|    time_elapsed     | 22144    |
|    total_timesteps  | 789143   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 172285   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.38     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7204     |
|    fps              | 35       |
|    time_elapsed     | 22145    |
|    total_timesteps  | 789431   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0453   |
|    n_updates        | 172357   |
----------------------------------
Eval num_timesteps=789500, episode_reward=3.66 +/- 4.75
Episode length: 122.74 +/- 57.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 789500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0268   |
|    n_updates        | 172374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.38     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7208     |
|    fps              | 35       |
|    time_elapsed     | 22160    |
|    total_timesteps  | 789969   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0524   |
|    n_updates        | 172492   |
----------------------------------
Eval num_timesteps=790000, episode_reward=3.28 +/- 6.65
Episode length: 116.66 +/- 70.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 790000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 172499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.56     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7212     |
|    fps              | 35       |
|    time_elapsed     | 22175    |
|    total_timesteps  | 790460   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 172614   |
----------------------------------
Eval num_timesteps=790500, episode_reward=4.60 +/- 6.24
Episode length: 137.66 +/- 73.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 790500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.155    |
|    n_updates        | 172624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.52     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7216     |
|    fps              | 35       |
|    time_elapsed     | 22197    |
|    total_timesteps  | 790824   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 172705   |
----------------------------------
Eval num_timesteps=791000, episode_reward=3.88 +/- 8.73
Episode length: 108.20 +/- 66.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 791000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0775   |
|    n_updates        | 172749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7220     |
|    fps              | 35       |
|    time_elapsed     | 22216    |
|    total_timesteps  | 791462   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0355   |
|    n_updates        | 172865   |
----------------------------------
Eval num_timesteps=791500, episode_reward=3.28 +/- 5.59
Episode length: 119.72 +/- 64.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 791500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 172874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7224     |
|    fps              | 35       |
|    time_elapsed     | 22236    |
|    total_timesteps  | 791940   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0811   |
|    n_updates        | 172984   |
----------------------------------
Eval num_timesteps=792000, episode_reward=2.58 +/- 2.99
Episode length: 115.88 +/- 44.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 2.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 792000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0395   |
|    n_updates        | 172999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.87     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7228     |
|    fps              | 35       |
|    time_elapsed     | 22252    |
|    total_timesteps  | 792159   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0381   |
|    n_updates        | 173039   |
----------------------------------
Eval num_timesteps=792500, episode_reward=3.74 +/- 6.50
Episode length: 114.94 +/- 62.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 792500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 173124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.1      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7232     |
|    fps              | 35       |
|    time_elapsed     | 22267    |
|    total_timesteps  | 792884   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0531   |
|    n_updates        | 173220   |
----------------------------------
Eval num_timesteps=793000, episode_reward=4.52 +/- 6.99
Episode length: 126.74 +/- 66.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 793000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0272   |
|    n_updates        | 173249   |
----------------------------------
Eval num_timesteps=793500, episode_reward=4.82 +/- 9.10
Episode length: 115.38 +/- 57.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 793500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0362   |
|    n_updates        | 173374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7236     |
|    fps              | 35       |
|    time_elapsed     | 22296    |
|    total_timesteps  | 793519   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0875   |
|    n_updates        | 173379   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7240     |
|    fps              | 35       |
|    time_elapsed     | 22297    |
|    total_timesteps  | 793936   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0809   |
|    n_updates        | 173483   |
----------------------------------
Eval num_timesteps=794000, episode_reward=4.92 +/- 9.64
Episode length: 118.24 +/- 70.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 794000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00896  |
|    n_updates        | 173499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7244     |
|    fps              | 35       |
|    time_elapsed     | 22312    |
|    total_timesteps  | 794400   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.15     |
|    n_updates        | 173599   |
----------------------------------
Eval num_timesteps=794500, episode_reward=4.34 +/- 7.60
Episode length: 124.24 +/- 65.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 794500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0914   |
|    n_updates        | 173624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7248     |
|    fps              | 35       |
|    time_elapsed     | 22333    |
|    total_timesteps  | 794881   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0831   |
|    n_updates        | 173720   |
----------------------------------
Eval num_timesteps=795000, episode_reward=3.34 +/- 4.48
Episode length: 113.10 +/- 51.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 795000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0705   |
|    n_updates        | 173749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7252     |
|    fps              | 35       |
|    time_elapsed     | 22351    |
|    total_timesteps  | 795218   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0241   |
|    n_updates        | 173804   |
----------------------------------
Eval num_timesteps=795500, episode_reward=4.04 +/- 6.33
Episode length: 123.84 +/- 65.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 795500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0757   |
|    n_updates        | 173874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7256     |
|    fps              | 35       |
|    time_elapsed     | 22371    |
|    total_timesteps  | 795712   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0284   |
|    n_updates        | 173927   |
----------------------------------
Eval num_timesteps=796000, episode_reward=4.66 +/- 7.64
Episode length: 128.92 +/- 77.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 4.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 796000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0874   |
|    n_updates        | 173999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7260     |
|    fps              | 35       |
|    time_elapsed     | 22386    |
|    total_timesteps  | 796184   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0385   |
|    n_updates        | 174045   |
----------------------------------
Eval num_timesteps=796500, episode_reward=3.22 +/- 4.30
Episode length: 116.12 +/- 48.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 796500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0841   |
|    n_updates        | 174124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7264     |
|    fps              | 35       |
|    time_elapsed     | 22400    |
|    total_timesteps  | 796577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 174144   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7268     |
|    fps              | 35       |
|    time_elapsed     | 22401    |
|    total_timesteps  | 796983   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 174245   |
----------------------------------
Eval num_timesteps=797000, episode_reward=3.62 +/- 6.15
Episode length: 110.34 +/- 54.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 797000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 174249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7272     |
|    fps              | 35       |
|    time_elapsed     | 22415    |
|    total_timesteps  | 797498   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0899   |
|    n_updates        | 174374   |
----------------------------------
Eval num_timesteps=797500, episode_reward=4.62 +/- 8.34
Episode length: 130.04 +/- 68.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 797500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7276     |
|    fps              | 35       |
|    time_elapsed     | 22431    |
|    total_timesteps  | 797829   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0503   |
|    n_updates        | 174457   |
----------------------------------
Eval num_timesteps=798000, episode_reward=2.88 +/- 5.87
Episode length: 118.86 +/- 80.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 798000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 174499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7280     |
|    fps              | 35       |
|    time_elapsed     | 22445    |
|    total_timesteps  | 798372   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0656   |
|    n_updates        | 174592   |
----------------------------------
Eval num_timesteps=798500, episode_reward=5.66 +/- 8.77
Episode length: 126.92 +/- 68.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 5.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 798500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0918   |
|    n_updates        | 174624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7284     |
|    fps              | 35       |
|    time_elapsed     | 22461    |
|    total_timesteps  | 798869   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0807   |
|    n_updates        | 174717   |
----------------------------------
Eval num_timesteps=799000, episode_reward=3.48 +/- 4.16
Episode length: 118.34 +/- 48.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 799000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0423   |
|    n_updates        | 174749   |
----------------------------------
Eval num_timesteps=799500, episode_reward=4.20 +/- 7.11
Episode length: 122.88 +/- 67.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 799500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0186   |
|    n_updates        | 174874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7288     |
|    fps              | 35       |
|    time_elapsed     | 22490    |
|    total_timesteps  | 799897   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 174974   |
----------------------------------
Eval num_timesteps=800000, episode_reward=4.26 +/- 7.34
Episode length: 118.26 +/- 63.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 800000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 174999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7292     |
|    fps              | 35       |
|    time_elapsed     | 22505    |
|    total_timesteps  | 800462   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.028    |
|    n_updates        | 175115   |
----------------------------------
Eval num_timesteps=800500, episode_reward=2.42 +/- 4.83
Episode length: 119.56 +/- 66.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 2.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 800500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0668   |
|    n_updates        | 175124   |
----------------------------------
Eval num_timesteps=801000, episode_reward=3.50 +/- 7.17
Episode length: 113.42 +/- 69.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 801000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0492   |
|    n_updates        | 175249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7296     |
|    fps              | 35       |
|    time_elapsed     | 22535    |
|    total_timesteps  | 801257   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0374   |
|    n_updates        | 175314   |
----------------------------------
Eval num_timesteps=801500, episode_reward=3.38 +/- 4.98
Episode length: 121.26 +/- 59.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 801500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 175374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.38     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7300     |
|    fps              | 35       |
|    time_elapsed     | 22549    |
|    total_timesteps  | 801635   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0644   |
|    n_updates        | 175408   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7304     |
|    fps              | 35       |
|    time_elapsed     | 22550    |
|    total_timesteps  | 801920   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.054    |
|    n_updates        | 175479   |
----------------------------------
Eval num_timesteps=802000, episode_reward=2.74 +/- 4.88
Episode length: 118.20 +/- 58.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 802000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 175499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.53     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7308     |
|    fps              | 35       |
|    time_elapsed     | 22565    |
|    total_timesteps  | 802455   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.167    |
|    n_updates        | 175613   |
----------------------------------
Eval num_timesteps=802500, episode_reward=4.36 +/- 6.75
Episode length: 133.12 +/- 72.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 802500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 175624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7312     |
|    fps              | 35       |
|    time_elapsed     | 22580    |
|    total_timesteps  | 802752   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0709   |
|    n_updates        | 175687   |
----------------------------------
Eval num_timesteps=803000, episode_reward=5.70 +/- 9.85
Episode length: 137.96 +/- 83.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 5.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 803000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.151    |
|    n_updates        | 175749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7316     |
|    fps              | 35       |
|    time_elapsed     | 22597    |
|    total_timesteps  | 803090   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.083    |
|    n_updates        | 175772   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7320     |
|    fps              | 35       |
|    time_elapsed     | 22598    |
|    total_timesteps  | 803434   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0268   |
|    n_updates        | 175858   |
----------------------------------
Eval num_timesteps=803500, episode_reward=4.44 +/- 8.63
Episode length: 118.66 +/- 67.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 803500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0597   |
|    n_updates        | 175874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7324     |
|    fps              | 35       |
|    time_elapsed     | 22617    |
|    total_timesteps  | 803834   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0534   |
|    n_updates        | 175958   |
----------------------------------
Eval num_timesteps=804000, episode_reward=3.92 +/- 5.62
Episode length: 131.24 +/- 89.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 3.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 804000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0853   |
|    n_updates        | 175999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7328     |
|    fps              | 35       |
|    time_elapsed     | 22639    |
|    total_timesteps  | 804412   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0586   |
|    n_updates        | 176102   |
----------------------------------
Eval num_timesteps=804500, episode_reward=3.56 +/- 5.36
Episode length: 129.46 +/- 69.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 804500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.028    |
|    n_updates        | 176124   |
----------------------------------
Eval num_timesteps=805000, episode_reward=4.32 +/- 6.93
Episode length: 117.16 +/- 62.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 805000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0578   |
|    n_updates        | 176249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7332     |
|    fps              | 35       |
|    time_elapsed     | 22672    |
|    total_timesteps  | 805005   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 176251   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7336     |
|    fps              | 35       |
|    time_elapsed     | 22674    |
|    total_timesteps  | 805383   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0453   |
|    n_updates        | 176345   |
----------------------------------
Eval num_timesteps=805500, episode_reward=3.10 +/- 5.32
Episode length: 117.96 +/- 53.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 805500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0457   |
|    n_updates        | 176374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.15     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7340     |
|    fps              | 35       |
|    time_elapsed     | 22689    |
|    total_timesteps  | 805737   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0963   |
|    n_updates        | 176434   |
----------------------------------
Eval num_timesteps=806000, episode_reward=3.76 +/- 5.99
Episode length: 115.02 +/- 52.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 806000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 176499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7344     |
|    fps              | 35       |
|    time_elapsed     | 22702    |
|    total_timesteps  | 806094   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0851   |
|    n_updates        | 176523   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7348     |
|    fps              | 35       |
|    time_elapsed     | 22704    |
|    total_timesteps  | 806466   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.059    |
|    n_updates        | 176616   |
----------------------------------
Eval num_timesteps=806500, episode_reward=4.02 +/- 6.21
Episode length: 136.58 +/- 66.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 806500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0428   |
|    n_updates        | 176624   |
----------------------------------
Eval num_timesteps=807000, episode_reward=3.64 +/- 6.05
Episode length: 130.86 +/- 77.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 807000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0453   |
|    n_updates        | 176749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.07     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7352     |
|    fps              | 35       |
|    time_elapsed     | 22734    |
|    total_timesteps  | 807118   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0893   |
|    n_updates        | 176779   |
----------------------------------
Eval num_timesteps=807500, episode_reward=6.40 +/- 10.11
Episode length: 130.26 +/- 76.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 6.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 807500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0598   |
|    n_updates        | 176874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7356     |
|    fps              | 35       |
|    time_elapsed     | 22750    |
|    total_timesteps  | 807624   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0486   |
|    n_updates        | 176905   |
----------------------------------
Eval num_timesteps=808000, episode_reward=3.18 +/- 4.61
Episode length: 121.14 +/- 56.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 808000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0351   |
|    n_updates        | 176999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7360     |
|    fps              | 35       |
|    time_elapsed     | 22765    |
|    total_timesteps  | 808186   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 177046   |
----------------------------------
Eval num_timesteps=808500, episode_reward=4.88 +/- 5.63
Episode length: 136.70 +/- 70.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 4.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 808500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0342   |
|    n_updates        | 177124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7364     |
|    fps              | 35       |
|    time_elapsed     | 22781    |
|    total_timesteps  | 808641   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0572   |
|    n_updates        | 177160   |
----------------------------------
Eval num_timesteps=809000, episode_reward=3.88 +/- 5.36
Episode length: 136.58 +/- 58.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 3.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 809000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.081    |
|    n_updates        | 177249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7368     |
|    fps              | 35       |
|    time_elapsed     | 22797    |
|    total_timesteps  | 809086   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.036    |
|    n_updates        | 177271   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7372     |
|    fps              | 35       |
|    time_elapsed     | 22798    |
|    total_timesteps  | 809424   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0924   |
|    n_updates        | 177355   |
----------------------------------
Eval num_timesteps=809500, episode_reward=4.72 +/- 6.84
Episode length: 132.76 +/- 71.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 809500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 177374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7376     |
|    fps              | 35       |
|    time_elapsed     | 22815    |
|    total_timesteps  | 809933   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.039    |
|    n_updates        | 177483   |
----------------------------------
Eval num_timesteps=810000, episode_reward=5.28 +/- 7.66
Episode length: 125.58 +/- 68.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 810000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0542   |
|    n_updates        | 177499   |
----------------------------------
Eval num_timesteps=810500, episode_reward=3.62 +/- 4.76
Episode length: 120.54 +/- 57.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 810500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0807   |
|    n_updates        | 177624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7380     |
|    fps              | 35       |
|    time_elapsed     | 22845    |
|    total_timesteps  | 810551   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0693   |
|    n_updates        | 177637   |
----------------------------------
Eval num_timesteps=811000, episode_reward=3.40 +/- 5.31
Episode length: 117.16 +/- 53.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 811000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0688   |
|    n_updates        | 177749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.62     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7384     |
|    fps              | 35       |
|    time_elapsed     | 22861    |
|    total_timesteps  | 811412   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 177852   |
----------------------------------
Eval num_timesteps=811500, episode_reward=4.14 +/- 7.60
Episode length: 119.24 +/- 70.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 811500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0592   |
|    n_updates        | 177874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.27     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7388     |
|    fps              | 35       |
|    time_elapsed     | 22875    |
|    total_timesteps  | 811795   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0832   |
|    n_updates        | 177948   |
----------------------------------
Eval num_timesteps=812000, episode_reward=4.78 +/- 7.57
Episode length: 138.70 +/- 72.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 812000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0702   |
|    n_updates        | 177999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7392     |
|    fps              | 35       |
|    time_elapsed     | 22891    |
|    total_timesteps  | 812271   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0594   |
|    n_updates        | 178067   |
----------------------------------
Eval num_timesteps=812500, episode_reward=4.70 +/- 8.21
Episode length: 140.32 +/- 74.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 4.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 812500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0519   |
|    n_updates        | 178124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7396     |
|    fps              | 35       |
|    time_elapsed     | 22909    |
|    total_timesteps  | 812932   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.078    |
|    n_updates        | 178232   |
----------------------------------
Eval num_timesteps=813000, episode_reward=3.20 +/- 5.47
Episode length: 134.34 +/- 86.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 3.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 813000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0541   |
|    n_updates        | 178249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7400     |
|    fps              | 35       |
|    time_elapsed     | 22924    |
|    total_timesteps  | 813278   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0289   |
|    n_updates        | 178319   |
----------------------------------
Eval num_timesteps=813500, episode_reward=6.00 +/- 8.38
Episode length: 148.56 +/- 72.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 6        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 813500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0227   |
|    n_updates        | 178374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.83     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7404     |
|    fps              | 35       |
|    time_elapsed     | 22941    |
|    total_timesteps  | 813643   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0508   |
|    n_updates        | 178410   |
----------------------------------
Eval num_timesteps=814000, episode_reward=3.84 +/- 4.54
Episode length: 126.02 +/- 62.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 814000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.04     |
|    n_updates        | 178499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.6      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7408     |
|    fps              | 35       |
|    time_elapsed     | 22956    |
|    total_timesteps  | 814040   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.134    |
|    n_updates        | 178509   |
----------------------------------
Eval num_timesteps=814500, episode_reward=3.38 +/- 4.49
Episode length: 110.02 +/- 36.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 814500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.067    |
|    n_updates        | 178624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7412     |
|    fps              | 35       |
|    time_elapsed     | 22970    |
|    total_timesteps  | 814541   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.061    |
|    n_updates        | 178635   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.83     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7416     |
|    fps              | 35       |
|    time_elapsed     | 22972    |
|    total_timesteps  | 814957   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0649   |
|    n_updates        | 178739   |
----------------------------------
Eval num_timesteps=815000, episode_reward=3.60 +/- 4.71
Episode length: 135.06 +/- 66.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 815000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0563   |
|    n_updates        | 178749   |
----------------------------------
Eval num_timesteps=815500, episode_reward=3.74 +/- 6.21
Episode length: 116.60 +/- 55.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 815500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0985   |
|    n_updates        | 178874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7420     |
|    fps              | 35       |
|    time_elapsed     | 23001    |
|    total_timesteps  | 815568   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 178891   |
----------------------------------
Eval num_timesteps=816000, episode_reward=4.82 +/- 7.27
Episode length: 128.82 +/- 68.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 4.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 816000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0525   |
|    n_updates        | 178999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7424     |
|    fps              | 35       |
|    time_elapsed     | 23017    |
|    total_timesteps  | 816119   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0243   |
|    n_updates        | 179029   |
----------------------------------
Eval num_timesteps=816500, episode_reward=4.14 +/- 5.73
Episode length: 133.10 +/- 70.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 816500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 179124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7428     |
|    fps              | 35       |
|    time_elapsed     | 23034    |
|    total_timesteps  | 816937   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 179234   |
----------------------------------
Eval num_timesteps=817000, episode_reward=5.72 +/- 8.69
Episode length: 128.58 +/- 70.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 817000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0475   |
|    n_updates        | 179249   |
----------------------------------
Eval num_timesteps=817500, episode_reward=4.70 +/- 8.73
Episode length: 120.50 +/- 67.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 817500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0254   |
|    n_updates        | 179374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7432     |
|    fps              | 35       |
|    time_elapsed     | 23068    |
|    total_timesteps  | 817692   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0363   |
|    n_updates        | 179422   |
----------------------------------
Eval num_timesteps=818000, episode_reward=4.58 +/- 6.57
Episode length: 121.28 +/- 60.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 818000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 179499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7436     |
|    fps              | 35       |
|    time_elapsed     | 23085    |
|    total_timesteps  | 818177   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0368   |
|    n_updates        | 179544   |
----------------------------------
Eval num_timesteps=818500, episode_reward=3.96 +/- 6.29
Episode length: 124.88 +/- 65.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 818500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.156    |
|    n_updates        | 179624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7440     |
|    fps              | 35       |
|    time_elapsed     | 23105    |
|    total_timesteps  | 818550   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0584   |
|    n_updates        | 179637   |
----------------------------------
Eval num_timesteps=819000, episode_reward=4.48 +/- 9.43
Episode length: 123.64 +/- 73.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 819000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0485   |
|    n_updates        | 179749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7444     |
|    fps              | 35       |
|    time_elapsed     | 23122    |
|    total_timesteps  | 819293   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0941   |
|    n_updates        | 179823   |
----------------------------------
Eval num_timesteps=819500, episode_reward=4.52 +/- 8.08
Episode length: 128.80 +/- 74.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 819500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 179874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7448     |
|    fps              | 35       |
|    time_elapsed     | 23138    |
|    total_timesteps  | 819758   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 179939   |
----------------------------------
Eval num_timesteps=820000, episode_reward=5.48 +/- 7.41
Episode length: 138.32 +/- 68.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 820000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0358   |
|    n_updates        | 179999   |
----------------------------------
Eval num_timesteps=820500, episode_reward=3.86 +/- 6.41
Episode length: 142.20 +/- 91.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 820500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 180124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 4.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7452     |
|    fps              | 35       |
|    time_elapsed     | 23171    |
|    total_timesteps  | 820525   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0334   |
|    n_updates        | 180131   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 4.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7456     |
|    fps              | 35       |
|    time_elapsed     | 23172    |
|    total_timesteps  | 820824   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0968   |
|    n_updates        | 180205   |
----------------------------------
Eval num_timesteps=821000, episode_reward=3.66 +/- 4.85
Episode length: 115.36 +/- 54.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 821000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 180249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 4.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7460     |
|    fps              | 35       |
|    time_elapsed     | 23187    |
|    total_timesteps  | 821394   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.107    |
|    n_updates        | 180348   |
----------------------------------
Eval num_timesteps=821500, episode_reward=2.26 +/- 3.30
Episode length: 105.44 +/- 53.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 821500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 180374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.02     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7464     |
|    fps              | 35       |
|    time_elapsed     | 23200    |
|    total_timesteps  | 821881   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 180470   |
----------------------------------
Eval num_timesteps=822000, episode_reward=4.96 +/- 7.69
Episode length: 119.48 +/- 59.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 822000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 180499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7468     |
|    fps              | 35       |
|    time_elapsed     | 23214    |
|    total_timesteps  | 822362   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0855   |
|    n_updates        | 180590   |
----------------------------------
Eval num_timesteps=822500, episode_reward=3.10 +/- 5.51
Episode length: 110.06 +/- 46.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 822500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0275   |
|    n_updates        | 180624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7472     |
|    fps              | 35       |
|    time_elapsed     | 23228    |
|    total_timesteps  | 822827   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0223   |
|    n_updates        | 180706   |
----------------------------------
Eval num_timesteps=823000, episode_reward=4.00 +/- 6.95
Episode length: 140.60 +/- 76.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 823000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 180749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7476     |
|    fps              | 35       |
|    time_elapsed     | 23244    |
|    total_timesteps  | 823304   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 180825   |
----------------------------------
Eval num_timesteps=823500, episode_reward=6.48 +/- 11.94
Episode length: 137.94 +/- 85.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 6.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 823500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0295   |
|    n_updates        | 180874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.44     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7480     |
|    fps              | 35       |
|    time_elapsed     | 23261    |
|    total_timesteps  | 823789   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0254   |
|    n_updates        | 180947   |
----------------------------------
Eval num_timesteps=824000, episode_reward=5.68 +/- 11.72
Episode length: 119.86 +/- 78.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 5.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 824000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.077    |
|    n_updates        | 180999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7484     |
|    fps              | 35       |
|    time_elapsed     | 23276    |
|    total_timesteps  | 824242   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0311   |
|    n_updates        | 181060   |
----------------------------------
Eval num_timesteps=824500, episode_reward=4.76 +/- 7.63
Episode length: 117.72 +/- 68.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 824500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0793   |
|    n_updates        | 181124   |
----------------------------------
Eval num_timesteps=825000, episode_reward=4.30 +/- 5.76
Episode length: 115.34 +/- 61.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 825000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.157    |
|    n_updates        | 181249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.03     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7488     |
|    fps              | 35       |
|    time_elapsed     | 23304    |
|    total_timesteps  | 825025   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0641   |
|    n_updates        | 181256   |
----------------------------------
Eval num_timesteps=825500, episode_reward=5.94 +/- 11.45
Episode length: 136.08 +/- 80.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 5.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 825500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 181374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 5.35     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7492     |
|    fps              | 35       |
|    time_elapsed     | 23322    |
|    total_timesteps  | 825785   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.164    |
|    n_updates        | 181446   |
----------------------------------
Eval num_timesteps=826000, episode_reward=5.20 +/- 7.23
Episode length: 140.78 +/- 83.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 5.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 826000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 181499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.18     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7496     |
|    fps              | 35       |
|    time_elapsed     | 23338    |
|    total_timesteps  | 826158   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0241   |
|    n_updates        | 181539   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7500     |
|    fps              | 35       |
|    time_elapsed     | 23340    |
|    total_timesteps  | 826445   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0791   |
|    n_updates        | 181611   |
----------------------------------
Eval num_timesteps=826500, episode_reward=7.20 +/- 9.24
Episode length: 158.82 +/- 70.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 7.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 826500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.037    |
|    n_updates        | 181624   |
----------------------------------
Eval num_timesteps=827000, episode_reward=4.64 +/- 6.94
Episode length: 134.14 +/- 65.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 827000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0708   |
|    n_updates        | 181749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7504     |
|    fps              | 35       |
|    time_elapsed     | 23374    |
|    total_timesteps  | 827078   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0698   |
|    n_updates        | 181769   |
----------------------------------
Eval num_timesteps=827500, episode_reward=3.56 +/- 5.06
Episode length: 117.38 +/- 48.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 827500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 181874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 6.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7508     |
|    fps              | 35       |
|    time_elapsed     | 23388    |
|    total_timesteps  | 827777   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0232   |
|    n_updates        | 181944   |
----------------------------------
Eval num_timesteps=828000, episode_reward=4.64 +/- 8.81
Episode length: 121.66 +/- 70.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 828000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0812   |
|    n_updates        | 181999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7512     |
|    fps              | 35       |
|    time_elapsed     | 23403    |
|    total_timesteps  | 828262   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0774   |
|    n_updates        | 182065   |
----------------------------------
Eval num_timesteps=828500, episode_reward=6.82 +/- 8.69
Episode length: 156.50 +/- 77.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 6.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 828500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0803   |
|    n_updates        | 182124   |
----------------------------------
Eval num_timesteps=829000, episode_reward=6.66 +/- 8.62
Episode length: 149.88 +/- 96.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 6.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 829000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 182249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 6.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7516     |
|    fps              | 35       |
|    time_elapsed     | 23443    |
|    total_timesteps  | 829307   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 182326   |
----------------------------------
Eval num_timesteps=829500, episode_reward=6.00 +/- 9.62
Episode length: 144.66 +/- 94.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 6        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 829500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 182374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 6.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7520     |
|    fps              | 35       |
|    time_elapsed     | 23465    |
|    total_timesteps  | 829656   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0573   |
|    n_updates        | 182413   |
----------------------------------
Eval num_timesteps=830000, episode_reward=4.98 +/- 8.59
Episode length: 122.18 +/- 64.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 830000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.26     |
|    n_updates        | 182499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 6.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7524     |
|    fps              | 35       |
|    time_elapsed     | 23486    |
|    total_timesteps  | 830301   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 182575   |
----------------------------------
Eval num_timesteps=830500, episode_reward=4.32 +/- 6.40
Episode length: 124.32 +/- 63.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 830500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.161    |
|    n_updates        | 182624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.71     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7528     |
|    fps              | 35       |
|    time_elapsed     | 23501    |
|    total_timesteps  | 830652   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0483   |
|    n_updates        | 182662   |
----------------------------------
Eval num_timesteps=831000, episode_reward=4.24 +/- 7.89
Episode length: 122.78 +/- 79.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 831000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 182749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 5.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7532     |
|    fps              | 35       |
|    time_elapsed     | 23517    |
|    total_timesteps  | 831205   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0205   |
|    n_updates        | 182801   |
----------------------------------
Eval num_timesteps=831500, episode_reward=3.44 +/- 5.23
Episode length: 128.08 +/- 57.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 831500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0646   |
|    n_updates        | 182874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 5.16     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7536     |
|    fps              | 35       |
|    time_elapsed     | 23532    |
|    total_timesteps  | 831801   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0791   |
|    n_updates        | 182950   |
----------------------------------
Eval num_timesteps=832000, episode_reward=3.84 +/- 5.58
Episode length: 136.68 +/- 67.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 832000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 182999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 5.26     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7540     |
|    fps              | 35       |
|    time_elapsed     | 23549    |
|    total_timesteps  | 832415   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 183103   |
----------------------------------
Eval num_timesteps=832500, episode_reward=6.98 +/- 12.51
Episode length: 133.94 +/- 74.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 6.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 832500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 183124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7544     |
|    fps              | 35       |
|    time_elapsed     | 23571    |
|    total_timesteps  | 832978   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0522   |
|    n_updates        | 183244   |
----------------------------------
Eval num_timesteps=833000, episode_reward=5.34 +/- 8.72
Episode length: 137.44 +/- 66.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 833000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0707   |
|    n_updates        | 183249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.03     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7548     |
|    fps              | 35       |
|    time_elapsed     | 23593    |
|    total_timesteps  | 833419   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0578   |
|    n_updates        | 183354   |
----------------------------------
Eval num_timesteps=833500, episode_reward=4.10 +/- 6.04
Episode length: 111.72 +/- 59.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 833500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 183374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7552     |
|    fps              | 35       |
|    time_elapsed     | 23607    |
|    total_timesteps  | 833908   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 183476   |
----------------------------------
Eval num_timesteps=834000, episode_reward=5.78 +/- 9.32
Episode length: 134.48 +/- 69.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 834000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0608   |
|    n_updates        | 183499   |
----------------------------------
Eval num_timesteps=834500, episode_reward=4.36 +/- 6.16
Episode length: 133.02 +/- 62.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 834500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0412   |
|    n_updates        | 183624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 5.1      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7556     |
|    fps              | 35       |
|    time_elapsed     | 23639    |
|    total_timesteps  | 834759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 183689   |
----------------------------------
Eval num_timesteps=835000, episode_reward=3.20 +/- 4.47
Episode length: 106.62 +/- 53.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 835000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.226    |
|    n_updates        | 183749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7560     |
|    fps              | 35       |
|    time_elapsed     | 23653    |
|    total_timesteps  | 835294   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.202    |
|    n_updates        | 183823   |
----------------------------------
Eval num_timesteps=835500, episode_reward=6.90 +/- 9.06
Episode length: 145.96 +/- 77.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 6.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 835500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0523   |
|    n_updates        | 183874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 4.89     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7564     |
|    fps              | 35       |
|    time_elapsed     | 23670    |
|    total_timesteps  | 835838   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 183959   |
----------------------------------
Eval num_timesteps=836000, episode_reward=4.72 +/- 7.58
Episode length: 140.76 +/- 69.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 836000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 183999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 4.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7568     |
|    fps              | 35       |
|    time_elapsed     | 23687    |
|    total_timesteps  | 836421   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 184105   |
----------------------------------
Eval num_timesteps=836500, episode_reward=4.24 +/- 6.05
Episode length: 133.34 +/- 72.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 836500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0662   |
|    n_updates        | 184124   |
----------------------------------
Eval num_timesteps=837000, episode_reward=3.78 +/- 6.57
Episode length: 110.44 +/- 50.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 837000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.05     |
|    n_updates        | 184249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 4.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7572     |
|    fps              | 35       |
|    time_elapsed     | 23715    |
|    total_timesteps  | 837057   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0607   |
|    n_updates        | 184264   |
----------------------------------
Eval num_timesteps=837500, episode_reward=3.78 +/- 5.72
Episode length: 116.36 +/- 64.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 837500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 184374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 5.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7576     |
|    fps              | 35       |
|    time_elapsed     | 23729    |
|    total_timesteps  | 837652   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 184412   |
----------------------------------
Eval num_timesteps=838000, episode_reward=5.28 +/- 9.47
Episode length: 144.42 +/- 81.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 5.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 838000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0754   |
|    n_updates        | 184499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7580     |
|    fps              | 35       |
|    time_elapsed     | 23746    |
|    total_timesteps  | 838008   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0683   |
|    n_updates        | 184501   |
----------------------------------
Eval num_timesteps=838500, episode_reward=2.90 +/- 4.33
Episode length: 107.14 +/- 44.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 838500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 184624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 5.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7584     |
|    fps              | 35       |
|    time_elapsed     | 23759    |
|    total_timesteps  | 838616   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 184653   |
----------------------------------
Eval num_timesteps=839000, episode_reward=5.12 +/- 9.93
Episode length: 124.34 +/- 66.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 839000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 184749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 5.03     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7588     |
|    fps              | 35       |
|    time_elapsed     | 23774    |
|    total_timesteps  | 839025   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 184756   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 4.66     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7592     |
|    fps              | 35       |
|    time_elapsed     | 23775    |
|    total_timesteps  | 839422   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 184855   |
----------------------------------
Eval num_timesteps=839500, episode_reward=5.44 +/- 8.08
Episode length: 129.46 +/- 64.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 839500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0262   |
|    n_updates        | 184874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 4.61     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7596     |
|    fps              | 35       |
|    time_elapsed     | 23790    |
|    total_timesteps  | 839703   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0636   |
|    n_updates        | 184925   |
----------------------------------
Eval num_timesteps=840000, episode_reward=4.04 +/- 6.03
Episode length: 112.96 +/- 61.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 840000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 184999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7600     |
|    fps              | 35       |
|    time_elapsed     | 23804    |
|    total_timesteps  | 840179   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 185044   |
----------------------------------
Eval num_timesteps=840500, episode_reward=6.16 +/- 11.11
Episode length: 131.22 +/- 78.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 840500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 185124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 4.57     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7604     |
|    fps              | 35       |
|    time_elapsed     | 23820    |
|    total_timesteps  | 840804   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0532   |
|    n_updates        | 185200   |
----------------------------------
Eval num_timesteps=841000, episode_reward=4.86 +/- 8.29
Episode length: 128.84 +/- 72.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 4.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 841000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.157    |
|    n_updates        | 185249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7608     |
|    fps              | 35       |
|    time_elapsed     | 23835    |
|    total_timesteps  | 841256   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 185313   |
----------------------------------
Eval num_timesteps=841500, episode_reward=3.80 +/- 6.26
Episode length: 129.54 +/- 93.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 841500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 185374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7612     |
|    fps              | 35       |
|    time_elapsed     | 23850    |
|    total_timesteps  | 841810   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0227   |
|    n_updates        | 185452   |
----------------------------------
Eval num_timesteps=842000, episode_reward=4.14 +/- 6.61
Episode length: 117.24 +/- 58.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 842000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0426   |
|    n_updates        | 185499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7616     |
|    fps              | 35       |
|    time_elapsed     | 23865    |
|    total_timesteps  | 842402   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 185600   |
----------------------------------
Eval num_timesteps=842500, episode_reward=3.42 +/- 5.10
Episode length: 114.52 +/- 55.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 842500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0259   |
|    n_updates        | 185624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.42     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7620     |
|    fps              | 35       |
|    time_elapsed     | 23878    |
|    total_timesteps  | 842780   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 185694   |
----------------------------------
Eval num_timesteps=843000, episode_reward=5.44 +/- 8.14
Episode length: 132.02 +/- 70.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 843000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 185749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7624     |
|    fps              | 35       |
|    time_elapsed     | 23894    |
|    total_timesteps  | 843252   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0436   |
|    n_updates        | 185812   |
----------------------------------
Eval num_timesteps=843500, episode_reward=3.76 +/- 6.54
Episode length: 113.62 +/- 65.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 843500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.174    |
|    n_updates        | 185874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.75     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7628     |
|    fps              | 35       |
|    time_elapsed     | 23908    |
|    total_timesteps  | 843779   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0388   |
|    n_updates        | 185944   |
----------------------------------
Eval num_timesteps=844000, episode_reward=4.60 +/- 7.09
Episode length: 127.36 +/- 73.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 844000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0645   |
|    n_updates        | 185999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.74     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7632     |
|    fps              | 35       |
|    time_elapsed     | 23923    |
|    total_timesteps  | 844172   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.153    |
|    n_updates        | 186042   |
----------------------------------
Eval num_timesteps=844500, episode_reward=3.72 +/- 5.59
Episode length: 111.28 +/- 48.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 844500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 186124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7636     |
|    fps              | 35       |
|    time_elapsed     | 23936    |
|    total_timesteps  | 844571   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 186142   |
----------------------------------
Eval num_timesteps=845000, episode_reward=3.28 +/- 5.59
Episode length: 101.04 +/- 47.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 845000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0262   |
|    n_updates        | 186249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7640     |
|    fps              | 35       |
|    time_elapsed     | 23949    |
|    total_timesteps  | 845081   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0877   |
|    n_updates        | 186270   |
----------------------------------
Eval num_timesteps=845500, episode_reward=3.88 +/- 6.80
Episode length: 122.06 +/- 52.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 845500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 186374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7644     |
|    fps              | 35       |
|    time_elapsed     | 23965    |
|    total_timesteps  | 845669   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0834   |
|    n_updates        | 186417   |
----------------------------------
Eval num_timesteps=846000, episode_reward=6.48 +/- 11.27
Episode length: 144.58 +/- 98.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 6.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 846000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0702   |
|    n_updates        | 186499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7648     |
|    fps              | 35       |
|    time_elapsed     | 23981    |
|    total_timesteps  | 846057   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0564   |
|    n_updates        | 186514   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7652     |
|    fps              | 35       |
|    time_elapsed     | 23983    |
|    total_timesteps  | 846394   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0805   |
|    n_updates        | 186598   |
----------------------------------
Eval num_timesteps=846500, episode_reward=2.68 +/- 3.53
Episode length: 104.46 +/- 40.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 846500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0746   |
|    n_updates        | 186624   |
----------------------------------
Eval num_timesteps=847000, episode_reward=1.66 +/- 2.63
Episode length: 103.80 +/- 45.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 1.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 847000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0649   |
|    n_updates        | 186749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7656     |
|    fps              | 35       |
|    time_elapsed     | 24008    |
|    total_timesteps  | 847144   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 186785   |
----------------------------------
Eval num_timesteps=847500, episode_reward=2.64 +/- 5.34
Episode length: 109.34 +/- 60.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 847500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0557   |
|    n_updates        | 186874   |
----------------------------------
Eval num_timesteps=848000, episode_reward=2.84 +/- 4.55
Episode length: 103.78 +/- 56.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 848000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 186999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7660     |
|    fps              | 35       |
|    time_elapsed     | 24035    |
|    total_timesteps  | 848007   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 187001   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7664     |
|    fps              | 35       |
|    time_elapsed     | 24036    |
|    total_timesteps  | 848393   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0263   |
|    n_updates        | 187098   |
----------------------------------
Eval num_timesteps=848500, episode_reward=3.50 +/- 4.42
Episode length: 124.92 +/- 53.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 848500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0771   |
|    n_updates        | 187124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7668     |
|    fps              | 35       |
|    time_elapsed     | 24051    |
|    total_timesteps  | 848728   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 187181   |
----------------------------------
Eval num_timesteps=849000, episode_reward=1.96 +/- 2.59
Episode length: 97.02 +/- 34.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 1.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 849000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0384   |
|    n_updates        | 187249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.34     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7672     |
|    fps              | 35       |
|    time_elapsed     | 24063    |
|    total_timesteps  | 849267   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 187316   |
----------------------------------
Eval num_timesteps=849500, episode_reward=3.84 +/- 4.92
Episode length: 114.38 +/- 69.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 849500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 187374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7676     |
|    fps              | 35       |
|    time_elapsed     | 24076    |
|    total_timesteps  | 849631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0444   |
|    n_updates        | 187407   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7680     |
|    fps              | 35       |
|    time_elapsed     | 24077    |
|    total_timesteps  | 849944   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0315   |
|    n_updates        | 187485   |
----------------------------------
Eval num_timesteps=850000, episode_reward=4.68 +/- 7.10
Episode length: 120.62 +/- 62.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 850000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0513   |
|    n_updates        | 187499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7684     |
|    fps              | 35       |
|    time_elapsed     | 24092    |
|    total_timesteps  | 850301   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0298   |
|    n_updates        | 187575   |
----------------------------------
Eval num_timesteps=850500, episode_reward=5.48 +/- 8.55
Episode length: 122.94 +/- 62.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 850500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 187624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.29     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7688     |
|    fps              | 35       |
|    time_elapsed     | 24107    |
|    total_timesteps  | 850917   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0776   |
|    n_updates        | 187729   |
----------------------------------
Eval num_timesteps=851000, episode_reward=2.64 +/- 3.10
Episode length: 98.04 +/- 46.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98       |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 851000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 187749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7692     |
|    fps              | 35       |
|    time_elapsed     | 24121    |
|    total_timesteps  | 851408   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 187851   |
----------------------------------
Eval num_timesteps=851500, episode_reward=4.86 +/- 7.63
Episode length: 124.00 +/- 57.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 851500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0495   |
|    n_updates        | 187874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7696     |
|    fps              | 35       |
|    time_elapsed     | 24137    |
|    total_timesteps  | 851827   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0446   |
|    n_updates        | 187956   |
----------------------------------
Eval num_timesteps=852000, episode_reward=4.90 +/- 7.37
Episode length: 119.46 +/- 57.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 852000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.037    |
|    n_updates        | 187999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.2      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7700     |
|    fps              | 35       |
|    time_elapsed     | 24152    |
|    total_timesteps  | 852194   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 188048   |
----------------------------------
Eval num_timesteps=852500, episode_reward=3.58 +/- 5.43
Episode length: 116.02 +/- 57.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 852500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 188124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7704     |
|    fps              | 35       |
|    time_elapsed     | 24166    |
|    total_timesteps  | 852675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 188168   |
----------------------------------
Eval num_timesteps=853000, episode_reward=4.58 +/- 7.02
Episode length: 136.00 +/- 92.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 853000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0696   |
|    n_updates        | 188249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.12     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7708     |
|    fps              | 35       |
|    time_elapsed     | 24183    |
|    total_timesteps  | 853466   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0573   |
|    n_updates        | 188366   |
----------------------------------
Eval num_timesteps=853500, episode_reward=5.06 +/- 8.81
Episode length: 126.48 +/- 65.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 853500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0503   |
|    n_updates        | 188374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7712     |
|    fps              | 35       |
|    time_elapsed     | 24198    |
|    total_timesteps  | 853798   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 188449   |
----------------------------------
Eval num_timesteps=854000, episode_reward=2.20 +/- 5.84
Episode length: 124.16 +/- 88.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 2.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 854000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 188499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7716     |
|    fps              | 35       |
|    time_elapsed     | 24212    |
|    total_timesteps  | 854176   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.064    |
|    n_updates        | 188543   |
----------------------------------
Eval num_timesteps=854500, episode_reward=6.48 +/- 10.46
Episode length: 167.74 +/- 116.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 168      |
|    mean_reward      | 6.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 854500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 188624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7720     |
|    fps              | 35       |
|    time_elapsed     | 24233    |
|    total_timesteps  | 854776   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0603   |
|    n_updates        | 188693   |
----------------------------------
Eval num_timesteps=855000, episode_reward=5.06 +/- 10.69
Episode length: 111.40 +/- 71.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 855000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0426   |
|    n_updates        | 188749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7724     |
|    fps              | 35       |
|    time_elapsed     | 24246    |
|    total_timesteps  | 855156   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0157   |
|    n_updates        | 188788   |
----------------------------------
Eval num_timesteps=855500, episode_reward=6.98 +/- 10.34
Episode length: 151.54 +/- 88.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 6.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 855500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0367   |
|    n_updates        | 188874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.65     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7728     |
|    fps              | 35       |
|    time_elapsed     | 24264    |
|    total_timesteps  | 855588   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0506   |
|    n_updates        | 188896   |
----------------------------------
Eval num_timesteps=856000, episode_reward=4.84 +/- 7.48
Episode length: 128.04 +/- 57.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 856000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 188999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7732     |
|    fps              | 35       |
|    time_elapsed     | 24280    |
|    total_timesteps  | 856335   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0409   |
|    n_updates        | 189083   |
----------------------------------
Eval num_timesteps=856500, episode_reward=5.50 +/- 9.38
Episode length: 138.46 +/- 94.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 5.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 856500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0599   |
|    n_updates        | 189124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.16     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7736     |
|    fps              | 35       |
|    time_elapsed     | 24298    |
|    total_timesteps  | 856971   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0715   |
|    n_updates        | 189242   |
----------------------------------
Eval num_timesteps=857000, episode_reward=3.84 +/- 7.01
Episode length: 139.68 +/- 95.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 857000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0343   |
|    n_updates        | 189249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7740     |
|    fps              | 35       |
|    time_elapsed     | 24314    |
|    total_timesteps  | 857397   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 189349   |
----------------------------------
Eval num_timesteps=857500, episode_reward=6.04 +/- 9.64
Episode length: 145.48 +/- 74.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 6.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 857500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0657   |
|    n_updates        | 189374   |
----------------------------------
Eval num_timesteps=858000, episode_reward=5.78 +/- 10.11
Episode length: 125.56 +/- 68.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 858000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00971  |
|    n_updates        | 189499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.38     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7744     |
|    fps              | 35       |
|    time_elapsed     | 24351    |
|    total_timesteps  | 858316   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0346   |
|    n_updates        | 189578   |
----------------------------------
Eval num_timesteps=858500, episode_reward=5.06 +/- 8.21
Episode length: 137.86 +/- 80.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 858500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0628   |
|    n_updates        | 189624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7748     |
|    fps              | 35       |
|    time_elapsed     | 24371    |
|    total_timesteps  | 858770   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 189692   |
----------------------------------
Eval num_timesteps=859000, episode_reward=5.26 +/- 8.57
Episode length: 118.30 +/- 60.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 5.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 859000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 189749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.6      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7752     |
|    fps              | 35       |
|    time_elapsed     | 24386    |
|    total_timesteps  | 859125   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0661   |
|    n_updates        | 189781   |
----------------------------------
Eval num_timesteps=859500, episode_reward=4.52 +/- 6.86
Episode length: 134.42 +/- 91.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 859500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0791   |
|    n_updates        | 189874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.62     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7756     |
|    fps              | 35       |
|    time_elapsed     | 24403    |
|    total_timesteps  | 859541   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.129    |
|    n_updates        | 189885   |
----------------------------------
Eval num_timesteps=860000, episode_reward=5.90 +/- 7.14
Episode length: 153.36 +/- 63.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 153      |
|    mean_reward      | 5.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 860000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0745   |
|    n_updates        | 189999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.61     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7760     |
|    fps              | 35       |
|    time_elapsed     | 24421    |
|    total_timesteps  | 860047   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0568   |
|    n_updates        | 190011   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7764     |
|    fps              | 35       |
|    time_elapsed     | 24422    |
|    total_timesteps  | 860317   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0744   |
|    n_updates        | 190079   |
----------------------------------
Eval num_timesteps=860500, episode_reward=5.00 +/- 7.52
Episode length: 127.72 +/- 71.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 860500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0753   |
|    n_updates        | 190124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7768     |
|    fps              | 35       |
|    time_elapsed     | 24437    |
|    total_timesteps  | 860743   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 190185   |
----------------------------------
Eval num_timesteps=861000, episode_reward=5.40 +/- 9.11
Episode length: 124.88 +/- 73.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 5.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 861000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0948   |
|    n_updates        | 190249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7772     |
|    fps              | 35       |
|    time_elapsed     | 24453    |
|    total_timesteps  | 861208   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0545   |
|    n_updates        | 190301   |
----------------------------------
Eval num_timesteps=861500, episode_reward=3.20 +/- 4.35
Episode length: 105.48 +/- 45.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 861500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 190374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.41     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7776     |
|    fps              | 35       |
|    time_elapsed     | 24468    |
|    total_timesteps  | 861690   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0312   |
|    n_updates        | 190422   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7780     |
|    fps              | 35       |
|    time_elapsed     | 24470    |
|    total_timesteps  | 861975   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0543   |
|    n_updates        | 190493   |
----------------------------------
Eval num_timesteps=862000, episode_reward=3.24 +/- 4.88
Episode length: 103.82 +/- 58.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 862000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0625   |
|    n_updates        | 190499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.61     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7784     |
|    fps              | 35       |
|    time_elapsed     | 24483    |
|    total_timesteps  | 862451   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 190612   |
----------------------------------
Eval num_timesteps=862500, episode_reward=3.76 +/- 7.73
Episode length: 107.84 +/- 57.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 862500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0514   |
|    n_updates        | 190624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7788     |
|    fps              | 35       |
|    time_elapsed     | 24497    |
|    total_timesteps  | 862942   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 190735   |
----------------------------------
Eval num_timesteps=863000, episode_reward=7.14 +/- 11.48
Episode length: 137.64 +/- 76.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 7.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 863000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0853   |
|    n_updates        | 190749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7792     |
|    fps              | 35       |
|    time_elapsed     | 24513    |
|    total_timesteps  | 863495   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0604   |
|    n_updates        | 190873   |
----------------------------------
Eval num_timesteps=863500, episode_reward=6.40 +/- 7.05
Episode length: 126.90 +/- 56.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 6.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 863500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0488   |
|    n_updates        | 190874   |
----------------------------------
Eval num_timesteps=864000, episode_reward=3.18 +/- 4.55
Episode length: 116.38 +/- 51.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 3.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 864000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0853   |
|    n_updates        | 190999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7796     |
|    fps              | 35       |
|    time_elapsed     | 24543    |
|    total_timesteps  | 864359   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0386   |
|    n_updates        | 191089   |
----------------------------------
Eval num_timesteps=864500, episode_reward=5.12 +/- 7.42
Episode length: 154.20 +/- 115.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 5.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 864500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 191124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7800     |
|    fps              | 35       |
|    time_elapsed     | 24561    |
|    total_timesteps  | 864889   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.066    |
|    n_updates        | 191222   |
----------------------------------
Eval num_timesteps=865000, episode_reward=4.26 +/- 6.43
Episode length: 124.60 +/- 82.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 865000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0592   |
|    n_updates        | 191249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 5.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7804     |
|    fps              | 35       |
|    time_elapsed     | 24576    |
|    total_timesteps  | 865273   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0635   |
|    n_updates        | 191318   |
----------------------------------
Eval num_timesteps=865500, episode_reward=5.18 +/- 7.68
Episode length: 124.88 +/- 61.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 5.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 865500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 191374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7808     |
|    fps              | 35       |
|    time_elapsed     | 24592    |
|    total_timesteps  | 865972   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0342   |
|    n_updates        | 191492   |
----------------------------------
Eval num_timesteps=866000, episode_reward=6.00 +/- 8.33
Episode length: 138.14 +/- 78.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 6        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 866000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 191499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.41     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7812     |
|    fps              | 35       |
|    time_elapsed     | 24608    |
|    total_timesteps  | 866464   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0766   |
|    n_updates        | 191615   |
----------------------------------
Eval num_timesteps=866500, episode_reward=5.12 +/- 8.66
Episode length: 124.50 +/- 71.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 866500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 191624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.53     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7816     |
|    fps              | 35       |
|    time_elapsed     | 24623    |
|    total_timesteps  | 866935   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 191733   |
----------------------------------
Eval num_timesteps=867000, episode_reward=3.94 +/- 6.44
Episode length: 135.72 +/- 89.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 867000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 191749   |
----------------------------------
Eval num_timesteps=867500, episode_reward=5.64 +/- 9.63
Episode length: 128.04 +/- 76.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 867500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0741   |
|    n_updates        | 191874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 6.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7820     |
|    fps              | 35       |
|    time_elapsed     | 24660    |
|    total_timesteps  | 867859   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0144   |
|    n_updates        | 191964   |
----------------------------------
Eval num_timesteps=868000, episode_reward=8.54 +/- 12.74
Episode length: 144.06 +/- 85.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 8.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 868000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0746   |
|    n_updates        | 191999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 6.13     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7824     |
|    fps              | 35       |
|    time_elapsed     | 24677    |
|    total_timesteps  | 868326   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0534   |
|    n_updates        | 192081   |
----------------------------------
Eval num_timesteps=868500, episode_reward=5.38 +/- 8.65
Episode length: 135.34 +/- 90.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 868500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 192124   |
----------------------------------
Eval num_timesteps=869000, episode_reward=5.30 +/- 9.99
Episode length: 145.08 +/- 98.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 5.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 869000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0516   |
|    n_updates        | 192249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 6.47     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7828     |
|    fps              | 35       |
|    time_elapsed     | 24710    |
|    total_timesteps  | 869050   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0584   |
|    n_updates        | 192262   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 6.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7832     |
|    fps              | 35       |
|    time_elapsed     | 24711    |
|    total_timesteps  | 869405   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0364   |
|    n_updates        | 192351   |
----------------------------------
Eval num_timesteps=869500, episode_reward=5.44 +/- 8.78
Episode length: 132.66 +/- 96.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 869500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 192374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7836     |
|    fps              | 35       |
|    time_elapsed     | 24727    |
|    total_timesteps  | 869774   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0766   |
|    n_updates        | 192443   |
----------------------------------
Eval num_timesteps=870000, episode_reward=6.40 +/- 8.85
Episode length: 172.82 +/- 135.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 173      |
|    mean_reward      | 6.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 870000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0423   |
|    n_updates        | 192499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 5.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7840     |
|    fps              | 35       |
|    time_elapsed     | 24746    |
|    total_timesteps  | 870069   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0932   |
|    n_updates        | 192517   |
----------------------------------
Eval num_timesteps=870500, episode_reward=3.64 +/- 4.72
Episode length: 123.04 +/- 57.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 870500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0738   |
|    n_updates        | 192624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 5.6      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7844     |
|    fps              | 35       |
|    time_elapsed     | 24762    |
|    total_timesteps  | 870569   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 192642   |
----------------------------------
Eval num_timesteps=871000, episode_reward=5.68 +/- 11.53
Episode length: 130.36 +/- 82.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 871000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0278   |
|    n_updates        | 192749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.71     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7848     |
|    fps              | 35       |
|    time_elapsed     | 24778    |
|    total_timesteps  | 871126   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0796   |
|    n_updates        | 192781   |
----------------------------------
Eval num_timesteps=871500, episode_reward=6.20 +/- 8.37
Episode length: 151.00 +/- 94.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 6.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 871500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0263   |
|    n_updates        | 192874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7852     |
|    fps              | 35       |
|    time_elapsed     | 24797    |
|    total_timesteps  | 871673   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0923   |
|    n_updates        | 192918   |
----------------------------------
Eval num_timesteps=872000, episode_reward=5.86 +/- 9.39
Episode length: 140.00 +/- 92.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 5.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 872000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 192999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.71     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7856     |
|    fps              | 35       |
|    time_elapsed     | 24814    |
|    total_timesteps  | 872304   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0865   |
|    n_updates        | 193075   |
----------------------------------
Eval num_timesteps=872500, episode_reward=3.32 +/- 5.53
Episode length: 159.90 +/- 117.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 160      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 872500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0175   |
|    n_updates        | 193124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.4      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7860     |
|    fps              | 35       |
|    time_elapsed     | 24833    |
|    total_timesteps  | 872801   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 193200   |
----------------------------------
Eval num_timesteps=873000, episode_reward=5.58 +/- 7.15
Episode length: 179.04 +/- 136.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 5.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 873000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0835   |
|    n_updates        | 193249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.63     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7864     |
|    fps              | 35       |
|    time_elapsed     | 24861    |
|    total_timesteps  | 873366   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0393   |
|    n_updates        | 193341   |
----------------------------------
Eval num_timesteps=873500, episode_reward=5.20 +/- 6.55
Episode length: 134.84 +/- 83.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 873500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 193374   |
----------------------------------
Eval num_timesteps=874000, episode_reward=6.34 +/- 9.78
Episode length: 140.56 +/- 99.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 6.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 874000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 193499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7868     |
|    fps              | 35       |
|    time_elapsed     | 24896    |
|    total_timesteps  | 874030   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.192    |
|    n_updates        | 193507   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.69     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7872     |
|    fps              | 35       |
|    time_elapsed     | 24899    |
|    total_timesteps  | 874416   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 193603   |
----------------------------------
Eval num_timesteps=874500, episode_reward=6.10 +/- 10.93
Episode length: 131.54 +/- 89.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 6.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 874500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 193624   |
----------------------------------
Eval num_timesteps=875000, episode_reward=6.06 +/- 11.63
Episode length: 145.58 +/- 85.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 6.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 875000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0327   |
|    n_updates        | 193749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 6.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7876     |
|    fps              | 35       |
|    time_elapsed     | 24941    |
|    total_timesteps  | 875136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0474   |
|    n_updates        | 193783   |
----------------------------------
Eval num_timesteps=875500, episode_reward=5.96 +/- 9.42
Episode length: 129.50 +/- 59.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 875500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 193874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 6.09     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7880     |
|    fps              | 35       |
|    time_elapsed     | 24962    |
|    total_timesteps  | 875522   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0621   |
|    n_updates        | 193880   |
----------------------------------
Eval num_timesteps=876000, episode_reward=6.26 +/- 7.56
Episode length: 168.96 +/- 77.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 169      |
|    mean_reward      | 6.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 876000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 193999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 6.11     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7884     |
|    fps              | 35       |
|    time_elapsed     | 24988    |
|    total_timesteps  | 876114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 194028   |
----------------------------------
Eval num_timesteps=876500, episode_reward=4.74 +/- 6.90
Episode length: 127.02 +/- 61.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 876500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 194124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 6.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7888     |
|    fps              | 35       |
|    time_elapsed     | 25005    |
|    total_timesteps  | 876691   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 194172   |
----------------------------------
Eval num_timesteps=877000, episode_reward=6.06 +/- 10.33
Episode length: 141.70 +/- 79.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 6.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 877000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 194249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 5.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7892     |
|    fps              | 35       |
|    time_elapsed     | 25022    |
|    total_timesteps  | 877059   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0527   |
|    n_updates        | 194264   |
----------------------------------
Eval num_timesteps=877500, episode_reward=5.20 +/- 5.89
Episode length: 139.80 +/- 67.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 5.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 877500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0468   |
|    n_updates        | 194374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7896     |
|    fps              | 35       |
|    time_elapsed     | 25039    |
|    total_timesteps  | 877606   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0542   |
|    n_updates        | 194401   |
----------------------------------
Eval num_timesteps=878000, episode_reward=7.74 +/- 11.63
Episode length: 161.70 +/- 97.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 162      |
|    mean_reward      | 7.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 878000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 194499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7900     |
|    fps              | 35       |
|    time_elapsed     | 25057    |
|    total_timesteps  | 878096   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 194523   |
----------------------------------
Eval num_timesteps=878500, episode_reward=3.64 +/- 6.67
Episode length: 156.82 +/- 111.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 157      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 878500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0651   |
|    n_updates        | 194624   |
----------------------------------
Eval num_timesteps=879000, episode_reward=5.28 +/- 7.39
Episode length: 118.26 +/- 66.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 5.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 879000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.036    |
|    n_updates        | 194749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 6.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7904     |
|    fps              | 35       |
|    time_elapsed     | 25090    |
|    total_timesteps  | 879109   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0216   |
|    n_updates        | 194777   |
----------------------------------
Eval num_timesteps=879500, episode_reward=4.42 +/- 8.35
Episode length: 117.94 +/- 56.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 879500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0453   |
|    n_updates        | 194874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 6.42     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7908     |
|    fps              | 35       |
|    time_elapsed     | 25106    |
|    total_timesteps  | 879861   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0501   |
|    n_updates        | 194965   |
----------------------------------
Eval num_timesteps=880000, episode_reward=4.50 +/- 6.98
Episode length: 117.04 +/- 61.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 880000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.142    |
|    n_updates        | 194999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 6.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7912     |
|    fps              | 35       |
|    time_elapsed     | 25121    |
|    total_timesteps  | 880332   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0265   |
|    n_updates        | 195082   |
----------------------------------
Eval num_timesteps=880500, episode_reward=3.44 +/- 4.44
Episode length: 112.26 +/- 45.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 880500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 195124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 6.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7916     |
|    fps              | 35       |
|    time_elapsed     | 25134    |
|    total_timesteps  | 880727   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 195181   |
----------------------------------
Eval num_timesteps=881000, episode_reward=4.24 +/- 5.90
Episode length: 152.84 +/- 104.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 153      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 881000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00976  |
|    n_updates        | 195249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.5      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7920     |
|    fps              | 35       |
|    time_elapsed     | 25152    |
|    total_timesteps  | 881239   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0234   |
|    n_updates        | 195309   |
----------------------------------
Eval num_timesteps=881500, episode_reward=6.66 +/- 8.44
Episode length: 148.06 +/- 67.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 6.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 881500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.161    |
|    n_updates        | 195374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.53     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7924     |
|    fps              | 35       |
|    time_elapsed     | 25170    |
|    total_timesteps  | 881631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 195407   |
----------------------------------
Eval num_timesteps=882000, episode_reward=3.52 +/- 4.28
Episode length: 132.28 +/- 85.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 3.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 882000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0558   |
|    n_updates        | 195499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7928     |
|    fps              | 35       |
|    time_elapsed     | 25186    |
|    total_timesteps  | 882193   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0302   |
|    n_updates        | 195548   |
----------------------------------
Eval num_timesteps=882500, episode_reward=3.68 +/- 5.32
Episode length: 132.00 +/- 47.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 3.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 882500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 195624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.51     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7932     |
|    fps              | 35       |
|    time_elapsed     | 25202    |
|    total_timesteps  | 882649   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 195662   |
----------------------------------
Eval num_timesteps=883000, episode_reward=4.64 +/- 7.30
Episode length: 119.82 +/- 72.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 883000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 195749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.49     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7936     |
|    fps              | 35       |
|    time_elapsed     | 25216    |
|    total_timesteps  | 883141   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 195785   |
----------------------------------
Eval num_timesteps=883500, episode_reward=5.34 +/- 8.10
Episode length: 140.34 +/- 90.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 883500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.071    |
|    n_updates        | 195874   |
----------------------------------
Eval num_timesteps=884000, episode_reward=4.88 +/- 6.56
Episode length: 126.06 +/- 86.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 884000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 195999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 6.11     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7940     |
|    fps              | 35       |
|    time_elapsed     | 25248    |
|    total_timesteps  | 884022   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0721   |
|    n_updates        | 196005   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 6.12     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7944     |
|    fps              | 35       |
|    time_elapsed     | 25250    |
|    total_timesteps  | 884438   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.067    |
|    n_updates        | 196109   |
----------------------------------
Eval num_timesteps=884500, episode_reward=4.88 +/- 6.40
Episode length: 145.60 +/- 90.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 4.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 884500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.168    |
|    n_updates        | 196124   |
----------------------------------
Eval num_timesteps=885000, episode_reward=5.22 +/- 9.51
Episode length: 156.82 +/- 108.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 157      |
|    mean_reward      | 5.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 885000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0993   |
|    n_updates        | 196249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 5.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7948     |
|    fps              | 35       |
|    time_elapsed     | 25285    |
|    total_timesteps  | 885392   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0452   |
|    n_updates        | 196347   |
----------------------------------
Eval num_timesteps=885500, episode_reward=7.40 +/- 11.36
Episode length: 159.42 +/- 125.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 7.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 885500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0622   |
|    n_updates        | 196374   |
----------------------------------
Eval num_timesteps=886000, episode_reward=5.12 +/- 6.17
Episode length: 135.98 +/- 60.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 5.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 886000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 196499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 5.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7952     |
|    fps              | 34       |
|    time_elapsed     | 25319    |
|    total_timesteps  | 886038   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 196509   |
----------------------------------
Eval num_timesteps=886500, episode_reward=4.80 +/- 6.60
Episode length: 112.72 +/- 53.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 886500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.047    |
|    n_updates        | 196624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 6.13     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7956     |
|    fps              | 34       |
|    time_elapsed     | 25334    |
|    total_timesteps  | 886678   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0364   |
|    n_updates        | 196669   |
----------------------------------
Eval num_timesteps=887000, episode_reward=4.44 +/- 6.88
Episode length: 118.08 +/- 59.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 887000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0758   |
|    n_updates        | 196749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 6.1      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7960     |
|    fps              | 35       |
|    time_elapsed     | 25348    |
|    total_timesteps  | 887255   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0827   |
|    n_updates        | 196813   |
----------------------------------
Eval num_timesteps=887500, episode_reward=5.18 +/- 6.48
Episode length: 129.40 +/- 60.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 887500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 196874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 5.97     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7964     |
|    fps              | 34       |
|    time_elapsed     | 25365    |
|    total_timesteps  | 887752   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 196937   |
----------------------------------
Eval num_timesteps=888000, episode_reward=5.06 +/- 5.48
Episode length: 129.52 +/- 59.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 888000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0306   |
|    n_updates        | 196999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 5.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7968     |
|    fps              | 35       |
|    time_elapsed     | 25381    |
|    total_timesteps  | 888401   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.131    |
|    n_updates        | 197100   |
----------------------------------
Eval num_timesteps=888500, episode_reward=4.92 +/- 6.03
Episode length: 136.48 +/- 66.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 4.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 888500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0478   |
|    n_updates        | 197124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 5.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7972     |
|    fps              | 34       |
|    time_elapsed     | 25398    |
|    total_timesteps  | 888818   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 197204   |
----------------------------------
Eval num_timesteps=889000, episode_reward=5.86 +/- 9.04
Episode length: 127.66 +/- 82.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 889000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.146    |
|    n_updates        | 197249   |
----------------------------------
Eval num_timesteps=889500, episode_reward=4.94 +/- 8.31
Episode length: 123.74 +/- 65.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 889500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 197374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 5.77     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7976     |
|    fps              | 34       |
|    time_elapsed     | 25427    |
|    total_timesteps  | 889527   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 197381   |
----------------------------------
Eval num_timesteps=890000, episode_reward=5.82 +/- 6.97
Episode length: 126.58 +/- 58.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 5.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 890000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0348   |
|    n_updates        | 197499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 5.78     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7980     |
|    fps              | 34       |
|    time_elapsed     | 25448    |
|    total_timesteps  | 890003   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0767   |
|    n_updates        | 197500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 5.55     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7984     |
|    fps              | 34       |
|    time_elapsed     | 25450    |
|    total_timesteps  | 890338   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 197584   |
----------------------------------
Eval num_timesteps=890500, episode_reward=7.12 +/- 9.60
Episode length: 166.14 +/- 105.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 7.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 890500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.202    |
|    n_updates        | 197624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 5.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7988     |
|    fps              | 34       |
|    time_elapsed     | 25469    |
|    total_timesteps  | 890694   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 197673   |
----------------------------------
Eval num_timesteps=891000, episode_reward=5.98 +/- 8.30
Episode length: 127.12 +/- 69.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 5.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 891000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0595   |
|    n_updates        | 197749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 5.57     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7992     |
|    fps              | 34       |
|    time_elapsed     | 25484    |
|    total_timesteps  | 891018   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0875   |
|    n_updates        | 197754   |
----------------------------------
Eval num_timesteps=891500, episode_reward=5.96 +/- 9.08
Episode length: 123.96 +/- 73.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 891500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0911   |
|    n_updates        | 197874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 5.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 7996     |
|    fps              | 34       |
|    time_elapsed     | 25502    |
|    total_timesteps  | 891684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0703   |
|    n_updates        | 197920   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 5.63     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8000     |
|    fps              | 34       |
|    time_elapsed     | 25504    |
|    total_timesteps  | 891994   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 197998   |
----------------------------------
Eval num_timesteps=892000, episode_reward=4.90 +/- 7.02
Episode length: 108.96 +/- 50.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 4.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 892000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0681   |
|    n_updates        | 197999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 4.73     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8004     |
|    fps              | 34       |
|    time_elapsed     | 25518    |
|    total_timesteps  | 892471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0794   |
|    n_updates        | 198117   |
----------------------------------
Eval num_timesteps=892500, episode_reward=4.34 +/- 6.60
Episode length: 117.46 +/- 56.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 892500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0735   |
|    n_updates        | 198124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.4      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8008     |
|    fps              | 34       |
|    time_elapsed     | 25532    |
|    total_timesteps  | 892976   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0739   |
|    n_updates        | 198243   |
----------------------------------
Eval num_timesteps=893000, episode_reward=6.64 +/- 9.69
Episode length: 112.10 +/- 53.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 6.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 893000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 198249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 4.41     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8012     |
|    fps              | 34       |
|    time_elapsed     | 25546    |
|    total_timesteps  | 893495   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 198373   |
----------------------------------
Eval num_timesteps=893500, episode_reward=4.56 +/- 5.80
Episode length: 143.80 +/- 83.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 4.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 893500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0733   |
|    n_updates        | 198374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8016     |
|    fps              | 34       |
|    time_elapsed     | 25563    |
|    total_timesteps  | 893881   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0702   |
|    n_updates        | 198470   |
----------------------------------
Eval num_timesteps=894000, episode_reward=7.10 +/- 11.37
Episode length: 144.22 +/- 82.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 7.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 894000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0321   |
|    n_updates        | 198499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.5      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8020     |
|    fps              | 34       |
|    time_elapsed     | 25580    |
|    total_timesteps  | 894358   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 198589   |
----------------------------------
Eval num_timesteps=894500, episode_reward=6.22 +/- 10.64
Episode length: 131.62 +/- 71.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 6.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 894500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0758   |
|    n_updates        | 198624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8024     |
|    fps              | 34       |
|    time_elapsed     | 25596    |
|    total_timesteps  | 894733   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 198683   |
----------------------------------
Eval num_timesteps=895000, episode_reward=4.84 +/- 7.09
Episode length: 143.60 +/- 95.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 895000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 198749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.27     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8028     |
|    fps              | 34       |
|    time_elapsed     | 25613    |
|    total_timesteps  | 895273   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 198818   |
----------------------------------
Eval num_timesteps=895500, episode_reward=3.42 +/- 4.49
Episode length: 112.90 +/- 45.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 895500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 198874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.12     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8032     |
|    fps              | 34       |
|    time_elapsed     | 25627    |
|    total_timesteps  | 895684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0531   |
|    n_updates        | 198920   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8036     |
|    fps              | 34       |
|    time_elapsed     | 25628    |
|    total_timesteps  | 895963   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 198990   |
----------------------------------
Eval num_timesteps=896000, episode_reward=4.48 +/- 8.05
Episode length: 121.10 +/- 63.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 896000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 198999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 3.45     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8040     |
|    fps              | 34       |
|    time_elapsed     | 25643    |
|    total_timesteps  | 896378   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.125    |
|    n_updates        | 199094   |
----------------------------------
Eval num_timesteps=896500, episode_reward=4.72 +/- 7.08
Episode length: 114.78 +/- 58.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 896500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 199124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8044     |
|    fps              | 34       |
|    time_elapsed     | 25658    |
|    total_timesteps  | 896881   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.042    |
|    n_updates        | 199220   |
----------------------------------
Eval num_timesteps=897000, episode_reward=6.44 +/- 8.78
Episode length: 119.70 +/- 63.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 6.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 897000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0454   |
|    n_updates        | 199249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8048     |
|    fps              | 34       |
|    time_elapsed     | 25673    |
|    total_timesteps  | 897306   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0366   |
|    n_updates        | 199326   |
----------------------------------
Eval num_timesteps=897500, episode_reward=4.54 +/- 6.01
Episode length: 130.72 +/- 88.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 4.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 897500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 199374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8052     |
|    fps              | 34       |
|    time_elapsed     | 25688    |
|    total_timesteps  | 897775   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0666   |
|    n_updates        | 199443   |
----------------------------------
Eval num_timesteps=898000, episode_reward=2.66 +/- 3.96
Episode length: 101.96 +/- 46.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 898000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 199499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8056     |
|    fps              | 34       |
|    time_elapsed     | 25701    |
|    total_timesteps  | 898130   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.14     |
|    n_updates        | 199532   |
----------------------------------
Eval num_timesteps=898500, episode_reward=3.42 +/- 4.99
Episode length: 112.38 +/- 74.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 898500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.041    |
|    n_updates        | 199624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8060     |
|    fps              | 34       |
|    time_elapsed     | 25714    |
|    total_timesteps  | 898524   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 199630   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.03     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8064     |
|    fps              | 34       |
|    time_elapsed     | 25716    |
|    total_timesteps  | 898990   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0784   |
|    n_updates        | 199747   |
----------------------------------
Eval num_timesteps=899000, episode_reward=5.44 +/- 7.26
Episode length: 133.64 +/- 68.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 899000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0524   |
|    n_updates        | 199749   |
----------------------------------
Eval num_timesteps=899500, episode_reward=3.64 +/- 5.71
Episode length: 100.22 +/- 47.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 899500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0617   |
|    n_updates        | 199874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 2.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8068     |
|    fps              | 34       |
|    time_elapsed     | 25743    |
|    total_timesteps  | 899505   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 199876   |
----------------------------------
Eval num_timesteps=900000, episode_reward=7.18 +/- 9.23
Episode length: 148.10 +/- 72.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 7.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 900000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 199999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8072     |
|    fps              | 34       |
|    time_elapsed     | 25761    |
|    total_timesteps  | 900110   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 200027   |
----------------------------------
Eval num_timesteps=900500, episode_reward=6.08 +/- 7.06
Episode length: 144.12 +/- 65.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 6.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 900500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 200124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8076     |
|    fps              | 34       |
|    time_elapsed     | 25779    |
|    total_timesteps  | 900531   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0515   |
|    n_updates        | 200132   |
----------------------------------
Eval num_timesteps=901000, episode_reward=4.00 +/- 5.01
Episode length: 116.50 +/- 52.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 901000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0663   |
|    n_updates        | 200249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8080     |
|    fps              | 34       |
|    time_elapsed     | 25793    |
|    total_timesteps  | 901025   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0352   |
|    n_updates        | 200256   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8084     |
|    fps              | 34       |
|    time_elapsed     | 25795    |
|    total_timesteps  | 901441   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0656   |
|    n_updates        | 200360   |
----------------------------------
Eval num_timesteps=901500, episode_reward=5.16 +/- 6.54
Episode length: 136.76 +/- 69.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 901500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.055    |
|    n_updates        | 200374   |
----------------------------------
Eval num_timesteps=902000, episode_reward=6.36 +/- 9.25
Episode length: 132.88 +/- 68.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 6.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 902000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 200499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8088     |
|    fps              | 34       |
|    time_elapsed     | 25828    |
|    total_timesteps  | 902138   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0416   |
|    n_updates        | 200534   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8092     |
|    fps              | 34       |
|    time_elapsed     | 25830    |
|    total_timesteps  | 902481   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 200620   |
----------------------------------
Eval num_timesteps=902500, episode_reward=6.68 +/- 10.77
Episode length: 137.54 +/- 92.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 6.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 902500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0562   |
|    n_updates        | 200624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.07     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8096     |
|    fps              | 34       |
|    time_elapsed     | 25850    |
|    total_timesteps  | 902953   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 200738   |
----------------------------------
Eval num_timesteps=903000, episode_reward=4.72 +/- 6.99
Episode length: 133.58 +/- 82.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 903000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0309   |
|    n_updates        | 200749   |
----------------------------------
Eval num_timesteps=903500, episode_reward=4.76 +/- 5.98
Episode length: 108.58 +/- 49.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 903500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 200874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8100     |
|    fps              | 34       |
|    time_elapsed     | 25882    |
|    total_timesteps  | 903548   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.038    |
|    n_updates        | 200886   |
----------------------------------
Eval num_timesteps=904000, episode_reward=4.98 +/- 7.96
Episode length: 117.86 +/- 66.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 904000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0205   |
|    n_updates        | 200999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8104     |
|    fps              | 34       |
|    time_elapsed     | 25897    |
|    total_timesteps  | 904184   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0471   |
|    n_updates        | 201045   |
----------------------------------
Eval num_timesteps=904500, episode_reward=5.90 +/- 8.13
Episode length: 131.74 +/- 70.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 5.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 904500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 201124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.2      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8108     |
|    fps              | 34       |
|    time_elapsed     | 25913    |
|    total_timesteps  | 904547   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0566   |
|    n_updates        | 201136   |
----------------------------------
Eval num_timesteps=905000, episode_reward=4.70 +/- 8.35
Episode length: 121.16 +/- 68.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 905000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0186   |
|    n_updates        | 201249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8112     |
|    fps              | 34       |
|    time_elapsed     | 25928    |
|    total_timesteps  | 905136   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0428   |
|    n_updates        | 201283   |
----------------------------------
Eval num_timesteps=905500, episode_reward=6.24 +/- 7.79
Episode length: 143.68 +/- 91.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 6.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 905500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0481   |
|    n_updates        | 201374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8116     |
|    fps              | 34       |
|    time_elapsed     | 25946    |
|    total_timesteps  | 905585   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0932   |
|    n_updates        | 201396   |
----------------------------------
Eval num_timesteps=906000, episode_reward=5.00 +/- 7.97
Episode length: 149.14 +/- 105.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 906000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0642   |
|    n_updates        | 201499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.47     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8120     |
|    fps              | 34       |
|    time_elapsed     | 25964    |
|    total_timesteps  | 906067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0126   |
|    n_updates        | 201516   |
----------------------------------
Eval num_timesteps=906500, episode_reward=6.94 +/- 11.06
Episode length: 149.56 +/- 74.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 6.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 906500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 201624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8124     |
|    fps              | 34       |
|    time_elapsed     | 25982    |
|    total_timesteps  | 906587   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 201646   |
----------------------------------
Eval num_timesteps=907000, episode_reward=3.54 +/- 5.49
Episode length: 122.36 +/- 61.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 907000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0566   |
|    n_updates        | 201749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 3.5      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8128     |
|    fps              | 34       |
|    time_elapsed     | 25998    |
|    total_timesteps  | 907291   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 201822   |
----------------------------------
Eval num_timesteps=907500, episode_reward=7.64 +/- 13.09
Episode length: 151.24 +/- 87.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 7.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 907500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 201874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 3.57     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8132     |
|    fps              | 34       |
|    time_elapsed     | 26023    |
|    total_timesteps  | 907903   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 201975   |
----------------------------------
Eval num_timesteps=908000, episode_reward=5.80 +/- 7.45
Episode length: 129.66 +/- 68.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 908000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 201999   |
----------------------------------
Eval num_timesteps=908500, episode_reward=5.80 +/- 6.95
Episode length: 182.64 +/- 111.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 183      |
|    mean_reward      | 5.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 908500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00978  |
|    n_updates        | 202124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8136     |
|    fps              | 34       |
|    time_elapsed     | 26068    |
|    total_timesteps  | 908822   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0415   |
|    n_updates        | 202205   |
----------------------------------
Eval num_timesteps=909000, episode_reward=4.44 +/- 6.69
Episode length: 125.68 +/- 84.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 909000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 202249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8140     |
|    fps              | 34       |
|    time_elapsed     | 26084    |
|    total_timesteps  | 909294   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 202323   |
----------------------------------
Eval num_timesteps=909500, episode_reward=5.08 +/- 6.95
Episode length: 126.16 +/- 57.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 909500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0564   |
|    n_updates        | 202374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.17     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8144     |
|    fps              | 34       |
|    time_elapsed     | 26103    |
|    total_timesteps  | 909871   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0544   |
|    n_updates        | 202467   |
----------------------------------
Eval num_timesteps=910000, episode_reward=6.66 +/- 9.42
Episode length: 138.32 +/- 83.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 6.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 910000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0677   |
|    n_updates        | 202499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 4.3      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8148     |
|    fps              | 34       |
|    time_elapsed     | 26126    |
|    total_timesteps  | 910452   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0104   |
|    n_updates        | 202612   |
----------------------------------
Eval num_timesteps=910500, episode_reward=6.54 +/- 7.74
Episode length: 147.46 +/- 76.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 6.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 910500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 202624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.28     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8152     |
|    fps              | 34       |
|    time_elapsed     | 26143    |
|    total_timesteps  | 910817   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0264   |
|    n_updates        | 202704   |
----------------------------------
Eval num_timesteps=911000, episode_reward=5.40 +/- 7.38
Episode length: 144.92 +/- 85.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 5.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 911000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 202749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 4.44     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8156     |
|    fps              | 34       |
|    time_elapsed     | 26161    |
|    total_timesteps  | 911359   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0669   |
|    n_updates        | 202839   |
----------------------------------
Eval num_timesteps=911500, episode_reward=6.94 +/- 7.62
Episode length: 141.30 +/- 66.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 6.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 911500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0504   |
|    n_updates        | 202874   |
----------------------------------
Eval num_timesteps=912000, episode_reward=7.80 +/- 10.55
Episode length: 173.38 +/- 116.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 173      |
|    mean_reward      | 7.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 912000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0949   |
|    n_updates        | 202999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 4.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8160     |
|    fps              | 34       |
|    time_elapsed     | 26197    |
|    total_timesteps  | 912218   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0537   |
|    n_updates        | 203054   |
----------------------------------
Eval num_timesteps=912500, episode_reward=6.40 +/- 9.09
Episode length: 158.54 +/- 95.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 6.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 912500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.137    |
|    n_updates        | 203124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 4.82     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8164     |
|    fps              | 34       |
|    time_elapsed     | 26216    |
|    total_timesteps  | 912700   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 203174   |
----------------------------------
Eval num_timesteps=913000, episode_reward=8.76 +/- 12.59
Episode length: 166.04 +/- 106.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 8.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 913000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0818   |
|    n_updates        | 203249   |
----------------------------------
Eval num_timesteps=913500, episode_reward=5.82 +/- 8.42
Episode length: 150.32 +/- 90.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 5.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 913500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0755   |
|    n_updates        | 203374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 4.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8168     |
|    fps              | 34       |
|    time_elapsed     | 26253    |
|    total_timesteps  | 913540   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.052    |
|    n_updates        | 203384   |
----------------------------------
Eval num_timesteps=914000, episode_reward=4.86 +/- 9.16
Episode length: 120.86 +/- 64.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 914000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 203499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 5.29     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8172     |
|    fps              | 34       |
|    time_elapsed     | 26269    |
|    total_timesteps  | 914340   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0576   |
|    n_updates        | 203584   |
----------------------------------
Eval num_timesteps=914500, episode_reward=5.74 +/- 7.03
Episode length: 151.08 +/- 94.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 5.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 914500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0447   |
|    n_updates        | 203624   |
----------------------------------
Eval num_timesteps=915000, episode_reward=5.22 +/- 6.16
Episode length: 149.12 +/- 99.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 5.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 915000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0655   |
|    n_updates        | 203749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 5.62     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8176     |
|    fps              | 34       |
|    time_elapsed     | 26306    |
|    total_timesteps  | 915066   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 203766   |
----------------------------------
Eval num_timesteps=915500, episode_reward=5.32 +/- 8.30
Episode length: 137.44 +/- 84.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 5.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 915500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 203874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 5.57     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8180     |
|    fps              | 34       |
|    time_elapsed     | 26327    |
|    total_timesteps  | 915732   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 203932   |
----------------------------------
Eval num_timesteps=916000, episode_reward=6.02 +/- 9.74
Episode length: 128.42 +/- 73.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 6.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 916000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 203999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 5.6      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8184     |
|    fps              | 34       |
|    time_elapsed     | 26343    |
|    total_timesteps  | 916230   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 204057   |
----------------------------------
Eval num_timesteps=916500, episode_reward=4.68 +/- 5.03
Episode length: 118.36 +/- 52.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 916500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.095    |
|    n_updates        | 204124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 5.64     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8188     |
|    fps              | 34       |
|    time_elapsed     | 26358    |
|    total_timesteps  | 916692   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0879   |
|    n_updates        | 204172   |
----------------------------------
Eval num_timesteps=917000, episode_reward=6.88 +/- 8.16
Episode length: 132.10 +/- 62.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 6.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 917000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 204249   |
----------------------------------
Eval num_timesteps=917500, episode_reward=6.44 +/- 9.15
Episode length: 147.36 +/- 64.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 6.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 917500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 204374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 151      |
|    ep_rew_mean      | 5.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8192     |
|    fps              | 34       |
|    time_elapsed     | 26390    |
|    total_timesteps  | 917540   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0632   |
|    n_updates        | 204384   |
----------------------------------
Eval num_timesteps=918000, episode_reward=4.58 +/- 5.91
Episode length: 144.08 +/- 61.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 918000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0839   |
|    n_updates        | 204499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 6.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8196     |
|    fps              | 34       |
|    time_elapsed     | 26408    |
|    total_timesteps  | 918152   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 204537   |
----------------------------------
Eval num_timesteps=918500, episode_reward=4.68 +/- 6.68
Episode length: 131.58 +/- 61.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 918500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0639   |
|    n_updates        | 204624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 153      |
|    ep_rew_mean      | 6.37     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8200     |
|    fps              | 34       |
|    time_elapsed     | 26424    |
|    total_timesteps  | 918883   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 204720   |
----------------------------------
Eval num_timesteps=919000, episode_reward=4.40 +/- 5.66
Episode length: 130.40 +/- 73.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 919000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 204749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 153      |
|    ep_rew_mean      | 6.44     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8204     |
|    fps              | 34       |
|    time_elapsed     | 26441    |
|    total_timesteps  | 919455   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 204863   |
----------------------------------
Eval num_timesteps=919500, episode_reward=4.92 +/- 7.21
Episode length: 140.66 +/- 57.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 4.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 919500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0787   |
|    n_updates        | 204874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 153      |
|    ep_rew_mean      | 6.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8208     |
|    fps              | 34       |
|    time_elapsed     | 26457    |
|    total_timesteps  | 919839   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 204959   |
----------------------------------
Eval num_timesteps=920000, episode_reward=5.12 +/- 6.86
Episode length: 146.26 +/- 83.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 5.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 920000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 204999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 151      |
|    ep_rew_mean      | 6.35     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8212     |
|    fps              | 34       |
|    time_elapsed     | 26474    |
|    total_timesteps  | 920248   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0613   |
|    n_updates        | 205061   |
----------------------------------
Eval num_timesteps=920500, episode_reward=4.48 +/- 7.66
Episode length: 122.46 +/- 62.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 920500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0747   |
|    n_updates        | 205124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 6.29     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8216     |
|    fps              | 34       |
|    time_elapsed     | 26489    |
|    total_timesteps  | 920814   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.119    |
|    n_updates        | 205203   |
----------------------------------
Eval num_timesteps=921000, episode_reward=5.00 +/- 7.84
Episode length: 126.32 +/- 62.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 921000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0405   |
|    n_updates        | 205249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 6.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8220     |
|    fps              | 34       |
|    time_elapsed     | 26505    |
|    total_timesteps  | 921279   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0341   |
|    n_updates        | 205319   |
----------------------------------
Eval num_timesteps=921500, episode_reward=3.32 +/- 4.34
Episode length: 114.58 +/- 53.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 921500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0914   |
|    n_updates        | 205374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 153      |
|    ep_rew_mean      | 6.47     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8224     |
|    fps              | 34       |
|    time_elapsed     | 26519    |
|    total_timesteps  | 921872   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 205467   |
----------------------------------
Eval num_timesteps=922000, episode_reward=6.22 +/- 10.38
Episode length: 139.92 +/- 97.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 6.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 922000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0493   |
|    n_updates        | 205499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 6.61     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8228     |
|    fps              | 34       |
|    time_elapsed     | 26536    |
|    total_timesteps  | 922441   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0643   |
|    n_updates        | 205610   |
----------------------------------
Eval num_timesteps=922500, episode_reward=5.86 +/- 7.42
Episode length: 126.80 +/- 65.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 5.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 922500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.191    |
|    n_updates        | 205624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 149      |
|    ep_rew_mean      | 6.58     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8232     |
|    fps              | 34       |
|    time_elapsed     | 26552    |
|    total_timesteps  | 922852   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 205712   |
----------------------------------
Eval num_timesteps=923000, episode_reward=8.04 +/- 11.65
Episode length: 132.92 +/- 64.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 8.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 923000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0468   |
|    n_updates        | 205749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 5.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8236     |
|    fps              | 34       |
|    time_elapsed     | 26568    |
|    total_timesteps  | 923184   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0515   |
|    n_updates        | 205795   |
----------------------------------
Eval num_timesteps=923500, episode_reward=3.56 +/- 5.27
Episode length: 132.78 +/- 89.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 133      |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 923500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0465   |
|    n_updates        | 205874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 5.87     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8240     |
|    fps              | 34       |
|    time_elapsed     | 26584    |
|    total_timesteps  | 923557   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0216   |
|    n_updates        | 205889   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 5.75     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8244     |
|    fps              | 34       |
|    time_elapsed     | 26585    |
|    total_timesteps  | 923970   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0312   |
|    n_updates        | 205992   |
----------------------------------
Eval num_timesteps=924000, episode_reward=4.32 +/- 6.27
Episode length: 111.70 +/- 44.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 924000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0719   |
|    n_updates        | 205999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 5.6      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8248     |
|    fps              | 34       |
|    time_elapsed     | 26599    |
|    total_timesteps  | 924323   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0641   |
|    n_updates        | 206080   |
----------------------------------
Eval num_timesteps=924500, episode_reward=4.86 +/- 6.74
Episode length: 128.44 +/- 71.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 924500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 206124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 5.61     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8252     |
|    fps              | 34       |
|    time_elapsed     | 26614    |
|    total_timesteps  | 924657   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.113    |
|    n_updates        | 206164   |
----------------------------------
Eval num_timesteps=925000, episode_reward=5.36 +/- 5.84
Episode length: 129.38 +/- 51.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 925000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0424   |
|    n_updates        | 206249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 5.85     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8256     |
|    fps              | 34       |
|    time_elapsed     | 26630    |
|    total_timesteps  | 925218   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 206304   |
----------------------------------
Eval num_timesteps=925500, episode_reward=5.38 +/- 9.12
Episode length: 137.32 +/- 94.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 5.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 925500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0761   |
|    n_updates        | 206374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.79     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8260     |
|    fps              | 34       |
|    time_elapsed     | 26646    |
|    total_timesteps  | 925611   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 206402   |
----------------------------------
Eval num_timesteps=926000, episode_reward=2.98 +/- 3.72
Episode length: 101.66 +/- 37.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 926000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0628   |
|    n_updates        | 206499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.71     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8264     |
|    fps              | 34       |
|    time_elapsed     | 26659    |
|    total_timesteps  | 926051   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0562   |
|    n_updates        | 206512   |
----------------------------------
Eval num_timesteps=926500, episode_reward=2.88 +/- 3.52
Episode length: 107.06 +/- 35.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 926500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 206624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 5.64     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8268     |
|    fps              | 34       |
|    time_elapsed     | 26674    |
|    total_timesteps  | 926998   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00874  |
|    n_updates        | 206749   |
----------------------------------
Eval num_timesteps=927000, episode_reward=3.32 +/- 5.68
Episode length: 125.80 +/- 89.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 927000   |
----------------------------------
Eval num_timesteps=927500, episode_reward=2.94 +/- 4.15
Episode length: 109.96 +/- 53.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 927500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0861   |
|    n_updates        | 206874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8272     |
|    fps              | 34       |
|    time_elapsed     | 26702    |
|    total_timesteps  | 927677   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 206919   |
----------------------------------
Eval num_timesteps=928000, episode_reward=4.66 +/- 7.15
Episode length: 115.94 +/- 68.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.66     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 928000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 206999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 4.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8276     |
|    fps              | 34       |
|    time_elapsed     | 26717    |
|    total_timesteps  | 928342   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 207085   |
----------------------------------
Eval num_timesteps=928500, episode_reward=5.60 +/- 11.25
Episode length: 124.02 +/- 75.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 928500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 207124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.06     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8280     |
|    fps              | 34       |
|    time_elapsed     | 26732    |
|    total_timesteps  | 928809   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 207202   |
----------------------------------
Eval num_timesteps=929000, episode_reward=5.24 +/- 8.25
Episode length: 142.78 +/- 93.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 5.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 929000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0582   |
|    n_updates        | 207249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8284     |
|    fps              | 34       |
|    time_elapsed     | 26749    |
|    total_timesteps  | 929179   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 207294   |
----------------------------------
Eval num_timesteps=929500, episode_reward=6.06 +/- 7.47
Episode length: 146.46 +/- 89.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 6.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 929500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0158   |
|    n_updates        | 207374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.02     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8288     |
|    fps              | 34       |
|    time_elapsed     | 26766    |
|    total_timesteps  | 929672   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.135    |
|    n_updates        | 207417   |
----------------------------------
Eval num_timesteps=930000, episode_reward=6.10 +/- 8.95
Episode length: 125.54 +/- 72.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 6.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 930000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 207499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8292     |
|    fps              | 34       |
|    time_elapsed     | 26782    |
|    total_timesteps  | 930260   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 207564   |
----------------------------------
Eval num_timesteps=930500, episode_reward=5.08 +/- 9.23
Episode length: 155.04 +/- 112.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 155      |
|    mean_reward      | 5.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 930500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0735   |
|    n_updates        | 207624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.54     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8296     |
|    fps              | 34       |
|    time_elapsed     | 26800    |
|    total_timesteps  | 930694   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0849   |
|    n_updates        | 207673   |
----------------------------------
Eval num_timesteps=931000, episode_reward=4.98 +/- 8.13
Episode length: 134.16 +/- 83.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 4.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 931000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.143    |
|    n_updates        | 207749   |
----------------------------------
Eval num_timesteps=931500, episode_reward=2.86 +/- 5.05
Episode length: 118.12 +/- 88.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 931500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 207874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8300     |
|    fps              | 34       |
|    time_elapsed     | 26830    |
|    total_timesteps  | 931594   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0683   |
|    n_updates        | 207898   |
----------------------------------
Eval num_timesteps=932000, episode_reward=5.24 +/- 8.31
Episode length: 134.60 +/- 83.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 932000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0493   |
|    n_updates        | 207999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.33     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8304     |
|    fps              | 34       |
|    time_elapsed     | 26847    |
|    total_timesteps  | 932143   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0268   |
|    n_updates        | 208035   |
----------------------------------
Eval num_timesteps=932500, episode_reward=3.46 +/- 8.16
Episode length: 110.64 +/- 57.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 932500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0387   |
|    n_updates        | 208124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.49     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8308     |
|    fps              | 34       |
|    time_elapsed     | 26861    |
|    total_timesteps  | 932690   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 208172   |
----------------------------------
Eval num_timesteps=933000, episode_reward=3.78 +/- 4.51
Episode length: 106.26 +/- 39.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 933000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0555   |
|    n_updates        | 208249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8312     |
|    fps              | 34       |
|    time_elapsed     | 26874    |
|    total_timesteps  | 933205   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 208301   |
----------------------------------
Eval num_timesteps=933500, episode_reward=3.72 +/- 6.54
Episode length: 117.72 +/- 87.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 933500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0633   |
|    n_updates        | 208374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.45     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8316     |
|    fps              | 34       |
|    time_elapsed     | 26888    |
|    total_timesteps  | 933723   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 208430   |
----------------------------------
Eval num_timesteps=934000, episode_reward=3.72 +/- 5.37
Episode length: 121.84 +/- 69.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 3.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 934000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.111    |
|    n_updates        | 208499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8320     |
|    fps              | 34       |
|    time_elapsed     | 26903    |
|    total_timesteps  | 934102   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 208525   |
----------------------------------
Eval num_timesteps=934500, episode_reward=3.76 +/- 4.94
Episode length: 111.22 +/- 53.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 934500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0496   |
|    n_updates        | 208624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8324     |
|    fps              | 34       |
|    time_elapsed     | 26916    |
|    total_timesteps  | 934512   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 208627   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8328     |
|    fps              | 34       |
|    time_elapsed     | 26918    |
|    total_timesteps  | 934892   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 208722   |
----------------------------------
Eval num_timesteps=935000, episode_reward=4.18 +/- 8.46
Episode length: 118.72 +/- 68.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 935000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 208749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8332     |
|    fps              | 34       |
|    time_elapsed     | 26933    |
|    total_timesteps  | 935394   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.103    |
|    n_updates        | 208848   |
----------------------------------
Eval num_timesteps=935500, episode_reward=4.32 +/- 7.14
Episode length: 122.88 +/- 90.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 935500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 208874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8336     |
|    fps              | 34       |
|    time_elapsed     | 26948    |
|    total_timesteps  | 935928   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0408   |
|    n_updates        | 208981   |
----------------------------------
Eval num_timesteps=936000, episode_reward=4.12 +/- 7.06
Episode length: 113.90 +/- 56.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 936000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 208999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8340     |
|    fps              | 34       |
|    time_elapsed     | 26962    |
|    total_timesteps  | 936432   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0293   |
|    n_updates        | 209107   |
----------------------------------
Eval num_timesteps=936500, episode_reward=2.80 +/- 4.76
Episode length: 96.50 +/- 48.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 936500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 209124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8344     |
|    fps              | 34       |
|    time_elapsed     | 26974    |
|    total_timesteps  | 936719   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0434   |
|    n_updates        | 209179   |
----------------------------------
Eval num_timesteps=937000, episode_reward=2.70 +/- 4.09
Episode length: 98.52 +/- 44.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.5     |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 937000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 209249   |
----------------------------------
Eval num_timesteps=937500, episode_reward=5.74 +/- 7.09
Episode length: 128.24 +/- 60.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 937500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0651   |
|    n_updates        | 209374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 4.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8348     |
|    fps              | 34       |
|    time_elapsed     | 27001    |
|    total_timesteps  | 937538   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.144    |
|    n_updates        | 209384   |
----------------------------------
Eval num_timesteps=938000, episode_reward=2.78 +/- 4.99
Episode length: 104.24 +/- 45.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 938000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 209499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 4.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8352     |
|    fps              | 34       |
|    time_elapsed     | 27016    |
|    total_timesteps  | 938081   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.07     |
|    n_updates        | 209520   |
----------------------------------
Eval num_timesteps=938500, episode_reward=2.40 +/- 3.83
Episode length: 97.16 +/- 40.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.2     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 938500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 209624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 4.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8356     |
|    fps              | 34       |
|    time_elapsed     | 27029    |
|    total_timesteps  | 938618   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.039    |
|    n_updates        | 209654   |
----------------------------------
Eval num_timesteps=939000, episode_reward=3.96 +/- 9.12
Episode length: 108.80 +/- 71.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 939000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0925   |
|    n_updates        | 209749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 4.84     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8360     |
|    fps              | 34       |
|    time_elapsed     | 27047    |
|    total_timesteps  | 939189   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 209797   |
----------------------------------
Eval num_timesteps=939500, episode_reward=6.12 +/- 11.76
Episode length: 116.66 +/- 69.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 6.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 939500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0692   |
|    n_updates        | 209874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 4.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8364     |
|    fps              | 34       |
|    time_elapsed     | 27065    |
|    total_timesteps  | 939783   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 209945   |
----------------------------------
Eval num_timesteps=940000, episode_reward=4.20 +/- 5.20
Episode length: 121.68 +/- 81.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 940000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 209999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8368     |
|    fps              | 34       |
|    time_elapsed     | 27084    |
|    total_timesteps  | 940430   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 210107   |
----------------------------------
Eval num_timesteps=940500, episode_reward=2.90 +/- 5.75
Episode length: 124.06 +/- 94.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 940500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.047    |
|    n_updates        | 210124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8372     |
|    fps              | 34       |
|    time_elapsed     | 27104    |
|    total_timesteps  | 940820   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 210204   |
----------------------------------
Eval num_timesteps=941000, episode_reward=4.08 +/- 5.24
Episode length: 143.90 +/- 124.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 4.08     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 941000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0456   |
|    n_updates        | 210249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 5.15     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8376     |
|    fps              | 34       |
|    time_elapsed     | 27120    |
|    total_timesteps  | 941211   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0286   |
|    n_updates        | 210302   |
----------------------------------
Eval num_timesteps=941500, episode_reward=3.68 +/- 6.75
Episode length: 113.90 +/- 52.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 941500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.044    |
|    n_updates        | 210374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 5.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8380     |
|    fps              | 34       |
|    time_elapsed     | 27134    |
|    total_timesteps  | 941632   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0332   |
|    n_updates        | 210407   |
----------------------------------
Eval num_timesteps=942000, episode_reward=5.34 +/- 6.61
Episode length: 149.14 +/- 88.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 942000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0354   |
|    n_updates        | 210499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8384     |
|    fps              | 34       |
|    time_elapsed     | 27151    |
|    total_timesteps  | 942057   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0534   |
|    n_updates        | 210514   |
----------------------------------
Eval num_timesteps=942500, episode_reward=4.76 +/- 5.70
Episode length: 137.94 +/- 58.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 942500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 210624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.88     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8388     |
|    fps              | 34       |
|    time_elapsed     | 27168    |
|    total_timesteps  | 942677   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.104    |
|    n_updates        | 210669   |
----------------------------------
Eval num_timesteps=943000, episode_reward=5.40 +/- 10.79
Episode length: 135.92 +/- 95.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 5.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 943000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.11     |
|    n_updates        | 210749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8392     |
|    fps              | 34       |
|    time_elapsed     | 27185    |
|    total_timesteps  | 943096   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 210773   |
----------------------------------
Eval num_timesteps=943500, episode_reward=5.56 +/- 8.41
Episode length: 118.08 +/- 54.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 5.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 943500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 210874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.83     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8396     |
|    fps              | 34       |
|    time_elapsed     | 27199    |
|    total_timesteps  | 943581   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 210895   |
----------------------------------
Eval num_timesteps=944000, episode_reward=4.86 +/- 7.73
Episode length: 119.96 +/- 68.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 944000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 210999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.81     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8400     |
|    fps              | 34       |
|    time_elapsed     | 27214    |
|    total_timesteps  | 944043   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0481   |
|    n_updates        | 211010   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8404     |
|    fps              | 34       |
|    time_elapsed     | 27215    |
|    total_timesteps  | 944393   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 211098   |
----------------------------------
Eval num_timesteps=944500, episode_reward=4.48 +/- 6.89
Episode length: 122.80 +/- 59.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 944500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0978   |
|    n_updates        | 211124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.67     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8408     |
|    fps              | 34       |
|    time_elapsed     | 27230    |
|    total_timesteps  | 944832   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.116    |
|    n_updates        | 211207   |
----------------------------------
Eval num_timesteps=945000, episode_reward=3.94 +/- 4.98
Episode length: 111.46 +/- 41.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 945000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 211249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.53     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8412     |
|    fps              | 34       |
|    time_elapsed     | 27243    |
|    total_timesteps  | 945300   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0187   |
|    n_updates        | 211324   |
----------------------------------
Eval num_timesteps=945500, episode_reward=5.12 +/- 7.84
Episode length: 124.30 +/- 64.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 945500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0213   |
|    n_updates        | 211374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.44     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8416     |
|    fps              | 34       |
|    time_elapsed     | 27259    |
|    total_timesteps  | 945759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0431   |
|    n_updates        | 211439   |
----------------------------------
Eval num_timesteps=946000, episode_reward=5.22 +/- 7.29
Episode length: 117.96 +/- 61.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 5.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 946000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 211499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8420     |
|    fps              | 34       |
|    time_elapsed     | 27273    |
|    total_timesteps  | 946244   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.136    |
|    n_updates        | 211560   |
----------------------------------
Eval num_timesteps=946500, episode_reward=5.34 +/- 9.97
Episode length: 117.66 +/- 67.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 946500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.086    |
|    n_updates        | 211624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8424     |
|    fps              | 34       |
|    time_elapsed     | 27287    |
|    total_timesteps  | 946636   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 211658   |
----------------------------------
Eval num_timesteps=947000, episode_reward=5.42 +/- 8.27
Episode length: 111.26 +/- 57.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 5.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 947000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.163    |
|    n_updates        | 211749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8428     |
|    fps              | 34       |
|    time_elapsed     | 27301    |
|    total_timesteps  | 947113   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0897   |
|    n_updates        | 211778   |
----------------------------------
Eval num_timesteps=947500, episode_reward=5.24 +/- 6.16
Episode length: 132.06 +/- 61.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 5.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 947500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0419   |
|    n_updates        | 211874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 4.9      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8432     |
|    fps              | 34       |
|    time_elapsed     | 27317    |
|    total_timesteps  | 947667   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 211916   |
----------------------------------
Eval num_timesteps=948000, episode_reward=3.86 +/- 4.43
Episode length: 113.58 +/- 47.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 948000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 211999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8436     |
|    fps              | 34       |
|    time_elapsed     | 27332    |
|    total_timesteps  | 948158   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0608   |
|    n_updates        | 212039   |
----------------------------------
Eval num_timesteps=948500, episode_reward=3.00 +/- 5.07
Episode length: 102.72 +/- 39.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 948500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0506   |
|    n_updates        | 212124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 5.24     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8440     |
|    fps              | 34       |
|    time_elapsed     | 27345    |
|    total_timesteps  | 948831   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.102    |
|    n_updates        | 212207   |
----------------------------------
Eval num_timesteps=949000, episode_reward=5.82 +/- 8.56
Episode length: 128.72 +/- 68.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 949000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.106    |
|    n_updates        | 212249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 5.37     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8444     |
|    fps              | 34       |
|    time_elapsed     | 27361    |
|    total_timesteps  | 949246   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0272   |
|    n_updates        | 212311   |
----------------------------------
Eval num_timesteps=949500, episode_reward=6.04 +/- 7.23
Episode length: 129.82 +/- 59.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 6.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 949500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.163    |
|    n_updates        | 212374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.85     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8448     |
|    fps              | 34       |
|    time_elapsed     | 27379    |
|    total_timesteps  | 949686   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0619   |
|    n_updates        | 212421   |
----------------------------------
Eval num_timesteps=950000, episode_reward=4.22 +/- 7.12
Episode length: 117.84 +/- 71.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 950000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0713   |
|    n_updates        | 212499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.71     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8452     |
|    fps              | 34       |
|    time_elapsed     | 27399    |
|    total_timesteps  | 950067   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0413   |
|    n_updates        | 212516   |
----------------------------------
Eval num_timesteps=950500, episode_reward=4.40 +/- 5.89
Episode length: 119.82 +/- 57.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 950500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0725   |
|    n_updates        | 212624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.65     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8456     |
|    fps              | 34       |
|    time_elapsed     | 27420    |
|    total_timesteps  | 950658   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0496   |
|    n_updates        | 212664   |
----------------------------------
Eval num_timesteps=951000, episode_reward=3.88 +/- 6.78
Episode length: 123.60 +/- 95.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 951000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0368   |
|    n_updates        | 212749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8460     |
|    fps              | 34       |
|    time_elapsed     | 27438    |
|    total_timesteps  | 951224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0526   |
|    n_updates        | 212805   |
----------------------------------
Eval num_timesteps=951500, episode_reward=4.02 +/- 4.88
Episode length: 106.24 +/- 40.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 951500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 212874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8464     |
|    fps              | 34       |
|    time_elapsed     | 27453    |
|    total_timesteps  | 951715   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0832   |
|    n_updates        | 212928   |
----------------------------------
Eval num_timesteps=952000, episode_reward=4.70 +/- 9.65
Episode length: 129.42 +/- 73.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 4.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 952000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 212999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8468     |
|    fps              | 34       |
|    time_elapsed     | 27468    |
|    total_timesteps  | 952151   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0295   |
|    n_updates        | 213037   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8472     |
|    fps              | 34       |
|    time_elapsed     | 27470    |
|    total_timesteps  | 952475   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 213118   |
----------------------------------
Eval num_timesteps=952500, episode_reward=5.52 +/- 7.68
Episode length: 130.82 +/- 59.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 5.52     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 952500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0474   |
|    n_updates        | 213124   |
----------------------------------
Eval num_timesteps=953000, episode_reward=4.32 +/- 6.46
Episode length: 121.72 +/- 56.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 953000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 213249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.83     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8476     |
|    fps              | 34       |
|    time_elapsed     | 27505    |
|    total_timesteps  | 953061   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 213265   |
----------------------------------
Eval num_timesteps=953500, episode_reward=4.50 +/- 7.20
Episode length: 117.56 +/- 61.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 953500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.032    |
|    n_updates        | 213374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.9      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8480     |
|    fps              | 34       |
|    time_elapsed     | 27520    |
|    total_timesteps  | 953503   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0642   |
|    n_updates        | 213375   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8484     |
|    fps              | 34       |
|    time_elapsed     | 27521    |
|    total_timesteps  | 953836   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.117    |
|    n_updates        | 213458   |
----------------------------------
Eval num_timesteps=954000, episode_reward=5.58 +/- 9.03
Episode length: 152.02 +/- 106.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 5.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 954000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0629   |
|    n_updates        | 213499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.74     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8488     |
|    fps              | 34       |
|    time_elapsed     | 27539    |
|    total_timesteps  | 954305   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0469   |
|    n_updates        | 213576   |
----------------------------------
Eval num_timesteps=954500, episode_reward=3.24 +/- 5.88
Episode length: 102.52 +/- 42.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 954500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0519   |
|    n_updates        | 213624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.86     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8492     |
|    fps              | 34       |
|    time_elapsed     | 27553    |
|    total_timesteps  | 954776   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 213693   |
----------------------------------
Eval num_timesteps=955000, episode_reward=2.30 +/- 3.16
Episode length: 111.40 +/- 41.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 955000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0836   |
|    n_updates        | 213749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8496     |
|    fps              | 34       |
|    time_elapsed     | 27567    |
|    total_timesteps  | 955312   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 213827   |
----------------------------------
Eval num_timesteps=955500, episode_reward=5.14 +/- 8.58
Episode length: 115.48 +/- 58.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 955500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 213874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.02     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8500     |
|    fps              | 34       |
|    time_elapsed     | 27581    |
|    total_timesteps  | 955764   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0445   |
|    n_updates        | 213940   |
----------------------------------
Eval num_timesteps=956000, episode_reward=4.22 +/- 8.80
Episode length: 115.92 +/- 57.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 956000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0436   |
|    n_updates        | 213999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.08     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8504     |
|    fps              | 34       |
|    time_elapsed     | 27595    |
|    total_timesteps  | 956096   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00552  |
|    n_updates        | 214023   |
----------------------------------
Eval num_timesteps=956500, episode_reward=5.46 +/- 9.50
Episode length: 123.70 +/- 70.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 956500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0756   |
|    n_updates        | 214124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.99     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8508     |
|    fps              | 34       |
|    time_elapsed     | 27610    |
|    total_timesteps  | 956613   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0875   |
|    n_updates        | 214153   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8512     |
|    fps              | 34       |
|    time_elapsed     | 27612    |
|    total_timesteps  | 956958   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 214239   |
----------------------------------
Eval num_timesteps=957000, episode_reward=4.46 +/- 8.55
Episode length: 120.24 +/- 60.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.46     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 957000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 214249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8516     |
|    fps              | 34       |
|    time_elapsed     | 27627    |
|    total_timesteps  | 957351   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0548   |
|    n_updates        | 214337   |
----------------------------------
Eval num_timesteps=957500, episode_reward=6.16 +/- 7.85
Episode length: 127.12 +/- 70.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 957500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0313   |
|    n_updates        | 214374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.8      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8520     |
|    fps              | 34       |
|    time_elapsed     | 27643    |
|    total_timesteps  | 957792   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0664   |
|    n_updates        | 214447   |
----------------------------------
Eval num_timesteps=958000, episode_reward=5.80 +/- 10.06
Episode length: 113.56 +/- 62.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 5.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 958000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 214499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8524     |
|    fps              | 34       |
|    time_elapsed     | 27656    |
|    total_timesteps  | 958165   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.105    |
|    n_updates        | 214541   |
----------------------------------
Eval num_timesteps=958500, episode_reward=5.48 +/- 7.26
Episode length: 130.80 +/- 68.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 958500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 214624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8528     |
|    fps              | 34       |
|    time_elapsed     | 27673    |
|    total_timesteps  | 958920   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 214729   |
----------------------------------
Eval num_timesteps=959000, episode_reward=3.20 +/- 7.43
Episode length: 115.16 +/- 92.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 959000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 214749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8532     |
|    fps              | 34       |
|    time_elapsed     | 27687    |
|    total_timesteps  | 959357   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0354   |
|    n_updates        | 214839   |
----------------------------------
Eval num_timesteps=959500, episode_reward=4.56 +/- 9.64
Episode length: 127.54 +/- 95.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 4.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 959500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0776   |
|    n_updates        | 214874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8536     |
|    fps              | 34       |
|    time_elapsed     | 27703    |
|    total_timesteps  | 959834   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0515   |
|    n_updates        | 214958   |
----------------------------------
Eval num_timesteps=960000, episode_reward=4.62 +/- 6.13
Episode length: 109.94 +/- 56.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 4.62     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 960000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0795   |
|    n_updates        | 214999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.13     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8540     |
|    fps              | 34       |
|    time_elapsed     | 27718    |
|    total_timesteps  | 960439   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0419   |
|    n_updates        | 215109   |
----------------------------------
Eval num_timesteps=960500, episode_reward=3.44 +/- 5.70
Episode length: 90.54 +/- 44.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.5     |
|    mean_reward      | 3.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 960500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 215124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.02     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8544     |
|    fps              | 34       |
|    time_elapsed     | 27729    |
|    total_timesteps  | 960840   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0485   |
|    n_updates        | 215209   |
----------------------------------
Eval num_timesteps=961000, episode_reward=3.94 +/- 6.65
Episode length: 104.72 +/- 43.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 961000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0552   |
|    n_updates        | 215249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.32     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8548     |
|    fps              | 34       |
|    time_elapsed     | 27743    |
|    total_timesteps  | 961437   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 215359   |
----------------------------------
Eval num_timesteps=961500, episode_reward=4.42 +/- 8.89
Episode length: 117.80 +/- 63.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 961500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0682   |
|    n_updates        | 215374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.54     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8552     |
|    fps              | 34       |
|    time_elapsed     | 27758    |
|    total_timesteps  | 961981   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0437   |
|    n_updates        | 215495   |
----------------------------------
Eval num_timesteps=962000, episode_reward=6.06 +/- 9.63
Episode length: 115.14 +/- 68.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 6.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 962000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0822   |
|    n_updates        | 215499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.35     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8556     |
|    fps              | 34       |
|    time_elapsed     | 27772    |
|    total_timesteps  | 962302   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.069    |
|    n_updates        | 215575   |
----------------------------------
Eval num_timesteps=962500, episode_reward=4.10 +/- 5.54
Episode length: 111.74 +/- 56.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 962500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 215624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.43     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8560     |
|    fps              | 34       |
|    time_elapsed     | 27786    |
|    total_timesteps  | 962858   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0513   |
|    n_updates        | 215714   |
----------------------------------
Eval num_timesteps=963000, episode_reward=3.64 +/- 6.10
Episode length: 106.14 +/- 55.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 963000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0683   |
|    n_updates        | 215749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8564     |
|    fps              | 34       |
|    time_elapsed     | 27799    |
|    total_timesteps  | 963207   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0683   |
|    n_updates        | 215801   |
----------------------------------
Eval num_timesteps=963500, episode_reward=5.00 +/- 6.09
Episode length: 126.08 +/- 50.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5        |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 963500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0474   |
|    n_updates        | 215874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8568     |
|    fps              | 34       |
|    time_elapsed     | 27814    |
|    total_timesteps  | 963675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 215918   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.36     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8572     |
|    fps              | 34       |
|    time_elapsed     | 27816    |
|    total_timesteps  | 963978   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0111   |
|    n_updates        | 215994   |
----------------------------------
Eval num_timesteps=964000, episode_reward=6.12 +/- 12.28
Episode length: 124.78 +/- 74.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 6.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 964000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0313   |
|    n_updates        | 215999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.16     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8576     |
|    fps              | 34       |
|    time_elapsed     | 27831    |
|    total_timesteps  | 964352   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0489   |
|    n_updates        | 216087   |
----------------------------------
Eval num_timesteps=964500, episode_reward=5.18 +/- 14.77
Episode length: 105.08 +/- 69.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 5.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 964500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 216124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.26     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8580     |
|    fps              | 34       |
|    time_elapsed     | 27845    |
|    total_timesteps  | 964947   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00561  |
|    n_updates        | 216236   |
----------------------------------
Eval num_timesteps=965000, episode_reward=2.82 +/- 4.59
Episode length: 106.36 +/- 47.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 965000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0695   |
|    n_updates        | 216249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.21     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8584     |
|    fps              | 34       |
|    time_elapsed     | 27858    |
|    total_timesteps  | 965345   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0452   |
|    n_updates        | 216336   |
----------------------------------
Eval num_timesteps=965500, episode_reward=3.78 +/- 5.64
Episode length: 107.28 +/- 49.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 965500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0734   |
|    n_updates        | 216374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8588     |
|    fps              | 34       |
|    time_elapsed     | 27872    |
|    total_timesteps  | 965730   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0626   |
|    n_updates        | 216432   |
----------------------------------
Eval num_timesteps=966000, episode_reward=2.64 +/- 4.85
Episode length: 97.20 +/- 40.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.2     |
|    mean_reward      | 2.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 966000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 216499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.31     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8592     |
|    fps              | 34       |
|    time_elapsed     | 27884    |
|    total_timesteps  | 966247   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 216561   |
----------------------------------
Eval num_timesteps=966500, episode_reward=4.42 +/- 6.24
Episode length: 109.44 +/- 51.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 966500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 216624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.3      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8596     |
|    fps              | 34       |
|    time_elapsed     | 27898    |
|    total_timesteps  | 966683   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0811   |
|    n_updates        | 216670   |
----------------------------------
Eval num_timesteps=967000, episode_reward=3.12 +/- 5.18
Episode length: 108.36 +/- 51.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 967000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00865  |
|    n_updates        | 216749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8600     |
|    fps              | 34       |
|    time_elapsed     | 27917    |
|    total_timesteps  | 967153   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0977   |
|    n_updates        | 216788   |
----------------------------------
Eval num_timesteps=967500, episode_reward=2.98 +/- 3.32
Episode length: 95.54 +/- 34.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.5     |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 967500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 216874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.4      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8604     |
|    fps              | 34       |
|    time_elapsed     | 27930    |
|    total_timesteps  | 967637   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0472   |
|    n_updates        | 216909   |
----------------------------------
Eval num_timesteps=968000, episode_reward=6.26 +/- 8.20
Episode length: 119.96 +/- 55.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 6.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 968000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.036    |
|    n_updates        | 216999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8608     |
|    fps              | 34       |
|    time_elapsed     | 27945    |
|    total_timesteps  | 968033   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0476   |
|    n_updates        | 217008   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.54     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8612     |
|    fps              | 34       |
|    time_elapsed     | 27948    |
|    total_timesteps  | 968460   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0357   |
|    n_updates        | 217114   |
----------------------------------
Eval num_timesteps=968500, episode_reward=4.26 +/- 5.58
Episode length: 116.14 +/- 51.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 968500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 217124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.59     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8616     |
|    fps              | 34       |
|    time_elapsed     | 27962    |
|    total_timesteps  | 968814   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.071    |
|    n_updates        | 217203   |
----------------------------------
Eval num_timesteps=969000, episode_reward=4.48 +/- 7.00
Episode length: 105.44 +/- 49.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 969000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0777   |
|    n_updates        | 217249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.62     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8620     |
|    fps              | 34       |
|    time_elapsed     | 27975    |
|    total_timesteps  | 969194   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 217298   |
----------------------------------
Eval num_timesteps=969500, episode_reward=4.88 +/- 8.23
Episode length: 105.74 +/- 51.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 969500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 217374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.63     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8624     |
|    fps              | 34       |
|    time_elapsed     | 27990    |
|    total_timesteps  | 969618   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00763  |
|    n_updates        | 217404   |
----------------------------------
Eval num_timesteps=970000, episode_reward=4.40 +/- 7.84
Episode length: 106.40 +/- 52.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 970000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 217499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.7      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8628     |
|    fps              | 34       |
|    time_elapsed     | 28005    |
|    total_timesteps  | 970036   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 217508   |
----------------------------------
Eval num_timesteps=970500, episode_reward=8.28 +/- 11.16
Episode length: 146.16 +/- 73.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 8.28     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 970500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 217624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.89     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8632     |
|    fps              | 34       |
|    time_elapsed     | 28029    |
|    total_timesteps  | 970508   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.078    |
|    n_updates        | 217626   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.77     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8636     |
|    fps              | 34       |
|    time_elapsed     | 28031    |
|    total_timesteps  | 970868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0327   |
|    n_updates        | 217716   |
----------------------------------
Eval num_timesteps=971000, episode_reward=5.56 +/- 9.43
Episode length: 119.50 +/- 67.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 5.56     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 971000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0816   |
|    n_updates        | 217749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.65     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8640     |
|    fps              | 34       |
|    time_elapsed     | 28048    |
|    total_timesteps  | 971264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0614   |
|    n_updates        | 217815   |
----------------------------------
Eval num_timesteps=971500, episode_reward=5.18 +/- 7.24
Episode length: 122.40 +/- 58.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 5.18     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 971500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0343   |
|    n_updates        | 217874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8644     |
|    fps              | 34       |
|    time_elapsed     | 28064    |
|    total_timesteps  | 971853   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0347   |
|    n_updates        | 217963   |
----------------------------------
Eval num_timesteps=972000, episode_reward=3.64 +/- 5.79
Episode length: 104.40 +/- 50.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 972000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0705   |
|    n_updates        | 217999   |
----------------------------------
Eval num_timesteps=972500, episode_reward=5.32 +/- 7.91
Episode length: 139.00 +/- 93.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 5.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 972500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0344   |
|    n_updates        | 218124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8648     |
|    fps              | 34       |
|    time_elapsed     | 28100    |
|    total_timesteps  | 972682   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0734   |
|    n_updates        | 218170   |
----------------------------------
Eval num_timesteps=973000, episode_reward=5.24 +/- 6.78
Episode length: 160.46 +/- 116.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 160      |
|    mean_reward      | 5.24     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 973000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0703   |
|    n_updates        | 218249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.84     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8652     |
|    fps              | 34       |
|    time_elapsed     | 28126    |
|    total_timesteps  | 973234   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0679   |
|    n_updates        | 218308   |
----------------------------------
Eval num_timesteps=973500, episode_reward=4.64 +/- 8.30
Episode length: 124.88 +/- 62.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 973500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0864   |
|    n_updates        | 218374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8656     |
|    fps              | 34       |
|    time_elapsed     | 28145    |
|    total_timesteps  | 973695   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0595   |
|    n_updates        | 218423   |
----------------------------------
Eval num_timesteps=974000, episode_reward=4.76 +/- 6.22
Episode length: 134.04 +/- 97.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 974000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.07     |
|    n_updates        | 218499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8660     |
|    fps              | 34       |
|    time_elapsed     | 28166    |
|    total_timesteps  | 974112   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 218527   |
----------------------------------
Eval num_timesteps=974500, episode_reward=4.40 +/- 5.70
Episode length: 119.70 +/- 46.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.4      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 974500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0548   |
|    n_updates        | 218624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8664     |
|    fps              | 34       |
|    time_elapsed     | 28183    |
|    total_timesteps  | 974625   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 218656   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.19     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8668     |
|    fps              | 34       |
|    time_elapsed     | 28185    |
|    total_timesteps  | 974988   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 218746   |
----------------------------------
Eval num_timesteps=975000, episode_reward=4.22 +/- 5.56
Episode length: 136.20 +/- 86.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 4.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 975000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.056    |
|    n_updates        | 218749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.46     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8672     |
|    fps              | 34       |
|    time_elapsed     | 28202    |
|    total_timesteps  | 975481   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 218870   |
----------------------------------
Eval num_timesteps=975500, episode_reward=4.34 +/- 7.03
Episode length: 157.90 +/- 133.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 158      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 975500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0418   |
|    n_updates        | 218874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 4.39     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8676     |
|    fps              | 34       |
|    time_elapsed     | 28220    |
|    total_timesteps  | 975886   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0484   |
|    n_updates        | 218971   |
----------------------------------
Eval num_timesteps=976000, episode_reward=4.90 +/- 8.17
Episode length: 113.46 +/- 55.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 4.9      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 976000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0636   |
|    n_updates        | 218999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 4.22     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8680     |
|    fps              | 34       |
|    time_elapsed     | 28239    |
|    total_timesteps  | 976256   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 219063   |
----------------------------------
Eval num_timesteps=976500, episode_reward=3.80 +/- 4.52
Episode length: 106.90 +/- 51.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 976500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 219124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 4.2      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8684     |
|    fps              | 34       |
|    time_elapsed     | 28252    |
|    total_timesteps  | 976586   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00811  |
|    n_updates        | 219146   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8688     |
|    fps              | 34       |
|    time_elapsed     | 28253    |
|    total_timesteps  | 976845   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 219211   |
----------------------------------
Eval num_timesteps=977000, episode_reward=5.50 +/- 8.27
Episode length: 121.06 +/- 57.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 5.5      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 977000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0752   |
|    n_updates        | 219249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8692     |
|    fps              | 34       |
|    time_elapsed     | 28268    |
|    total_timesteps  | 977303   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0369   |
|    n_updates        | 219325   |
----------------------------------
Eval num_timesteps=977500, episode_reward=4.60 +/- 6.86
Episode length: 122.70 +/- 59.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 977500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.019    |
|    n_updates        | 219374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8696     |
|    fps              | 34       |
|    time_elapsed     | 28283    |
|    total_timesteps  | 977828   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 219456   |
----------------------------------
Eval num_timesteps=978000, episode_reward=5.60 +/- 7.19
Episode length: 125.44 +/- 66.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 5.6      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 978000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0603   |
|    n_updates        | 219499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8700     |
|    fps              | 34       |
|    time_elapsed     | 28298    |
|    total_timesteps  | 978175   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 219543   |
----------------------------------
Eval num_timesteps=978500, episode_reward=5.80 +/- 9.85
Episode length: 142.98 +/- 93.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 5.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 978500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 219624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8704     |
|    fps              | 34       |
|    time_elapsed     | 28314    |
|    total_timesteps  | 978502   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0632   |
|    n_updates        | 219625   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8708     |
|    fps              | 34       |
|    time_elapsed     | 28316    |
|    total_timesteps  | 978934   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0713   |
|    n_updates        | 219733   |
----------------------------------
Eval num_timesteps=979000, episode_reward=4.20 +/- 5.94
Episode length: 117.80 +/- 59.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 979000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 219749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8712     |
|    fps              | 34       |
|    time_elapsed     | 28330    |
|    total_timesteps  | 979272   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 219817   |
----------------------------------
Eval num_timesteps=979500, episode_reward=4.94 +/- 7.28
Episode length: 118.80 +/- 58.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 979500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0693   |
|    n_updates        | 219874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.94     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8716     |
|    fps              | 34       |
|    time_elapsed     | 28345    |
|    total_timesteps  | 979729   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0555   |
|    n_updates        | 219932   |
----------------------------------
Eval num_timesteps=980000, episode_reward=2.70 +/- 2.83
Episode length: 112.62 +/- 38.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 980000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00956  |
|    n_updates        | 219999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8720     |
|    fps              | 34       |
|    time_elapsed     | 28360    |
|    total_timesteps  | 980209   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 220052   |
----------------------------------
Eval num_timesteps=980500, episode_reward=4.72 +/- 5.41
Episode length: 130.00 +/- 73.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 980500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0782   |
|    n_updates        | 220124   |
----------------------------------
Eval num_timesteps=981000, episode_reward=4.04 +/- 6.02
Episode length: 130.64 +/- 78.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 981000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0567   |
|    n_updates        | 220249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 4.05     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8724     |
|    fps              | 34       |
|    time_elapsed     | 28392    |
|    total_timesteps  | 981000   |
----------------------------------
Eval num_timesteps=981500, episode_reward=4.68 +/- 8.09
Episode length: 122.60 +/- 53.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 981500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 220374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.2      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8728     |
|    fps              | 34       |
|    time_elapsed     | 28408    |
|    total_timesteps  | 981613   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0502   |
|    n_updates        | 220403   |
----------------------------------
Eval num_timesteps=982000, episode_reward=3.78 +/- 5.91
Episode length: 117.96 +/- 44.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 982000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0995   |
|    n_updates        | 220499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 4.01     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8732     |
|    fps              | 34       |
|    time_elapsed     | 28423    |
|    total_timesteps  | 982132   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.108    |
|    n_updates        | 220532   |
----------------------------------
Eval num_timesteps=982500, episode_reward=6.42 +/- 8.76
Episode length: 130.14 +/- 68.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 6.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 982500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0542   |
|    n_updates        | 220624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8736     |
|    fps              | 34       |
|    time_elapsed     | 28443    |
|    total_timesteps  | 982622   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0416   |
|    n_updates        | 220655   |
----------------------------------
Eval num_timesteps=983000, episode_reward=7.36 +/- 12.18
Episode length: 125.60 +/- 89.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 7.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 983000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.114    |
|    n_updates        | 220749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.23     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8740     |
|    fps              | 34       |
|    time_elapsed     | 28465    |
|    total_timesteps  | 983263   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 220815   |
----------------------------------
Eval num_timesteps=983500, episode_reward=4.36 +/- 7.61
Episode length: 117.66 +/- 58.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.36     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 983500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0262   |
|    n_updates        | 220874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.25     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8744     |
|    fps              | 34       |
|    time_elapsed     | 28485    |
|    total_timesteps  | 983908   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0601   |
|    n_updates        | 220976   |
----------------------------------
Eval num_timesteps=984000, episode_reward=4.48 +/- 7.50
Episode length: 124.24 +/- 78.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 984000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0368   |
|    n_updates        | 220999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8748     |
|    fps              | 34       |
|    time_elapsed     | 28501    |
|    total_timesteps  | 984347   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0216   |
|    n_updates        | 221086   |
----------------------------------
Eval num_timesteps=984500, episode_reward=4.76 +/- 7.60
Episode length: 124.66 +/- 49.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.76     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 984500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0395   |
|    n_updates        | 221124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8752     |
|    fps              | 34       |
|    time_elapsed     | 28516    |
|    total_timesteps  | 984716   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 221178   |
----------------------------------
Eval num_timesteps=985000, episode_reward=3.38 +/- 4.03
Episode length: 115.34 +/- 36.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 985000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 221249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 3.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8756     |
|    fps              | 34       |
|    time_elapsed     | 28535    |
|    total_timesteps  | 985326   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0243   |
|    n_updates        | 221331   |
----------------------------------
Eval num_timesteps=985500, episode_reward=4.88 +/- 10.17
Episode length: 150.90 +/- 111.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 4.88     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 985500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 221374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.81     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8760     |
|    fps              | 34       |
|    time_elapsed     | 28556    |
|    total_timesteps  | 985988   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0639   |
|    n_updates        | 221496   |
----------------------------------
Eval num_timesteps=986000, episode_reward=3.86 +/- 6.44
Episode length: 136.94 +/- 59.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 986000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0316   |
|    n_updates        | 221499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 3.53     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8764     |
|    fps              | 34       |
|    time_elapsed     | 28578    |
|    total_timesteps  | 986320   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0416   |
|    n_updates        | 221579   |
----------------------------------
Eval num_timesteps=986500, episode_reward=3.74 +/- 5.85
Episode length: 116.90 +/- 75.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 986500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0495   |
|    n_updates        | 221624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8768     |
|    fps              | 34       |
|    time_elapsed     | 28598    |
|    total_timesteps  | 986805   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 221701   |
----------------------------------
Eval num_timesteps=987000, episode_reward=3.68 +/- 6.39
Episode length: 126.02 +/- 88.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 987000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 221749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.69     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8772     |
|    fps              | 34       |
|    time_elapsed     | 28616    |
|    total_timesteps  | 987390   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0636   |
|    n_updates        | 221847   |
----------------------------------
Eval num_timesteps=987500, episode_reward=6.94 +/- 9.48
Episode length: 141.56 +/- 88.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 6.94     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 987500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0666   |
|    n_updates        | 221874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8776     |
|    fps              | 34       |
|    time_elapsed     | 28634    |
|    total_timesteps  | 987789   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 221947   |
----------------------------------
Eval num_timesteps=988000, episode_reward=6.68 +/- 10.20
Episode length: 155.04 +/- 101.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 155      |
|    mean_reward      | 6.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 988000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0936   |
|    n_updates        | 221999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8780     |
|    fps              | 34       |
|    time_elapsed     | 28652    |
|    total_timesteps  | 988135   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.122    |
|    n_updates        | 222033   |
----------------------------------
Eval num_timesteps=988500, episode_reward=3.68 +/- 4.93
Episode length: 112.40 +/- 54.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 988500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0943   |
|    n_updates        | 222124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8784     |
|    fps              | 34       |
|    time_elapsed     | 28667    |
|    total_timesteps  | 988788   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0474   |
|    n_updates        | 222196   |
----------------------------------
Eval num_timesteps=989000, episode_reward=4.64 +/- 5.60
Episode length: 126.68 +/- 67.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 989000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0816   |
|    n_updates        | 222249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8788     |
|    fps              | 34       |
|    time_elapsed     | 28682    |
|    total_timesteps  | 989271   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0116   |
|    n_updates        | 222317   |
----------------------------------
Eval num_timesteps=989500, episode_reward=3.12 +/- 5.82
Episode length: 109.82 +/- 57.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 989500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0126   |
|    n_updates        | 222374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.07     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8792     |
|    fps              | 34       |
|    time_elapsed     | 28696    |
|    total_timesteps  | 989806   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 222451   |
----------------------------------
Eval num_timesteps=990000, episode_reward=4.68 +/- 7.72
Episode length: 132.38 +/- 61.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 990000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0506   |
|    n_updates        | 222499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 3.95     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8796     |
|    fps              | 34       |
|    time_elapsed     | 28711    |
|    total_timesteps  | 990195   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0603   |
|    n_updates        | 222548   |
----------------------------------
Eval num_timesteps=990500, episode_reward=4.10 +/- 7.16
Episode length: 107.12 +/- 62.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 990500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0576   |
|    n_updates        | 222624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 3.97     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8800     |
|    fps              | 34       |
|    time_elapsed     | 28724    |
|    total_timesteps  | 990672   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0985   |
|    n_updates        | 222667   |
----------------------------------
Eval num_timesteps=991000, episode_reward=3.32 +/- 5.95
Episode length: 105.62 +/- 51.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 991000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0909   |
|    n_updates        | 222749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.12     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8804     |
|    fps              | 34       |
|    time_elapsed     | 28738    |
|    total_timesteps  | 991257   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 222814   |
----------------------------------
Eval num_timesteps=991500, episode_reward=5.92 +/- 8.31
Episode length: 149.08 +/- 85.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 5.92     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 991500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 222874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8808     |
|    fps              | 34       |
|    time_elapsed     | 28762    |
|    total_timesteps  | 991722   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0357   |
|    n_updates        | 222930   |
----------------------------------
Eval num_timesteps=992000, episode_reward=6.10 +/- 9.01
Episode length: 137.32 +/- 71.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 6.1      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 992000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.139    |
|    n_updates        | 222999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8812     |
|    fps              | 34       |
|    time_elapsed     | 28781    |
|    total_timesteps  | 992169   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00607  |
|    n_updates        | 223042   |
----------------------------------
Eval num_timesteps=992500, episode_reward=5.22 +/- 6.59
Episode length: 130.38 +/- 69.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.22     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 992500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0338   |
|    n_updates        | 223124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8816     |
|    fps              | 34       |
|    time_elapsed     | 28797    |
|    total_timesteps  | 992592   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0255   |
|    n_updates        | 223147   |
----------------------------------
Eval num_timesteps=993000, episode_reward=5.30 +/- 7.47
Episode length: 153.78 +/- 94.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 5.3      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 993000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 223249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 4.1      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8820     |
|    fps              | 34       |
|    time_elapsed     | 28817    |
|    total_timesteps  | 993170   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 223292   |
----------------------------------
Eval num_timesteps=993500, episode_reward=5.96 +/- 7.87
Episode length: 149.22 +/- 88.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 5.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 993500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0343   |
|    n_updates        | 223374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 4.04     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8824     |
|    fps              | 34       |
|    time_elapsed     | 28834    |
|    total_timesteps  | 993561   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 223390   |
----------------------------------
Eval num_timesteps=994000, episode_reward=3.96 +/- 4.85
Episode length: 124.00 +/- 83.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 994000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 223499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 3.92     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8828     |
|    fps              | 34       |
|    time_elapsed     | 28849    |
|    total_timesteps  | 994022   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 223505   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 3.96     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8832     |
|    fps              | 34       |
|    time_elapsed     | 28850    |
|    total_timesteps  | 994462   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0537   |
|    n_updates        | 223615   |
----------------------------------
Eval num_timesteps=994500, episode_reward=7.42 +/- 10.03
Episode length: 144.38 +/- 73.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 7.42     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 994500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 223624   |
----------------------------------
Eval num_timesteps=995000, episode_reward=5.16 +/- 7.66
Episode length: 110.78 +/- 47.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 995000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00872  |
|    n_updates        | 223749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.11     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8836     |
|    fps              | 34       |
|    time_elapsed     | 28879    |
|    total_timesteps  | 995012   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 223752   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 3.98     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8840     |
|    fps              | 34       |
|    time_elapsed     | 28881    |
|    total_timesteps  | 995422   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0364   |
|    n_updates        | 223855   |
----------------------------------
Eval num_timesteps=995500, episode_reward=4.54 +/- 5.05
Episode length: 119.58 +/- 53.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.54     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 995500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0341   |
|    n_updates        | 223874   |
----------------------------------
Eval num_timesteps=996000, episode_reward=4.80 +/- 7.88
Episode length: 160.42 +/- 122.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 160      |
|    mean_reward      | 4.8      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 996000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0568   |
|    n_updates        | 223999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 4.4      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8844     |
|    fps              | 34       |
|    time_elapsed     | 28914    |
|    total_timesteps  | 996284   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0486   |
|    n_updates        | 224070   |
----------------------------------
Eval num_timesteps=996500, episode_reward=5.32 +/- 7.66
Episode length: 137.16 +/- 78.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 5.32     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 996500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.026    |
|    n_updates        | 224124   |
----------------------------------
Eval num_timesteps=997000, episode_reward=5.06 +/- 7.57
Episode length: 120.48 +/- 59.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 997000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.13     |
|    n_updates        | 224249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.72     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8848     |
|    fps              | 34       |
|    time_elapsed     | 28944    |
|    total_timesteps  | 997100   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 224274   |
----------------------------------
Eval num_timesteps=997500, episode_reward=5.26 +/- 6.13
Episode length: 122.82 +/- 62.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 5.26     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 997500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0268   |
|    n_updates        | 224374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.7      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8852     |
|    fps              | 34       |
|    time_elapsed     | 28959    |
|    total_timesteps  | 997531   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0317   |
|    n_updates        | 224382   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.76     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8856     |
|    fps              | 34       |
|    time_elapsed     | 28961    |
|    total_timesteps  | 997994   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0116   |
|    n_updates        | 224498   |
----------------------------------
Eval num_timesteps=998000, episode_reward=5.14 +/- 7.71
Episode length: 151.54 +/- 138.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 998000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0382   |
|    n_updates        | 224499   |
----------------------------------
Eval num_timesteps=998500, episode_reward=5.72 +/- 8.20
Episode length: 132.04 +/- 70.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 5.72     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 998500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0409   |
|    n_updates        | 224624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 4.91     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8860     |
|    fps              | 34       |
|    time_elapsed     | 28994    |
|    total_timesteps  | 998727   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0881   |
|    n_updates        | 224681   |
----------------------------------
Eval num_timesteps=999000, episode_reward=5.48 +/- 9.80
Episode length: 130.18 +/- 95.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 999000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0247   |
|    n_updates        | 224749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 5.28     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8864     |
|    fps              | 34       |
|    time_elapsed     | 29010    |
|    total_timesteps  | 999290   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0608   |
|    n_updates        | 224822   |
----------------------------------
Eval num_timesteps=999500, episode_reward=4.58 +/- 6.89
Episode length: 120.30 +/- 56.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.58     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 999500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00914  |
|    n_updates        | 224874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 5.14     |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 8868     |
|    fps              | 34       |
|    time_elapsed     | 29025    |
|    total_timesteps  | 999751   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0775   |
|    n_updates        | 224937   |
----------------------------------
Eval num_timesteps=1000000, episode_reward=4.44 +/- 6.92
Episode length: 129.72 +/- 87.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 4.44     |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 1000000  |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0111   |
|    n_updates        | 224999   |
----------------------------------
/mnt/c/Proyecto/.venv/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/deathmatch/dqn-2-btn(menos)-fs(7)-steps(1000000)/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
