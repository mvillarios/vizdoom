/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=315.36 +/- 41.22
Episode length: 103.84 +/- 10.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 420      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 177      |
|    time_elapsed     | 2        |
|    total_timesteps  | 520      |
----------------------------------
Eval num_timesteps=1000, episode_reward=311.52 +/- 38.41
Episode length: 102.88 +/- 9.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 8        |
|    fps              | 173      |
|    time_elapsed     | 5        |
|    total_timesteps  | 1016     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.96     |
|    n_updates        | 3        |
----------------------------------
Eval num_timesteps=1500, episode_reward=308.96 +/- 36.37
Episode length: 102.24 +/- 9.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.64     |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 12       |
|    fps              | 175      |
|    time_elapsed     | 8        |
|    total_timesteps  | 1568     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 141      |
----------------------------------
Eval num_timesteps=2000, episode_reward=315.36 +/- 40.72
Episode length: 103.84 +/- 10.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.414    |
|    n_updates        | 249      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 420      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 16       |
|    fps              | 172      |
|    time_elapsed     | 12       |
|    total_timesteps  | 2080     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.466    |
|    n_updates        | 269      |
----------------------------------
Eval num_timesteps=2500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.353    |
|    n_updates        | 374      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 431      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 20       |
|    fps              | 176      |
|    time_elapsed     | 15       |
|    total_timesteps  | 2656     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.245    |
|    n_updates        | 413      |
----------------------------------
Eval num_timesteps=3000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.106    |
|    n_updates        | 499      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 447      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 24       |
|    fps              | 181      |
|    time_elapsed     | 18       |
|    total_timesteps  | 3280     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.72     |
|    n_updates        | 569      |
----------------------------------
Eval num_timesteps=3500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.58     |
|    n_updates        | 624      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 442      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 28       |
|    fps              | 180      |
|    time_elapsed     | 21       |
|    total_timesteps  | 3792     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.57     |
|    n_updates        | 697      |
----------------------------------
Eval num_timesteps=4000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.023    |
|    n_updates        | 749      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 431      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 32       |
|    fps              | 177      |
|    time_elapsed     | 23       |
|    total_timesteps  | 4248     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.57     |
|    n_updates        | 811      |
----------------------------------
Eval num_timesteps=4500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 429      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 36       |
|    fps              | 177      |
|    time_elapsed     | 26       |
|    total_timesteps  | 4760     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.12     |
|    n_updates        | 939      |
----------------------------------
Eval num_timesteps=5000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 999      |
----------------------------------
Eval num_timesteps=5500, episode_reward=303.20 +/- 37.32
Episode length: 100.80 +/- 9.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 452      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 40       |
|    fps              | 168      |
|    time_elapsed     | 32       |
|    total_timesteps  | 5520     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.57     |
|    n_updates        | 1129     |
----------------------------------
Eval num_timesteps=6000, episode_reward=285.28 +/- 6.27
Episode length: 96.32 +/- 1.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 445      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 44       |
|    fps              | 168      |
|    time_elapsed     | 35       |
|    total_timesteps  | 6000     |
----------------------------------
Eval num_timesteps=6500, episode_reward=303.20 +/- 37.32
Episode length: 100.80 +/- 9.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 442      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 48       |
|    fps              | 167      |
|    time_elapsed     | 38       |
|    total_timesteps  | 6504     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00674  |
|    n_updates        | 1375     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 434      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 52       |
|    fps              | 177      |
|    time_elapsed     | 39       |
|    total_timesteps  | 6936     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00597  |
|    n_updates        | 1483     |
----------------------------------
Eval num_timesteps=7000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00651  |
|    n_updates        | 1499     |
----------------------------------
Eval num_timesteps=7500, episode_reward=314.72 +/- 55.04
Episode length: 103.68 +/- 13.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0034   |
|    n_updates        | 1624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 443      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 56       |
|    fps              | 169      |
|    time_elapsed     | 44       |
|    total_timesteps  | 7600     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.67     |
|    n_updates        | 1649     |
----------------------------------
Eval num_timesteps=8000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 434      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 60       |
|    fps              | 167      |
|    time_elapsed     | 47       |
|    total_timesteps  | 8016     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00381  |
|    n_updates        | 1753     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 430      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 64       |
|    fps              | 176      |
|    time_elapsed     | 48       |
|    total_timesteps  | 8488     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1871     |
----------------------------------
Eval num_timesteps=8500, episode_reward=285.28 +/- 8.96
Episode length: 96.32 +/- 2.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 427      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 68       |
|    fps              | 175      |
|    time_elapsed     | 51       |
|    total_timesteps  | 8952     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00287  |
|    n_updates        | 1987     |
----------------------------------
Eval num_timesteps=9000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9000     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00282  |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 424      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 72       |
|    fps              | 174      |
|    time_elapsed     | 54       |
|    total_timesteps  | 9432     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00193  |
|    n_updates        | 2107     |
----------------------------------
Eval num_timesteps=9500, episode_reward=322.40 +/- 39.97
Episode length: 105.60 +/- 9.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9500     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00168  |
|    n_updates        | 2124     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 422      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 76       |
|    fps              | 173      |
|    time_elapsed     | 57       |
|    total_timesteps  | 9920     |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00242  |
|    n_updates        | 2229     |
----------------------------------
Eval num_timesteps=10000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.56     |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 420      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 80       |
|    fps              | 173      |
|    time_elapsed     | 60       |
|    total_timesteps  | 10400    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0282   |
|    n_updates        | 2349     |
----------------------------------
Eval num_timesteps=10500, episode_reward=333.28 +/- 84.47
Episode length: 108.32 +/- 21.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 333      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.63     |
|    n_updates        | 2374     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 422      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 84       |
|    fps              | 172      |
|    time_elapsed     | 63       |
|    total_timesteps  | 10952    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00981  |
|    n_updates        | 2487     |
----------------------------------
Eval num_timesteps=11000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0128   |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 418      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 88       |
|    fps              | 171      |
|    time_elapsed     | 66       |
|    total_timesteps  | 11392    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0079   |
|    n_updates        | 2597     |
----------------------------------
Eval num_timesteps=11500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0082   |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 418      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 92       |
|    fps              | 171      |
|    time_elapsed     | 69       |
|    total_timesteps  | 11912    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 2727     |
----------------------------------
Eval num_timesteps=12000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00701  |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 416      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 96       |
|    fps              | 171      |
|    time_elapsed     | 72       |
|    total_timesteps  | 12392    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00476  |
|    n_updates        | 2847     |
----------------------------------
Eval num_timesteps=12500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 416      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 100      |
|    fps              | 171      |
|    time_elapsed     | 75       |
|    total_timesteps  | 12896    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00535  |
|    n_updates        | 2973     |
----------------------------------
Eval num_timesteps=13000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 417      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 104      |
|    fps              | 172      |
|    time_elapsed     | 78       |
|    total_timesteps  | 13440    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3109     |
----------------------------------
Eval num_timesteps=13500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00256  |
|    n_updates        | 3124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 415      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 108      |
|    fps              | 171      |
|    time_elapsed     | 80       |
|    total_timesteps  | 13880    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.23     |
|    n_updates        | 3219     |
----------------------------------
Eval num_timesteps=14000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00329  |
|    n_updates        | 3249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 112      |
|    fps              | 171      |
|    time_elapsed     | 83       |
|    total_timesteps  | 14384    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00385  |
|    n_updates        | 3345     |
----------------------------------
Eval num_timesteps=14500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 116      |
|    fps              | 171      |
|    time_elapsed     | 86       |
|    total_timesteps  | 14888    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00293  |
|    n_updates        | 3471     |
----------------------------------
Eval num_timesteps=15000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0025   |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 120      |
|    fps              | 171      |
|    time_elapsed     | 89       |
|    total_timesteps  | 15376    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00311  |
|    n_updates        | 3593     |
----------------------------------
Eval num_timesteps=15500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0035   |
|    n_updates        | 3624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 124      |
|    fps              | 171      |
|    time_elapsed     | 92       |
|    total_timesteps  | 15904    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00373  |
|    n_updates        | 3725     |
----------------------------------
Eval num_timesteps=16000, episode_reward=350.56 +/- 62.98
Episode length: 112.64 +/- 15.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 351      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00196  |
|    n_updates        | 3749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 128      |
|    fps              | 170      |
|    time_elapsed     | 96       |
|    total_timesteps  | 16392    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3847     |
----------------------------------
Eval num_timesteps=16500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00315  |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 132      |
|    fps              | 170      |
|    time_elapsed     | 99       |
|    total_timesteps  | 16872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 3967     |
----------------------------------
Eval num_timesteps=17000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00217  |
|    n_updates        | 3999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 136      |
|    fps              | 169      |
|    time_elapsed     | 102      |
|    total_timesteps  | 17312    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00212  |
|    n_updates        | 4077     |
----------------------------------
Eval num_timesteps=17500, episode_reward=316.00 +/- 33.87
Episode length: 104.00 +/- 8.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00358  |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 140      |
|    fps              | 170      |
|    time_elapsed     | 105      |
|    total_timesteps  | 17912    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00387  |
|    n_updates        | 4227     |
----------------------------------
Eval num_timesteps=18000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 144      |
|    fps              | 170      |
|    time_elapsed     | 108      |
|    total_timesteps  | 18432    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4357     |
----------------------------------
Eval num_timesteps=18500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 148      |
|    fps              | 170      |
|    time_elapsed     | 111      |
|    total_timesteps  | 18912    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00175  |
|    n_updates        | 4477     |
----------------------------------
Eval num_timesteps=19000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0022   |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 152      |
|    fps              | 170      |
|    time_elapsed     | 114      |
|    total_timesteps  | 19416    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.23     |
|    n_updates        | 4603     |
----------------------------------
Eval num_timesteps=19500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00312  |
|    n_updates        | 4624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 156      |
|    fps              | 169      |
|    time_elapsed     | 116      |
|    total_timesteps  | 19872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4717     |
----------------------------------
Eval num_timesteps=20000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.62     |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 160      |
|    fps              | 170      |
|    time_elapsed     | 119      |
|    total_timesteps  | 20424    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.69     |
|    n_updates        | 4855     |
----------------------------------
Eval num_timesteps=20500, episode_reward=308.32 +/- 37.08
Episode length: 102.08 +/- 9.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.36     |
|    n_updates        | 4874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 164      |
|    fps              | 170      |
|    time_elapsed     | 122      |
|    total_timesteps  | 20912    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 4977     |
----------------------------------
Eval num_timesteps=21000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 168      |
|    fps              | 169      |
|    time_elapsed     | 125      |
|    total_timesteps  | 21320    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5079     |
----------------------------------
Eval num_timesteps=21500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00553  |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 172      |
|    fps              | 169      |
|    time_elapsed     | 128      |
|    total_timesteps  | 21872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.02     |
|    n_updates        | 5217     |
----------------------------------
Eval num_timesteps=22000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00438  |
|    n_updates        | 5249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 176      |
|    fps              | 169      |
|    time_elapsed     | 131      |
|    total_timesteps  | 22360    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00623  |
|    n_updates        | 5339     |
----------------------------------
Eval num_timesteps=22500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 180      |
|    fps              | 169      |
|    time_elapsed     | 134      |
|    total_timesteps  | 22816    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.67     |
|    n_updates        | 5453     |
----------------------------------
Eval num_timesteps=23000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00629  |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 184      |
|    fps              | 169      |
|    time_elapsed     | 137      |
|    total_timesteps  | 23280    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5569     |
----------------------------------
Eval num_timesteps=23500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 188      |
|    fps              | 168      |
|    time_elapsed     | 140      |
|    total_timesteps  | 23736    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5683     |
----------------------------------
Eval num_timesteps=24000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00588  |
|    n_updates        | 5749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 192      |
|    fps              | 168      |
|    time_elapsed     | 143      |
|    total_timesteps  | 24200    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 5799     |
----------------------------------
Eval num_timesteps=24500, episode_reward=392.16 +/- 130.20
Episode length: 123.04 +/- 32.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 392      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00398  |
|    n_updates        | 5874     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 196      |
|    fps              | 167      |
|    time_elapsed     | 147      |
|    total_timesteps  | 24704    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00237  |
|    n_updates        | 5925     |
----------------------------------
Eval num_timesteps=25000, episode_reward=287.84 +/- 18.81
Episode length: 96.96 +/- 4.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0043   |
|    n_updates        | 5999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 200      |
|    fps              | 167      |
|    time_elapsed     | 150      |
|    total_timesteps  | 25200    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.35     |
|    n_updates        | 6049     |
----------------------------------
Eval num_timesteps=25500, episode_reward=976.80 +/- 671.61
Episode length: 264.20 +/- 159.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 264      |
|    mean_reward      | 977      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00307  |
|    n_updates        | 6124     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 204      |
|    fps              | 163      |
|    time_elapsed     | 157      |
|    total_timesteps  | 25680    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00351  |
|    n_updates        | 6169     |
----------------------------------
Eval num_timesteps=26000, episode_reward=335.20 +/- 90.28
Episode length: 108.80 +/- 22.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 335      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00341  |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 208      |
|    fps              | 162      |
|    time_elapsed     | 160      |
|    total_timesteps  | 26232    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6307     |
----------------------------------
Eval num_timesteps=26500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00311  |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 212      |
|    fps              | 163      |
|    time_elapsed     | 163      |
|    total_timesteps  | 26768    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6441     |
----------------------------------
Eval num_timesteps=27000, episode_reward=294.88 +/- 29.05
Episode length: 98.72 +/- 7.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 295      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00304  |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 216      |
|    fps              | 162      |
|    time_elapsed     | 166      |
|    total_timesteps  | 27200    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0018   |
|    n_updates        | 6549     |
----------------------------------
Eval num_timesteps=27500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00515  |
|    n_updates        | 6624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 220      |
|    fps              | 163      |
|    time_elapsed     | 169      |
|    total_timesteps  | 27736    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6683     |
----------------------------------
Eval num_timesteps=28000, episode_reward=456.16 +/- 187.67
Episode length: 139.04 +/- 46.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 456      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00398  |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 224      |
|    fps              | 162      |
|    time_elapsed     | 173      |
|    total_timesteps  | 28168    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6791     |
----------------------------------
Eval num_timesteps=28500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00386  |
|    n_updates        | 6874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 228      |
|    fps              | 162      |
|    time_elapsed     | 176      |
|    total_timesteps  | 28640    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0034   |
|    n_updates        | 6909     |
----------------------------------
Eval num_timesteps=29000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 6999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 232      |
|    fps              | 162      |
|    time_elapsed     | 179      |
|    total_timesteps  | 29144    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00374  |
|    n_updates        | 7035     |
----------------------------------
Eval num_timesteps=29500, episode_reward=319.84 +/- 43.71
Episode length: 104.96 +/- 10.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00378  |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 236      |
|    fps              | 162      |
|    time_elapsed     | 182      |
|    total_timesteps  | 29616    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.68     |
|    n_updates        | 7153     |
----------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00393  |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 240      |
|    fps              | 162      |
|    time_elapsed     | 185      |
|    total_timesteps  | 30144    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.105    |
|    n_updates        | 7285     |
----------------------------------
Eval num_timesteps=30500, episode_reward=1464.48 +/- 743.53
Episode length: 378.12 +/- 174.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 378      |
|    mean_reward      | 1.46e+03 |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00854  |
|    n_updates        | 7374     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 244      |
|    fps              | 156      |
|    time_elapsed     | 196      |
|    total_timesteps  | 30736    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00528  |
|    n_updates        | 7433     |
----------------------------------
Eval num_timesteps=31000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00523  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 248      |
|    fps              | 156      |
|    time_elapsed     | 199      |
|    total_timesteps  | 31192    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 7547     |
----------------------------------
Eval num_timesteps=31500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 252      |
|    fps              | 157      |
|    time_elapsed     | 202      |
|    total_timesteps  | 31800    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 7699     |
----------------------------------
Eval num_timesteps=32000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 256      |
|    fps              | 157      |
|    time_elapsed     | 205      |
|    total_timesteps  | 32288    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.47     |
|    n_updates        | 7821     |
----------------------------------
Eval num_timesteps=32500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 7874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 260      |
|    fps              | 157      |
|    time_elapsed     | 208      |
|    total_timesteps  | 32744    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.47     |
|    n_updates        | 7935     |
----------------------------------
Eval num_timesteps=33000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0046   |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 264      |
|    fps              | 157      |
|    time_elapsed     | 211      |
|    total_timesteps  | 33272    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00405  |
|    n_updates        | 8067     |
----------------------------------
Eval num_timesteps=33500, episode_reward=307.04 +/- 31.38
Episode length: 101.76 +/- 7.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 268      |
|    fps              | 157      |
|    time_elapsed     | 214      |
|    total_timesteps  | 33816    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00539  |
|    n_updates        | 8203     |
----------------------------------
Eval num_timesteps=34000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 8249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 272      |
|    fps              | 158      |
|    time_elapsed     | 217      |
|    total_timesteps  | 34312    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00331  |
|    n_updates        | 8327     |
----------------------------------
Eval num_timesteps=34500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00585  |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 276      |
|    fps              | 158      |
|    time_elapsed     | 220      |
|    total_timesteps  | 34792    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00387  |
|    n_updates        | 8447     |
----------------------------------
Eval num_timesteps=35000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00387  |
|    n_updates        | 8499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 280      |
|    fps              | 157      |
|    time_elapsed     | 223      |
|    total_timesteps  | 35232    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 8557     |
----------------------------------
Eval num_timesteps=35500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 284      |
|    fps              | 158      |
|    time_elapsed     | 226      |
|    total_timesteps  | 35760    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00466  |
|    n_updates        | 8689     |
----------------------------------
Eval num_timesteps=36000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.19     |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 288      |
|    fps              | 158      |
|    time_elapsed     | 228      |
|    total_timesteps  | 36240    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 8809     |
----------------------------------
Eval num_timesteps=36500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00367  |
|    n_updates        | 8874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 292      |
|    fps              | 158      |
|    time_elapsed     | 231      |
|    total_timesteps  | 36632    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00314  |
|    n_updates        | 8907     |
----------------------------------
Eval num_timesteps=37000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 296      |
|    fps              | 158      |
|    time_elapsed     | 234      |
|    total_timesteps  | 37088    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 9021     |
----------------------------------
Eval num_timesteps=37500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 300      |
|    fps              | 158      |
|    time_elapsed     | 237      |
|    total_timesteps  | 37584    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00413  |
|    n_updates        | 9145     |
----------------------------------
Eval num_timesteps=38000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0061   |
|    n_updates        | 9249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 304      |
|    fps              | 158      |
|    time_elapsed     | 240      |
|    total_timesteps  | 38016    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00594  |
|    n_updates        | 9253     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 308      |
|    fps              | 159      |
|    time_elapsed     | 240      |
|    total_timesteps  | 38440    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 9359     |
----------------------------------
Eval num_timesteps=38500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00345  |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 312      |
|    fps              | 159      |
|    time_elapsed     | 244      |
|    total_timesteps  | 38896    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00403  |
|    n_updates        | 9473     |
----------------------------------
Eval num_timesteps=39000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.74     |
|    n_updates        | 9499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 316      |
|    fps              | 159      |
|    time_elapsed     | 246      |
|    total_timesteps  | 39336    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00312  |
|    n_updates        | 9583     |
----------------------------------
Eval num_timesteps=39500, episode_reward=484.96 +/- 236.02
Episode length: 146.24 +/- 59.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 485      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.73     |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 320      |
|    fps              | 158      |
|    time_elapsed     | 251      |
|    total_timesteps  | 39752    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0066   |
|    n_updates        | 9687     |
----------------------------------
Eval num_timesteps=40000, episode_reward=287.20 +/- 11.54
Episode length: 96.80 +/- 2.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.8     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.47     |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 324      |
|    fps              | 158      |
|    time_elapsed     | 254      |
|    total_timesteps  | 40208    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.6      |
|    n_updates        | 9801     |
----------------------------------
Eval num_timesteps=40500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 328      |
|    fps              | 158      |
|    time_elapsed     | 256      |
|    total_timesteps  | 40656    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00483  |
|    n_updates        | 9913     |
----------------------------------
Eval num_timesteps=41000, episode_reward=302.56 +/- 32.06
Episode length: 100.64 +/- 8.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00474  |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 332      |
|    fps              | 158      |
|    time_elapsed     | 259      |
|    total_timesteps  | 41128    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10031    |
----------------------------------
Eval num_timesteps=41500, episode_reward=320.48 +/- 36.77
Episode length: 105.12 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 336      |
|    fps              | 158      |
|    time_elapsed     | 263      |
|    total_timesteps  | 41624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00462  |
|    n_updates        | 10155    |
----------------------------------
Eval num_timesteps=42000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00532  |
|    n_updates        | 10249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 340      |
|    fps              | 158      |
|    time_elapsed     | 265      |
|    total_timesteps  | 42056    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10263    |
----------------------------------
Eval num_timesteps=42500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 344      |
|    fps              | 158      |
|    time_elapsed     | 268      |
|    total_timesteps  | 42608    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00755  |
|    n_updates        | 10401    |
----------------------------------
Eval num_timesteps=43000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 348      |
|    fps              | 158      |
|    time_elapsed     | 271      |
|    total_timesteps  | 43072    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00564  |
|    n_updates        | 10517    |
----------------------------------
Eval num_timesteps=43500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00511  |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 352      |
|    fps              | 158      |
|    time_elapsed     | 274      |
|    total_timesteps  | 43624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00328  |
|    n_updates        | 10655    |
----------------------------------
Eval num_timesteps=44000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 356      |
|    fps              | 158      |
|    time_elapsed     | 277      |
|    total_timesteps  | 44080    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00394  |
|    n_updates        | 10769    |
----------------------------------
Eval num_timesteps=44500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 10874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 360      |
|    fps              | 158      |
|    time_elapsed     | 280      |
|    total_timesteps  | 44536    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00581  |
|    n_updates        | 10883    |
----------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00761  |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 364      |
|    fps              | 158      |
|    time_elapsed     | 283      |
|    total_timesteps  | 45016    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.37     |
|    n_updates        | 11003    |
----------------------------------
Eval num_timesteps=45500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 368      |
|    fps              | 158      |
|    time_elapsed     | 286      |
|    total_timesteps  | 45520    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00364  |
|    n_updates        | 11129    |
----------------------------------
Eval num_timesteps=46000, episode_reward=325.60 +/- 38.53
Episode length: 106.40 +/- 9.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00517  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 372      |
|    fps              | 158      |
|    time_elapsed     | 289      |
|    total_timesteps  | 46040    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00625  |
|    n_updates        | 11259    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 376      |
|    fps              | 160      |
|    time_elapsed     | 290      |
|    total_timesteps  | 46448    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00546  |
|    n_updates        | 11361    |
----------------------------------
Eval num_timesteps=46500, episode_reward=292.96 +/- 28.65
Episode length: 98.24 +/- 7.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.2     |
|    mean_reward      | 293      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00609  |
|    n_updates        | 11374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 380      |
|    fps              | 160      |
|    time_elapsed     | 293      |
|    total_timesteps  | 46968    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00436  |
|    n_updates        | 11491    |
----------------------------------
Eval num_timesteps=47000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11499    |
----------------------------------
Eval num_timesteps=47500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.57     |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 384      |
|    fps              | 159      |
|    time_elapsed     | 298      |
|    total_timesteps  | 47584    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0061   |
|    n_updates        | 11645    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 388      |
|    fps              | 160      |
|    time_elapsed     | 299      |
|    total_timesteps  | 47992    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00401  |
|    n_updates        | 11747    |
----------------------------------
Eval num_timesteps=48000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11749    |
----------------------------------
Eval num_timesteps=48500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00528  |
|    n_updates        | 11874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 392      |
|    fps              | 159      |
|    time_elapsed     | 304      |
|    total_timesteps  | 48528    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 11881    |
----------------------------------
Eval num_timesteps=49000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00416  |
|    n_updates        | 11999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 396      |
|    fps              | 159      |
|    time_elapsed     | 307      |
|    total_timesteps  | 49000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 400      |
|    fps              | 160      |
|    time_elapsed     | 307      |
|    total_timesteps  | 49440    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.58     |
|    n_updates        | 12109    |
----------------------------------
Eval num_timesteps=49500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.79     |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 404      |
|    fps              | 160      |
|    time_elapsed     | 310      |
|    total_timesteps  | 49880    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00322  |
|    n_updates        | 12219    |
----------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00603  |
|    n_updates        | 12249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 408      |
|    fps              | 160      |
|    time_elapsed     | 313      |
|    total_timesteps  | 50312    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00852  |
|    n_updates        | 12327    |
----------------------------------
Eval num_timesteps=50500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.68     |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 412      |
|    fps              | 160      |
|    time_elapsed     | 316      |
|    total_timesteps  | 50840    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.85     |
|    n_updates        | 12459    |
----------------------------------
Eval num_timesteps=51000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.68     |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 416      |
|    fps              | 160      |
|    time_elapsed     | 319      |
|    total_timesteps  | 51384    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 12595    |
----------------------------------
Eval num_timesteps=51500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.85     |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 420      |
|    fps              | 160      |
|    time_elapsed     | 322      |
|    total_timesteps  | 51832    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 12707    |
----------------------------------
Eval num_timesteps=52000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.52     |
|    n_updates        | 12749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 424      |
|    fps              | 160      |
|    time_elapsed     | 325      |
|    total_timesteps  | 52256    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0067   |
|    n_updates        | 12813    |
----------------------------------
Eval num_timesteps=52500, episode_reward=356.32 +/- 119.53
Episode length: 114.08 +/- 29.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 356      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 12874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 428      |
|    fps              | 160      |
|    time_elapsed     | 328      |
|    total_timesteps  | 52688    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00623  |
|    n_updates        | 12921    |
----------------------------------
Eval num_timesteps=53000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.68     |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 432      |
|    fps              | 160      |
|    time_elapsed     | 331      |
|    total_timesteps  | 53224    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 13055    |
----------------------------------
Eval num_timesteps=53500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00468  |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 436      |
|    fps              | 160      |
|    time_elapsed     | 334      |
|    total_timesteps  | 53680    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.68     |
|    n_updates        | 13169    |
----------------------------------
Eval num_timesteps=54000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 13249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 440      |
|    fps              | 160      |
|    time_elapsed     | 337      |
|    total_timesteps  | 54112    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00437  |
|    n_updates        | 13277    |
----------------------------------
Eval num_timesteps=54500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00471  |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 444      |
|    fps              | 160      |
|    time_elapsed     | 340      |
|    total_timesteps  | 54712    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00581  |
|    n_updates        | 13427    |
----------------------------------
Eval num_timesteps=55000, episode_reward=344.16 +/- 80.86
Episode length: 111.04 +/- 20.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00696  |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 448      |
|    fps              | 160      |
|    time_elapsed     | 343      |
|    total_timesteps  | 55176    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 13543    |
----------------------------------
Eval num_timesteps=55500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00642  |
|    n_updates        | 13624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 452      |
|    fps              | 160      |
|    time_elapsed     | 346      |
|    total_timesteps  | 55800    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00445  |
|    n_updates        | 13699    |
----------------------------------
Eval num_timesteps=56000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00368  |
|    n_updates        | 13749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 456      |
|    fps              | 160      |
|    time_elapsed     | 349      |
|    total_timesteps  | 56240    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00416  |
|    n_updates        | 13809    |
----------------------------------
Eval num_timesteps=56500, episode_reward=295.52 +/- 36.63
Episode length: 98.88 +/- 9.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 296      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.68     |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 460      |
|    fps              | 161      |
|    time_elapsed     | 352      |
|    total_timesteps  | 56792    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00463  |
|    n_updates        | 13947    |
----------------------------------
Eval num_timesteps=57000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00568  |
|    n_updates        | 13999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 464      |
|    fps              | 161      |
|    time_elapsed     | 355      |
|    total_timesteps  | 57368    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00663  |
|    n_updates        | 14091    |
----------------------------------
Eval num_timesteps=57500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00375  |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 468      |
|    fps              | 161      |
|    time_elapsed     | 358      |
|    total_timesteps  | 57920    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00356  |
|    n_updates        | 14229    |
----------------------------------
Eval num_timesteps=58000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 472      |
|    fps              | 161      |
|    time_elapsed     | 361      |
|    total_timesteps  | 58336    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 14333    |
----------------------------------
Eval num_timesteps=58500, episode_reward=456.16 +/- 176.54
Episode length: 139.04 +/- 44.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 456      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.84     |
|    n_updates        | 14374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 476      |
|    fps              | 160      |
|    time_elapsed     | 365      |
|    total_timesteps  | 58792    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00443  |
|    n_updates        | 14447    |
----------------------------------
Eval num_timesteps=59000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00474  |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 480      |
|    fps              | 161      |
|    time_elapsed     | 368      |
|    total_timesteps  | 59352    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00479  |
|    n_updates        | 14587    |
----------------------------------
Eval num_timesteps=59500, episode_reward=305.76 +/- 30.99
Episode length: 101.44 +/- 7.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00784  |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 484      |
|    fps              | 161      |
|    time_elapsed     | 371      |
|    total_timesteps  | 59832    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00487  |
|    n_updates        | 14707    |
----------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00628  |
|    n_updates        | 14749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 488      |
|    fps              | 161      |
|    time_elapsed     | 374      |
|    total_timesteps  | 60296    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00982  |
|    n_updates        | 14823    |
----------------------------------
Eval num_timesteps=60500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 492      |
|    fps              | 161      |
|    time_elapsed     | 377      |
|    total_timesteps  | 60800    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.66     |
|    n_updates        | 14949    |
----------------------------------
Eval num_timesteps=61000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00552  |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 496      |
|    fps              | 161      |
|    time_elapsed     | 380      |
|    total_timesteps  | 61336    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00592  |
|    n_updates        | 15083    |
----------------------------------
Eval num_timesteps=61500, episode_reward=308.96 +/- 34.04
Episode length: 102.24 +/- 8.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00699  |
|    n_updates        | 15124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 500      |
|    fps              | 161      |
|    time_elapsed     | 383      |
|    total_timesteps  | 61744    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00739  |
|    n_updates        | 15185    |
----------------------------------
Eval num_timesteps=62000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 504      |
|    fps              | 161      |
|    time_elapsed     | 386      |
|    total_timesteps  | 62320    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.91     |
|    n_updates        | 15329    |
----------------------------------
Eval num_timesteps=62500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00738  |
|    n_updates        | 15374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 508      |
|    fps              | 161      |
|    time_elapsed     | 389      |
|    total_timesteps  | 62856    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00908  |
|    n_updates        | 15463    |
----------------------------------
Eval num_timesteps=63000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 512      |
|    fps              | 161      |
|    time_elapsed     | 392      |
|    total_timesteps  | 63336    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.79     |
|    n_updates        | 15583    |
----------------------------------
Eval num_timesteps=63500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 516      |
|    fps              | 161      |
|    time_elapsed     | 394      |
|    total_timesteps  | 63872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 15717    |
----------------------------------
Eval num_timesteps=64000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 15749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 520      |
|    fps              | 161      |
|    time_elapsed     | 397      |
|    total_timesteps  | 64336    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0061   |
|    n_updates        | 15833    |
----------------------------------
Eval num_timesteps=64500, episode_reward=349.92 +/- 61.86
Episode length: 112.48 +/- 15.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 350      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0065   |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 524      |
|    fps              | 161      |
|    time_elapsed     | 401      |
|    total_timesteps  | 64840    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00542  |
|    n_updates        | 15959    |
----------------------------------
Eval num_timesteps=65000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00587  |
|    n_updates        | 15999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 528      |
|    fps              | 161      |
|    time_elapsed     | 404      |
|    total_timesteps  | 65376    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 16093    |
----------------------------------
Eval num_timesteps=65500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 532      |
|    fps              | 161      |
|    time_elapsed     | 407      |
|    total_timesteps  | 65776    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00386  |
|    n_updates        | 16193    |
----------------------------------
Eval num_timesteps=66000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 536      |
|    fps              | 161      |
|    time_elapsed     | 410      |
|    total_timesteps  | 66296    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00585  |
|    n_updates        | 16323    |
----------------------------------
Eval num_timesteps=66500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 16374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 540      |
|    fps              | 161      |
|    time_elapsed     | 412      |
|    total_timesteps  | 66736    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00798  |
|    n_updates        | 16433    |
----------------------------------
Eval num_timesteps=67000, episode_reward=329.44 +/- 82.48
Episode length: 107.36 +/- 20.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 329      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00684  |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 544      |
|    fps              | 161      |
|    time_elapsed     | 416      |
|    total_timesteps  | 67184    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.79     |
|    n_updates        | 16545    |
----------------------------------
Eval num_timesteps=67500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 548      |
|    fps              | 161      |
|    time_elapsed     | 419      |
|    total_timesteps  | 67624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.79     |
|    n_updates        | 16655    |
----------------------------------
Eval num_timesteps=68000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 68000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00598  |
|    n_updates        | 16749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 552      |
|    fps              | 161      |
|    time_elapsed     | 422      |
|    total_timesteps  | 68104    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 16775    |
----------------------------------
Eval num_timesteps=68500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.89     |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 556      |
|    fps              | 161      |
|    time_elapsed     | 424      |
|    total_timesteps  | 68576    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00447  |
|    n_updates        | 16893    |
----------------------------------
Eval num_timesteps=69000, episode_reward=312.80 +/- 43.05
Episode length: 103.20 +/- 10.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00574  |
|    n_updates        | 16999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 560      |
|    fps              | 161      |
|    time_elapsed     | 427      |
|    total_timesteps  | 69064    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.9      |
|    n_updates        | 17015    |
----------------------------------
Eval num_timesteps=69500, episode_reward=317.92 +/- 37.54
Episode length: 104.48 +/- 9.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00406  |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 564      |
|    fps              | 161      |
|    time_elapsed     | 431      |
|    total_timesteps  | 69624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00726  |
|    n_updates        | 17155    |
----------------------------------
Eval num_timesteps=70000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00387  |
|    n_updates        | 17249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 568      |
|    fps              | 161      |
|    time_elapsed     | 433      |
|    total_timesteps  | 70104    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.136    |
|    n_updates        | 17275    |
----------------------------------
Eval num_timesteps=70500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00956  |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 572      |
|    fps              | 161      |
|    time_elapsed     | 436      |
|    total_timesteps  | 70592    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00476  |
|    n_updates        | 17397    |
----------------------------------
Eval num_timesteps=71000, episode_reward=309.60 +/- 35.05
Episode length: 102.40 +/- 8.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 17499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 576      |
|    fps              | 161      |
|    time_elapsed     | 439      |
|    total_timesteps  | 71048    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00811  |
|    n_updates        | 17511    |
----------------------------------
Eval num_timesteps=71500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0059   |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 580      |
|    fps              | 161      |
|    time_elapsed     | 442      |
|    total_timesteps  | 71504    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00537  |
|    n_updates        | 17625    |
----------------------------------
Eval num_timesteps=72000, episode_reward=316.64 +/- 44.57
Episode length: 104.16 +/- 11.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00803  |
|    n_updates        | 17749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 584      |
|    fps              | 161      |
|    time_elapsed     | 445      |
|    total_timesteps  | 72024    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 17755    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 588      |
|    fps              | 162      |
|    time_elapsed     | 446      |
|    total_timesteps  | 72456    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0058   |
|    n_updates        | 17863    |
----------------------------------
Eval num_timesteps=72500, episode_reward=316.00 +/- 37.32
Episode length: 104.00 +/- 9.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 17874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 592      |
|    fps              | 162      |
|    time_elapsed     | 449      |
|    total_timesteps  | 72944    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0073   |
|    n_updates        | 17985    |
----------------------------------
Eval num_timesteps=73000, episode_reward=351.20 +/- 129.31
Episode length: 112.80 +/- 32.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 351      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0067   |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 596      |
|    fps              | 162      |
|    time_elapsed     | 452      |
|    total_timesteps  | 73400    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00698  |
|    n_updates        | 18099    |
----------------------------------
Eval num_timesteps=73500, episode_reward=316.00 +/- 34.47
Episode length: 104.00 +/- 8.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0082   |
|    n_updates        | 18124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 600      |
|    fps              | 162      |
|    time_elapsed     | 455      |
|    total_timesteps  | 73888    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00794  |
|    n_updates        | 18221    |
----------------------------------
Eval num_timesteps=74000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00924  |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 604      |
|    fps              | 162      |
|    time_elapsed     | 458      |
|    total_timesteps  | 74296    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00709  |
|    n_updates        | 18323    |
----------------------------------
Eval num_timesteps=74500, episode_reward=388.32 +/- 116.23
Episode length: 122.08 +/- 29.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 388      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 18374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 608      |
|    fps              | 161      |
|    time_elapsed     | 462      |
|    total_timesteps  | 74824    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00564  |
|    n_updates        | 18455    |
----------------------------------
Eval num_timesteps=75000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0071   |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 612      |
|    fps              | 162      |
|    time_elapsed     | 465      |
|    total_timesteps  | 75368    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 18591    |
----------------------------------
Eval num_timesteps=75500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00761  |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 616      |
|    fps              | 162      |
|    time_elapsed     | 467      |
|    total_timesteps  | 75840    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.89     |
|    n_updates        | 18709    |
----------------------------------
Eval num_timesteps=76000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00492  |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 620      |
|    fps              | 162      |
|    time_elapsed     | 470      |
|    total_timesteps  | 76360    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00767  |
|    n_updates        | 18839    |
----------------------------------
Eval num_timesteps=76500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 18874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 624      |
|    fps              | 162      |
|    time_elapsed     | 473      |
|    total_timesteps  | 76904    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.88     |
|    n_updates        | 18975    |
----------------------------------
Eval num_timesteps=77000, episode_reward=388.32 +/- 134.22
Episode length: 122.08 +/- 33.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 388      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.89     |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 628      |
|    fps              | 162      |
|    time_elapsed     | 477      |
|    total_timesteps  | 77424    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 19105    |
----------------------------------
Eval num_timesteps=77500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 632      |
|    fps              | 162      |
|    time_elapsed     | 480      |
|    total_timesteps  | 77920    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 19229    |
----------------------------------
Eval num_timesteps=78000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 78000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00549  |
|    n_updates        | 19249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 636      |
|    fps              | 162      |
|    time_elapsed     | 483      |
|    total_timesteps  | 78352    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00734  |
|    n_updates        | 19337    |
----------------------------------
Eval num_timesteps=78500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00667  |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 640      |
|    fps              | 162      |
|    time_elapsed     | 486      |
|    total_timesteps  | 78808    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 19451    |
----------------------------------
Eval num_timesteps=79000, episode_reward=310.88 +/- 38.59
Episode length: 102.72 +/- 9.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00329  |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 644      |
|    fps              | 161      |
|    time_elapsed     | 489      |
|    total_timesteps  | 79256    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00868  |
|    n_updates        | 19563    |
----------------------------------
Eval num_timesteps=79500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.89     |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 648      |
|    fps              | 161      |
|    time_elapsed     | 492      |
|    total_timesteps  | 79688    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.95     |
|    n_updates        | 19671    |
----------------------------------
Eval num_timesteps=80000, episode_reward=463.20 +/- 185.27
Episode length: 140.80 +/- 46.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 463      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.94     |
|    n_updates        | 19749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 652      |
|    fps              | 161      |
|    time_elapsed     | 496      |
|    total_timesteps  | 80256    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00822  |
|    n_updates        | 19813    |
----------------------------------
Eval num_timesteps=80500, episode_reward=291.68 +/- 26.04
Episode length: 97.92 +/- 6.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.9     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2        |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 656      |
|    fps              | 161      |
|    time_elapsed     | 499      |
|    total_timesteps  | 80848    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00911  |
|    n_updates        | 19961    |
----------------------------------
Eval num_timesteps=81000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2        |
|    n_updates        | 19999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 660      |
|    fps              | 161      |
|    time_elapsed     | 502      |
|    total_timesteps  | 81384    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 20095    |
----------------------------------
Eval num_timesteps=81500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 664      |
|    fps              | 162      |
|    time_elapsed     | 505      |
|    total_timesteps  | 81872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 20217    |
----------------------------------
Eval num_timesteps=82000, episode_reward=294.24 +/- 28.93
Episode length: 98.56 +/- 7.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 20249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 668      |
|    fps              | 161      |
|    time_elapsed     | 508      |
|    total_timesteps  | 82304    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 20325    |
----------------------------------
Eval num_timesteps=82500, episode_reward=442.08 +/- 173.44
Episode length: 135.52 +/- 43.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 442      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00855  |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 672      |
|    fps              | 161      |
|    time_elapsed     | 512      |
|    total_timesteps  | 82760    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 20439    |
----------------------------------
Eval num_timesteps=83000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0101   |
|    n_updates        | 20499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 676      |
|    fps              | 161      |
|    time_elapsed     | 515      |
|    total_timesteps  | 83240    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00963  |
|    n_updates        | 20559    |
----------------------------------
Eval num_timesteps=83500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 680      |
|    fps              | 161      |
|    time_elapsed     | 518      |
|    total_timesteps  | 83792    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2        |
|    n_updates        | 20697    |
----------------------------------
Eval num_timesteps=84000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00737  |
|    n_updates        | 20749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 684      |
|    fps              | 161      |
|    time_elapsed     | 521      |
|    total_timesteps  | 84280    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00966  |
|    n_updates        | 20819    |
----------------------------------
Eval num_timesteps=84500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00796  |
|    n_updates        | 20874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 688      |
|    fps              | 161      |
|    time_elapsed     | 524      |
|    total_timesteps  | 84784    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0092   |
|    n_updates        | 20945    |
----------------------------------
Eval num_timesteps=85000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.98     |
|    n_updates        | 20999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 692      |
|    fps              | 161      |
|    time_elapsed     | 527      |
|    total_timesteps  | 85296    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.98     |
|    n_updates        | 21073    |
----------------------------------
Eval num_timesteps=85500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.96     |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 696      |
|    fps              | 161      |
|    time_elapsed     | 530      |
|    total_timesteps  | 85776    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 21193    |
----------------------------------
Eval num_timesteps=86000, episode_reward=317.92 +/- 42.17
Episode length: 104.48 +/- 10.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00847  |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 700      |
|    fps              | 161      |
|    time_elapsed     | 533      |
|    total_timesteps  | 86328    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 21331    |
----------------------------------
Eval num_timesteps=86500, episode_reward=313.44 +/- 37.23
Episode length: 103.36 +/- 9.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00446  |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 704      |
|    fps              | 161      |
|    time_elapsed     | 536      |
|    total_timesteps  | 86808    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00801  |
|    n_updates        | 21451    |
----------------------------------
Eval num_timesteps=87000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0041   |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 708      |
|    fps              | 161      |
|    time_elapsed     | 539      |
|    total_timesteps  | 87248    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 3.98     |
|    n_updates        | 21561    |
----------------------------------
Eval num_timesteps=87500, episode_reward=317.92 +/- 33.51
Episode length: 104.48 +/- 8.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0119   |
|    n_updates        | 21624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 712      |
|    fps              | 161      |
|    time_elapsed     | 542      |
|    total_timesteps  | 87800    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00935  |
|    n_updates        | 21699    |
----------------------------------
Eval num_timesteps=88000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0096   |
|    n_updates        | 21749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 716      |
|    fps              | 161      |
|    time_elapsed     | 545      |
|    total_timesteps  | 88296    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 21823    |
----------------------------------
Eval num_timesteps=88500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00576  |
|    n_updates        | 21874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 720      |
|    fps              | 161      |
|    time_elapsed     | 548      |
|    total_timesteps  | 88824    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00841  |
|    n_updates        | 21955    |
----------------------------------
Eval num_timesteps=89000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00783  |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 724      |
|    fps              | 162      |
|    time_elapsed     | 551      |
|    total_timesteps  | 89344    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0101   |
|    n_updates        | 22085    |
----------------------------------
Eval num_timesteps=89500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 22124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 728      |
|    fps              | 161      |
|    time_elapsed     | 554      |
|    total_timesteps  | 89760    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00602  |
|    n_updates        | 22189    |
----------------------------------
Eval num_timesteps=90000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.99     |
|    n_updates        | 22249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 732      |
|    fps              | 161      |
|    time_elapsed     | 557      |
|    total_timesteps  | 90192    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0426   |
|    n_updates        | 22297    |
----------------------------------
Eval num_timesteps=90500, episode_reward=301.92 +/- 30.13
Episode length: 100.48 +/- 7.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 302      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 736      |
|    fps              | 161      |
|    time_elapsed     | 560      |
|    total_timesteps  | 90672    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 10.2     |
|    n_updates        | 22417    |
----------------------------------
Eval num_timesteps=91000, episode_reward=335.20 +/- 74.09
Episode length: 108.80 +/- 18.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 335      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00894  |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 740      |
|    fps              | 161      |
|    time_elapsed     | 563      |
|    total_timesteps  | 91144    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 22535    |
----------------------------------
Eval num_timesteps=91500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 744      |
|    fps              | 161      |
|    time_elapsed     | 566      |
|    total_timesteps  | 91624    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00906  |
|    n_updates        | 22655    |
----------------------------------
Eval num_timesteps=92000, episode_reward=344.80 +/- 68.41
Episode length: 111.20 +/- 17.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 345      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 748      |
|    fps              | 161      |
|    time_elapsed     | 569      |
|    total_timesteps  | 92080    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00601  |
|    n_updates        | 22769    |
----------------------------------
Eval num_timesteps=92500, episode_reward=343.52 +/- 64.96
Episode length: 110.88 +/- 16.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.07     |
|    n_updates        | 22874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 752      |
|    fps              | 161      |
|    time_elapsed     | 572      |
|    total_timesteps  | 92552    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 22887    |
----------------------------------
Eval num_timesteps=93000, episode_reward=316.00 +/- 37.86
Episode length: 104.00 +/- 9.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 22999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 756      |
|    fps              | 161      |
|    time_elapsed     | 575      |
|    total_timesteps  | 93072    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00757  |
|    n_updates        | 23017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 760      |
|    fps              | 162      |
|    time_elapsed     | 576      |
|    total_timesteps  | 93480    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00532  |
|    n_updates        | 23119    |
----------------------------------
Eval num_timesteps=93500, episode_reward=317.92 +/- 56.31
Episode length: 104.48 +/- 14.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00864  |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 764      |
|    fps              | 162      |
|    time_elapsed     | 579      |
|    total_timesteps  | 93976    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00931  |
|    n_updates        | 23243    |
----------------------------------
Eval num_timesteps=94000, episode_reward=376.16 +/- 117.95
Episode length: 119.04 +/- 29.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 376      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00859  |
|    n_updates        | 23249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 768      |
|    fps              | 162      |
|    time_elapsed     | 582      |
|    total_timesteps  | 94456    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 23363    |
----------------------------------
Eval num_timesteps=94500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 94500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 772      |
|    fps              | 161      |
|    time_elapsed     | 585      |
|    total_timesteps  | 94864    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0116   |
|    n_updates        | 23465    |
----------------------------------
Eval num_timesteps=95000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 95000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 776      |
|    fps              | 161      |
|    time_elapsed     | 588      |
|    total_timesteps  | 95344    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00665  |
|    n_updates        | 23585    |
----------------------------------
Eval num_timesteps=95500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.05     |
|    n_updates        | 23624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 780      |
|    fps              | 161      |
|    time_elapsed     | 591      |
|    total_timesteps  | 95832    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0056   |
|    n_updates        | 23707    |
----------------------------------
Eval num_timesteps=96000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00538  |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 784      |
|    fps              | 161      |
|    time_elapsed     | 594      |
|    total_timesteps  | 96264    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00812  |
|    n_updates        | 23815    |
----------------------------------
Eval num_timesteps=96500, episode_reward=313.44 +/- 37.23
Episode length: 103.36 +/- 9.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0119   |
|    n_updates        | 23874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 788      |
|    fps              | 162      |
|    time_elapsed     | 597      |
|    total_timesteps  | 96872    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.05     |
|    n_updates        | 23967    |
----------------------------------
Eval num_timesteps=97000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00978  |
|    n_updates        | 23999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 792      |
|    fps              | 162      |
|    time_elapsed     | 600      |
|    total_timesteps  | 97352    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00632  |
|    n_updates        | 24087    |
----------------------------------
Eval num_timesteps=97500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00957  |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 796      |
|    fps              | 162      |
|    time_elapsed     | 603      |
|    total_timesteps  | 97800    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.07     |
|    n_updates        | 24199    |
----------------------------------
Eval num_timesteps=98000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0085   |
|    n_updates        | 24249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 800      |
|    fps              | 161      |
|    time_elapsed     | 606      |
|    total_timesteps  | 98304    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00664  |
|    n_updates        | 24325    |
----------------------------------
Eval num_timesteps=98500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 804      |
|    fps              | 162      |
|    time_elapsed     | 609      |
|    total_timesteps  | 98840    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 24459    |
----------------------------------
Eval num_timesteps=99000, episode_reward=319.84 +/- 70.00
Episode length: 104.96 +/- 17.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.04     |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 808      |
|    fps              | 161      |
|    time_elapsed     | 612      |
|    total_timesteps  | 99240    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00619  |
|    n_updates        | 24559    |
----------------------------------
Eval num_timesteps=99500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00752  |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 812      |
|    fps              | 161      |
|    time_elapsed     | 615      |
|    total_timesteps  | 99736    |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 24683    |
----------------------------------
Eval num_timesteps=100000, episode_reward=358.88 +/- 84.81
Episode length: 114.72 +/- 21.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 359      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00899  |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 816      |
|    fps              | 161      |
|    time_elapsed     | 619      |
|    total_timesteps  | 100216   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0195   |
|    n_updates        | 24803    |
----------------------------------
Eval num_timesteps=100500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 820      |
|    fps              | 161      |
|    time_elapsed     | 622      |
|    total_timesteps  | 100688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.16     |
|    n_updates        | 24921    |
----------------------------------
Eval num_timesteps=101000, episode_reward=338.40 +/- 60.12
Episode length: 109.60 +/- 15.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 338      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.16     |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 824      |
|    fps              | 161      |
|    time_elapsed     | 625      |
|    total_timesteps  | 101136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00892  |
|    n_updates        | 25033    |
----------------------------------
Eval num_timesteps=101500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0106   |
|    n_updates        | 25124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 828      |
|    fps              | 161      |
|    time_elapsed     | 628      |
|    total_timesteps  | 101680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.17     |
|    n_updates        | 25169    |
----------------------------------
Eval num_timesteps=102000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 25249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 832      |
|    fps              | 161      |
|    time_elapsed     | 631      |
|    total_timesteps  | 102184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 25295    |
----------------------------------
Eval num_timesteps=102500, episode_reward=321.76 +/- 42.78
Episode length: 105.44 +/- 10.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.1      |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 836      |
|    fps              | 161      |
|    time_elapsed     | 634      |
|    total_timesteps  | 102720   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00571  |
|    n_updates        | 25429    |
----------------------------------
Eval num_timesteps=103000, episode_reward=308.32 +/- 31.72
Episode length: 102.08 +/- 7.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 840      |
|    fps              | 161      |
|    time_elapsed     | 637      |
|    total_timesteps  | 103208   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00857  |
|    n_updates        | 25551    |
----------------------------------
Eval num_timesteps=103500, episode_reward=367.84 +/- 85.36
Episode length: 116.96 +/- 21.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 368      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.08     |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 844      |
|    fps              | 161      |
|    time_elapsed     | 640      |
|    total_timesteps  | 103688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 25671    |
----------------------------------
Eval num_timesteps=104000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00923  |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 848      |
|    fps              | 161      |
|    time_elapsed     | 643      |
|    total_timesteps  | 104288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 25821    |
----------------------------------
Eval num_timesteps=104500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.17     |
|    n_updates        | 25874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 852      |
|    fps              | 161      |
|    time_elapsed     | 646      |
|    total_timesteps  | 104696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00836  |
|    n_updates        | 25923    |
----------------------------------
Eval num_timesteps=105000, episode_reward=322.40 +/- 49.57
Episode length: 105.60 +/- 12.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 322      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.17     |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 856      |
|    fps              | 161      |
|    time_elapsed     | 649      |
|    total_timesteps  | 105176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0103   |
|    n_updates        | 26043    |
----------------------------------
Eval num_timesteps=105500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0102   |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 860      |
|    fps              | 161      |
|    time_elapsed     | 652      |
|    total_timesteps  | 105696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0066   |
|    n_updates        | 26173    |
----------------------------------
Eval num_timesteps=106000, episode_reward=290.40 +/- 30.02
Episode length: 97.60 +/- 7.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.17     |
|    n_updates        | 26249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 864      |
|    fps              | 161      |
|    time_elapsed     | 655      |
|    total_timesteps  | 106152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 26287    |
----------------------------------
Eval num_timesteps=106500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00866  |
|    n_updates        | 26374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 868      |
|    fps              | 161      |
|    time_elapsed     | 658      |
|    total_timesteps  | 106592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00938  |
|    n_updates        | 26397    |
----------------------------------
Eval num_timesteps=107000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00841  |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 872      |
|    fps              | 161      |
|    time_elapsed     | 661      |
|    total_timesteps  | 107064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.17     |
|    n_updates        | 26515    |
----------------------------------
Eval num_timesteps=107500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00826  |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 876      |
|    fps              | 161      |
|    time_elapsed     | 664      |
|    total_timesteps  | 107600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00849  |
|    n_updates        | 26649    |
----------------------------------
Eval num_timesteps=108000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 26749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 880      |
|    fps              | 161      |
|    time_elapsed     | 667      |
|    total_timesteps  | 108112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00729  |
|    n_updates        | 26777    |
----------------------------------
Eval num_timesteps=108500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 884      |
|    fps              | 162      |
|    time_elapsed     | 670      |
|    total_timesteps  | 108592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 26897    |
----------------------------------
Eval num_timesteps=109000, episode_reward=532.96 +/- 327.61
Episode length: 158.24 +/- 81.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 158      |
|    mean_reward      | 533      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 26999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 888      |
|    fps              | 161      |
|    time_elapsed     | 674      |
|    total_timesteps  | 109112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 27027    |
----------------------------------
Eval num_timesteps=109500, episode_reward=320.48 +/- 42.94
Episode length: 105.12 +/- 10.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00658  |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 892      |
|    fps              | 161      |
|    time_elapsed     | 677      |
|    total_timesteps  | 109576   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 27143    |
----------------------------------
Eval num_timesteps=110000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.09     |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 896      |
|    fps              | 161      |
|    time_elapsed     | 680      |
|    total_timesteps  | 110032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.812    |
|    n_updates        | 27257    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 900      |
|    fps              | 162      |
|    time_elapsed     | 681      |
|    total_timesteps  | 110464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 27365    |
----------------------------------
Eval num_timesteps=110500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.14     |
|    n_updates        | 27374    |
----------------------------------
Eval num_timesteps=111000, episode_reward=315.36 +/- 40.22
Episode length: 103.84 +/- 10.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 27499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 904      |
|    fps              | 161      |
|    time_elapsed     | 686      |
|    total_timesteps  | 111000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 908      |
|    fps              | 162      |
|    time_elapsed     | 687      |
|    total_timesteps  | 111448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.25     |
|    n_updates        | 27611    |
----------------------------------
Eval num_timesteps=111500, episode_reward=311.52 +/- 33.26
Episode length: 102.88 +/- 8.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 912      |
|    fps              | 162      |
|    time_elapsed     | 690      |
|    total_timesteps  | 111952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 27737    |
----------------------------------
Eval num_timesteps=112000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00786  |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 916      |
|    fps              | 162      |
|    time_elapsed     | 693      |
|    total_timesteps  | 112432   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00837  |
|    n_updates        | 27857    |
----------------------------------
Eval num_timesteps=112500, episode_reward=326.88 +/- 100.71
Episode length: 106.72 +/- 25.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 327      |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0103   |
|    n_updates        | 27874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 920      |
|    fps              | 162      |
|    time_elapsed     | 696      |
|    total_timesteps  | 112912   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.25     |
|    n_updates        | 27977    |
----------------------------------
Eval num_timesteps=113000, episode_reward=335.84 +/- 52.73
Episode length: 108.96 +/- 13.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0134   |
|    n_updates        | 27999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 924      |
|    fps              | 162      |
|    time_elapsed     | 699      |
|    total_timesteps  | 113456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 28113    |
----------------------------------
Eval num_timesteps=113500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 28124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 928      |
|    fps              | 162      |
|    time_elapsed     | 702      |
|    total_timesteps  | 113888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 28221    |
----------------------------------
Eval num_timesteps=114000, episode_reward=302.56 +/- 29.39
Episode length: 100.64 +/- 7.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 28249    |
----------------------------------
Eval num_timesteps=114500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.14     |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 932      |
|    fps              | 161      |
|    time_elapsed     | 707      |
|    total_timesteps  | 114504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.27     |
|    n_updates        | 28375    |
----------------------------------
Eval num_timesteps=115000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0141   |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 936      |
|    fps              | 161      |
|    time_elapsed     | 710      |
|    total_timesteps  | 115040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.14     |
|    n_updates        | 28509    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 940      |
|    fps              | 162      |
|    time_elapsed     | 711      |
|    total_timesteps  | 115464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00843  |
|    n_updates        | 28615    |
----------------------------------
Eval num_timesteps=115500, episode_reward=324.32 +/- 38.83
Episode length: 106.08 +/- 9.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 28624    |
----------------------------------
Eval num_timesteps=116000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 28749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 944      |
|    fps              | 161      |
|    time_elapsed     | 717      |
|    total_timesteps  | 116016   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.14     |
|    n_updates        | 28753    |
----------------------------------
Eval num_timesteps=116500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.98     |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 948      |
|    fps              | 161      |
|    time_elapsed     | 720      |
|    total_timesteps  | 116536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.25     |
|    n_updates        | 28883    |
----------------------------------
Eval num_timesteps=117000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0113   |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 952      |
|    fps              | 161      |
|    time_elapsed     | 723      |
|    total_timesteps  | 117064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00765  |
|    n_updates        | 29015    |
----------------------------------
Eval num_timesteps=117500, episode_reward=374.24 +/- 119.51
Episode length: 118.56 +/- 29.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 374      |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 29124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 956      |
|    fps              | 161      |
|    time_elapsed     | 726      |
|    total_timesteps  | 117640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 29159    |
----------------------------------
Eval num_timesteps=118000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 960      |
|    fps              | 161      |
|    time_elapsed     | 729      |
|    total_timesteps  | 118096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.14     |
|    n_updates        | 29273    |
----------------------------------
Eval num_timesteps=118500, episode_reward=316.00 +/- 38.40
Episode length: 104.00 +/- 9.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 29374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 964      |
|    fps              | 161      |
|    time_elapsed     | 732      |
|    total_timesteps  | 118592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00757  |
|    n_updates        | 29397    |
----------------------------------
Eval num_timesteps=119000, episode_reward=336.48 +/- 67.66
Episode length: 109.12 +/- 16.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.971    |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00967  |
|    n_updates        | 29499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 968      |
|    fps              | 161      |
|    time_elapsed     | 735      |
|    total_timesteps  | 119112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00821  |
|    n_updates        | 29527    |
----------------------------------
Eval num_timesteps=119500, episode_reward=320.48 +/- 37.87
Episode length: 105.12 +/- 9.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0203   |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 972      |
|    fps              | 161      |
|    time_elapsed     | 738      |
|    total_timesteps  | 119640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.13     |
|    n_updates        | 29659    |
----------------------------------
Eval num_timesteps=120000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.16     |
|    n_updates        | 29749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 976      |
|    fps              | 161      |
|    time_elapsed     | 741      |
|    total_timesteps  | 120120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.41     |
|    n_updates        | 29779    |
----------------------------------
Eval num_timesteps=120500, episode_reward=323.04 +/- 51.32
Episode length: 105.76 +/- 12.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 323      |
| rollout/            |          |
|    exploration_rate | 0.965    |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 29874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 980      |
|    fps              | 162      |
|    time_elapsed     | 744      |
|    total_timesteps  | 120696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00785  |
|    n_updates        | 29923    |
----------------------------------
Eval num_timesteps=121000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.963    |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 29999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 984      |
|    fps              | 162      |
|    time_elapsed     | 747      |
|    total_timesteps  | 121200   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0123   |
|    n_updates        | 30049    |
----------------------------------
Eval num_timesteps=121500, episode_reward=354.40 +/- 56.16
Episode length: 113.60 +/- 14.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 354      |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00647  |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 988      |
|    fps              | 161      |
|    time_elapsed     | 751      |
|    total_timesteps  | 121648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 30161    |
----------------------------------
Eval num_timesteps=122000, episode_reward=319.20 +/- 36.90
Episode length: 104.80 +/- 9.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.19     |
|    n_updates        | 30249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 992      |
|    fps              | 161      |
|    time_elapsed     | 754      |
|    total_timesteps  | 122096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00775  |
|    n_updates        | 30273    |
----------------------------------
Eval num_timesteps=122500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0087   |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 996      |
|    fps              | 161      |
|    time_elapsed     | 757      |
|    total_timesteps  | 122568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 30391    |
----------------------------------
Eval num_timesteps=123000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.955    |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 161      |
|    time_elapsed     | 760      |
|    total_timesteps  | 123168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 30541    |
----------------------------------
Eval num_timesteps=123500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.952    |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.33     |
|    n_updates        | 30624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 162      |
|    time_elapsed     | 763      |
|    total_timesteps  | 123664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 30665    |
----------------------------------
Eval num_timesteps=124000, episode_reward=396.00 +/- 117.53
Episode length: 124.00 +/- 29.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 396      |
| rollout/            |          |
|    exploration_rate | 0.95     |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 161      |
|    time_elapsed     | 767      |
|    total_timesteps  | 124144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 30785    |
----------------------------------
Eval num_timesteps=124500, episode_reward=294.24 +/- 59.50
Episode length: 98.56 +/- 14.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.6     |
|    mean_reward      | 294      |
| rollout/            |          |
|    exploration_rate | 0.948    |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0153   |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 161      |
|    time_elapsed     | 769      |
|    total_timesteps  | 124528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00828  |
|    n_updates        | 30881    |
----------------------------------
Eval num_timesteps=125000, episode_reward=308.96 +/- 35.22
Episode length: 102.24 +/- 8.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 309      |
| rollout/            |          |
|    exploration_rate | 0.946    |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00725  |
|    n_updates        | 30999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 161      |
|    time_elapsed     | 772      |
|    total_timesteps  | 125008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00821  |
|    n_updates        | 31001    |
----------------------------------
Eval num_timesteps=125500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.943    |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0103   |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 161      |
|    time_elapsed     | 775      |
|    total_timesteps  | 125576   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 31143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 162      |
|    time_elapsed     | 776      |
|    total_timesteps  | 125992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 31247    |
----------------------------------
Eval num_timesteps=126000, episode_reward=310.24 +/- 61.95
Episode length: 102.56 +/- 15.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0451   |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 162      |
|    time_elapsed     | 779      |
|    total_timesteps  | 126448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00963  |
|    n_updates        | 31361    |
----------------------------------
Eval num_timesteps=126500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.938    |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00654  |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 162      |
|    time_elapsed     | 782      |
|    total_timesteps  | 126928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 31481    |
----------------------------------
Eval num_timesteps=127000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.936    |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00587  |
|    n_updates        | 31499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 162      |
|    time_elapsed     | 785      |
|    total_timesteps  | 127392   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 31597    |
----------------------------------
Eval num_timesteps=127500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 162      |
|    time_elapsed     | 788      |
|    total_timesteps  | 127816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00822  |
|    n_updates        | 31703    |
----------------------------------
Eval num_timesteps=128000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0083   |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 162      |
|    time_elapsed     | 791      |
|    total_timesteps  | 128432   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.17     |
|    n_updates        | 31857    |
----------------------------------
Eval num_timesteps=128500, episode_reward=314.08 +/- 35.87
Episode length: 103.52 +/- 8.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 0.928    |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 31874    |
----------------------------------
Eval num_timesteps=129000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.926    |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 31999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 161      |
|    time_elapsed     | 796      |
|    total_timesteps  | 129024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.18     |
|    n_updates        | 32005    |
----------------------------------
Eval num_timesteps=129500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.923    |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.19     |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 161      |
|    time_elapsed     | 799      |
|    total_timesteps  | 129528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 32131    |
----------------------------------
Eval num_timesteps=130000, episode_reward=490.72 +/- 191.27
Episode length: 147.68 +/- 47.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 491      |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00557  |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 161      |
|    time_elapsed     | 804      |
|    total_timesteps  | 130104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0576   |
|    n_updates        | 32275    |
----------------------------------
Eval num_timesteps=130500, episode_reward=290.40 +/- 32.00
Episode length: 97.60 +/- 8.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.918    |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.42     |
|    n_updates        | 32374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 161      |
|    time_elapsed     | 807      |
|    total_timesteps  | 130688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0091   |
|    n_updates        | 32421    |
----------------------------------
Eval num_timesteps=131000, episode_reward=339.04 +/- 47.05
Episode length: 109.76 +/- 11.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 339      |
| rollout/            |          |
|    exploration_rate | 0.915    |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.24     |
|    n_updates        | 32499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 161      |
|    time_elapsed     | 810      |
|    total_timesteps  | 131224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 32555    |
----------------------------------
Eval num_timesteps=131500, episode_reward=313.44 +/- 37.23
Episode length: 103.36 +/- 9.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.913    |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0157   |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 161      |
|    time_elapsed     | 813      |
|    total_timesteps  | 131704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 32675    |
----------------------------------
Eval num_timesteps=132000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.91     |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00776  |
|    n_updates        | 32749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 161      |
|    time_elapsed     | 816      |
|    total_timesteps  | 132264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0142   |
|    n_updates        | 32815    |
----------------------------------
Eval num_timesteps=132500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.907    |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.009    |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 161      |
|    time_elapsed     | 819      |
|    total_timesteps  | 132728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.42     |
|    n_updates        | 32931    |
----------------------------------
Eval num_timesteps=133000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.904    |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.903    |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 161      |
|    time_elapsed     | 822      |
|    total_timesteps  | 133184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 33045    |
----------------------------------
Eval num_timesteps=133500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.901    |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.42     |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 161      |
|    time_elapsed     | 825      |
|    total_timesteps  | 133608   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00809  |
|    n_updates        | 33151    |
----------------------------------
Eval num_timesteps=134000, episode_reward=316.64 +/- 36.48
Episode length: 104.16 +/- 9.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.899    |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 33249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 161      |
|    time_elapsed     | 828      |
|    total_timesteps  | 134144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0153   |
|    n_updates        | 33285    |
----------------------------------
Eval num_timesteps=134500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 161      |
|    time_elapsed     | 831      |
|    total_timesteps  | 134624   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0339   |
|    n_updates        | 33405    |
----------------------------------
Eval num_timesteps=135000, episode_reward=291.04 +/- 22.44
Episode length: 97.76 +/- 5.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.8     |
|    mean_reward      | 291      |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 161      |
|    time_elapsed     | 834      |
|    total_timesteps  | 135056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.84     |
|    n_updates        | 33513    |
----------------------------------
Eval num_timesteps=135500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.89     |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00677  |
|    n_updates        | 33624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 161      |
|    time_elapsed     | 837      |
|    total_timesteps  | 135528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00883  |
|    n_updates        | 33631    |
----------------------------------
Eval num_timesteps=136000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.887    |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0202   |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 161      |
|    time_elapsed     | 840      |
|    total_timesteps  | 136032   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.23     |
|    n_updates        | 33757    |
----------------------------------
Eval num_timesteps=136500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 33874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.884    |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 161      |
|    time_elapsed     | 843      |
|    total_timesteps  | 136544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0082   |
|    n_updates        | 33885    |
----------------------------------
Eval num_timesteps=137000, episode_reward=321.12 +/- 38.06
Episode length: 105.28 +/- 9.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.881    |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.881    |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 161      |
|    time_elapsed     | 846      |
|    total_timesteps  | 137024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.42     |
|    n_updates        | 34005    |
----------------------------------
Eval num_timesteps=137500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.878    |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 161      |
|    time_elapsed     | 849      |
|    total_timesteps  | 137528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.42     |
|    n_updates        | 34131    |
----------------------------------
Eval num_timesteps=138000, episode_reward=333.92 +/- 67.17
Episode length: 108.48 +/- 16.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 334      |
| rollout/            |          |
|    exploration_rate | 0.875    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00848  |
|    n_updates        | 34249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 161      |
|    time_elapsed     | 852      |
|    total_timesteps  | 138000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 162      |
|    time_elapsed     | 853      |
|    total_timesteps  | 138456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0137   |
|    n_updates        | 34363    |
----------------------------------
Eval num_timesteps=138500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.872    |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.21     |
|    n_updates        | 34374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 162      |
|    time_elapsed     | 856      |
|    total_timesteps  | 138888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0113   |
|    n_updates        | 34471    |
----------------------------------
Eval num_timesteps=139000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.869    |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0123   |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 162      |
|    time_elapsed     | 858      |
|    total_timesteps  | 139272   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.22     |
|    n_updates        | 34567    |
----------------------------------
Eval num_timesteps=139500, episode_reward=458.08 +/- 234.55
Episode length: 139.52 +/- 58.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 458      |
| rollout/            |          |
|    exploration_rate | 0.865    |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.42     |
|    n_updates        | 34624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 161      |
|    time_elapsed     | 863      |
|    total_timesteps  | 139728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0128   |
|    n_updates        | 34681    |
----------------------------------
Eval num_timesteps=140000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.862    |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0178   |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 161      |
|    time_elapsed     | 866      |
|    total_timesteps  | 140144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0412   |
|    n_updates        | 34785    |
----------------------------------
Eval num_timesteps=140500, episode_reward=349.28 +/- 71.26
Episode length: 112.32 +/- 17.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 349      |
| rollout/            |          |
|    exploration_rate | 0.859    |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 34874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 161      |
|    time_elapsed     | 869      |
|    total_timesteps  | 140672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 34917    |
----------------------------------
Eval num_timesteps=141000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.856    |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.25     |
|    n_updates        | 34999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 161      |
|    time_elapsed     | 872      |
|    total_timesteps  | 141136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 35033    |
----------------------------------
Eval num_timesteps=141500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.853    |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 161      |
|    time_elapsed     | 875      |
|    total_timesteps  | 141568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 35141    |
----------------------------------
Eval num_timesteps=142000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.849    |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 35249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 161      |
|    time_elapsed     | 878      |
|    total_timesteps  | 142080   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00641  |
|    n_updates        | 35269    |
----------------------------------
Eval num_timesteps=142500, episode_reward=335.84 +/- 49.94
Episode length: 108.96 +/- 12.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 161      |
|    time_elapsed     | 881      |
|    total_timesteps  | 142512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 35377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 162      |
|    time_elapsed     | 881      |
|    total_timesteps  | 142984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 35495    |
----------------------------------
Eval num_timesteps=143000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 35499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 162      |
|    time_elapsed     | 884      |
|    total_timesteps  | 143464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0119   |
|    n_updates        | 35615    |
----------------------------------
Eval num_timesteps=143500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.839    |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 162      |
|    time_elapsed     | 887      |
|    total_timesteps  | 143920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 35729    |
----------------------------------
Eval num_timesteps=144000, episode_reward=287.84 +/- 26.88
Episode length: 96.96 +/- 6.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 162      |
|    time_elapsed     | 890      |
|    total_timesteps  | 144400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.25     |
|    n_updates        | 35849    |
----------------------------------
Eval num_timesteps=144500, episode_reward=300.00 +/- 44.91
Episode length: 100.00 +/- 11.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 300      |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.5      |
|    n_updates        | 35874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 162      |
|    time_elapsed     | 893      |
|    total_timesteps  | 144816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 35953    |
----------------------------------
Eval num_timesteps=145000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.829    |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 35999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 162      |
|    time_elapsed     | 896      |
|    total_timesteps  | 145264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 36065    |
----------------------------------
Eval num_timesteps=145500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.826    |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0142   |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 162      |
|    time_elapsed     | 899      |
|    total_timesteps  | 145784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00859  |
|    n_updates        | 36195    |
----------------------------------
Eval num_timesteps=146000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.822    |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00958  |
|    n_updates        | 36249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 162      |
|    time_elapsed     | 902      |
|    total_timesteps  | 146312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00798  |
|    n_updates        | 36327    |
----------------------------------
Eval num_timesteps=146500, episode_reward=378.08 +/- 144.59
Episode length: 119.52 +/- 36.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 378      |
| rollout/            |          |
|    exploration_rate | 0.819    |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00909  |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 162      |
|    time_elapsed     | 906      |
|    total_timesteps  | 146816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.27     |
|    n_updates        | 36453    |
----------------------------------
Eval num_timesteps=147000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 36499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.813    |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 162      |
|    time_elapsed     | 909      |
|    total_timesteps  | 147384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 36595    |
----------------------------------
Eval num_timesteps=147500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.812    |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 36624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.81     |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 162      |
|    time_elapsed     | 912      |
|    total_timesteps  | 147824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 36705    |
----------------------------------
Eval num_timesteps=148000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.808    |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 162      |
|    time_elapsed     | 915      |
|    total_timesteps  | 148296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 36823    |
----------------------------------
Eval num_timesteps=148500, episode_reward=504.80 +/- 254.01
Episode length: 151.20 +/- 63.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 505      |
| rollout/            |          |
|    exploration_rate | 0.805    |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.27     |
|    n_updates        | 36874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 161      |
|    time_elapsed     | 919      |
|    total_timesteps  | 148712   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 36927    |
----------------------------------
Eval num_timesteps=149000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.801    |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0079   |
|    n_updates        | 36999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 161      |
|    time_elapsed     | 922      |
|    total_timesteps  | 149256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0125   |
|    n_updates        | 37063    |
----------------------------------
Eval num_timesteps=149500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.25     |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.797    |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 161      |
|    time_elapsed     | 925      |
|    total_timesteps  | 149640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 37159    |
----------------------------------
Eval num_timesteps=150000, episode_reward=429.92 +/- 185.29
Episode length: 132.48 +/- 46.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 430      |
| rollout/            |          |
|    exploration_rate | 0.794    |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.26     |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 161      |
|    time_elapsed     | 929      |
|    total_timesteps  | 150048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5        |
|    n_updates        | 37261    |
----------------------------------
Eval num_timesteps=150500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0236   |
|    n_updates        | 37374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 161      |
|    time_elapsed     | 932      |
|    total_timesteps  | 150704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 37425    |
----------------------------------
Eval num_timesteps=151000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.787    |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 161      |
|    time_elapsed     | 935      |
|    total_timesteps  | 151168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 37541    |
----------------------------------
Eval num_timesteps=151500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.783    |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.782    |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 161      |
|    time_elapsed     | 938      |
|    total_timesteps  | 151640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0303   |
|    n_updates        | 37659    |
----------------------------------
Eval num_timesteps=152000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 37749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 161      |
|    time_elapsed     | 941      |
|    total_timesteps  | 152144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 37785    |
----------------------------------
Eval num_timesteps=152500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.776    |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 161      |
|    time_elapsed     | 944      |
|    total_timesteps  | 152560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0103   |
|    n_updates        | 37889    |
----------------------------------
Eval num_timesteps=153000, episode_reward=289.76 +/- 29.81
Episode length: 97.44 +/- 7.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.4     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.772    |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 37999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.771    |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 161      |
|    time_elapsed     | 947      |
|    total_timesteps  | 153096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.31     |
|    n_updates        | 38023    |
----------------------------------
Eval num_timesteps=153500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.768    |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00857  |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.767    |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 161      |
|    time_elapsed     | 950      |
|    total_timesteps  | 153648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 38161    |
----------------------------------
Eval num_timesteps=154000, episode_reward=312.80 +/- 35.20
Episode length: 103.20 +/- 8.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.764    |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.29     |
|    n_updates        | 38249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 161      |
|    time_elapsed     | 953      |
|    total_timesteps  | 154152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00838  |
|    n_updates        | 38287    |
----------------------------------
Eval num_timesteps=154500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.761    |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 38374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 161      |
|    time_elapsed     | 956      |
|    total_timesteps  | 154680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.29     |
|    n_updates        | 38419    |
----------------------------------
Eval num_timesteps=155000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.757    |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0137   |
|    n_updates        | 38499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.755    |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 161      |
|    time_elapsed     | 959      |
|    total_timesteps  | 155176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0119   |
|    n_updates        | 38543    |
----------------------------------
Eval num_timesteps=155500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.59     |
|    n_updates        | 38624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 161      |
|    time_elapsed     | 962      |
|    total_timesteps  | 155744   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 38685    |
----------------------------------
Eval num_timesteps=156000, episode_reward=310.88 +/- 34.68
Episode length: 102.72 +/- 8.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 311      |
| rollout/            |          |
|    exploration_rate | 0.749    |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 161      |
|    time_elapsed     | 965      |
|    total_timesteps  | 156136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 38783    |
----------------------------------
Eval num_timesteps=156500, episode_reward=308.32 +/- 37.62
Episode length: 102.08 +/- 9.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.745    |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 38874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 161      |
|    time_elapsed     | 968      |
|    total_timesteps  | 156696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.59     |
|    n_updates        | 38923    |
----------------------------------
Eval num_timesteps=157000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.741    |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00996  |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 161      |
|    time_elapsed     | 971      |
|    total_timesteps  | 157208   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0172   |
|    n_updates        | 39051    |
----------------------------------
Eval num_timesteps=157500, episode_reward=289.76 +/- 29.81
Episode length: 97.44 +/- 7.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.4     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.737    |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00828  |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 161      |
|    time_elapsed     | 974      |
|    total_timesteps  | 157616   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.31     |
|    n_updates        | 39153    |
----------------------------------
Eval num_timesteps=158000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.733    |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.29     |
|    n_updates        | 39249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.732    |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 161      |
|    time_elapsed     | 977      |
|    total_timesteps  | 158120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.011    |
|    n_updates        | 39279    |
----------------------------------
Eval num_timesteps=158500, episode_reward=349.28 +/- 65.25
Episode length: 112.32 +/- 16.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 349      |
| rollout/            |          |
|    exploration_rate | 0.729    |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00843  |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.728    |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 161      |
|    time_elapsed     | 980      |
|    total_timesteps  | 158632   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00955  |
|    n_updates        | 39407    |
----------------------------------
Eval num_timesteps=159000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.725    |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 39499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.724    |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 161      |
|    time_elapsed     | 983      |
|    total_timesteps  | 159152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00931  |
|    n_updates        | 39537    |
----------------------------------
Eval num_timesteps=159500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.721    |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.3      |
|    n_updates        | 39624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.72     |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 161      |
|    time_elapsed     | 986      |
|    total_timesteps  | 159632   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.31     |
|    n_updates        | 39657    |
----------------------------------
Eval num_timesteps=160000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.717    |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.01     |
|    n_updates        | 39749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 161      |
|    time_elapsed     | 989      |
|    total_timesteps  | 160088   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 39771    |
----------------------------------
Eval num_timesteps=160500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.713    |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 39874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 161      |
|    time_elapsed     | 992      |
|    total_timesteps  | 160592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00631  |
|    n_updates        | 39897    |
----------------------------------
Eval num_timesteps=161000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.709    |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 161      |
|    time_elapsed     | 995      |
|    total_timesteps  | 161064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0206   |
|    n_updates        | 40015    |
----------------------------------
Eval num_timesteps=161500, episode_reward=317.92 +/- 38.08
Episode length: 104.48 +/- 9.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 318      |
| rollout/            |          |
|    exploration_rate | 0.705    |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00841  |
|    n_updates        | 40124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 161      |
|    time_elapsed     | 998      |
|    total_timesteps  | 161520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.012    |
|    n_updates        | 40129    |
----------------------------------
Eval num_timesteps=162000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.701    |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.7      |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 161      |
|    time_elapsed     | 1001     |
|    total_timesteps  | 162064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 40265    |
----------------------------------
Eval num_timesteps=162500, episode_reward=338.40 +/- 63.44
Episode length: 109.60 +/- 15.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 338      |
| rollout/            |          |
|    exploration_rate | 0.697    |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 40374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.696    |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 161      |
|    time_elapsed     | 1005     |
|    total_timesteps  | 162560   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0138   |
|    n_updates        | 40389    |
----------------------------------
Eval num_timesteps=163000, episode_reward=289.12 +/- 28.16
Episode length: 97.28 +/- 7.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 289      |
| rollout/            |          |
|    exploration_rate | 0.693    |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0096   |
|    n_updates        | 40499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 161      |
|    time_elapsed     | 1008     |
|    total_timesteps  | 163104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 40525    |
----------------------------------
Eval num_timesteps=163500, episode_reward=312.16 +/- 66.09
Episode length: 103.04 +/- 16.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.689    |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 40624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.687    |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 161      |
|    time_elapsed     | 1011     |
|    total_timesteps  | 163704   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.34     |
|    n_updates        | 40675    |
----------------------------------
Eval num_timesteps=164000, episode_reward=339.04 +/- 64.33
Episode length: 109.76 +/- 16.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 339      |
| rollout/            |          |
|    exploration_rate | 0.684    |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00929  |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.683    |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 161      |
|    time_elapsed     | 1014     |
|    total_timesteps  | 164200   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 40799    |
----------------------------------
Eval num_timesteps=164500, episode_reward=338.40 +/- 62.79
Episode length: 109.60 +/- 15.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 338      |
| rollout/            |          |
|    exploration_rate | 0.68     |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.32     |
|    n_updates        | 40874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.678    |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 161      |
|    time_elapsed     | 1018     |
|    total_timesteps  | 164728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00816  |
|    n_updates        | 40931    |
----------------------------------
Eval num_timesteps=165000, episode_reward=335.84 +/- 45.66
Episode length: 108.96 +/- 11.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 336      |
| rollout/            |          |
|    exploration_rate | 0.676    |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.65     |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 161      |
|    time_elapsed     | 1021     |
|    total_timesteps  | 165232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00859  |
|    n_updates        | 41057    |
----------------------------------
Eval num_timesteps=165500, episode_reward=323.04 +/- 88.98
Episode length: 105.76 +/- 22.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 323      |
| rollout/            |          |
|    exploration_rate | 0.672    |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 41124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.671    |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 161      |
|    time_elapsed     | 1024     |
|    total_timesteps  | 165640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 41159    |
----------------------------------
Eval num_timesteps=166000, episode_reward=309.60 +/- 35.63
Episode length: 102.40 +/- 8.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.668    |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00947  |
|    n_updates        | 41249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.667    |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 161      |
|    time_elapsed     | 1027     |
|    total_timesteps  | 166112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 41277    |
----------------------------------
Eval num_timesteps=166500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.663    |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 161      |
|    time_elapsed     | 1030     |
|    total_timesteps  | 166592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 41397    |
----------------------------------
Eval num_timesteps=167000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.659    |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0138   |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.659    |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 161      |
|    time_elapsed     | 1033     |
|    total_timesteps  | 167024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00831  |
|    n_updates        | 41505    |
----------------------------------
Eval num_timesteps=167500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.655    |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00774  |
|    n_updates        | 41624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.654    |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 161      |
|    time_elapsed     | 1036     |
|    total_timesteps  | 167552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 41637    |
----------------------------------
Eval num_timesteps=168000, episode_reward=319.20 +/- 39.58
Episode length: 104.80 +/- 9.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 319      |
| rollout/            |          |
|    exploration_rate | 0.65     |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0166   |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.649    |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 161      |
|    time_elapsed     | 1039     |
|    total_timesteps  | 168160   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.34     |
|    n_updates        | 41789    |
----------------------------------
Eval num_timesteps=168500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.646    |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.645    |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 161      |
|    time_elapsed     | 1042     |
|    total_timesteps  | 168640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00943  |
|    n_updates        | 41909    |
----------------------------------
Eval num_timesteps=169000, episode_reward=288.48 +/- 22.18
Episode length: 97.12 +/- 5.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.1     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.642    |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0168   |
|    n_updates        | 41999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 161      |
|    time_elapsed     | 1045     |
|    total_timesteps  | 169120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.65     |
|    n_updates        | 42029    |
----------------------------------
Eval num_timesteps=169500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.637    |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.637    |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 161      |
|    time_elapsed     | 1048     |
|    total_timesteps  | 169528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.33     |
|    n_updates        | 42131    |
----------------------------------
Eval num_timesteps=170000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.633    |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.633    |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 161      |
|    time_elapsed     | 1051     |
|    total_timesteps  | 170016   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.31     |
|    n_updates        | 42253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.628    |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 162      |
|    time_elapsed     | 1052     |
|    total_timesteps  | 170496   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.36     |
|    n_updates        | 42373    |
----------------------------------
Eval num_timesteps=170500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.628    |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00732  |
|    n_updates        | 42374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.624    |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 162      |
|    time_elapsed     | 1055     |
|    total_timesteps  | 170992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.71     |
|    n_updates        | 42497    |
----------------------------------
Eval num_timesteps=171000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.624    |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.62     |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 162      |
|    time_elapsed     | 1058     |
|    total_timesteps  | 171472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.36     |
|    n_updates        | 42617    |
----------------------------------
Eval num_timesteps=171500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.37     |
|    n_updates        | 42624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.616    |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 162      |
|    time_elapsed     | 1061     |
|    total_timesteps  | 171928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 42731    |
----------------------------------
Eval num_timesteps=172000, episode_reward=317.28 +/- 39.95
Episode length: 104.32 +/- 9.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.615    |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.611    |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 161      |
|    time_elapsed     | 1064     |
|    total_timesteps  | 172400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 42849    |
----------------------------------
Eval num_timesteps=172500, episode_reward=315.36 +/- 35.91
Episode length: 103.84 +/- 8.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.611    |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 42874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 161      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 172856   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 42963    |
----------------------------------
Eval num_timesteps=173000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.606    |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.603    |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 161      |
|    time_elapsed     | 1070     |
|    total_timesteps  | 173336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0143   |
|    n_updates        | 43083    |
----------------------------------
Eval num_timesteps=173500, episode_reward=332.00 +/- 81.52
Episode length: 108.00 +/- 20.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 332      |
| rollout/            |          |
|    exploration_rate | 0.602    |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0143   |
|    n_updates        | 43124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.598    |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 161      |
|    time_elapsed     | 1073     |
|    total_timesteps  | 173840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.39     |
|    n_updates        | 43209    |
----------------------------------
Eval num_timesteps=174000, episode_reward=314.72 +/- 43.39
Episode length: 103.68 +/- 10.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.597    |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 43249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.595    |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 161      |
|    time_elapsed     | 1076     |
|    total_timesteps  | 174272   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.72     |
|    n_updates        | 43317    |
----------------------------------
Eval num_timesteps=174500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.592    |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.59     |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 161      |
|    time_elapsed     | 1079     |
|    total_timesteps  | 174800   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0132   |
|    n_updates        | 43449    |
----------------------------------
Eval num_timesteps=175000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.588    |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.37     |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.585    |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 161      |
|    time_elapsed     | 1082     |
|    total_timesteps  | 175288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 43571    |
----------------------------------
Eval num_timesteps=175500, episode_reward=468.96 +/- 145.56
Episode length: 142.24 +/- 36.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 142      |
|    mean_reward      | 469      |
| rollout/            |          |
|    exploration_rate | 0.583    |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.36     |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.58     |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 161      |
|    time_elapsed     | 1087     |
|    total_timesteps  | 175808   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.37     |
|    n_updates        | 43701    |
----------------------------------
Eval num_timesteps=176000, episode_reward=295.52 +/- 36.63
Episode length: 98.88 +/- 9.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.9     |
|    mean_reward      | 296      |
| rollout/            |          |
|    exploration_rate | 0.579    |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0181   |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 161      |
|    time_elapsed     | 1090     |
|    total_timesteps  | 176264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0157   |
|    n_updates        | 43815    |
----------------------------------
Eval num_timesteps=176500, episode_reward=288.48 +/- 19.21
Episode length: 97.12 +/- 4.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.1     |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.574    |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.72     |
|    n_updates        | 43874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.571    |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 161      |
|    time_elapsed     | 1093     |
|    total_timesteps  | 176824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 43955    |
----------------------------------
Eval num_timesteps=177000, episode_reward=594.88 +/- 309.61
Episode length: 173.22 +/- 75.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 173      |
|    mean_reward      | 595      |
| rollout/            |          |
|    exploration_rate | 0.569    |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.72     |
|    n_updates        | 43999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.567    |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 161      |
|    time_elapsed     | 1098     |
|    total_timesteps  | 177280   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.35     |
|    n_updates        | 44069    |
----------------------------------
Eval num_timesteps=177500, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.565    |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0106   |
|    n_updates        | 44124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.563    |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 161      |
|    time_elapsed     | 1101     |
|    total_timesteps  | 177728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00693  |
|    n_updates        | 44181    |
----------------------------------
Eval num_timesteps=178000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.56     |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0222   |
|    n_updates        | 44249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.558    |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 161      |
|    time_elapsed     | 1104     |
|    total_timesteps  | 178208   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 44301    |
----------------------------------
Eval num_timesteps=178500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.555    |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.39     |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.554    |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 161      |
|    time_elapsed     | 1107     |
|    total_timesteps  | 178648   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.37     |
|    n_updates        | 44411    |
----------------------------------
Eval num_timesteps=179000, episode_reward=326.24 +/- 54.47
Episode length: 106.56 +/- 13.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.551    |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0128   |
|    n_updates        | 44499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.55     |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 161      |
|    time_elapsed     | 1110     |
|    total_timesteps  | 179104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.71     |
|    n_updates        | 44525    |
----------------------------------
Eval num_timesteps=179500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.546    |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00852  |
|    n_updates        | 44624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 161      |
|    time_elapsed     | 1113     |
|    total_timesteps  | 179520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.017    |
|    n_updates        | 44629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.542    |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 161      |
|    time_elapsed     | 1113     |
|    total_timesteps  | 179952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0114   |
|    n_updates        | 44737    |
----------------------------------
Eval num_timesteps=180000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.541    |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0193   |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.537    |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 161      |
|    time_elapsed     | 1116     |
|    total_timesteps  | 180456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0213   |
|    n_updates        | 44863    |
----------------------------------
Eval num_timesteps=180500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.537    |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.41     |
|    n_updates        | 44874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.532    |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 161      |
|    time_elapsed     | 1119     |
|    total_timesteps  | 180928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 44981    |
----------------------------------
Eval num_timesteps=181000, episode_reward=319.84 +/- 41.30
Episode length: 104.96 +/- 10.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.532    |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 44999    |
----------------------------------
Eval num_timesteps=181500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.527    |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0119   |
|    n_updates        | 45124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.527    |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 161      |
|    time_elapsed     | 1125     |
|    total_timesteps  | 181520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.39     |
|    n_updates        | 45129    |
----------------------------------
Eval num_timesteps=182000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.522    |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0104   |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.522    |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 161      |
|    time_elapsed     | 1128     |
|    total_timesteps  | 182040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0158   |
|    n_updates        | 45259    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.517    |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 161      |
|    time_elapsed     | 1128     |
|    total_timesteps  | 182488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 45371    |
----------------------------------
Eval num_timesteps=182500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.517    |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0177   |
|    n_updates        | 45374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.513    |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 161      |
|    time_elapsed     | 1131     |
|    total_timesteps  | 182920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0108   |
|    n_updates        | 45479    |
----------------------------------
Eval num_timesteps=183000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.513    |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0098   |
|    n_updates        | 45499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.508    |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 161      |
|    time_elapsed     | 1134     |
|    total_timesteps  | 183424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.78     |
|    n_updates        | 45605    |
----------------------------------
Eval num_timesteps=183500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.508    |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.504    |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 161      |
|    time_elapsed     | 1137     |
|    total_timesteps  | 183920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 45729    |
----------------------------------
Eval num_timesteps=184000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.503    |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.019    |
|    n_updates        | 45749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.498    |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 161      |
|    time_elapsed     | 1140     |
|    total_timesteps  | 184488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 45871    |
----------------------------------
Eval num_timesteps=184500, episode_reward=325.60 +/- 55.15
Episode length: 106.40 +/- 13.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.498    |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.39     |
|    n_updates        | 45874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.494    |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 161      |
|    time_elapsed     | 1144     |
|    total_timesteps  | 184920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0128   |
|    n_updates        | 45979    |
----------------------------------
Eval num_timesteps=185000, episode_reward=370.40 +/- 110.71
Episode length: 117.60 +/- 27.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 370      |
| rollout/            |          |
|    exploration_rate | 0.493    |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.77     |
|    n_updates        | 45999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.489    |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 161      |
|    time_elapsed     | 1147     |
|    total_timesteps  | 185392   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.39     |
|    n_updates        | 46097    |
----------------------------------
Eval num_timesteps=185500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.488    |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 46124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.484    |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 161      |
|    time_elapsed     | 1150     |
|    total_timesteps  | 185928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 46231    |
----------------------------------
Eval num_timesteps=186000, episode_reward=358.88 +/- 96.98
Episode length: 114.72 +/- 24.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 359      |
| rollout/            |          |
|    exploration_rate | 0.483    |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.479    |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 161      |
|    time_elapsed     | 1154     |
|    total_timesteps  | 186456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0249   |
|    n_updates        | 46363    |
----------------------------------
Eval num_timesteps=186500, episode_reward=307.68 +/- 52.70
Episode length: 101.92 +/- 13.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.478    |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.473    |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 161      |
|    time_elapsed     | 1157     |
|    total_timesteps  | 186984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 46495    |
----------------------------------
Eval num_timesteps=187000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.473    |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 46499    |
----------------------------------
Eval num_timesteps=187500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.468    |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.79     |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.468    |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 161      |
|    time_elapsed     | 1163     |
|    total_timesteps  | 187504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 46625    |
----------------------------------
Eval num_timesteps=188000, episode_reward=324.96 +/- 43.43
Episode length: 106.24 +/- 10.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 325      |
| rollout/            |          |
|    exploration_rate | 0.463    |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 46749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.463    |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 161      |
|    time_elapsed     | 1166     |
|    total_timesteps  | 188056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0149   |
|    n_updates        | 46763    |
----------------------------------
Eval num_timesteps=188500, episode_reward=312.16 +/- 34.84
Episode length: 103.04 +/- 8.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.458    |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.457    |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 161      |
|    time_elapsed     | 1169     |
|    total_timesteps  | 188592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.41     |
|    n_updates        | 46897    |
----------------------------------
Eval num_timesteps=189000, episode_reward=309.60 +/- 34.47
Episode length: 102.40 +/- 8.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.453    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.4      |
|    n_updates        | 46999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.453    |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 161      |
|    time_elapsed     | 1172     |
|    total_timesteps  | 189024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0167   |
|    n_updates        | 47005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.448    |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 161      |
|    time_elapsed     | 1173     |
|    total_timesteps  | 189488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0197   |
|    n_updates        | 47121    |
----------------------------------
Eval num_timesteps=189500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.448    |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0178   |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.444    |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 161      |
|    time_elapsed     | 1176     |
|    total_timesteps  | 189944   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 47235    |
----------------------------------
Eval num_timesteps=190000, episode_reward=320.48 +/- 37.32
Episode length: 105.12 +/- 9.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.443    |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0194   |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.439    |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 161      |
|    time_elapsed     | 1179     |
|    total_timesteps  | 190384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0261   |
|    n_updates        | 47345    |
----------------------------------
Eval num_timesteps=190500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.438    |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0187   |
|    n_updates        | 47374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.435    |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 161      |
|    time_elapsed     | 1182     |
|    total_timesteps  | 190824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 47455    |
----------------------------------
Eval num_timesteps=191000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.433    |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0199   |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 161      |
|    time_elapsed     | 1185     |
|    total_timesteps  | 191368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 47591    |
----------------------------------
Eval num_timesteps=191500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.428    |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 47624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.425    |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 161      |
|    time_elapsed     | 1188     |
|    total_timesteps  | 191824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.42     |
|    n_updates        | 47705    |
----------------------------------
Eval num_timesteps=192000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.423    |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0183   |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.42     |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 161      |
|    time_elapsed     | 1191     |
|    total_timesteps  | 192304   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 47825    |
----------------------------------
Eval num_timesteps=192500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.418    |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 47874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.415    |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 161      |
|    time_elapsed     | 1194     |
|    total_timesteps  | 192760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0807   |
|    n_updates        | 47939    |
----------------------------------
Eval num_timesteps=193000, episode_reward=371.04 +/- 224.00
Episode length: 117.76 +/- 56.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 371      |
| rollout/            |          |
|    exploration_rate | 0.413    |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0233   |
|    n_updates        | 47999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.411    |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 161      |
|    time_elapsed     | 1197     |
|    total_timesteps  | 193192   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 48047    |
----------------------------------
Eval num_timesteps=193500, episode_reward=305.76 +/- 32.91
Episode length: 101.44 +/- 8.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 306      |
| rollout/            |          |
|    exploration_rate | 0.407    |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.406    |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 161      |
|    time_elapsed     | 1200     |
|    total_timesteps  | 193600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.84     |
|    n_updates        | 48149    |
----------------------------------
Eval num_timesteps=194000, episode_reward=328.80 +/- 57.60
Episode length: 107.20 +/- 14.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 329      |
| rollout/            |          |
|    exploration_rate | 0.402    |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.84     |
|    n_updates        | 48249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.401    |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 161      |
|    time_elapsed     | 1204     |
|    total_timesteps  | 194080   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 48269    |
----------------------------------
Eval num_timesteps=194500, episode_reward=312.80 +/- 34.61
Episode length: 103.20 +/- 8.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.397    |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.84     |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.397    |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 161      |
|    time_elapsed     | 1207     |
|    total_timesteps  | 194520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00927  |
|    n_updates        | 48379    |
----------------------------------
Eval num_timesteps=195000, episode_reward=715.04 +/- 445.23
Episode length: 202.76 +/- 108.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 715      |
| rollout/            |          |
|    exploration_rate | 0.392    |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.44     |
|    n_updates        | 48499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.391    |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 160      |
|    time_elapsed     | 1213     |
|    total_timesteps  | 195040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0259   |
|    n_updates        | 48509    |
----------------------------------
Eval num_timesteps=195500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.387    |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.44     |
|    n_updates        | 48624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.387    |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 160      |
|    time_elapsed     | 1216     |
|    total_timesteps  | 195504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.45     |
|    n_updates        | 48625    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.382    |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 161      |
|    time_elapsed     | 1216     |
|    total_timesteps  | 195944   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0211   |
|    n_updates        | 48735    |
----------------------------------
Eval num_timesteps=196000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.381    |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.42     |
|    n_updates        | 48749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.377    |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 161      |
|    time_elapsed     | 1219     |
|    total_timesteps  | 196448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 48861    |
----------------------------------
Eval num_timesteps=196500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.376    |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 48874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.371    |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 161      |
|    time_elapsed     | 1222     |
|    total_timesteps  | 196984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.009    |
|    n_updates        | 48995    |
----------------------------------
Eval num_timesteps=197000, episode_reward=308.32 +/- 32.98
Episode length: 102.08 +/- 8.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.371    |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.84     |
|    n_updates        | 48999    |
----------------------------------
Eval num_timesteps=197500, episode_reward=493.28 +/- 232.37
Episode length: 148.32 +/- 58.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 493      |
| rollout/            |          |
|    exploration_rate | 0.366    |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 49124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.365    |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 160      |
|    time_elapsed     | 1230     |
|    total_timesteps  | 197544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 49135    |
----------------------------------
Eval num_timesteps=198000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.36     |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 49249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.36     |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 160      |
|    time_elapsed     | 1233     |
|    total_timesteps  | 198048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 49261    |
----------------------------------
Eval num_timesteps=198500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.355    |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.44     |
|    n_updates        | 49374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.355    |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 160      |
|    time_elapsed     | 1236     |
|    total_timesteps  | 198504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0215   |
|    n_updates        | 49375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.35     |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 160      |
|    time_elapsed     | 1236     |
|    total_timesteps  | 198952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00811  |
|    n_updates        | 49487    |
----------------------------------
Eval num_timesteps=199000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.35     |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.43     |
|    n_updates        | 49499    |
----------------------------------
Eval num_timesteps=199500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.344    |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.344    |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 160      |
|    time_elapsed     | 1242     |
|    total_timesteps  | 199520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.018    |
|    n_updates        | 49629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.339    |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 160      |
|    time_elapsed     | 1242     |
|    total_timesteps  | 199976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 49743    |
----------------------------------
Eval num_timesteps=200000, episode_reward=308.32 +/- 33.60
Episode length: 102.08 +/- 8.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.339    |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.03     |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.335    |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 160      |
|    time_elapsed     | 1245     |
|    total_timesteps  | 200384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.9      |
|    n_updates        | 49845    |
----------------------------------
Eval num_timesteps=200500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.334    |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.011    |
|    n_updates        | 49874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.33     |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 160      |
|    time_elapsed     | 1248     |
|    total_timesteps  | 200840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.47     |
|    n_updates        | 49959    |
----------------------------------
Eval num_timesteps=201000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.328    |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.325    |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 160      |
|    time_elapsed     | 1251     |
|    total_timesteps  | 201296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0221   |
|    n_updates        | 50073    |
----------------------------------
Eval num_timesteps=201500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.323    |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 50124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.32     |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 160      |
|    time_elapsed     | 1254     |
|    total_timesteps  | 201752   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 50187    |
----------------------------------
Eval num_timesteps=202000, episode_reward=309.60 +/- 35.05
Episode length: 102.40 +/- 8.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.317    |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0228   |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.315    |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 160      |
|    time_elapsed     | 1257     |
|    total_timesteps  | 202240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.47     |
|    n_updates        | 50309    |
----------------------------------
Eval num_timesteps=202500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.312    |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0201   |
|    n_updates        | 50374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.309    |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 160      |
|    time_elapsed     | 1260     |
|    total_timesteps  | 202744   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 50435    |
----------------------------------
Eval num_timesteps=203000, episode_reward=332.64 +/- 58.74
Episode length: 108.16 +/- 14.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 333      |
| rollout/            |          |
|    exploration_rate | 0.307    |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.91     |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.304    |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 160      |
|    time_elapsed     | 1264     |
|    total_timesteps  | 203224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0143   |
|    n_updates        | 50555    |
----------------------------------
Eval num_timesteps=203500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.301    |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.299    |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 160      |
|    time_elapsed     | 1267     |
|    total_timesteps  | 203656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.45     |
|    n_updates        | 50663    |
----------------------------------
Eval num_timesteps=204000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.296    |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 50749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.295    |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 160      |
|    time_elapsed     | 1270     |
|    total_timesteps  | 204064   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.91     |
|    n_updates        | 50765    |
----------------------------------
Eval num_timesteps=204500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.29     |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.289    |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 160      |
|    time_elapsed     | 1273     |
|    total_timesteps  | 204592   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0133   |
|    n_updates        | 50897    |
----------------------------------
Eval num_timesteps=205000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.019    |
|    n_updates        | 50999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.283    |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 160      |
|    time_elapsed     | 1276     |
|    total_timesteps  | 205168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0146   |
|    n_updates        | 51041    |
----------------------------------
Eval num_timesteps=205500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.279    |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 51124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.278    |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 160      |
|    time_elapsed     | 1279     |
|    total_timesteps  | 205624   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 51155    |
----------------------------------
Eval num_timesteps=206000, episode_reward=315.36 +/- 38.13
Episode length: 103.84 +/- 9.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.274    |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.273    |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 160      |
|    time_elapsed     | 1282     |
|    total_timesteps  | 206080   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 51269    |
----------------------------------
Eval num_timesteps=206500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.268    |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0229   |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.267    |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 160      |
|    time_elapsed     | 1285     |
|    total_timesteps  | 206584   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00793  |
|    n_updates        | 51395    |
----------------------------------
Eval num_timesteps=207000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.263    |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0236   |
|    n_updates        | 51499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.262    |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 160      |
|    time_elapsed     | 1288     |
|    total_timesteps  | 207072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00905  |
|    n_updates        | 51517    |
----------------------------------
Eval num_timesteps=207500, episode_reward=320.48 +/- 42.46
Episode length: 105.12 +/- 10.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 320      |
| rollout/            |          |
|    exploration_rate | 0.257    |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.45     |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.255    |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 160      |
|    time_elapsed     | 1291     |
|    total_timesteps  | 207672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 51667    |
----------------------------------
Eval num_timesteps=208000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.251    |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 51749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 160      |
|    time_elapsed     | 1294     |
|    total_timesteps  | 208144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.014    |
|    n_updates        | 51785    |
----------------------------------
Eval num_timesteps=208500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.246    |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00916  |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 160      |
|    time_elapsed     | 1297     |
|    total_timesteps  | 208528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0073   |
|    n_updates        | 51881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.241    |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 160      |
|    time_elapsed     | 1298     |
|    total_timesteps  | 208960   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 51989    |
----------------------------------
Eval num_timesteps=209000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.24     |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0137   |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.236    |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 160      |
|    time_elapsed     | 1301     |
|    total_timesteps  | 209392   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 9.8      |
|    n_updates        | 52097    |
----------------------------------
Eval num_timesteps=209500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.235    |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.46     |
|    n_updates        | 52124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.23     |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 160      |
|    time_elapsed     | 1304     |
|    total_timesteps  | 209928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 52231    |
----------------------------------
Eval num_timesteps=210000, episode_reward=312.16 +/- 41.79
Episode length: 103.04 +/- 10.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.229    |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0413   |
|    n_updates        | 52249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.223    |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 160      |
|    time_elapsed     | 1307     |
|    total_timesteps  | 210488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 52371    |
----------------------------------
Eval num_timesteps=210500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.223    |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.97     |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.218    |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 160      |
|    time_elapsed     | 1310     |
|    total_timesteps  | 210968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.96     |
|    n_updates        | 52491    |
----------------------------------
Eval num_timesteps=211000, episode_reward=312.80 +/- 35.20
Episode length: 103.20 +/- 8.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.218    |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0233   |
|    n_updates        | 52499    |
----------------------------------
Eval num_timesteps=211500, episode_reward=356.96 +/- 80.96
Episode length: 114.24 +/- 20.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 357      |
| rollout/            |          |
|    exploration_rate | 0.212    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.48     |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.212    |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 160      |
|    time_elapsed     | 1316     |
|    total_timesteps  | 211520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.96     |
|    n_updates        | 52629    |
----------------------------------
Eval num_timesteps=212000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.206    |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0146   |
|    n_updates        | 52749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.206    |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 160      |
|    time_elapsed     | 1319     |
|    total_timesteps  | 212040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0219   |
|    n_updates        | 52759    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.201    |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 160      |
|    time_elapsed     | 1320     |
|    total_timesteps  | 212488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 52871    |
----------------------------------
Eval num_timesteps=212500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.201    |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.96     |
|    n_updates        | 52874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.195    |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 160      |
|    time_elapsed     | 1323     |
|    total_timesteps  | 212968   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0158   |
|    n_updates        | 52991    |
----------------------------------
Eval num_timesteps=213000, episode_reward=396.64 +/- 102.65
Episode length: 124.16 +/- 25.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 397      |
| rollout/            |          |
|    exploration_rate | 0.195    |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.189    |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 160      |
|    time_elapsed     | 1326     |
|    total_timesteps  | 213472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 53117    |
----------------------------------
Eval num_timesteps=213500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.189    |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.019    |
|    n_updates        | 53124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.185    |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 160      |
|    time_elapsed     | 1329     |
|    total_timesteps  | 213872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 53217    |
----------------------------------
Eval num_timesteps=214000, episode_reward=297.44 +/- 37.37
Episode length: 99.36 +/- 9.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 0.183    |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.18     |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 160      |
|    time_elapsed     | 1333     |
|    total_timesteps  | 214328   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0129   |
|    n_updates        | 53331    |
----------------------------------
Eval num_timesteps=214500, episode_reward=316.00 +/- 35.63
Episode length: 104.00 +/- 8.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00664  |
|    n_updates        | 53374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.174    |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 160      |
|    time_elapsed     | 1336     |
|    total_timesteps  | 214784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0266   |
|    n_updates        | 53445    |
----------------------------------
Eval num_timesteps=215000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.172    |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.169    |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 160      |
|    time_elapsed     | 1339     |
|    total_timesteps  | 215224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 53555    |
----------------------------------
Eval num_timesteps=215500, episode_reward=286.56 +/- 14.08
Episode length: 96.64 +/- 3.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.166    |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0111   |
|    n_updates        | 53624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.164    |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 160      |
|    time_elapsed     | 1342     |
|    total_timesteps  | 215656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.97     |
|    n_updates        | 53663    |
----------------------------------
Eval num_timesteps=216000, episode_reward=328.16 +/- 48.69
Episode length: 107.04 +/- 12.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 328      |
| rollout/            |          |
|    exploration_rate | 0.16     |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0129   |
|    n_updates        | 53749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.159    |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 160      |
|    time_elapsed     | 1345     |
|    total_timesteps  | 216152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 53787    |
----------------------------------
Eval num_timesteps=216500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.155    |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0227   |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.154    |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 160      |
|    time_elapsed     | 1348     |
|    total_timesteps  | 216544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0638   |
|    n_updates        | 53885    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.149    |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 160      |
|    time_elapsed     | 1348     |
|    total_timesteps  | 216960   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.48     |
|    n_updates        | 53989    |
----------------------------------
Eval num_timesteps=217000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.149    |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0334   |
|    n_updates        | 53999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.144    |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 160      |
|    time_elapsed     | 1351     |
|    total_timesteps  | 217416   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.5      |
|    n_updates        | 54103    |
----------------------------------
Eval num_timesteps=217500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.143    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 4.97     |
|    n_updates        | 54124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.138    |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 160      |
|    time_elapsed     | 1354     |
|    total_timesteps  | 217888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 54221    |
----------------------------------
Eval num_timesteps=218000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.137    |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0484   |
|    n_updates        | 54249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.133    |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 160      |
|    time_elapsed     | 1358     |
|    total_timesteps  | 218360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.5      |
|    n_updates        | 54339    |
----------------------------------
Eval num_timesteps=218500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.131    |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0595   |
|    n_updates        | 54374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.127    |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 160      |
|    time_elapsed     | 1361     |
|    total_timesteps  | 218864   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.48     |
|    n_updates        | 54465    |
----------------------------------
Eval num_timesteps=219000, episode_reward=401.76 +/- 101.89
Episode length: 125.44 +/- 25.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 402      |
| rollout/            |          |
|    exploration_rate | 0.125    |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.48     |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.119    |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 160      |
|    time_elapsed     | 1365     |
|    total_timesteps  | 219496   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.49     |
|    n_updates        | 54623    |
----------------------------------
Eval num_timesteps=219500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.119    |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 54624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.114    |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 160      |
|    time_elapsed     | 1368     |
|    total_timesteps  | 219928   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0289   |
|    n_updates        | 54731    |
----------------------------------
Eval num_timesteps=220000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.113    |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 160      |
|    time_elapsed     | 1371     |
|    total_timesteps  | 220464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.01     |
|    n_updates        | 54865    |
----------------------------------
Eval num_timesteps=220500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.108    |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 54874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.103    |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 160      |
|    time_elapsed     | 1374     |
|    total_timesteps  | 220920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 54979    |
----------------------------------
Eval num_timesteps=221000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.102    |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.0968   |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 160      |
|    time_elapsed     | 1377     |
|    total_timesteps  | 221408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0155   |
|    n_updates        | 55101    |
----------------------------------
Eval num_timesteps=221500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0957   |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0173   |
|    n_updates        | 55124    |
----------------------------------
Eval num_timesteps=222000, episode_reward=290.40 +/- 23.95
Episode length: 97.60 +/- 5.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 290      |
| rollout/            |          |
|    exploration_rate | 0.0897   |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0272   |
|    n_updates        | 55249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0889   |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 160      |
|    time_elapsed     | 1383     |
|    total_timesteps  | 222072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0154   |
|    n_updates        | 55267    |
----------------------------------
Eval num_timesteps=222500, episode_reward=720.48 +/- 426.59
Episode length: 205.12 +/- 106.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 720      |
| rollout/            |          |
|    exploration_rate | 0.0838   |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 55374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.0835   |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 160      |
|    time_elapsed     | 1389     |
|    total_timesteps  | 222520   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.02     |
|    n_updates        | 55379    |
----------------------------------
Eval num_timesteps=223000, episode_reward=406.88 +/- 150.28
Episode length: 126.72 +/- 37.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 407      |
| rollout/            |          |
|    exploration_rate | 0.0778   |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0238   |
|    n_updates        | 55499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0766   |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 160      |
|    time_elapsed     | 1392     |
|    total_timesteps  | 223096   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0197   |
|    n_updates        | 55523    |
----------------------------------
Eval num_timesteps=223500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0718   |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0696   |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 160      |
|    time_elapsed     | 1396     |
|    total_timesteps  | 223680   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 55669    |
----------------------------------
Eval num_timesteps=224000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0658   |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0248   |
|    n_updates        | 55749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.0643   |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 160      |
|    time_elapsed     | 1399     |
|    total_timesteps  | 224120   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0164   |
|    n_updates        | 55779    |
----------------------------------
Eval num_timesteps=224500, episode_reward=364.00 +/- 100.63
Episode length: 116.00 +/- 25.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 364      |
| rollout/            |          |
|    exploration_rate | 0.0598   |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 55874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.0589   |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 160      |
|    time_elapsed     | 1402     |
|    total_timesteps  | 224568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0265   |
|    n_updates        | 55891    |
----------------------------------
Eval num_timesteps=225000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0537   |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0172   |
|    n_updates        | 55999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.0537   |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 160      |
|    time_elapsed     | 1405     |
|    total_timesteps  | 225000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0488   |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 160      |
|    time_elapsed     | 1405     |
|    total_timesteps  | 225408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 56101    |
----------------------------------
Eval num_timesteps=225500, episode_reward=348.00 +/- 56.52
Episode length: 112.00 +/- 14.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 348      |
| rollout/            |          |
|    exploration_rate | 0.0477   |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 56124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 160      |
|    time_elapsed     | 1409     |
|    total_timesteps  | 225840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0254   |
|    n_updates        | 56209    |
----------------------------------
Eval num_timesteps=226000, episode_reward=343.52 +/- 68.04
Episode length: 110.88 +/- 17.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 344      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.02     |
|    n_updates        | 56249    |
----------------------------------
Eval num_timesteps=226500, episode_reward=324.96 +/- 53.56
Episode length: 106.24 +/- 13.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 325      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0262   |
|    n_updates        | 56374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 160      |
|    time_elapsed     | 1415     |
|    total_timesteps  | 226688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0232   |
|    n_updates        | 56421    |
----------------------------------
Eval num_timesteps=227000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0176   |
|    n_updates        | 56499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 160      |
|    time_elapsed     | 1418     |
|    total_timesteps  | 227224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 56555    |
----------------------------------
Eval num_timesteps=227500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0374   |
|    n_updates        | 56624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 160      |
|    time_elapsed     | 1421     |
|    total_timesteps  | 227656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0147   |
|    n_updates        | 56663    |
----------------------------------
Eval num_timesteps=228000, episode_reward=286.56 +/- 14.08
Episode length: 96.64 +/- 3.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.52     |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 160      |
|    time_elapsed     | 1425     |
|    total_timesteps  | 228232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.02     |
|    n_updates        | 56807    |
----------------------------------
Eval num_timesteps=228500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.03     |
|    n_updates        | 56874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 160      |
|    time_elapsed     | 1428     |
|    total_timesteps  | 228760   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0233   |
|    n_updates        | 56939    |
----------------------------------
Eval num_timesteps=229000, episode_reward=285.28 +/- 6.27
Episode length: 96.32 +/- 1.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0324   |
|    n_updates        | 56999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 160      |
|    time_elapsed     | 1431     |
|    total_timesteps  | 229176   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 57043    |
----------------------------------
Eval num_timesteps=229500, episode_reward=312.16 +/- 35.43
Episode length: 103.04 +/- 8.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 57124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 160      |
|    time_elapsed     | 1434     |
|    total_timesteps  | 229656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 57163    |
----------------------------------
Eval num_timesteps=230000, episode_reward=285.92 +/- 13.44
Episode length: 96.48 +/- 3.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00862  |
|    n_updates        | 57249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 160      |
|    time_elapsed     | 1437     |
|    total_timesteps  | 230112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 57277    |
----------------------------------
Eval num_timesteps=230500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.53     |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 414      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 160      |
|    time_elapsed     | 1440     |
|    total_timesteps  | 230736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0283   |
|    n_updates        | 57433    |
----------------------------------
Eval num_timesteps=231000, episode_reward=307.04 +/- 35.08
Episode length: 101.76 +/- 8.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.56     |
|    n_updates        | 57499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 414      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 160      |
|    time_elapsed     | 1443     |
|    total_timesteps  | 231200   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 57549    |
----------------------------------
Eval num_timesteps=231500, episode_reward=314.72 +/- 31.33
Episode length: 103.68 +/- 7.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0146   |
|    n_updates        | 57624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 160      |
|    time_elapsed     | 1446     |
|    total_timesteps  | 231656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 57663    |
----------------------------------
Eval num_timesteps=232000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0184   |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 160      |
|    time_elapsed     | 1449     |
|    total_timesteps  | 232184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00872  |
|    n_updates        | 57795    |
----------------------------------
Eval num_timesteps=232500, episode_reward=602.08 +/- 314.67
Episode length: 175.52 +/- 78.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 176      |
|    mean_reward      | 602      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0114   |
|    n_updates        | 57874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 130      |
|    ep_rew_mean      | 418      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 160      |
|    time_elapsed     | 1455     |
|    total_timesteps  | 232888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.04     |
|    n_updates        | 57971    |
----------------------------------
Eval num_timesteps=233000, episode_reward=308.32 +/- 33.60
Episode length: 102.08 +/- 8.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0102   |
|    n_updates        | 57999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 416      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 160      |
|    time_elapsed     | 1458     |
|    total_timesteps  | 233352   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0123   |
|    n_updates        | 58087    |
----------------------------------
Eval num_timesteps=233500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 58124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 417      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 160      |
|    time_elapsed     | 1461     |
|    total_timesteps  | 233856   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 58213    |
----------------------------------
Eval num_timesteps=234000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0128   |
|    n_updates        | 58249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 159      |
|    time_elapsed     | 1464     |
|    total_timesteps  | 234240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.09     |
|    n_updates        | 58309    |
----------------------------------
Eval num_timesteps=234500, episode_reward=1466.72 +/- 642.24
Episode length: 381.68 +/- 150.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 382      |
|    mean_reward      | 1.47e+03 |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0117   |
|    n_updates        | 58374    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 159      |
|    time_elapsed     | 1475     |
|    total_timesteps  | 234672   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 58417    |
----------------------------------
Eval num_timesteps=235000, episode_reward=285.92 +/- 7.60
Episode length: 96.48 +/- 1.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0243   |
|    n_updates        | 58499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 159      |
|    time_elapsed     | 1478     |
|    total_timesteps  | 235136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.56     |
|    n_updates        | 58533    |
----------------------------------
Eval num_timesteps=235500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0276   |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 159      |
|    time_elapsed     | 1481     |
|    total_timesteps  | 235640   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0137   |
|    n_updates        | 58659    |
----------------------------------
Eval num_timesteps=236000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00926  |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 159      |
|    time_elapsed     | 1484     |
|    total_timesteps  | 236072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0122   |
|    n_updates        | 58767    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 159      |
|    time_elapsed     | 1484     |
|    total_timesteps  | 236480   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.08     |
|    n_updates        | 58869    |
----------------------------------
Eval num_timesteps=236500, episode_reward=394.72 +/- 136.40
Episode length: 123.68 +/- 34.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 395      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00944  |
|    n_updates        | 58874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 159      |
|    time_elapsed     | 1488     |
|    total_timesteps  | 236984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0227   |
|    n_updates        | 58995    |
----------------------------------
Eval num_timesteps=237000, episode_reward=323.68 +/- 40.76
Episode length: 105.92 +/- 10.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 324      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0316   |
|    n_updates        | 58999    |
----------------------------------
Eval num_timesteps=237500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.54     |
|    n_updates        | 59124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 158      |
|    time_elapsed     | 1494     |
|    total_timesteps  | 237528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.55     |
|    n_updates        | 59131    |
----------------------------------
Eval num_timesteps=238000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0318   |
|    n_updates        | 59249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 158      |
|    time_elapsed     | 1498     |
|    total_timesteps  | 238056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0162   |
|    n_updates        | 59263    |
----------------------------------
Eval num_timesteps=238500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0256   |
|    n_updates        | 59374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 158      |
|    time_elapsed     | 1501     |
|    total_timesteps  | 238512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.54     |
|    n_updates        | 59377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 159      |
|    time_elapsed     | 1501     |
|    total_timesteps  | 238952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0213   |
|    n_updates        | 59487    |
----------------------------------
Eval num_timesteps=239000, episode_reward=321.12 +/- 48.05
Episode length: 105.28 +/- 12.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 321      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.6      |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 159      |
|    time_elapsed     | 1504     |
|    total_timesteps  | 239432   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.54     |
|    n_updates        | 59607    |
----------------------------------
Eval num_timesteps=239500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.54     |
|    n_updates        | 59624    |
----------------------------------
Eval num_timesteps=240000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.07     |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 158      |
|    time_elapsed     | 1510     |
|    total_timesteps  | 240008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 1.15     |
|    n_updates        | 59751    |
----------------------------------
Eval num_timesteps=240500, episode_reward=451.68 +/- 168.43
Episode length: 137.92 +/- 42.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 452      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0259   |
|    n_updates        | 59874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 158      |
|    time_elapsed     | 1514     |
|    total_timesteps  | 240624   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0167   |
|    n_updates        | 59905    |
----------------------------------
Eval num_timesteps=241000, episode_reward=325.60 +/- 42.09
Episode length: 106.40 +/- 10.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 326      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 59999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 158      |
|    time_elapsed     | 1518     |
|    total_timesteps  | 241056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0242   |
|    n_updates        | 60013    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 158      |
|    time_elapsed     | 1518     |
|    total_timesteps  | 241440   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0246   |
|    n_updates        | 60109    |
----------------------------------
Eval num_timesteps=241500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 60124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 159      |
|    time_elapsed     | 1521     |
|    total_timesteps  | 241976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 60243    |
----------------------------------
Eval num_timesteps=242000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 60249    |
----------------------------------
Eval num_timesteps=242500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 60374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 158      |
|    time_elapsed     | 1527     |
|    total_timesteps  | 242504   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 60375    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 159      |
|    time_elapsed     | 1527     |
|    total_timesteps  | 242960   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 60489    |
----------------------------------
Eval num_timesteps=243000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 60499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 159      |
|    time_elapsed     | 1530     |
|    total_timesteps  | 243440   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 60609    |
----------------------------------
Eval num_timesteps=243500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 159      |
|    time_elapsed     | 1533     |
|    total_timesteps  | 243912   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0123   |
|    n_updates        | 60727    |
----------------------------------
Eval num_timesteps=244000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.13     |
|    n_updates        | 60749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 159      |
|    time_elapsed     | 1537     |
|    total_timesteps  | 244472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0101   |
|    n_updates        | 60867    |
----------------------------------
Eval num_timesteps=244500, episode_reward=1195.52 +/- 648.81
Episode length: 318.38 +/- 154.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 318      |
|    mean_reward      | 1.2e+03  |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.016    |
|    n_updates        | 60874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 158      |
|    time_elapsed     | 1546     |
|    total_timesteps  | 244976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.59     |
|    n_updates        | 60993    |
----------------------------------
Eval num_timesteps=245000, episode_reward=312.80 +/- 40.10
Episode length: 103.20 +/- 10.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0137   |
|    n_updates        | 60999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 158      |
|    time_elapsed     | 1549     |
|    total_timesteps  | 245456   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.13     |
|    n_updates        | 61113    |
----------------------------------
Eval num_timesteps=245500, episode_reward=644.32 +/- 328.14
Episode length: 186.08 +/- 82.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 644      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 61124    |
----------------------------------
Eval num_timesteps=246000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 61249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 158      |
|    time_elapsed     | 1557     |
|    total_timesteps  | 246128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0226   |
|    n_updates        | 61281    |
----------------------------------
Eval num_timesteps=246500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 61374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 158      |
|    time_elapsed     | 1560     |
|    total_timesteps  | 246600   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00845  |
|    n_updates        | 61399    |
----------------------------------
Eval num_timesteps=247000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.58     |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 158      |
|    time_elapsed     | 1563     |
|    total_timesteps  | 247104   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 61525    |
----------------------------------
Eval num_timesteps=247500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0183   |
|    n_updates        | 61624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 158      |
|    time_elapsed     | 1566     |
|    total_timesteps  | 247616   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0296   |
|    n_updates        | 61653    |
----------------------------------
Eval num_timesteps=248000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 61749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 158      |
|    time_elapsed     | 1569     |
|    total_timesteps  | 248088   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.59     |
|    n_updates        | 61771    |
----------------------------------
Eval num_timesteps=248500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0118   |
|    n_updates        | 61874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 158      |
|    time_elapsed     | 1572     |
|    total_timesteps  | 248528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 61881    |
----------------------------------
Eval num_timesteps=249000, episode_reward=927.36 +/- 537.72
Episode length: 255.34 +/- 131.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 255      |
|    mean_reward      | 927      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 61999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 157      |
|    time_elapsed     | 1580     |
|    total_timesteps  | 249128   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.57     |
|    n_updates        | 62031    |
----------------------------------
Eval num_timesteps=249500, episode_reward=291.68 +/- 32.36
Episode length: 97.92 +/- 8.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.9     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.15     |
|    n_updates        | 62124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 157      |
|    time_elapsed     | 1583     |
|    total_timesteps  | 249544   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0179   |
|    n_updates        | 62135    |
----------------------------------
Eval num_timesteps=250000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00877  |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 157      |
|    time_elapsed     | 1586     |
|    total_timesteps  | 250024   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.79     |
|    n_updates        | 62255    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 157      |
|    time_elapsed     | 1586     |
|    total_timesteps  | 250408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00804  |
|    n_updates        | 62351    |
----------------------------------
Eval num_timesteps=250500, episode_reward=305.12 +/- 32.39
Episode length: 101.28 +/- 8.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0195   |
|    n_updates        | 62374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 157      |
|    time_elapsed     | 1589     |
|    total_timesteps  | 250872   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 62467    |
----------------------------------
Eval num_timesteps=251000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 62499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 157      |
|    time_elapsed     | 1592     |
|    total_timesteps  | 251368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 62591    |
----------------------------------
Eval num_timesteps=251500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.18     |
|    n_updates        | 62624    |
----------------------------------
Eval num_timesteps=252000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 62749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 157      |
|    time_elapsed     | 1598     |
|    total_timesteps  | 252016   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0151   |
|    n_updates        | 62753    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 157      |
|    time_elapsed     | 1598     |
|    total_timesteps  | 252424   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0103   |
|    n_updates        | 62855    |
----------------------------------
Eval num_timesteps=252500, episode_reward=314.72 +/- 37.84
Episode length: 103.68 +/- 9.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0127   |
|    n_updates        | 62874    |
----------------------------------
Eval num_timesteps=253000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0172   |
|    n_updates        | 62999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 157      |
|    time_elapsed     | 1604     |
|    total_timesteps  | 253112   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.016    |
|    n_updates        | 63027    |
----------------------------------
Eval num_timesteps=253500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0169   |
|    n_updates        | 63124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 157      |
|    time_elapsed     | 1607     |
|    total_timesteps  | 253568   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00916  |
|    n_updates        | 63141    |
----------------------------------
Eval num_timesteps=254000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.028    |
|    n_updates        | 63249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 157      |
|    time_elapsed     | 1610     |
|    total_timesteps  | 254008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 63251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 157      |
|    time_elapsed     | 1611     |
|    total_timesteps  | 254464   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 63365    |
----------------------------------
Eval num_timesteps=254500, episode_reward=305.12 +/- 33.63
Episode length: 101.28 +/- 8.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 305      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00971  |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 157      |
|    time_elapsed     | 1614     |
|    total_timesteps  | 254992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0111   |
|    n_updates        | 63497    |
----------------------------------
Eval num_timesteps=255000, episode_reward=787.52 +/- 601.20
Episode length: 219.38 +/- 144.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 788      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.61     |
|    n_updates        | 63499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 157      |
|    time_elapsed     | 1620     |
|    total_timesteps  | 255384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0228   |
|    n_updates        | 63595    |
----------------------------------
Eval num_timesteps=255500, episode_reward=307.04 +/- 39.47
Episode length: 101.76 +/- 9.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 63624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 157      |
|    time_elapsed     | 1623     |
|    total_timesteps  | 255848   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 63711    |
----------------------------------
Eval num_timesteps=256000, episode_reward=309.60 +/- 32.63
Episode length: 102.40 +/- 8.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0126   |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 157      |
|    time_elapsed     | 1626     |
|    total_timesteps  | 256296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0169   |
|    n_updates        | 63823    |
----------------------------------
Eval num_timesteps=256500, episode_reward=316.64 +/- 37.04
Episode length: 104.16 +/- 9.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.64     |
|    n_updates        | 63874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 157      |
|    time_elapsed     | 1630     |
|    total_timesteps  | 256832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 63957    |
----------------------------------
Eval num_timesteps=257000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 63999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 157      |
|    time_elapsed     | 1633     |
|    total_timesteps  | 257288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.61     |
|    n_updates        | 64071    |
----------------------------------
Eval num_timesteps=257500, episode_reward=310.24 +/- 38.23
Episode length: 102.56 +/- 9.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 64124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 157      |
|    time_elapsed     | 1636     |
|    total_timesteps  | 257816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 64203    |
----------------------------------
Eval num_timesteps=258000, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.78     |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 157      |
|    time_elapsed     | 1639     |
|    total_timesteps  | 258320   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 10.4     |
|    n_updates        | 64329    |
----------------------------------
Eval num_timesteps=258500, episode_reward=286.56 +/- 14.08
Episode length: 96.64 +/- 3.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 287      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.19     |
|    n_updates        | 64374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 157      |
|    time_elapsed     | 1642     |
|    total_timesteps  | 258816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 64453    |
----------------------------------
Eval num_timesteps=259000, episode_reward=291.68 +/- 30.40
Episode length: 97.92 +/- 7.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.9     |
|    mean_reward      | 292      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 157      |
|    time_elapsed     | 1645     |
|    total_timesteps  | 259240   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0152   |
|    n_updates        | 64559    |
----------------------------------
Eval num_timesteps=259500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.62     |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 157      |
|    time_elapsed     | 1648     |
|    total_timesteps  | 259656   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.6      |
|    n_updates        | 64663    |
----------------------------------
Eval num_timesteps=260000, episode_reward=303.84 +/- 30.62
Episode length: 100.96 +/- 7.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0135   |
|    n_updates        | 64749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 157      |
|    time_elapsed     | 1651     |
|    total_timesteps  | 260072   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.131    |
|    n_updates        | 64767    |
----------------------------------
Eval num_timesteps=260500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 157      |
|    time_elapsed     | 1654     |
|    total_timesteps  | 260528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 64881    |
----------------------------------
Eval num_timesteps=261000, episode_reward=489.44 +/- 217.80
Episode length: 147.36 +/- 54.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 489      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 157      |
|    time_elapsed     | 1659     |
|    total_timesteps  | 261056   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 65013    |
----------------------------------
Eval num_timesteps=261500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 65124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 157      |
|    time_elapsed     | 1662     |
|    total_timesteps  | 261512   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.032    |
|    n_updates        | 65127    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 157      |
|    time_elapsed     | 1662     |
|    total_timesteps  | 261920   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0126   |
|    n_updates        | 65229    |
----------------------------------
Eval num_timesteps=262000, episode_reward=285.92 +/- 9.94
Episode length: 96.48 +/- 2.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 286      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0157   |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 157      |
|    time_elapsed     | 1665     |
|    total_timesteps  | 262360   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0106   |
|    n_updates        | 65339    |
----------------------------------
Eval num_timesteps=262500, episode_reward=285.28 +/- 6.27
Episode length: 96.32 +/- 1.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 65374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 157      |
|    time_elapsed     | 1668     |
|    total_timesteps  | 262832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0172   |
|    n_updates        | 65457    |
----------------------------------
Eval num_timesteps=263000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0126   |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 157      |
|    time_elapsed     | 1671     |
|    total_timesteps  | 263296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0141   |
|    n_updates        | 65573    |
----------------------------------
Eval num_timesteps=263500, episode_reward=308.32 +/- 63.54
Episode length: 102.08 +/- 15.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0146   |
|    n_updates        | 65624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 157      |
|    time_elapsed     | 1674     |
|    total_timesteps  | 263736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0096   |
|    n_updates        | 65683    |
----------------------------------
Eval num_timesteps=264000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 65749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 157      |
|    time_elapsed     | 1678     |
|    total_timesteps  | 264256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.86     |
|    n_updates        | 65813    |
----------------------------------
Eval num_timesteps=264500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0201   |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 157      |
|    time_elapsed     | 1681     |
|    total_timesteps  | 264736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0153   |
|    n_updates        | 65933    |
----------------------------------
Eval num_timesteps=265000, episode_reward=313.44 +/- 66.77
Episode length: 103.36 +/- 16.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.24     |
|    n_updates        | 65999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 157      |
|    time_elapsed     | 1684     |
|    total_timesteps  | 265184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0146   |
|    n_updates        | 66045    |
----------------------------------
Eval num_timesteps=265500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0107   |
|    n_updates        | 66124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 157      |
|    time_elapsed     | 1687     |
|    total_timesteps  | 265632   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 66157    |
----------------------------------
Eval num_timesteps=266000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0161   |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 157      |
|    time_elapsed     | 1690     |
|    total_timesteps  | 266136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.62     |
|    n_updates        | 66283    |
----------------------------------
Eval num_timesteps=266500, episode_reward=307.68 +/- 35.52
Episode length: 101.92 +/- 8.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 308      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 66374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 157      |
|    time_elapsed     | 1693     |
|    total_timesteps  | 266696   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0209   |
|    n_updates        | 66423    |
----------------------------------
Eval num_timesteps=267000, episode_reward=627.36 +/- 409.69
Episode length: 180.84 +/- 98.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 627      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00809  |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 157      |
|    time_elapsed     | 1699     |
|    total_timesteps  | 267168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 66541    |
----------------------------------
Eval num_timesteps=267500, episode_reward=1604.16 +/- 526.83
Episode length: 415.54 +/- 122.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 416      |
|    mean_reward      | 1.6e+03  |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0119   |
|    n_updates        | 66624    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 156      |
|    time_elapsed     | 1710     |
|    total_timesteps  | 267576   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 66643    |
----------------------------------
Eval num_timesteps=268000, episode_reward=1273.60 +/- 589.15
Episode length: 337.90 +/- 139.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 338      |
|    mean_reward      | 1.27e+03 |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.019    |
|    n_updates        | 66749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 155      |
|    time_elapsed     | 1720     |
|    total_timesteps  | 268048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.24     |
|    n_updates        | 66761    |
----------------------------------
Eval num_timesteps=268500, episode_reward=317.28 +/- 37.30
Episode length: 104.32 +/- 9.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.23     |
|    n_updates        | 66874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 155      |
|    time_elapsed     | 1723     |
|    total_timesteps  | 268616   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0111   |
|    n_updates        | 66903    |
----------------------------------
Eval num_timesteps=269000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0111   |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 155      |
|    time_elapsed     | 1726     |
|    total_timesteps  | 269048   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.64     |
|    n_updates        | 67011    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 156      |
|    time_elapsed     | 1726     |
|    total_timesteps  | 269496   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 67123    |
----------------------------------
Eval num_timesteps=269500, episode_reward=936.64 +/- 633.31
Episode length: 254.66 +/- 150.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 255      |
|    mean_reward      | 937      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0182   |
|    n_updates        | 67124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 155      |
|    time_elapsed     | 1734     |
|    total_timesteps  | 269992   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.26     |
|    n_updates        | 67247    |
----------------------------------
Eval num_timesteps=270000, episode_reward=314.72 +/- 53.15
Episode length: 103.68 +/- 13.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.63     |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 155      |
|    time_elapsed     | 1737     |
|    total_timesteps  | 270400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0102   |
|    n_updates        | 67349    |
----------------------------------
Eval num_timesteps=270500, episode_reward=316.64 +/- 38.13
Episode length: 104.16 +/- 9.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 67374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 155      |
|    time_elapsed     | 1740     |
|    total_timesteps  | 270832   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 67457    |
----------------------------------
Eval num_timesteps=271000, episode_reward=531.04 +/- 367.65
Episode length: 157.76 +/- 91.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 158      |
|    mean_reward      | 531      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0523   |
|    n_updates        | 67499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 155      |
|    time_elapsed     | 1745     |
|    total_timesteps  | 271288   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.3      |
|    n_updates        | 67571    |
----------------------------------
Eval num_timesteps=271500, episode_reward=314.08 +/- 36.44
Episode length: 103.52 +/- 9.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 314      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0165   |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 155      |
|    time_elapsed     | 1748     |
|    total_timesteps  | 271744   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0205   |
|    n_updates        | 67685    |
----------------------------------
Eval num_timesteps=272000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0136   |
|    n_updates        | 67749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 155      |
|    time_elapsed     | 1751     |
|    total_timesteps  | 272168   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.67     |
|    n_updates        | 67791    |
----------------------------------
Eval num_timesteps=272500, episode_reward=355.04 +/- 75.53
Episode length: 113.76 +/- 18.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 355      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 155      |
|    time_elapsed     | 1754     |
|    total_timesteps  | 272624   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.29     |
|    n_updates        | 67905    |
----------------------------------
Eval num_timesteps=273000, episode_reward=345.44 +/- 94.46
Episode length: 111.36 +/- 23.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 345      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0187   |
|    n_updates        | 67999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 116      |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 155      |
|    time_elapsed     | 1758     |
|    total_timesteps  | 273152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68037    |
----------------------------------
Eval num_timesteps=273500, episode_reward=364.64 +/- 86.40
Episode length: 116.16 +/- 21.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 365      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00856  |
|    n_updates        | 68124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 155      |
|    time_elapsed     | 1762     |
|    total_timesteps  | 273728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68181    |
----------------------------------
Eval num_timesteps=274000, episode_reward=285.28 +/- 6.27
Episode length: 96.32 +/- 1.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 155      |
|    time_elapsed     | 1765     |
|    total_timesteps  | 274184   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 68295    |
----------------------------------
Eval num_timesteps=274500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 68374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 155      |
|    time_elapsed     | 1768     |
|    total_timesteps  | 274736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.076    |
|    n_updates        | 68433    |
----------------------------------
Eval num_timesteps=275000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0205   |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 155      |
|    time_elapsed     | 1771     |
|    total_timesteps  | 275152   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 68537    |
----------------------------------
Eval num_timesteps=275500, episode_reward=285.28 +/- 8.96
Episode length: 96.32 +/- 2.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 68624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 155      |
|    time_elapsed     | 1774     |
|    total_timesteps  | 275584   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.67     |
|    n_updates        | 68645    |
----------------------------------
Eval num_timesteps=276000, episode_reward=388.96 +/- 181.93
Episode length: 122.24 +/- 45.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 389      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 155      |
|    time_elapsed     | 1778     |
|    total_timesteps  | 276088   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0201   |
|    n_updates        | 68771    |
----------------------------------
Eval num_timesteps=276500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0126   |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 155      |
|    time_elapsed     | 1781     |
|    total_timesteps  | 276528   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0132   |
|    n_updates        | 68881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 155      |
|    time_elapsed     | 1781     |
|    total_timesteps  | 276976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 68993    |
----------------------------------
Eval num_timesteps=277000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0199   |
|    n_updates        | 68999    |
----------------------------------
Eval num_timesteps=277500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00855  |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 155      |
|    time_elapsed     | 1787     |
|    total_timesteps  | 277552   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0132   |
|    n_updates        | 69137    |
----------------------------------
Eval num_timesteps=278000, episode_reward=296.80 +/- 45.71
Episode length: 99.20 +/- 11.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.2     |
|    mean_reward      | 297      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0156   |
|    n_updates        | 69249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 155      |
|    time_elapsed     | 1790     |
|    total_timesteps  | 278040   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.32     |
|    n_updates        | 69259    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 155      |
|    time_elapsed     | 1791     |
|    total_timesteps  | 278448   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.67     |
|    n_updates        | 69361    |
----------------------------------
Eval num_timesteps=278500, episode_reward=284.64 +/- 4.48
Episode length: 96.16 +/- 1.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.2     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 69374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 155      |
|    time_elapsed     | 1794     |
|    total_timesteps  | 278856   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 69463    |
----------------------------------
Eval num_timesteps=279000, episode_reward=302.56 +/- 29.39
Episode length: 100.64 +/- 7.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 303      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2288     |
|    fps              | 155      |
|    time_elapsed     | 1797     |
|    total_timesteps  | 279264   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.65     |
|    n_updates        | 69565    |
----------------------------------
Eval num_timesteps=279500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0115   |
|    n_updates        | 69624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2292     |
|    fps              | 155      |
|    time_elapsed     | 1800     |
|    total_timesteps  | 279800   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.66     |
|    n_updates        | 69699    |
----------------------------------
Eval num_timesteps=280000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 7.93     |
|    n_updates        | 69749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 155      |
|    time_elapsed     | 1803     |
|    total_timesteps  | 280384   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 69845    |
----------------------------------
Eval num_timesteps=280500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0148   |
|    n_updates        | 69874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2300     |
|    fps              | 155      |
|    time_elapsed     | 1806     |
|    total_timesteps  | 280816   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0159   |
|    n_updates        | 69953    |
----------------------------------
Eval num_timesteps=281000, episode_reward=312.16 +/- 41.30
Episode length: 103.04 +/- 10.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.34     |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2304     |
|    fps              | 155      |
|    time_elapsed     | 1809     |
|    total_timesteps  | 281400   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 70099    |
----------------------------------
Eval num_timesteps=281500, episode_reward=355.04 +/- 55.53
Episode length: 113.76 +/- 13.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 355      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.69     |
|    n_updates        | 70124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2308     |
|    fps              | 155      |
|    time_elapsed     | 1813     |
|    total_timesteps  | 281824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 70205    |
----------------------------------
Eval num_timesteps=282000, episode_reward=316.00 +/- 36.77
Episode length: 104.00 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 316      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0347   |
|    n_updates        | 70249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2312     |
|    fps              | 155      |
|    time_elapsed     | 1816     |
|    total_timesteps  | 282232   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0153   |
|    n_updates        | 70307    |
----------------------------------
Eval num_timesteps=282500, episode_reward=309.60 +/- 35.63
Episode length: 102.40 +/- 8.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 310      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0163   |
|    n_updates        | 70374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2316     |
|    fps              | 155      |
|    time_elapsed     | 1819     |
|    total_timesteps  | 282664   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 70415    |
----------------------------------
Eval num_timesteps=283000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.67     |
|    n_updates        | 70499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2320     |
|    fps              | 155      |
|    time_elapsed     | 1822     |
|    total_timesteps  | 283136   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0121   |
|    n_updates        | 70533    |
----------------------------------
Eval num_timesteps=283500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.015    |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2324     |
|    fps              | 155      |
|    time_elapsed     | 1825     |
|    total_timesteps  | 283800   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0085   |
|    n_updates        | 70699    |
----------------------------------
Eval num_timesteps=284000, episode_reward=287.84 +/- 26.88
Episode length: 96.96 +/- 6.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 70749    |
----------------------------------
Eval num_timesteps=284500, episode_reward=328.80 +/- 53.93
Episode length: 107.20 +/- 13.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 329      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0175   |
|    n_updates        | 70874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2328     |
|    fps              | 155      |
|    time_elapsed     | 1832     |
|    total_timesteps  | 284536   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 70883    |
----------------------------------
Eval num_timesteps=285000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.36     |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2332     |
|    fps              | 155      |
|    time_elapsed     | 1835     |
|    total_timesteps  | 285008   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0131   |
|    n_updates        | 71001    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2336     |
|    fps              | 155      |
|    time_elapsed     | 1835     |
|    total_timesteps  | 285488   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0199   |
|    n_updates        | 71121    |
----------------------------------
Eval num_timesteps=285500, episode_reward=285.28 +/- 6.27
Episode length: 96.32 +/- 1.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.3     |
|    mean_reward      | 285      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.35     |
|    n_updates        | 71124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2340     |
|    fps              | 155      |
|    time_elapsed     | 1838     |
|    total_timesteps  | 285976   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 71243    |
----------------------------------
Eval num_timesteps=286000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0145   |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2344     |
|    fps              | 155      |
|    time_elapsed     | 1841     |
|    total_timesteps  | 286408   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0212   |
|    n_updates        | 71351    |
----------------------------------
Eval num_timesteps=286500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0266   |
|    n_updates        | 71374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2348     |
|    fps              | 155      |
|    time_elapsed     | 1844     |
|    total_timesteps  | 286848   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0171   |
|    n_updates        | 71461    |
----------------------------------
Eval num_timesteps=287000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.69     |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2352     |
|    fps              | 155      |
|    time_elapsed     | 1847     |
|    total_timesteps  | 287312   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 71577    |
----------------------------------
Eval num_timesteps=287500, episode_reward=304.48 +/- 31.20
Episode length: 101.12 +/- 7.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 304      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0256   |
|    n_updates        | 71624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2356     |
|    fps              | 155      |
|    time_elapsed     | 1850     |
|    total_timesteps  | 287840   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0139   |
|    n_updates        | 71709    |
----------------------------------
Eval num_timesteps=288000, episode_reward=313.44 +/- 41.40
Episode length: 103.36 +/- 10.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2360     |
|    fps              | 155      |
|    time_elapsed     | 1854     |
|    total_timesteps  | 288296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0112   |
|    n_updates        | 71823    |
----------------------------------
Eval num_timesteps=288500, episode_reward=317.28 +/- 36.74
Episode length: 104.32 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0204   |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2364     |
|    fps              | 155      |
|    time_elapsed     | 1857     |
|    total_timesteps  | 288784   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 71945    |
----------------------------------
Eval num_timesteps=289000, episode_reward=1770.56 +/- 529.45
Episode length: 451.14 +/- 122.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 451      |
|    mean_reward      | 1.77e+03 |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0134   |
|    n_updates        | 71999    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2368     |
|    fps              | 154      |
|    time_elapsed     | 1870     |
|    total_timesteps  | 289352   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.68     |
|    n_updates        | 72087    |
----------------------------------
Eval num_timesteps=289500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.67     |
|    n_updates        | 72124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 154      |
|    time_elapsed     | 1873     |
|    total_timesteps  | 289880   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0181   |
|    n_updates        | 72219    |
----------------------------------
Eval num_timesteps=290000, episode_reward=688.96 +/- 376.76
Episode length: 196.74 +/- 92.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 689      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.69     |
|    n_updates        | 72249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2376     |
|    fps              | 154      |
|    time_elapsed     | 1879     |
|    total_timesteps  | 290368   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0144   |
|    n_updates        | 72341    |
----------------------------------
Eval num_timesteps=290500, episode_reward=521.44 +/- 290.34
Episode length: 155.36 +/- 72.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 155      |
|    mean_reward      | 521      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 72374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 154      |
|    time_elapsed     | 1883     |
|    total_timesteps  | 290952   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0869   |
|    n_updates        | 72487    |
----------------------------------
Eval num_timesteps=291000, episode_reward=358.24 +/- 86.68
Episode length: 114.56 +/- 21.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 358      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 72499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2384     |
|    fps              | 154      |
|    time_elapsed     | 1887     |
|    total_timesteps  | 291472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0058   |
|    n_updates        | 72617    |
----------------------------------
Eval num_timesteps=291500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0144   |
|    n_updates        | 72624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2388     |
|    fps              | 154      |
|    time_elapsed     | 1890     |
|    total_timesteps  | 291984   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0106   |
|    n_updates        | 72745    |
----------------------------------
Eval num_timesteps=292000, episode_reward=307.04 +/- 33.28
Episode length: 101.76 +/- 8.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 307      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 72749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 127      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2392     |
|    fps              | 154      |
|    time_elapsed     | 1893     |
|    total_timesteps  | 292472   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0105   |
|    n_updates        | 72867    |
----------------------------------
Eval num_timesteps=292500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 72874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2396     |
|    fps              | 154      |
|    time_elapsed     | 1896     |
|    total_timesteps  | 292888   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0109   |
|    n_updates        | 72971    |
----------------------------------
Eval num_timesteps=293000, episode_reward=601.44 +/- 331.74
Episode length: 175.36 +/- 82.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 601      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 72999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2400     |
|    fps              | 154      |
|    time_elapsed     | 1901     |
|    total_timesteps  | 293336   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.42     |
|    n_updates        | 73083    |
----------------------------------
Eval num_timesteps=293500, episode_reward=675.52 +/- 438.87
Episode length: 193.38 +/- 108.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 676      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.73     |
|    n_updates        | 73124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2404     |
|    fps              | 154      |
|    time_elapsed     | 1907     |
|    total_timesteps  | 293824   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0198   |
|    n_updates        | 73205    |
----------------------------------
Eval num_timesteps=294000, episode_reward=287.84 +/- 16.49
Episode length: 96.96 +/- 4.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 288      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00946  |
|    n_updates        | 73249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2408     |
|    fps              | 154      |
|    time_elapsed     | 1910     |
|    total_timesteps  | 294296   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.011    |
|    n_updates        | 73323    |
----------------------------------
Eval num_timesteps=294500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0129   |
|    n_updates        | 73374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2412     |
|    fps              | 154      |
|    time_elapsed     | 1913     |
|    total_timesteps  | 294776   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.02     |
|    n_updates        | 73443    |
----------------------------------
Eval num_timesteps=295000, episode_reward=315.36 +/- 39.71
Episode length: 103.84 +/- 9.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 73499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2416     |
|    fps              | 154      |
|    time_elapsed     | 1916     |
|    total_timesteps  | 295224   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 73555    |
----------------------------------
Eval num_timesteps=295500, episode_reward=312.80 +/- 38.53
Episode length: 103.20 +/- 9.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 313      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 5.41     |
|    n_updates        | 73624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 126      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 154      |
|    time_elapsed     | 1919     |
|    total_timesteps  | 295688   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 73671    |
----------------------------------
Eval num_timesteps=296000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 73749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 123      |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 154      |
|    time_elapsed     | 1922     |
|    total_timesteps  | 296144   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0123   |
|    n_updates        | 73785    |
----------------------------------
Eval num_timesteps=296500, episode_reward=360.16 +/- 78.35
Episode length: 115.04 +/- 19.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 360      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.024    |
|    n_updates        | 73874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2428     |
|    fps              | 154      |
|    time_elapsed     | 1926     |
|    total_timesteps  | 296728   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.068    |
|    n_updates        | 73931    |
----------------------------------
Eval num_timesteps=297000, episode_reward=311.52 +/- 33.26
Episode length: 102.88 +/- 8.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 312      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0124   |
|    n_updates        | 73999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2432     |
|    fps              | 154      |
|    time_elapsed     | 1929     |
|    total_timesteps  | 297256   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0437   |
|    n_updates        | 74063    |
----------------------------------
Eval num_timesteps=297500, episode_reward=398.56 +/- 171.50
Episode length: 124.64 +/- 42.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 399      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.72     |
|    n_updates        | 74124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2436     |
|    fps              | 154      |
|    time_elapsed     | 1933     |
|    total_timesteps  | 297960   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.71     |
|    n_updates        | 74239    |
----------------------------------
Eval num_timesteps=298000, episode_reward=315.36 +/- 32.94
Episode length: 103.84 +/- 8.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 315      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00742  |
|    n_updates        | 74249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2440     |
|    fps              | 154      |
|    time_elapsed     | 1936     |
|    total_timesteps  | 298416   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 74353    |
----------------------------------
Eval num_timesteps=298500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.013    |
|    n_updates        | 74374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2444     |
|    fps              | 154      |
|    time_elapsed     | 1939     |
|    total_timesteps  | 298864   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 74465    |
----------------------------------
Eval num_timesteps=299000, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00915  |
|    n_updates        | 74499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2448     |
|    fps              | 154      |
|    time_elapsed     | 1942     |
|    total_timesteps  | 299320   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.0174   |
|    n_updates        | 74579    |
----------------------------------
Eval num_timesteps=299500, episode_reward=284.00 +/- 0.00
Episode length: 96.00 +/- 0.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 284      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 8.09     |
|    n_updates        | 74624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 124      |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.0469   |
| time/               |          |
|    episodes         | 2452     |
|    fps              | 154      |
|    time_elapsed     | 1945     |
|    total_timesteps  | 299736   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 2.7      |
|    n_updates        | 74683    |
----------------------------------
Eval num_timesteps=300000, episode_reward=316.64 +/- 34.76
Episode length: 104.16 +/- 8.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 317      |
| rollout/            |          |
|    exploration_rate | 0.0469   |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 1.81e-05 |
|    loss             | 0.00958  |
|    n_updates        | 74749    |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/health-gathering/dqn-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
