/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-878.07 +/- 179.93
Episode length: 20.66 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.7     |
|    mean_reward     | -878     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-869.31 +/- 186.20
Episode length: 21.20 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | -869     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-879.11 +/- 180.28
Episode length: 21.46 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.5     |
|    mean_reward     | -879     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-883.46 +/- 140.26
Episode length: 23.54 +/- 8.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -883     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.3     |
|    ep_rew_mean     | -840     |
| time/              |          |
|    fps             | 419      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-903.17 +/- 140.78
Episode length: 26.84 +/- 8.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 26.8      |
|    mean_reward          | -903      |
| time/                   |           |
|    total_timesteps      | 2500      |
| train/                  |           |
|    approx_kl            | 1.0630018 |
|    clip_fraction        | 0.61      |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.74     |
|    explained_variance   | -0.000123 |
|    learning_rate        | 0.0001    |
|    loss                 | 2.29e+04  |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.157     |
|    value_loss           | 5.65e+04  |
---------------------------------------
Eval num_timesteps=3000, episode_reward=-919.81 +/- 144.89
Episode length: 24.60 +/- 8.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-946.37 +/- 145.17
Episode length: 24.74 +/- 8.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-925.91 +/- 147.16
Episode length: 25.18 +/- 8.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -902     |
| time/              |          |
|    fps             | 377      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-892.33 +/- 171.64
Episode length: 24.36 +/- 10.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 24.4      |
|    mean_reward          | -892      |
| time/                   |           |
|    total_timesteps      | 4500      |
| train/                  |           |
|    approx_kl            | 3.0879052 |
|    clip_fraction        | 0.905     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.761    |
|    explained_variance   | -0.00576  |
|    learning_rate        | 0.0001    |
|    loss                 | 2.45e+04  |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.329     |
|    value_loss           | 5.31e+04  |
---------------------------------------
Eval num_timesteps=5000, episode_reward=-921.16 +/- 147.99
Episode length: 23.26 +/- 8.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-892.11 +/- 173.58
Episode length: 24.32 +/- 10.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-934.21 +/- 151.41
Episode length: 25.52 +/- 9.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -923     |
| time/              |          |
|    fps             | 369      |
|    iterations      | 3        |
|    time_elapsed    | 16       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-930.76 +/- 131.51
Episode length: 23.68 +/- 8.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23.7       |
|    mean_reward          | -931       |
| time/                   |            |
|    total_timesteps      | 6500       |
| train/                  |            |
|    approx_kl            | 0.02904313 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.562     |
|    explained_variance   | 0.203      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.21e+04   |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0107     |
|    value_loss           | 4.21e+04   |
----------------------------------------
Eval num_timesteps=7000, episode_reward=-910.18 +/- 163.66
Episode length: 22.62 +/- 9.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-927.16 +/- 141.61
Episode length: 23.48 +/- 7.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-951.17 +/- 133.01
Episode length: 25.98 +/- 10.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 367      |
|    iterations      | 4        |
|    time_elapsed    | 22       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-928.13 +/- 168.76
Episode length: 24.72 +/- 9.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.7        |
|    mean_reward          | -928        |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.008729262 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.87e+04    |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00353     |
|    value_loss           | 3.77e+04    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=-946.36 +/- 124.91
Episode length: 24.52 +/- 9.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-929.56 +/- 107.45
Episode length: 23.30 +/- 7.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-930.69 +/- 135.43
Episode length: 22.86 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | -938     |
| time/              |          |
|    fps             | 365      |
|    iterations      | 5        |
|    time_elapsed    | 28       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=-907.95 +/- 170.98
Episode length: 23.10 +/- 9.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.1         |
|    mean_reward          | -908         |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0025722636 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.89e+04     |
|    n_updates            | 50           |
|    policy_gradient_loss | 0.00164      |
|    value_loss           | 3.53e+04     |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-923.54 +/- 142.08
Episode length: 22.28 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-905.56 +/- 135.43
Episode length: 24.06 +/- 10.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-907.94 +/- 166.71
Episode length: 23.04 +/- 8.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 366      |
|    iterations      | 6        |
|    time_elapsed    | 33       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=-965.55 +/- 112.15
Episode length: 25.66 +/- 9.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.7         |
|    mean_reward          | -966         |
| time/                   |              |
|    total_timesteps      | 12500        |
| train/                  |              |
|    approx_kl            | 0.0007172908 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.133       |
|    explained_variance   | 0.508        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.5e+04      |
|    n_updates            | 60           |
|    policy_gradient_loss | 0.00196      |
|    value_loss           | 3.38e+04     |
------------------------------------------
Eval num_timesteps=13000, episode_reward=-965.57 +/- 112.80
Episode length: 24.22 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -966     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-914.89 +/- 176.95
Episode length: 22.42 +/- 7.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-893.32 +/- 191.37
Episode length: 23.04 +/- 10.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -893     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.4     |
|    ep_rew_mean     | -929     |
| time/              |          |
|    fps             | 365      |
|    iterations      | 7        |
|    time_elapsed    | 39       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=-929.56 +/- 120.70
Episode length: 23.76 +/- 7.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.8        |
|    mean_reward          | -930        |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.001593978 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.45e+04    |
|    n_updates            | 70          |
|    policy_gradient_loss | 0.000413    |
|    value_loss           | 2.71e+04    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-898.35 +/- 135.74
Episode length: 23.88 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-883.96 +/- 204.73
Episode length: 22.78 +/- 9.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -884     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-925.97 +/- 129.93
Episode length: 25.88 +/- 10.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -907     |
| time/              |          |
|    fps             | 363      |
|    iterations      | 8        |
|    time_elapsed    | 45       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=-947.55 +/- 111.94
Episode length: 23.66 +/- 9.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.7         |
|    mean_reward          | -948         |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0016576104 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.178       |
|    explained_variance   | 0.592        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.09e+04     |
|    n_updates            | 80           |
|    policy_gradient_loss | 0.0022       |
|    value_loss           | 2.64e+04     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=-904.30 +/- 163.85
Episode length: 24.22 +/- 9.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-892.35 +/- 172.82
Episode length: 22.16 +/- 7.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-939.02 +/- 140.47
Episode length: 26.30 +/- 8.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.3     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -937     |
| time/              |          |
|    fps             | 362      |
|    iterations      | 9        |
|    time_elapsed    | 50       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=-915.16 +/- 134.40
Episode length: 24.96 +/- 9.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | -915         |
| time/                   |              |
|    total_timesteps      | 18500        |
| train/                  |              |
|    approx_kl            | 0.0072601214 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.162       |
|    explained_variance   | 0.603        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.2e+04      |
|    n_updates            | 90           |
|    policy_gradient_loss | 0.0019       |
|    value_loss           | 2.58e+04     |
------------------------------------------
Eval num_timesteps=19000, episode_reward=-937.84 +/- 150.88
Episode length: 22.90 +/- 10.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-897.90 +/- 196.39
Episode length: 22.80 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-912.75 +/- 146.53
Episode length: 24.68 +/- 9.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -932     |
| time/              |          |
|    fps             | 362      |
|    iterations      | 10       |
|    time_elapsed    | 56       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=-942.77 +/- 148.57
Episode length: 25.84 +/- 8.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | -943         |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0020151394 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.137       |
|    explained_variance   | 0.667        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57e+04     |
|    n_updates            | 100          |
|    policy_gradient_loss | -6.56e-05    |
|    value_loss           | 2.29e+04     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=-940.36 +/- 119.27
Episode length: 24.56 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-909.15 +/- 177.70
Episode length: 23.50 +/- 9.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-906.75 +/- 176.65
Episode length: 25.54 +/- 10.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-948.75 +/- 140.77
Episode length: 26.24 +/- 9.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 11       |
|    time_elapsed    | 63       |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=-899.52 +/- 173.26
Episode length: 23.24 +/- 8.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.2        |
|    mean_reward          | -900        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.004645153 |
|    clip_fraction        | 0.0142      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.38e+03    |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00107     |
|    value_loss           | 1.72e+04    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-942.69 +/- 160.45
Episode length: 25.22 +/- 9.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-927.14 +/- 159.75
Episode length: 23.50 +/- 8.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-965.56 +/- 115.96
Episode length: 27.76 +/- 10.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.8     |
|    mean_reward     | -966     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -912     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 12       |
|    time_elapsed    | 69       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-911.45 +/- 154.93
Episode length: 23.06 +/- 8.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.1         |
|    mean_reward          | -911         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0015124037 |
|    clip_fraction        | 0.019        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0846      |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.72e+03     |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000473    |
|    value_loss           | 1.64e+04     |
------------------------------------------
Eval num_timesteps=25500, episode_reward=-898.35 +/- 140.43
Episode length: 24.02 +/- 8.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-904.35 +/- 149.90
Episode length: 25.46 +/- 12.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-919.95 +/- 140.97
Episode length: 23.44 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 356      |
|    iterations      | 13       |
|    time_elapsed    | 74       |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=-945.16 +/- 126.84
Episode length: 25.82 +/- 10.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | -945         |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0010163203 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0762      |
|    explained_variance   | 0.82         |
|    learning_rate        | 0.0001       |
|    loss                 | 4.6e+03      |
|    n_updates            | 130          |
|    policy_gradient_loss | 0.000883     |
|    value_loss           | 1.33e+04     |
------------------------------------------
Eval num_timesteps=27500, episode_reward=-919.95 +/- 164.10
Episode length: 25.48 +/- 9.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-937.95 +/- 129.93
Episode length: 25.86 +/- 8.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-928.36 +/- 141.09
Episode length: 25.26 +/- 9.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -952     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 14       |
|    time_elapsed    | 80       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=-934.33 +/- 152.19
Episode length: 25.04 +/- 11.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | -934          |
| time/                   |               |
|    total_timesteps      | 29000         |
| train/                  |               |
|    approx_kl            | 0.00059259735 |
|    clip_fraction        | 0.00737       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0623       |
|    explained_variance   | 0.85          |
|    learning_rate        | 0.0001        |
|    loss                 | 5e+03         |
|    n_updates            | 140           |
|    policy_gradient_loss | -0.000167     |
|    value_loss           | 1.16e+04      |
-------------------------------------------
Eval num_timesteps=29500, episode_reward=-916.34 +/- 132.87
Episode length: 23.44 +/- 8.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-940.35 +/- 126.89
Episode length: 25.72 +/- 11.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-894.66 +/- 167.73
Episode length: 22.26 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -902     |
| time/              |          |
|    fps             | 356      |
|    iterations      | 15       |
|    time_elapsed    | 86       |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=-912.71 +/- 159.70
Episode length: 25.52 +/- 10.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.5         |
|    mean_reward          | -913         |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0011236148 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0594      |
|    explained_variance   | 0.812        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.85e+03     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000127    |
|    value_loss           | 1.37e+04     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-901.96 +/- 172.75
Episode length: 23.14 +/- 8.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-887.56 +/- 152.26
Episode length: 24.64 +/- 8.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -888     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-913.95 +/- 149.04
Episode length: 24.84 +/- 9.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -944     |
| time/              |          |
|    fps             | 356      |
|    iterations      | 16       |
|    time_elapsed    | 91       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=-915.15 +/- 126.67
Episode length: 23.64 +/- 8.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.6        |
|    mean_reward          | -915        |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.002158859 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0693     |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.61e+03    |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 1.11e+04    |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=-935.55 +/- 137.67
Episode length: 25.92 +/- 9.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-916.34 +/- 173.77
Episode length: 25.80 +/- 9.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-931.87 +/- 127.61
Episode length: 25.46 +/- 8.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | -934     |
| time/              |          |
|    fps             | 356      |
|    iterations      | 17       |
|    time_elapsed    | 97       |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-923.45 +/- 154.22
Episode length: 25.12 +/- 10.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.1         |
|    mean_reward          | -923         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0019344729 |
|    clip_fraction        | 0.00737      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0818      |
|    explained_variance   | 0.84         |
|    learning_rate        | 0.0001       |
|    loss                 | 8.59e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.000441     |
|    value_loss           | 1.24e+04     |
------------------------------------------
Eval num_timesteps=35500, episode_reward=-906.51 +/- 175.34
Episode length: 22.90 +/- 8.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-905.55 +/- 185.66
Episode length: 22.66 +/- 9.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-929.56 +/- 119.51
Episode length: 24.08 +/- 9.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | -932     |
| time/              |          |
|    fps             | 356      |
|    iterations      | 18       |
|    time_elapsed    | 103      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-905.55 +/- 154.32
Episode length: 24.40 +/- 9.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.4        |
|    mean_reward          | -906        |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.001375134 |
|    clip_fraction        | 0.01        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0515     |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.49e+03    |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.00182     |
|    value_loss           | 1.06e+04    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=-905.33 +/- 186.14
Episode length: 23.00 +/- 10.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-930.76 +/- 115.78
Episode length: 24.48 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-958.34 +/- 129.78
Episode length: 23.62 +/- 9.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -958     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -938     |
| time/              |          |
|    fps             | 357      |
|    iterations      | 19       |
|    time_elapsed    | 108      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-940.36 +/- 124.02
Episode length: 25.02 +/- 9.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | -940         |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0022228004 |
|    clip_fraction        | 0.00937      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0485      |
|    explained_variance   | 0.87         |
|    learning_rate        | 0.0001       |
|    loss                 | 5.17e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.00114      |
|    value_loss           | 9.92e+03     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-971.56 +/- 84.70
Episode length: 25.72 +/- 9.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -972     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-909.10 +/- 201.25
Episode length: 23.62 +/- 8.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-912.77 +/- 139.44
Episode length: 24.22 +/- 8.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 357      |
|    iterations      | 20       |
|    time_elapsed    | 114      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-918.76 +/- 142.44
Episode length: 24.30 +/- 9.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.3         |
|    mean_reward          | -919         |
| time/                   |              |
|    total_timesteps      | 41000        |
| train/                  |              |
|    approx_kl            | 0.0010220043 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0639      |
|    explained_variance   | 0.836        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.98e+03     |
|    n_updates            | 200          |
|    policy_gradient_loss | 0.000718     |
|    value_loss           | 1.18e+04     |
------------------------------------------
Eval num_timesteps=41500, episode_reward=-913.81 +/- 153.38
Episode length: 22.18 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-899.32 +/- 156.03
Episode length: 22.64 +/- 10.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -899     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-946.35 +/- 116.57
Episode length: 23.62 +/- 9.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-930.58 +/- 140.15
Episode length: 24.52 +/- 8.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -911     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 21       |
|    time_elapsed    | 120      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=-916.35 +/- 145.31
Episode length: 23.80 +/- 8.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | -916          |
| time/                   |               |
|    total_timesteps      | 43500         |
| train/                  |               |
|    approx_kl            | 0.00089174823 |
|    clip_fraction        | 0.00552       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0576       |
|    explained_variance   | 0.842         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.47e+03      |
|    n_updates            | 210           |
|    policy_gradient_loss | -0.000602     |
|    value_loss           | 1.17e+04      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=-913.63 +/- 167.93
Episode length: 23.32 +/- 8.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-933.16 +/- 131.41
Episode length: 26.98 +/- 8.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-934.15 +/- 143.64
Episode length: 24.96 +/- 8.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -934     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 22       |
|    time_elapsed    | 126      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=-943.95 +/- 142.00
Episode length: 25.08 +/- 8.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.1         |
|    mean_reward          | -944         |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0014470648 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0347      |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.0001       |
|    loss                 | 5.41e+03     |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000755    |
|    value_loss           | 8.56e+03     |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-939.15 +/- 119.43
Episode length: 24.10 +/- 9.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-891.01 +/- 187.15
Episode length: 23.54 +/- 8.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -891     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-988.35 +/- 79.79
Episode length: 25.26 +/- 9.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -988     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | -934     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 23       |
|    time_elapsed    | 132      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=-895.97 +/- 181.99
Episode length: 22.94 +/- 7.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22.9       |
|    mean_reward          | -896       |
| time/                   |            |
|    total_timesteps      | 47500      |
| train/                  |            |
|    approx_kl            | 0.05937561 |
|    clip_fraction        | 0.0113     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.191     |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.82e+03   |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.00303   |
|    value_loss           | 9.11e+03   |
----------------------------------------
Eval num_timesteps=48000, episode_reward=-946.26 +/- 147.22
Episode length: 24.68 +/- 8.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-957.16 +/- 107.66
Episode length: 24.94 +/- 8.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-929.57 +/- 130.44
Episode length: 26.06 +/- 9.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | -939     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 24       |
|    time_elapsed    | 138      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=-919.96 +/- 139.42
Episode length: 26.18 +/- 8.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.2        |
|    mean_reward          | -920        |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.011843113 |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.267      |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0001      |
|    loss                 | 5.07e+03    |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.0114      |
|    value_loss           | 1e+04       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-930.76 +/- 144.05
Episode length: 24.72 +/- 9.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-910.37 +/- 139.72
Episode length: 24.34 +/- 8.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-899.55 +/- 154.81
Episode length: 23.46 +/- 9.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -900     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -908     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 25       |
|    time_elapsed    | 143      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=-919.76 +/- 176.87
Episode length: 23.72 +/- 8.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23.7       |
|    mean_reward          | -920       |
| time/                   |            |
|    total_timesteps      | 51500      |
| train/                  |            |
|    approx_kl            | 0.18943667 |
|    clip_fraction        | 0.0601     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.78e+03   |
|    n_updates            | 250        |
|    policy_gradient_loss | 0.00837    |
|    value_loss           | 1.15e+04   |
----------------------------------------
Eval num_timesteps=52000, episode_reward=-940.36 +/- 134.59
Episode length: 25.64 +/- 10.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-941.40 +/- 138.29
Episode length: 25.80 +/- 8.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -941     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-943.94 +/- 134.17
Episode length: 24.02 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 26       |
|    time_elapsed    | 149      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=-900.63 +/- 178.93
Episode length: 22.50 +/- 8.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.5        |
|    mean_reward          | -901        |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.007570304 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.872       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.88e+03    |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.00257     |
|    value_loss           | 8.61e+03    |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=-903.00 +/- 169.04
Episode length: 25.08 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-949.89 +/- 144.16
Episode length: 24.48 +/- 10.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-883.94 +/- 152.74
Episode length: 23.30 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -884     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -939     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 27       |
|    time_elapsed    | 155      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=-952.35 +/- 126.66
Episode length: 25.44 +/- 9.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.4       |
|    mean_reward          | -952       |
| time/                   |            |
|    total_timesteps      | 55500      |
| train/                  |            |
|    approx_kl            | 0.02011453 |
|    clip_fraction        | 0.073      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.245     |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.81e+03   |
|    n_updates            | 270        |
|    policy_gradient_loss | 0.018      |
|    value_loss           | 8.74e+03   |
----------------------------------------
Eval num_timesteps=56000, episode_reward=-907.71 +/- 166.17
Episode length: 24.40 +/- 9.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-914.92 +/- 176.06
Episode length: 25.12 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-911.55 +/- 143.94
Episode length: 23.28 +/- 8.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -947     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 28       |
|    time_elapsed    | 161      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=-940.34 +/- 142.95
Episode length: 25.40 +/- 8.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.4        |
|    mean_reward          | -940        |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.023191445 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.86e+03    |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.00812     |
|    value_loss           | 7.47e+03    |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=-910.35 +/- 160.35
Episode length: 23.26 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-957.17 +/- 139.18
Episode length: 26.30 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.3     |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-960.58 +/- 121.55
Episode length: 25.64 +/- 10.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -961     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | -919     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 29       |
|    time_elapsed    | 167      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-917.52 +/- 163.54
Episode length: 24.82 +/- 9.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | -918         |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0029746632 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0359      |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.86e+03     |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000318    |
|    value_loss           | 9.75e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-967.95 +/- 116.35
Episode length: 23.08 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-887.57 +/- 148.92
Episode length: 25.42 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -888     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-900.65 +/- 168.20
Episode length: 23.70 +/- 9.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -945     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 30       |
|    time_elapsed    | 172      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-946.35 +/- 150.53
Episode length: 25.50 +/- 9.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.5         |
|    mean_reward          | -946         |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0007662154 |
|    clip_fraction        | 0.00376      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0467      |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.86e+03     |
|    n_updates            | 300          |
|    policy_gradient_loss | 0.00109      |
|    value_loss           | 7.31e+03     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-946.33 +/- 128.98
Episode length: 26.22 +/- 8.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-905.54 +/- 136.50
Episode length: 22.22 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-900.76 +/- 162.26
Episode length: 24.06 +/- 9.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -929     |
| time/              |          |
|    fps             | 355      |
|    iterations      | 31       |
|    time_elapsed    | 178      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-888.63 +/- 149.96
Episode length: 24.68 +/- 8.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.7          |
|    mean_reward          | -889          |
| time/                   |               |
|    total_timesteps      | 63500         |
| train/                  |               |
|    approx_kl            | 0.00073643797 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0319       |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.79e+03      |
|    n_updates            | 310           |
|    policy_gradient_loss | -6.83e-05     |
|    value_loss           | 8.65e+03      |
-------------------------------------------
Eval num_timesteps=64000, episode_reward=-947.33 +/- 138.01
Episode length: 24.46 +/- 9.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -947     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-946.11 +/- 149.48
Episode length: 25.08 +/- 11.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-937.76 +/- 150.67
Episode length: 22.84 +/- 8.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-864.42 +/- 196.66
Episode length: 21.54 +/- 8.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.5     |
|    mean_reward     | -864     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -920     |
| time/              |          |
|    fps             | 354      |
|    iterations      | 32       |
|    time_elapsed    | 185      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-930.76 +/- 120.68
Episode length: 23.10 +/- 8.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.1         |
|    mean_reward          | -931         |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0013908818 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0323      |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.49e+03     |
|    n_updates            | 320          |
|    policy_gradient_loss | -8.83e-05    |
|    value_loss           | 7.39e+03     |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-937.95 +/- 110.14
Episode length: 25.42 +/- 9.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-951.14 +/- 122.87
Episode length: 24.96 +/- 8.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-934.18 +/- 136.84
Episode length: 26.36 +/- 11.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 354      |
|    iterations      | 33       |
|    time_elapsed    | 190      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-906.76 +/- 129.64
Episode length: 26.64 +/- 9.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.6         |
|    mean_reward          | -907         |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0006399092 |
|    clip_fraction        | 0.00313      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00816     |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.56e+03     |
|    n_updates            | 330          |
|    policy_gradient_loss | 0.00037      |
|    value_loss           | 8.37e+03     |
------------------------------------------
Eval num_timesteps=68500, episode_reward=-951.14 +/- 110.53
Episode length: 23.22 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-925.95 +/- 144.13
Episode length: 22.42 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-901.97 +/- 184.45
Episode length: 23.32 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 34       |
|    time_elapsed    | 196      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-954.76 +/- 130.63
Episode length: 26.80 +/- 9.99
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 26.8           |
|    mean_reward          | -955           |
| time/                   |                |
|    total_timesteps      | 70000          |
| train/                  |                |
|    approx_kl            | 0.000107324944 |
|    clip_fraction        | 0.000488       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00895       |
|    explained_variance   | 0.887          |
|    learning_rate        | 0.0001         |
|    loss                 | 3.71e+03       |
|    n_updates            | 340            |
|    policy_gradient_loss | -1.99e-05      |
|    value_loss           | 7.93e+03       |
--------------------------------------------
Eval num_timesteps=70500, episode_reward=-912.74 +/- 136.82
Episode length: 23.52 +/- 9.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-917.54 +/- 149.82
Episode length: 25.18 +/- 10.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-904.34 +/- 141.99
Episode length: 23.96 +/- 9.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -908     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 35       |
|    time_elapsed    | 202      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-912.74 +/- 163.25
Episode length: 23.18 +/- 7.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.2        |
|    mean_reward          | -913        |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.000196242 |
|    clip_fraction        | 0.00112     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00923    |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.05e+04    |
|    n_updates            | 350         |
|    policy_gradient_loss | -5.72e-05   |
|    value_loss           | 1.28e+04    |
-----------------------------------------
Eval num_timesteps=72500, episode_reward=-913.95 +/- 150.97
Episode length: 24.64 +/- 10.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-891.17 +/- 173.41
Episode length: 23.92 +/- 8.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -891     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-933.14 +/- 139.44
Episode length: 24.22 +/- 11.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 36       |
|    time_elapsed    | 208      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-941.56 +/- 131.21
Episode length: 24.38 +/- 8.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.4        |
|    mean_reward          | -942        |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 6.98783e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0141     |
|    explained_variance   | 0.855       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.8e+03     |
|    n_updates            | 360         |
|    policy_gradient_loss | 1.88e-05    |
|    value_loss           | 1.02e+04    |
-----------------------------------------
Eval num_timesteps=74500, episode_reward=-921.08 +/- 162.58
Episode length: 25.66 +/- 9.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-934.35 +/- 139.31
Episode length: 25.54 +/- 9.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-937.69 +/- 163.14
Episode length: 26.58 +/- 9.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 37       |
|    time_elapsed    | 214      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-896.71 +/- 192.41
Episode length: 23.88 +/- 8.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.9         |
|    mean_reward          | -897         |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 9.004609e-06 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0118      |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.58e+03     |
|    n_updates            | 370          |
|    policy_gradient_loss | -4.17e-05    |
|    value_loss           | 7.34e+03     |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-927.16 +/- 146.59
Episode length: 24.18 +/- 9.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-931.92 +/- 167.16
Episode length: 24.94 +/- 10.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-868.35 +/- 145.11
Episode length: 22.24 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -868     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -930     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 38       |
|    time_elapsed    | 220      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-945.09 +/- 123.90
Episode length: 24.98 +/- 8.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | -945          |
| time/                   |               |
|    total_timesteps      | 78000         |
| train/                  |               |
|    approx_kl            | 2.6547292e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0.886         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.31e+03      |
|    n_updates            | 380           |
|    policy_gradient_loss | -3.66e-05     |
|    value_loss           | 8.03e+03      |
-------------------------------------------
Eval num_timesteps=78500, episode_reward=-905.56 +/- 159.82
Episode length: 24.22 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-880.37 +/- 195.28
Episode length: 23.84 +/- 8.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -880     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-904.19 +/- 147.52
Episode length: 23.38 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -907     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 39       |
|    time_elapsed    | 226      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-922.35 +/- 159.28
Episode length: 23.08 +/- 7.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.1         |
|    mean_reward          | -922         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0005277933 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0132      |
|    explained_variance   | 0.863        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.76e+03     |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.000167    |
|    value_loss           | 9.11e+03     |
------------------------------------------
Eval num_timesteps=80500, episode_reward=-967.98 +/- 121.19
Episode length: 27.14 +/- 8.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.1     |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-959.55 +/- 100.69
Episode length: 25.28 +/- 8.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -960     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-876.75 +/- 161.64
Episode length: 24.38 +/- 8.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -877     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -938     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 40       |
|    time_elapsed    | 232      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-898.13 +/- 154.30
Episode length: 22.72 +/- 9.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.7          |
|    mean_reward          | -898          |
| time/                   |               |
|    total_timesteps      | 82000         |
| train/                  |               |
|    approx_kl            | 0.00049027614 |
|    clip_fraction        | 0.00444       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0216       |
|    explained_variance   | 0.882         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.06e+03      |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.000209     |
|    value_loss           | 8.03e+03      |
-------------------------------------------
Eval num_timesteps=82500, episode_reward=-895.92 +/- 150.88
Episode length: 22.34 +/- 8.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-935.47 +/- 152.39
Episode length: 24.60 +/- 8.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -935     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-919.90 +/- 151.53
Episode length: 24.66 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -940     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 41       |
|    time_elapsed    | 237      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-909.15 +/- 171.09
Episode length: 23.26 +/- 9.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.3         |
|    mean_reward          | -909         |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0001338172 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0153      |
|    explained_variance   | 0.887        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.04e+03     |
|    n_updates            | 410          |
|    policy_gradient_loss | 4.65e-05     |
|    value_loss           | 7.62e+03     |
------------------------------------------
Eval num_timesteps=84500, episode_reward=-927.05 +/- 143.50
Episode length: 25.16 +/- 8.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-912.75 +/- 134.20
Episode length: 22.56 +/- 9.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-941.56 +/- 136.60
Episode length: 24.46 +/- 8.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-957.16 +/- 120.29
Episode length: 23.94 +/- 7.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -914     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 42       |
|    time_elapsed    | 244      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=-927.15 +/- 131.03
Episode length: 22.82 +/- 6.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.8        |
|    mean_reward          | -927        |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.024947852 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.31e+03    |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 8.36e+03    |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=-927.13 +/- 150.64
Episode length: 24.30 +/- 9.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-902.88 +/- 161.30
Episode length: 23.84 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-942.76 +/- 128.30
Episode length: 23.56 +/- 8.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -915     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 43       |
|    time_elapsed    | 249      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=-922.28 +/- 133.29
Episode length: 24.06 +/- 6.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 24.1       |
|    mean_reward          | -922       |
| time/                   |            |
|    total_timesteps      | 88500      |
| train/                  |            |
|    approx_kl            | 0.09934153 |
|    clip_fraction        | 0.0801     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.13      |
|    explained_variance   | 0.872      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.73e+03   |
|    n_updates            | 430        |
|    policy_gradient_loss | 0.011      |
|    value_loss           | 8.47e+03   |
----------------------------------------
Eval num_timesteps=89000, episode_reward=-903.05 +/- 157.44
Episode length: 23.74 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-940.34 +/- 140.89
Episode length: 23.30 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-892.19 +/- 174.58
Episode length: 22.32 +/- 8.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | -915     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 44       |
|    time_elapsed    | 255      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=-909.14 +/- 141.14
Episode length: 22.90 +/- 9.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22.9       |
|    mean_reward          | -909       |
| time/                   |            |
|    total_timesteps      | 90500      |
| train/                  |            |
|    approx_kl            | 0.02976125 |
|    clip_fraction        | 0.0181     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.068     |
|    explained_variance   | 0.884      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.05e+03   |
|    n_updates            | 440        |
|    policy_gradient_loss | 0.00469    |
|    value_loss           | 8.46e+03   |
----------------------------------------
Eval num_timesteps=91000, episode_reward=-922.36 +/- 124.23
Episode length: 24.38 +/- 8.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-919.95 +/- 164.11
Episode length: 21.84 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-941.55 +/- 112.92
Episode length: 24.08 +/- 8.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -931     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 45       |
|    time_elapsed    | 261      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=-925.95 +/- 164.20
Episode length: 24.40 +/- 7.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | -926          |
| time/                   |               |
|    total_timesteps      | 92500         |
| train/                  |               |
|    approx_kl            | 0.00060970674 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0159       |
|    explained_variance   | 0.912         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.68e+03      |
|    n_updates            | 450           |
|    policy_gradient_loss | 2.91e-05      |
|    value_loss           | 6.4e+03       |
-------------------------------------------
Eval num_timesteps=93000, episode_reward=-924.77 +/- 138.52
Episode length: 26.86 +/- 10.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.9     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-892.24 +/- 171.96
Episode length: 24.02 +/- 8.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-927.16 +/- 152.38
Episode length: 24.04 +/- 10.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -923     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 46       |
|    time_elapsed    | 266      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-916.36 +/- 124.48
Episode length: 26.04 +/- 9.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | -916          |
| time/                   |               |
|    total_timesteps      | 94500         |
| train/                  |               |
|    approx_kl            | 0.00014873958 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00995      |
|    explained_variance   | 0.893         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.5e+03       |
|    n_updates            | 460           |
|    policy_gradient_loss | 6.07e-05      |
|    value_loss           | 7.81e+03      |
-------------------------------------------
Eval num_timesteps=95000, episode_reward=-912.75 +/- 154.61
Episode length: 24.12 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-899.54 +/- 146.23
Episode length: 25.44 +/- 9.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -900     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-918.76 +/- 150.31
Episode length: 24.30 +/- 9.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -923     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 47       |
|    time_elapsed    | 272      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-925.94 +/- 144.13
Episode length: 26.08 +/- 10.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.1        |
|    mean_reward          | -926        |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.000276819 |
|    clip_fraction        | 0.000635    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0102     |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.95e+03    |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.000121   |
|    value_loss           | 8.78e+03    |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=-957.15 +/- 107.65
Episode length: 25.24 +/- 10.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-918.74 +/- 141.93
Episode length: 25.84 +/- 8.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-936.76 +/- 131.15
Episode length: 24.58 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 48       |
|    time_elapsed    | 278      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-924.76 +/- 150.95
Episode length: 23.60 +/- 9.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.6        |
|    mean_reward          | -925        |
| time/                   |             |
|    total_timesteps      | 98500       |
| train/                  |             |
|    approx_kl            | 0.075482205 |
|    clip_fraction        | 0.0282      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.45e+03    |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00548    |
|    value_loss           | 5.86e+03    |
-----------------------------------------
Eval num_timesteps=99000, episode_reward=-926.93 +/- 138.34
Episode length: 22.40 +/- 8.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-900.78 +/- 144.92
Episode length: 26.42 +/- 9.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-939.15 +/- 134.73
Episode length: 24.06 +/- 10.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 49       |
|    time_elapsed    | 284      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-950.97 +/- 136.51
Episode length: 25.92 +/- 9.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.9       |
|    mean_reward          | -951       |
| time/                   |            |
|    total_timesteps      | 100500     |
| train/                  |            |
|    approx_kl            | 0.20269208 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.167     |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.5e+03    |
|    n_updates            | 490        |
|    policy_gradient_loss | 0.0163     |
|    value_loss           | 7.26e+03   |
----------------------------------------
Eval num_timesteps=101000, episode_reward=-934.20 +/- 139.74
Episode length: 25.54 +/- 8.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-922.37 +/- 152.34
Episode length: 23.96 +/- 8.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-913.93 +/- 161.58
Episode length: 23.36 +/- 8.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -938     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 50       |
|    time_elapsed    | 289      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-909.15 +/- 140.61
Episode length: 26.14 +/- 10.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.1        |
|    mean_reward          | -909        |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.004321814 |
|    clip_fraction        | 0.00806     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0205     |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.18e+03    |
|    n_updates            | 500         |
|    policy_gradient_loss | 0.00117     |
|    value_loss           | 7.74e+03    |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=-928.35 +/- 118.32
Episode length: 24.04 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-903.16 +/- 153.52
Episode length: 25.82 +/- 11.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-947.44 +/- 151.68
Episode length: 27.00 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | -947     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -926     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 51       |
|    time_elapsed    | 295      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-942.76 +/- 125.45
Episode length: 24.52 +/- 8.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.5          |
|    mean_reward          | -943          |
| time/                   |               |
|    total_timesteps      | 104500        |
| train/                  |               |
|    approx_kl            | 0.00020782501 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0215       |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.75e+03      |
|    n_updates            | 510           |
|    policy_gradient_loss | 0.000186      |
|    value_loss           | 5.89e+03      |
-------------------------------------------
Eval num_timesteps=105000, episode_reward=-916.36 +/- 151.61
Episode length: 24.00 +/- 10.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-883.64 +/- 208.36
Episode length: 22.38 +/- 8.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -884     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-907.96 +/- 135.77
Episode length: 24.14 +/- 8.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 52       |
|    time_elapsed    | 301      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-891.12 +/- 176.05
Episode length: 24.22 +/- 7.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | -891         |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0001543739 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.024       |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.5e+03      |
|    n_updates            | 520          |
|    policy_gradient_loss | 9.86e-05     |
|    value_loss           | 7.58e+03     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=-909.15 +/- 153.81
Episode length: 22.90 +/- 7.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-918.60 +/- 173.53
Episode length: 23.60 +/- 8.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-919.96 +/- 139.95
Episode length: 23.30 +/- 7.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-886.38 +/- 150.53
Episode length: 25.14 +/- 9.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -886     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -910     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 53       |
|    time_elapsed    | 307      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-933.15 +/- 135.18
Episode length: 22.88 +/- 7.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.9          |
|    mean_reward          | -933          |
| time/                   |               |
|    total_timesteps      | 109000        |
| train/                  |               |
|    approx_kl            | 1.7190148e-05 |
|    clip_fraction        | 0.000537      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0176       |
|    explained_variance   | 0.876         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.78e+03      |
|    n_updates            | 530           |
|    policy_gradient_loss | 8.77e-06      |
|    value_loss           | 8.94e+03      |
-------------------------------------------
Eval num_timesteps=109500, episode_reward=-905.57 +/- 161.61
Episode length: 24.92 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-965.56 +/- 127.77
Episode length: 25.34 +/- 9.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -966     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-929.33 +/- 165.37
Episode length: 25.92 +/- 10.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -929     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 54       |
|    time_elapsed    | 313      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-958.27 +/- 146.13
Episode length: 26.18 +/- 8.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.2         |
|    mean_reward          | -958         |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 3.609166e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0184      |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.59e+03     |
|    n_updates            | 540          |
|    policy_gradient_loss | -1.76e-05    |
|    value_loss           | 8.15e+03     |
------------------------------------------
Eval num_timesteps=111500, episode_reward=-934.34 +/- 147.34
Episode length: 24.52 +/- 9.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-943.76 +/- 135.95
Episode length: 25.38 +/- 9.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-941.56 +/- 114.18
Episode length: 24.10 +/- 8.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | -889     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 55       |
|    time_elapsed    | 319      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-934.35 +/- 136.70
Episode length: 23.22 +/- 7.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | -934          |
| time/                   |               |
|    total_timesteps      | 113000        |
| train/                  |               |
|    approx_kl            | 0.00023078482 |
|    clip_fraction        | 0.00225       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0211       |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.57e+03      |
|    n_updates            | 550           |
|    policy_gradient_loss | 0.000101      |
|    value_loss           | 9.18e+03      |
-------------------------------------------
Eval num_timesteps=113500, episode_reward=-900.73 +/- 160.89
Episode length: 23.90 +/- 8.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-930.75 +/- 117.63
Episode length: 23.74 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-913.95 +/- 148.56
Episode length: 23.40 +/- 8.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -912     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 56       |
|    time_elapsed    | 324      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-927.06 +/- 152.73
Episode length: 23.54 +/- 7.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.5        |
|    mean_reward          | -927        |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.022852512 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.03e+03    |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.0024     |
|    value_loss           | 7.11e+03    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=-916.35 +/- 135.54
Episode length: 22.56 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-922.33 +/- 146.07
Episode length: 23.58 +/- 8.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-862.94 +/- 198.13
Episode length: 22.58 +/- 9.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -863     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.1     |
|    ep_rew_mean     | -898     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 57       |
|    time_elapsed    | 330      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-903.13 +/- 153.50
Episode length: 22.18 +/- 8.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.2        |
|    mean_reward          | -903        |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.042849656 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.42e+03    |
|    n_updates            | 570         |
|    policy_gradient_loss | 0.0225      |
|    value_loss           | 1.08e+04    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=-984.75 +/- 117.12
Episode length: 25.72 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -985     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-907.95 +/- 162.77
Episode length: 24.02 +/- 11.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-930.57 +/- 172.88
Episode length: 25.44 +/- 11.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -912     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 58       |
|    time_elapsed    | 336      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-945.03 +/- 150.38
Episode length: 26.08 +/- 9.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.1        |
|    mean_reward          | -945        |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.075590804 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.46e+03    |
|    n_updates            | 580         |
|    policy_gradient_loss | 0.00791     |
|    value_loss           | 7.59e+03    |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=-940.36 +/- 128.57
Episode length: 23.30 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-939.10 +/- 135.18
Episode length: 26.10 +/- 10.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-925.92 +/- 167.31
Episode length: 24.76 +/- 10.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -915     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 59       |
|    time_elapsed    | 342      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-905.56 +/- 135.95
Episode length: 23.88 +/- 7.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.9          |
|    mean_reward          | -906          |
| time/                   |               |
|    total_timesteps      | 121000        |
| train/                  |               |
|    approx_kl            | 0.00047597816 |
|    clip_fraction        | 0.00552       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0652       |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.37e+03      |
|    n_updates            | 590           |
|    policy_gradient_loss | 0.000194      |
|    value_loss           | 6.49e+03      |
-------------------------------------------
Eval num_timesteps=121500, episode_reward=-905.33 +/- 155.68
Episode length: 23.90 +/- 9.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-909.16 +/- 134.31
Episode length: 22.36 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-931.97 +/- 125.86
Episode length: 24.54 +/- 9.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 60       |
|    time_elapsed    | 347      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-883.79 +/- 156.34
Episode length: 22.16 +/- 7.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.2        |
|    mean_reward          | -884        |
| time/                   |             |
|    total_timesteps      | 123000      |
| train/                  |             |
|    approx_kl            | 0.020975076 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0562     |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.58e+03    |
|    n_updates            | 600         |
|    policy_gradient_loss | 0.00551     |
|    value_loss           | 8.58e+03    |
-----------------------------------------
Eval num_timesteps=123500, episode_reward=-907.94 +/- 135.76
Episode length: 22.96 +/- 7.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-903.90 +/- 199.83
Episode length: 24.26 +/- 9.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-937.95 +/- 126.58
Episode length: 26.92 +/- 11.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.9     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -907     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 61       |
|    time_elapsed    | 353      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-939.14 +/- 145.51
Episode length: 24.32 +/- 8.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.3          |
|    mean_reward          | -939          |
| time/                   |               |
|    total_timesteps      | 125000        |
| train/                  |               |
|    approx_kl            | 0.00021574012 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0.892         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.57e+03      |
|    n_updates            | 610           |
|    policy_gradient_loss | 0.000116      |
|    value_loss           | 6.99e+03      |
-------------------------------------------
Eval num_timesteps=125500, episode_reward=-893.54 +/- 171.01
Episode length: 23.80 +/- 8.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -894     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-900.73 +/- 186.22
Episode length: 23.60 +/- 10.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-928.34 +/- 150.98
Episode length: 24.66 +/- 8.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -923     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 62       |
|    time_elapsed    | 359      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-905.57 +/- 145.16
Episode length: 25.02 +/- 8.70
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 25             |
|    mean_reward          | -906           |
| time/                   |                |
|    total_timesteps      | 127000         |
| train/                  |                |
|    approx_kl            | 0.000122043304 |
|    clip_fraction        | 0.00166        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0108        |
|    explained_variance   | 0.906          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.8e+03        |
|    n_updates            | 620            |
|    policy_gradient_loss | -0.000225      |
|    value_loss           | 6.92e+03       |
--------------------------------------------
Eval num_timesteps=127500, episode_reward=-915.16 +/- 124.96
Episode length: 25.10 +/- 8.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-966.76 +/- 108.46
Episode length: 25.76 +/- 8.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-917.48 +/- 171.82
Episode length: 24.24 +/- 9.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -917     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-886.33 +/- 158.92
Episode length: 20.70 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.7     |
|    mean_reward     | -886     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -940     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 63       |
|    time_elapsed    | 365      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-874.19 +/- 179.55
Episode length: 22.66 +/- 8.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.7         |
|    mean_reward          | -874         |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 0.0011163572 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.27e+03     |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000223    |
|    value_loss           | 5.77e+03     |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-922.34 +/- 123.65
Episode length: 23.40 +/- 8.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-927.15 +/- 139.03
Episode length: 22.86 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-906.77 +/- 152.59
Episode length: 24.64 +/- 8.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -940     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 64       |
|    time_elapsed    | 371      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-917.57 +/- 160.44
Episode length: 23.74 +/- 8.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.7         |
|    mean_reward          | -918         |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0011236677 |
|    clip_fraction        | 0.00132      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0146      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.26e+03     |
|    n_updates            | 640          |
|    policy_gradient_loss | -2.81e-05    |
|    value_loss           | 6.46e+03     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-913.93 +/- 151.03
Episode length: 23.44 +/- 8.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-884.47 +/- 208.91
Episode length: 22.86 +/- 8.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -884     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-912.76 +/- 164.13
Episode length: 24.60 +/- 10.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | -938     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 65       |
|    time_elapsed    | 377      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-931.95 +/- 124.14
Episode length: 24.28 +/- 9.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.3         |
|    mean_reward          | -932         |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0022704648 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0438      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.39e+03     |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.000357    |
|    value_loss           | 6.09e+03     |
------------------------------------------
Eval num_timesteps=134000, episode_reward=-913.88 +/- 159.73
Episode length: 22.36 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-949.95 +/- 137.48
Episode length: 23.00 +/- 8.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-914.95 +/- 168.77
Episode length: 25.68 +/- 9.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26       |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 66       |
|    time_elapsed    | 382      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-941.53 +/- 159.06
Episode length: 24.32 +/- 9.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.3        |
|    mean_reward          | -942        |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.001103472 |
|    clip_fraction        | 0.00986     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.5e+03     |
|    n_updates            | 660         |
|    policy_gradient_loss | 0.000942    |
|    value_loss           | 7.18e+03    |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=-909.12 +/- 150.62
Episode length: 23.48 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-904.34 +/- 132.55
Episode length: 23.24 +/- 7.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-933.10 +/- 152.91
Episode length: 24.38 +/- 8.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -908     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 67       |
|    time_elapsed    | 388      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-907.94 +/- 137.37
Episode length: 23.82 +/- 10.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.8        |
|    mean_reward          | -908        |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.027555527 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0527     |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.47e+03    |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.00375     |
|    value_loss           | 7.69e+03    |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=-930.79 +/- 141.01
Episode length: 25.38 +/- 9.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-971.54 +/- 105.85
Episode length: 26.26 +/- 9.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.3     |
|    mean_reward     | -972     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-885.02 +/- 160.69
Episode length: 23.52 +/- 8.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -885     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -906     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 68       |
|    time_elapsed    | 394      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-941.46 +/- 118.53
Episode length: 25.06 +/- 7.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.1          |
|    mean_reward          | -941          |
| time/                   |               |
|    total_timesteps      | 139500        |
| train/                  |               |
|    approx_kl            | 0.00019089418 |
|    clip_fraction        | 0.00107       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0138       |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.1e+03       |
|    n_updates            | 680           |
|    policy_gradient_loss | -5.78e-05     |
|    value_loss           | 8.38e+03      |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-893.53 +/- 161.90
Episode length: 24.20 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -894     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-911.54 +/- 148.82
Episode length: 25.06 +/- 9.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-918.75 +/- 175.09
Episode length: 23.28 +/- 8.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -926     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 69       |
|    time_elapsed    | 400      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-924.74 +/- 116.48
Episode length: 23.74 +/- 8.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.7          |
|    mean_reward          | -925          |
| time/                   |               |
|    total_timesteps      | 141500        |
| train/                  |               |
|    approx_kl            | 0.00021798976 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0232       |
|    explained_variance   | 0.888         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.56e+03      |
|    n_updates            | 690           |
|    policy_gradient_loss | -0.000156     |
|    value_loss           | 7.93e+03      |
-------------------------------------------
Eval num_timesteps=142000, episode_reward=-945.16 +/- 127.38
Episode length: 25.10 +/- 9.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -945     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-918.77 +/- 157.78
Episode length: 25.02 +/- 9.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-928.36 +/- 119.54
Episode length: 24.14 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 70       |
|    time_elapsed    | 405      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-942.76 +/- 109.52
Episode length: 23.94 +/- 7.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.9         |
|    mean_reward          | -943         |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 0.0010857263 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0322      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.12e+03     |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.000202    |
|    value_loss           | 5.51e+03     |
------------------------------------------
Eval num_timesteps=144000, episode_reward=-915.01 +/- 153.47
Episode length: 25.16 +/- 8.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-925.95 +/- 153.33
Episode length: 22.92 +/- 8.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-963.16 +/- 123.92
Episode length: 24.10 +/- 9.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -963     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -910     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 71       |
|    time_elapsed    | 411      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-949.96 +/- 127.15
Episode length: 23.36 +/- 8.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.4        |
|    mean_reward          | -950        |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.017495845 |
|    clip_fraction        | 0.00732     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0922     |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.85e+03    |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.000226    |
|    value_loss           | 7.68e+03    |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=-940.36 +/- 125.75
Episode length: 24.68 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-921.16 +/- 132.05
Episode length: 24.90 +/- 9.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-913.76 +/- 159.91
Episode length: 22.60 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 72       |
|    time_elapsed    | 417      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-935.49 +/- 149.92
Episode length: 26.36 +/- 8.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.4        |
|    mean_reward          | -935        |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.035478998 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.26e+03    |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.00365     |
|    value_loss           | 7.18e+03    |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=-904.36 +/- 165.84
Episode length: 22.96 +/- 8.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-922.31 +/- 155.82
Episode length: 25.34 +/- 8.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-922.05 +/- 172.28
Episode length: 25.22 +/- 10.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-905.42 +/- 210.76
Episode length: 23.90 +/- 9.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -930     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 73       |
|    time_elapsed    | 423      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-916.34 +/- 148.73
Episode length: 25.80 +/- 10.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 25.8      |
|    mean_reward          | -916      |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0215475 |
|    clip_fraction        | 0.0348    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.124    |
|    explained_variance   | 0.906     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.92e+03  |
|    n_updates            | 730       |
|    policy_gradient_loss | 0.00443   |
|    value_loss           | 5.67e+03  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=-912.75 +/- 139.97
Episode length: 25.46 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-941.55 +/- 138.16
Episode length: 24.18 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-923.56 +/- 149.97
Episode length: 25.94 +/- 8.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 74       |
|    time_elapsed    | 429      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-943.91 +/- 132.00
Episode length: 24.74 +/- 7.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 24.7       |
|    mean_reward          | -944       |
| time/                   |            |
|    total_timesteps      | 152000     |
| train/                  |            |
|    approx_kl            | 0.01618521 |
|    clip_fraction        | 0.00962    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.045     |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0001     |
|    loss                 | 2.63e+03   |
|    n_updates            | 740        |
|    policy_gradient_loss | 0.00164    |
|    value_loss           | 5.72e+03   |
----------------------------------------
Eval num_timesteps=152500, episode_reward=-921.15 +/- 151.84
Episode length: 22.64 +/- 7.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-954.76 +/- 131.73
Episode length: 25.64 +/- 10.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -955     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-895.95 +/- 167.99
Episode length: 22.68 +/- 9.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 75       |
|    time_elapsed    | 435      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-929.55 +/- 146.07
Episode length: 24.62 +/- 8.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | -930          |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | 0.00028610163 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0139       |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.51e+03      |
|    n_updates            | 750           |
|    policy_gradient_loss | 3.85e-05      |
|    value_loss           | 6.39e+03      |
-------------------------------------------
Eval num_timesteps=154500, episode_reward=-916.21 +/- 120.26
Episode length: 25.02 +/- 9.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-936.52 +/- 154.93
Episode length: 24.00 +/- 9.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-923.54 +/- 138.51
Episode length: 22.98 +/- 9.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | -883     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 76       |
|    time_elapsed    | 441      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-901.96 +/- 148.55
Episode length: 24.76 +/- 10.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | -902         |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0003589516 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0157      |
|    explained_variance   | 0.858        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.28e+03     |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.000274    |
|    value_loss           | 9.96e+03     |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-947.55 +/- 117.59
Episode length: 25.04 +/- 9.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-917.34 +/- 168.87
Episode length: 22.18 +/- 9.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -917     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-870.58 +/- 194.45
Episode length: 23.64 +/- 8.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -871     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -903     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 77       |
|    time_elapsed    | 446      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-918.75 +/- 141.93
Episode length: 23.92 +/- 8.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.9          |
|    mean_reward          | -919          |
| time/                   |               |
|    total_timesteps      | 158000        |
| train/                  |               |
|    approx_kl            | 0.00063354603 |
|    clip_fraction        | 0.00239       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00818      |
|    explained_variance   | 0.907         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.88e+03      |
|    n_updates            | 770           |
|    policy_gradient_loss | -0.000143     |
|    value_loss           | 6.41e+03      |
-------------------------------------------
Eval num_timesteps=158500, episode_reward=-916.37 +/- 167.41
Episode length: 25.66 +/- 10.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-934.36 +/- 135.65
Episode length: 25.06 +/- 9.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-917.29 +/- 185.17
Episode length: 23.36 +/- 8.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -917     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | -904     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 78       |
|    time_elapsed    | 452      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-913.95 +/- 160.21
Episode length: 22.02 +/- 7.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22            |
|    mean_reward          | -914          |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 3.6117854e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00712      |
|    explained_variance   | 0.913         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.16e+03      |
|    n_updates            | 780           |
|    policy_gradient_loss | -2.3e-05      |
|    value_loss           | 6.54e+03      |
-------------------------------------------
Eval num_timesteps=160500, episode_reward=-923.49 +/- 146.80
Episode length: 24.06 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -923     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-924.76 +/- 136.94
Episode length: 25.36 +/- 10.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-918.70 +/- 126.39
Episode length: 24.08 +/- 8.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 79       |
|    time_elapsed    | 458      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-891.14 +/- 143.43
Episode length: 21.88 +/- 6.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.9          |
|    mean_reward          | -891          |
| time/                   |               |
|    total_timesteps      | 162000        |
| train/                  |               |
|    approx_kl            | 8.6494256e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00883      |
|    explained_variance   | 0.882         |
|    learning_rate        | 0.0001        |
|    loss                 | 7.45e+03      |
|    n_updates            | 790           |
|    policy_gradient_loss | -0.000182     |
|    value_loss           | 7.35e+03      |
-------------------------------------------
Eval num_timesteps=162500, episode_reward=-895.94 +/- 152.75
Episode length: 24.78 +/- 9.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-879.15 +/- 172.67
Episode length: 24.22 +/- 10.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -879     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-894.59 +/- 158.03
Episode length: 22.24 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 80       |
|    time_elapsed    | 463      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-893.55 +/- 166.26
Episode length: 25.32 +/- 8.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.3         |
|    mean_reward          | -894         |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0024145946 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.855        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.65e+03     |
|    n_updates            | 800          |
|    policy_gradient_loss | 0.000633     |
|    value_loss           | 9.69e+03     |
------------------------------------------
Eval num_timesteps=164500, episode_reward=-966.75 +/- 122.20
Episode length: 26.46 +/- 9.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.5     |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-939.16 +/- 147.97
Episode length: 26.20 +/- 9.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-924.52 +/- 182.71
Episode length: 25.52 +/- 8.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 81       |
|    time_elapsed    | 469      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-927.15 +/- 129.93
Episode length: 24.90 +/- 8.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.9          |
|    mean_reward          | -927          |
| time/                   |               |
|    total_timesteps      | 166000        |
| train/                  |               |
|    approx_kl            | 3.5157427e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00429      |
|    explained_variance   | 0.899         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.93e+03      |
|    n_updates            | 810           |
|    policy_gradient_loss | -1.39e-05     |
|    value_loss           | 7.09e+03      |
-------------------------------------------
Eval num_timesteps=166500, episode_reward=-949.92 +/- 129.98
Episode length: 25.18 +/- 8.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-877.89 +/- 150.05
Episode length: 22.20 +/- 8.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -878     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-922.35 +/- 142.56
Episode length: 24.00 +/- 10.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 82       |
|    time_elapsed    | 475      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-937.96 +/- 140.59
Episode length: 24.04 +/- 8.18
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24             |
|    mean_reward          | -938           |
| time/                   |                |
|    total_timesteps      | 168000         |
| train/                  |                |
|    approx_kl            | 0.000121295016 |
|    clip_fraction        | 0.000391       |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00669       |
|    explained_variance   | 0.921          |
|    learning_rate        | 0.0001         |
|    loss                 | 3.32e+03       |
|    n_updates            | 820            |
|    policy_gradient_loss | -0.000111      |
|    value_loss           | 6.2e+03        |
--------------------------------------------
Eval num_timesteps=168500, episode_reward=-930.58 +/- 122.33
Episode length: 24.36 +/- 10.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-936.76 +/- 129.49
Episode length: 24.46 +/- 8.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-937.96 +/- 152.86
Episode length: 24.38 +/- 8.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -915     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 83       |
|    time_elapsed    | 481      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-899.39 +/- 165.48
Episode length: 23.04 +/- 7.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23           |
|    mean_reward          | -899         |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 4.870558e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00464     |
|    explained_variance   | 0.903        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.84e+03     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.000145    |
|    value_loss           | 7.06e+03     |
------------------------------------------
Eval num_timesteps=170500, episode_reward=-920.93 +/- 168.47
Episode length: 26.10 +/- 10.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-922.35 +/- 160.19
Episode length: 23.70 +/- 8.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-911.55 +/- 137.76
Episode length: 23.74 +/- 9.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-894.77 +/- 162.09
Episode length: 23.56 +/- 7.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -901     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 84       |
|    time_elapsed    | 487      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-919.70 +/- 169.70
Episode length: 23.52 +/- 8.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.5          |
|    mean_reward          | -920          |
| time/                   |               |
|    total_timesteps      | 172500        |
| train/                  |               |
|    approx_kl            | 3.0655414e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00537      |
|    explained_variance   | 0.894         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.14e+03      |
|    n_updates            | 840           |
|    policy_gradient_loss | -4.14e-05     |
|    value_loss           | 7.99e+03      |
-------------------------------------------
Eval num_timesteps=173000, episode_reward=-935.55 +/- 126.22
Episode length: 25.42 +/- 9.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-915.04 +/- 165.90
Episode length: 23.52 +/- 9.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-871.74 +/- 203.76
Episode length: 23.42 +/- 8.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -872     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | -949     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 85       |
|    time_elapsed    | 493      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-930.75 +/- 135.82
Episode length: 24.78 +/- 9.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | -931         |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 9.676808e-05 |
|    clip_fraction        | 0.00103      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00411     |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.35e+03     |
|    n_updates            | 850          |
|    policy_gradient_loss | 0.000211     |
|    value_loss           | 5.72e+03     |
------------------------------------------
Eval num_timesteps=175000, episode_reward=-943.96 +/- 124.71
Episode length: 25.98 +/- 8.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-930.76 +/- 159.71
Episode length: 24.76 +/- 8.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-893.56 +/- 149.87
Episode length: 24.46 +/- 8.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -894     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 86       |
|    time_elapsed    | 498      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-872.79 +/- 199.96
Episode length: 23.74 +/- 9.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.7          |
|    mean_reward          | -873          |
| time/                   |               |
|    total_timesteps      | 176500        |
| train/                  |               |
|    approx_kl            | 1.3038516e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00321      |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.47e+03      |
|    n_updates            | 860           |
|    policy_gradient_loss | -1.65e-05     |
|    value_loss           | 5.89e+03      |
-------------------------------------------
Eval num_timesteps=177000, episode_reward=-910.25 +/- 185.82
Episode length: 23.30 +/- 8.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-927.16 +/- 145.12
Episode length: 23.50 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-925.81 +/- 129.25
Episode length: 23.30 +/- 8.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -909     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 87       |
|    time_elapsed    | 504      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-952.34 +/- 134.38
Episode length: 25.44 +/- 10.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | -952          |
| time/                   |               |
|    total_timesteps      | 178500        |
| train/                  |               |
|    approx_kl            | 1.1152588e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00586      |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.67e+03      |
|    n_updates            | 870           |
|    policy_gradient_loss | 1.29e-06      |
|    value_loss           | 6.51e+03      |
-------------------------------------------
Eval num_timesteps=179000, episode_reward=-900.76 +/- 147.37
Episode length: 24.14 +/- 9.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-909.07 +/- 172.67
Episode length: 22.22 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-936.76 +/- 150.10
Episode length: 24.08 +/- 9.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | -893     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 88       |
|    time_elapsed    | 510      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-936.59 +/- 155.35
Episode length: 23.82 +/- 8.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | -937          |
| time/                   |               |
|    total_timesteps      | 180500        |
| train/                  |               |
|    approx_kl            | 9.9290395e-05 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00696      |
|    explained_variance   | 0.851         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.05e+03      |
|    n_updates            | 880           |
|    policy_gradient_loss | 2.01e-05      |
|    value_loss           | 9.8e+03       |
-------------------------------------------
Eval num_timesteps=181000, episode_reward=-898.33 +/- 134.66
Episode length: 25.80 +/- 9.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-948.76 +/- 123.92
Episode length: 22.46 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-948.77 +/- 107.09
Episode length: 24.14 +/- 8.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -917     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 89       |
|    time_elapsed    | 515      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-909.15 +/- 169.40
Episode length: 25.54 +/- 11.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.5          |
|    mean_reward          | -909          |
| time/                   |               |
|    total_timesteps      | 182500        |
| train/                  |               |
|    approx_kl            | 0.00025517773 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00242      |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.08e+03      |
|    n_updates            | 890           |
|    policy_gradient_loss | -0.00019      |
|    value_loss           | 5.21e+03      |
-------------------------------------------
Eval num_timesteps=183000, episode_reward=-906.24 +/- 191.34
Episode length: 23.72 +/- 9.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-912.66 +/- 145.75
Episode length: 23.90 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-929.55 +/- 125.95
Episode length: 22.70 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 90       |
|    time_elapsed    | 521      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-945.13 +/- 140.33
Episode length: 24.20 +/- 7.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | -945          |
| time/                   |               |
|    total_timesteps      | 184500        |
| train/                  |               |
|    approx_kl            | 4.8637972e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00202      |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.94e+03      |
|    n_updates            | 900           |
|    policy_gradient_loss | -6.13e-06     |
|    value_loss           | 6.78e+03      |
-------------------------------------------
Eval num_timesteps=185000, episode_reward=-946.16 +/- 146.93
Episode length: 25.40 +/- 9.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-928.34 +/- 146.58
Episode length: 24.12 +/- 9.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-875.57 +/- 159.48
Episode length: 24.42 +/- 10.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -876     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 91       |
|    time_elapsed    | 527      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-901.84 +/- 161.68
Episode length: 24.76 +/- 10.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | -902          |
| time/                   |               |
|    total_timesteps      | 186500        |
| train/                  |               |
|    approx_kl            | 1.1117663e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00152      |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.65e+03      |
|    n_updates            | 910           |
|    policy_gradient_loss | -3.1e-06      |
|    value_loss           | 5.71e+03      |
-------------------------------------------
Eval num_timesteps=187000, episode_reward=-939.16 +/- 118.24
Episode length: 26.20 +/- 10.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-900.78 +/- 155.93
Episode length: 24.66 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-902.96 +/- 172.83
Episode length: 25.24 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 92       |
|    time_elapsed    | 533      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-918.76 +/- 140.91
Episode length: 23.70 +/- 8.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.7          |
|    mean_reward          | -919          |
| time/                   |               |
|    total_timesteps      | 188500        |
| train/                  |               |
|    approx_kl            | 2.6106136e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00242      |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.72e+03      |
|    n_updates            | 920           |
|    policy_gradient_loss | -2.82e-07     |
|    value_loss           | 6.47e+03      |
-------------------------------------------
Eval num_timesteps=189000, episode_reward=-969.09 +/- 120.30
Episode length: 24.24 +/- 9.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -969     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-905.38 +/- 150.69
Episode length: 22.26 +/- 8.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-948.75 +/- 148.76
Episode length: 22.20 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 353      |
|    iterations      | 93       |
|    time_elapsed    | 539      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-942.70 +/- 151.48
Episode length: 27.46 +/- 11.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.5          |
|    mean_reward          | -943          |
| time/                   |               |
|    total_timesteps      | 190500        |
| train/                  |               |
|    approx_kl            | 9.8989665e-05 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00237      |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.62e+03      |
|    n_updates            | 930           |
|    policy_gradient_loss | -0.00015      |
|    value_loss           | 6.95e+03      |
-------------------------------------------
Eval num_timesteps=191000, episode_reward=-924.74 +/- 141.09
Episode length: 23.00 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-967.96 +/- 117.57
Episode length: 25.46 +/- 9.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-929.39 +/- 169.37
Episode length: 25.30 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -929     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-927.15 +/- 131.57
Episode length: 23.96 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -943     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 94       |
|    time_elapsed    | 545      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-940.36 +/- 111.81
Episode length: 24.64 +/- 8.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | -940         |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0003070299 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00178     |
|    explained_variance   | 0.942        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.18e+03     |
|    n_updates            | 940          |
|    policy_gradient_loss | 9.91e-05     |
|    value_loss           | 4.3e+03      |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-882.76 +/- 182.09
Episode length: 22.12 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -883     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-933.14 +/- 155.04
Episode length: 25.04 +/- 10.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-905.43 +/- 189.28
Episode length: 25.06 +/- 9.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | -867     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 95       |
|    time_elapsed    | 551      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-925.96 +/- 125.43
Episode length: 23.14 +/- 7.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.1          |
|    mean_reward          | -926          |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 2.5029294e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00126      |
|    explained_variance   | 0.855         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.98e+03      |
|    n_updates            | 950           |
|    policy_gradient_loss | -4.43e-06     |
|    value_loss           | 1e+04         |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=-913.94 +/- 164.21
Episode length: 24.38 +/- 8.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-893.21 +/- 153.41
Episode length: 22.80 +/- 10.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -893     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-942.76 +/- 123.14
Episode length: 25.30 +/- 9.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -893     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 96       |
|    time_elapsed    | 557      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-918.77 +/- 151.75
Episode length: 24.88 +/- 7.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.9          |
|    mean_reward          | -919          |
| time/                   |               |
|    total_timesteps      | 197000        |
| train/                  |               |
|    approx_kl            | 2.1827873e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00134      |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.07e+03      |
|    n_updates            | 960           |
|    policy_gradient_loss | -6.2e-06      |
|    value_loss           | 7.74e+03      |
-------------------------------------------
Eval num_timesteps=197500, episode_reward=-891.15 +/- 162.70
Episode length: 22.10 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -891     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-927.16 +/- 138.00
Episode length: 23.52 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-923.55 +/- 136.93
Episode length: 24.56 +/- 9.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 97       |
|    time_elapsed    | 563      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-912.63 +/- 154.45
Episode length: 24.32 +/- 11.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.3          |
|    mean_reward          | -913          |
| time/                   |               |
|    total_timesteps      | 199000        |
| train/                  |               |
|    approx_kl            | 1.8975697e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00127      |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.81e+03      |
|    n_updates            | 970           |
|    policy_gradient_loss | -1.98e-06     |
|    value_loss           | 6.55e+03      |
-------------------------------------------
Eval num_timesteps=199500, episode_reward=-929.55 +/- 160.66
Episode length: 26.62 +/- 11.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-894.75 +/- 148.20
Episode length: 21.92 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.9     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-928.35 +/- 140.06
Episode length: 22.86 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -915     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 98       |
|    time_elapsed    | 569      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-907.85 +/- 131.03
Episode length: 25.22 +/- 9.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | -908          |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 1.5919795e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00174      |
|    explained_variance   | 0.902         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.1e+03       |
|    n_updates            | 980           |
|    policy_gradient_loss | -6.07e-06     |
|    value_loss           | 7.47e+03      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=-933.15 +/- 141.43
Episode length: 25.42 +/- 7.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-951.17 +/- 135.15
Episode length: 25.52 +/- 8.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-931.96 +/- 133.63
Episode length: 28.30 +/- 10.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.3     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 99       |
|    time_elapsed    | 575      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-954.75 +/- 114.16
Episode length: 24.94 +/- 8.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.9         |
|    mean_reward          | -955         |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 2.066372e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00122     |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.42e+03     |
|    n_updates            | 990          |
|    policy_gradient_loss | -6.15e-06    |
|    value_loss           | 6.36e+03     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=-923.55 +/- 115.86
Episode length: 24.60 +/- 10.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-901.95 +/- 167.67
Episode length: 21.52 +/- 8.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.5     |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-915.03 +/- 126.79
Episode length: 27.10 +/- 10.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.1     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 100      |
|    time_elapsed    | 580      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-900.75 +/- 148.84
Episode length: 20.86 +/- 6.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.9          |
|    mean_reward          | -901          |
| time/                   |               |
|    total_timesteps      | 205000        |
| train/                  |               |
|    approx_kl            | 2.3457687e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00204      |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.9e+03       |
|    n_updates            | 1000          |
|    policy_gradient_loss | -1.85e-06     |
|    value_loss           | 7.25e+03      |
-------------------------------------------
Eval num_timesteps=205500, episode_reward=-864.75 +/- 162.88
Episode length: 23.56 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -865     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-937.95 +/- 151.92
Episode length: 24.76 +/- 8.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-943.96 +/- 105.29
Episode length: 23.68 +/- 7.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -932     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 101      |
|    time_elapsed    | 586      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-965.51 +/- 119.75
Episode length: 24.26 +/- 8.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.3          |
|    mean_reward          | -966          |
| time/                   |               |
|    total_timesteps      | 207000        |
| train/                  |               |
|    approx_kl            | 1.9557774e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0011       |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.08e+03      |
|    n_updates            | 1010          |
|    policy_gradient_loss | 1.94e-06      |
|    value_loss           | 6.54e+03      |
-------------------------------------------
Eval num_timesteps=207500, episode_reward=-941.22 +/- 150.01
Episode length: 24.54 +/- 9.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -941     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-912.70 +/- 154.28
Episode length: 24.96 +/- 9.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-888.68 +/- 138.91
Episode length: 23.68 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -889     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 102      |
|    time_elapsed    | 592      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-909.03 +/- 162.71
Episode length: 23.78 +/- 8.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | -909          |
| time/                   |               |
|    total_timesteps      | 209000        |
| train/                  |               |
|    approx_kl            | 2.6193447e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000714     |
|    explained_variance   | 0.892         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.15e+03      |
|    n_updates            | 1020          |
|    policy_gradient_loss | -2.78e-06     |
|    value_loss           | 7.78e+03      |
-------------------------------------------
Eval num_timesteps=209500, episode_reward=-931.95 +/- 130.91
Episode length: 23.92 +/- 9.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-931.91 +/- 159.79
Episode length: 24.36 +/- 10.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-895.76 +/- 183.61
Episode length: 21.86 +/- 9.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.9     |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 103      |
|    time_elapsed    | 597      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-904.36 +/- 141.48
Episode length: 21.22 +/- 6.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.2          |
|    mean_reward          | -904          |
| time/                   |               |
|    total_timesteps      | 211000        |
| train/                  |               |
|    approx_kl            | 1.9499566e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000855     |
|    explained_variance   | 0.9           |
|    learning_rate        | 0.0001        |
|    loss                 | 2.71e+03      |
|    n_updates            | 1030          |
|    policy_gradient_loss | -9.57e-07     |
|    value_loss           | 6.98e+03      |
-------------------------------------------
Eval num_timesteps=211500, episode_reward=-924.57 +/- 140.74
Episode length: 23.38 +/- 8.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-947.53 +/- 125.34
Episode length: 23.72 +/- 9.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-949.95 +/- 120.76
Episode length: 23.22 +/- 8.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -900     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 104      |
|    time_elapsed    | 603      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-928.35 +/- 145.59
Episode length: 24.16 +/- 8.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | -928          |
| time/                   |               |
|    total_timesteps      | 213000        |
| train/                  |               |
|    approx_kl            | 3.3760443e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00101      |
|    explained_variance   | 0.866         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.55e+03      |
|    n_updates            | 1040          |
|    policy_gradient_loss | -2.13e-06     |
|    value_loss           | 9.11e+03      |
-------------------------------------------
Eval num_timesteps=213500, episode_reward=-936.76 +/- 127.82
Episode length: 24.24 +/- 8.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-917.36 +/- 154.65
Episode length: 23.88 +/- 9.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -917     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-918.74 +/- 136.23
Episode length: 23.74 +/- 10.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-934.13 +/- 155.85
Episode length: 23.74 +/- 10.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 105      |
|    time_elapsed    | 609      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-916.35 +/- 138.19
Episode length: 23.08 +/- 8.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.1          |
|    mean_reward          | -916          |
| time/                   |               |
|    total_timesteps      | 215500        |
| train/                  |               |
|    approx_kl            | 2.8288923e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00168      |
|    explained_variance   | 0.886         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.73e+03      |
|    n_updates            | 1050          |
|    policy_gradient_loss | -2e-06        |
|    value_loss           | 7.72e+03      |
-------------------------------------------
Eval num_timesteps=216000, episode_reward=-900.56 +/- 191.36
Episode length: 23.50 +/- 8.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-917.55 +/- 124.57
Episode length: 24.66 +/- 10.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-937.95 +/- 132.67
Episode length: 22.98 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -917     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 106      |
|    time_elapsed    | 615      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-879.92 +/- 187.39
Episode length: 22.70 +/- 8.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.7         |
|    mean_reward          | -880         |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 2.378365e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00602     |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.93e+03     |
|    n_updates            | 1060         |
|    policy_gradient_loss | -4.01e-05    |
|    value_loss           | 7.83e+03     |
------------------------------------------
Eval num_timesteps=218000, episode_reward=-924.61 +/- 151.44
Episode length: 24.20 +/- 7.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-886.35 +/- 184.49
Episode length: 23.66 +/- 9.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -886     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-888.47 +/- 199.95
Episode length: 21.24 +/- 8.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.2     |
|    mean_reward     | -888     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 107      |
|    time_elapsed    | 621      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-918.75 +/- 143.95
Episode length: 23.96 +/- 9.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24           |
|    mean_reward          | -919         |
| time/                   |              |
|    total_timesteps      | 219500       |
| train/                  |              |
|    approx_kl            | 7.619767e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00436     |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.0001       |
|    loss                 | 3.54e+03     |
|    n_updates            | 1070         |
|    policy_gradient_loss | 5.86e-05     |
|    value_loss           | 6.97e+03     |
------------------------------------------
Eval num_timesteps=220000, episode_reward=-939.15 +/- 143.52
Episode length: 23.02 +/- 9.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-912.76 +/- 153.23
Episode length: 24.60 +/- 8.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-946.35 +/- 123.75
Episode length: 24.96 +/- 8.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -919     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 108      |
|    time_elapsed    | 627      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-931.91 +/- 139.90
Episode length: 22.80 +/- 8.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.8         |
|    mean_reward          | -932         |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0006344446 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00586     |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.63e+03     |
|    n_updates            | 1080         |
|    policy_gradient_loss | -0.000209    |
|    value_loss           | 6.01e+03     |
------------------------------------------
Eval num_timesteps=222000, episode_reward=-893.56 +/- 156.44
Episode length: 24.42 +/- 8.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -894     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-922.37 +/- 129.33
Episode length: 26.84 +/- 8.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-913.94 +/- 130.48
Episode length: 23.90 +/- 10.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -905     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 109      |
|    time_elapsed    | 632      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-929.39 +/- 152.39
Episode length: 27.08 +/- 10.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.1          |
|    mean_reward          | -929          |
| time/                   |               |
|    total_timesteps      | 223500        |
| train/                  |               |
|    approx_kl            | 2.8910814e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00689      |
|    explained_variance   | 0.903         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.03e+03      |
|    n_updates            | 1090          |
|    policy_gradient_loss | -5.09e-05     |
|    value_loss           | 7.06e+03      |
-------------------------------------------
Eval num_timesteps=224000, episode_reward=-915.03 +/- 165.56
Episode length: 23.52 +/- 9.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-955.90 +/- 145.12
Episode length: 22.52 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -956     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-905.54 +/- 180.16
Episode length: 23.46 +/- 9.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -930     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 110      |
|    time_elapsed    | 638      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-893.55 +/- 151.76
Episode length: 21.32 +/- 9.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.3          |
|    mean_reward          | -894          |
| time/                   |               |
|    total_timesteps      | 225500        |
| train/                  |               |
|    approx_kl            | 0.00023487507 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00503      |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.18e+03      |
|    n_updates            | 1100          |
|    policy_gradient_loss | -0.000206     |
|    value_loss           | 4.76e+03      |
-------------------------------------------
Eval num_timesteps=226000, episode_reward=-915.15 +/- 119.07
Episode length: 25.02 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-928.33 +/- 127.21
Episode length: 24.12 +/- 10.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-887.37 +/- 184.98
Episode length: 24.24 +/- 9.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -887     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -909     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 111      |
|    time_elapsed    | 644      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-933.15 +/- 122.90
Episode length: 25.34 +/- 9.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.3         |
|    mean_reward          | -933         |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 6.398867e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00642     |
|    explained_variance   | 0.873        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.54e+03     |
|    n_updates            | 1110         |
|    policy_gradient_loss | -5.94e-05    |
|    value_loss           | 9.62e+03     |
------------------------------------------
Eval num_timesteps=228000, episode_reward=-942.76 +/- 142.18
Episode length: 25.40 +/- 10.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-924.76 +/- 123.69
Episode length: 24.24 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-900.73 +/- 164.03
Episode length: 22.22 +/- 9.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -902     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 112      |
|    time_elapsed    | 650      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-913.98 +/- 143.63
Episode length: 27.66 +/- 10.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.7          |
|    mean_reward          | -914          |
| time/                   |               |
|    total_timesteps      | 229500        |
| train/                  |               |
|    approx_kl            | 0.00019347447 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.903         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.85e+03      |
|    n_updates            | 1120          |
|    policy_gradient_loss | -0.000125     |
|    value_loss           | 6.92e+03      |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=-961.97 +/- 102.71
Episode length: 25.86 +/- 8.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -962     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-895.93 +/- 142.53
Episode length: 23.68 +/- 9.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-924.76 +/- 141.10
Episode length: 25.94 +/- 10.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -934     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 113      |
|    time_elapsed    | 656      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-914.92 +/- 186.56
Episode length: 26.30 +/- 10.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.3        |
|    mean_reward          | -915        |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.012838589 |
|    clip_fraction        | 0.00654     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0143     |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.29e+03    |
|    n_updates            | 1130        |
|    policy_gradient_loss | 0.00464     |
|    value_loss           | 8.13e+03    |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=-905.54 +/- 164.68
Episode length: 22.42 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-907.95 +/- 129.25
Episode length: 25.00 +/- 9.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-895.95 +/- 150.84
Episode length: 23.96 +/- 9.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 114      |
|    time_elapsed    | 661      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-959.56 +/- 133.84
Episode length: 24.08 +/- 8.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.1          |
|    mean_reward          | -960          |
| time/                   |               |
|    total_timesteps      | 233500        |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000191     |
|    explained_variance   | 0.889         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.4e+03       |
|    n_updates            | 1140          |
|    policy_gradient_loss | -4.88e-07     |
|    value_loss           | 6.9e+03       |
-------------------------------------------
Eval num_timesteps=234000, episode_reward=-916.03 +/- 166.80
Episode length: 23.34 +/- 10.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-924.75 +/- 150.96
Episode length: 24.04 +/- 9.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-929.56 +/- 132.10
Episode length: 23.76 +/- 10.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-927.05 +/- 153.04
Episode length: 24.06 +/- 9.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -902     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 115      |
|    time_elapsed    | 668      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-934.18 +/- 152.80
Episode length: 25.16 +/- 8.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | -934          |
| time/                   |               |
|    total_timesteps      | 236000        |
| train/                  |               |
|    approx_kl            | 1.1059456e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000242     |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.71e+03      |
|    n_updates            | 1150          |
|    policy_gradient_loss | -2.86e-07     |
|    value_loss           | 8.12e+03      |
-------------------------------------------
Eval num_timesteps=236500, episode_reward=-930.74 +/- 128.17
Episode length: 24.54 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-903.16 +/- 159.94
Episode length: 24.24 +/- 8.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-907.97 +/- 143.50
Episode length: 25.50 +/- 10.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 116      |
|    time_elapsed    | 674      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-915.17 +/- 141.19
Episode length: 24.10 +/- 7.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.1          |
|    mean_reward          | -915          |
| time/                   |               |
|    total_timesteps      | 238000        |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000178     |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.13e+03      |
|    n_updates            | 1160          |
|    policy_gradient_loss | -2.05e-07     |
|    value_loss           | 5.7e+03       |
-------------------------------------------
Eval num_timesteps=238500, episode_reward=-910.13 +/- 173.92
Episode length: 24.68 +/- 10.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-899.56 +/- 160.73
Episode length: 24.34 +/- 9.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -900     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-913.85 +/- 153.46
Episode length: 25.66 +/- 9.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 117      |
|    time_elapsed    | 680      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-900.76 +/- 157.30
Episode length: 24.44 +/- 7.89
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.4           |
|    mean_reward          | -901           |
| time/                   |                |
|    total_timesteps      | 240000         |
| train/                  |                |
|    approx_kl            | -1.7462298e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000236      |
|    explained_variance   | 0.896          |
|    learning_rate        | 0.0001         |
|    loss                 | 3.53e+03       |
|    n_updates            | 1170           |
|    policy_gradient_loss | -1.5e-07       |
|    value_loss           | 7.35e+03       |
--------------------------------------------
Eval num_timesteps=240500, episode_reward=-919.62 +/- 198.88
Episode length: 25.12 +/- 10.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-923.45 +/- 150.75
Episode length: 23.50 +/- 8.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -923     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-934.27 +/- 133.25
Episode length: 23.06 +/- 8.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | -935     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 118      |
|    time_elapsed    | 685      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-935.54 +/- 132.33
Episode length: 25.82 +/- 10.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | -936         |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 9.546056e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000362    |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.15e+03     |
|    n_updates            | 1180         |
|    policy_gradient_loss | -4.98e-07    |
|    value_loss           | 5.78e+03     |
------------------------------------------
Eval num_timesteps=242500, episode_reward=-927.13 +/- 128.84
Episode length: 23.20 +/- 8.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-943.96 +/- 113.85
Episode length: 24.84 +/- 8.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-910.34 +/- 163.90
Episode length: 21.78 +/- 8.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 119      |
|    time_elapsed    | 691      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-917.43 +/- 127.32
Episode length: 22.20 +/- 7.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | -917          |
| time/                   |               |
|    total_timesteps      | 244000        |
| train/                  |               |
|    approx_kl            | 5.9080776e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000463     |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.93e+03      |
|    n_updates            | 1190          |
|    policy_gradient_loss | 2.66e-07      |
|    value_loss           | 6.65e+03      |
-------------------------------------------
Eval num_timesteps=244500, episode_reward=-921.03 +/- 155.49
Episode length: 24.38 +/- 10.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-893.19 +/- 141.80
Episode length: 22.94 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -893     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-922.35 +/- 143.59
Episode length: 22.94 +/- 8.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 120      |
|    time_elapsed    | 696      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-923.53 +/- 146.62
Episode length: 23.42 +/- 9.50
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.4           |
|    mean_reward          | -924           |
| time/                   |                |
|    total_timesteps      | 246000         |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00031       |
|    explained_variance   | 0.897          |
|    learning_rate        | 0.0001         |
|    loss                 | 1.69e+03       |
|    n_updates            | 1200           |
|    policy_gradient_loss | -3.07e-07      |
|    value_loss           | 6.75e+03       |
--------------------------------------------
Eval num_timesteps=246500, episode_reward=-915.14 +/- 158.48
Episode length: 22.76 +/- 9.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-924.76 +/- 127.13
Episode length: 25.76 +/- 9.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-918.76 +/- 175.10
Episode length: 25.58 +/- 9.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -915     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 121      |
|    time_elapsed    | 702      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-913.93 +/- 163.81
Episode length: 25.78 +/- 9.17
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 25.8           |
|    mean_reward          | -914           |
| time/                   |                |
|    total_timesteps      | 248000         |
| train/                  |                |
|    approx_kl            | -2.6775524e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000176      |
|    explained_variance   | 0.873          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.51e+03       |
|    n_updates            | 1210           |
|    policy_gradient_loss | -4.52e-09      |
|    value_loss           | 8.33e+03       |
--------------------------------------------
Eval num_timesteps=248500, episode_reward=-906.46 +/- 181.14
Episode length: 23.98 +/- 9.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-937.89 +/- 146.12
Episode length: 24.40 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-928.36 +/- 132.12
Episode length: 24.04 +/- 9.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 122      |
|    time_elapsed    | 708      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-906.73 +/- 168.79
Episode length: 23.84 +/- 7.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | -907          |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 0.00029016242 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000337     |
|    explained_variance   | 0.9           |
|    learning_rate        | 0.0001        |
|    loss                 | 3.78e+03      |
|    n_updates            | 1220          |
|    policy_gradient_loss | -2.8e-05      |
|    value_loss           | 6.67e+03      |
-------------------------------------------
Eval num_timesteps=250500, episode_reward=-947.55 +/- 129.80
Episode length: 25.04 +/- 8.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-952.00 +/- 153.92
Episode length: 24.60 +/- 8.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -952     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-936.76 +/- 165.60
Episode length: 24.84 +/- 8.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -900     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 123      |
|    time_elapsed    | 714      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-930.76 +/- 136.34
Episode length: 23.16 +/- 8.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.2           |
|    mean_reward          | -931           |
| time/                   |                |
|    total_timesteps      | 252000         |
| train/                  |                |
|    approx_kl            | -2.0081643e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000238      |
|    explained_variance   | 0.9            |
|    learning_rate        | 0.0001         |
|    loss                 | 4.09e+03       |
|    n_updates            | 1230           |
|    policy_gradient_loss | 8.31e-08       |
|    value_loss           | 6.52e+03       |
--------------------------------------------
Eval num_timesteps=252500, episode_reward=-881.53 +/- 201.79
Episode length: 23.02 +/- 10.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -882     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-921.15 +/- 138.44
Episode length: 23.74 +/- 9.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-943.77 +/- 143.20
Episode length: 27.34 +/- 9.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.3     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -912     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 124      |
|    time_elapsed    | 720      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-911.14 +/- 182.45
Episode length: 24.78 +/- 10.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | -911          |
| time/                   |               |
|    total_timesteps      | 254000        |
| train/                  |               |
|    approx_kl            | 6.1118044e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000318     |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.7e+03       |
|    n_updates            | 1240          |
|    policy_gradient_loss | 1.75e-07      |
|    value_loss           | 6.3e+03       |
-------------------------------------------
Eval num_timesteps=254500, episode_reward=-892.35 +/- 165.13
Episode length: 21.62 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-948.64 +/- 126.70
Episode length: 24.48 +/- 9.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-899.55 +/- 167.72
Episode length: 24.44 +/- 8.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -900     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-933.08 +/- 153.88
Episode length: 25.96 +/- 9.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -885     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 125      |
|    time_elapsed    | 726      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-965.50 +/- 104.13
Episode length: 25.32 +/- 8.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.3          |
|    mean_reward          | -966          |
| time/                   |               |
|    total_timesteps      | 256500        |
| train/                  |               |
|    approx_kl            | 4.5401976e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000367     |
|    explained_variance   | 0.827         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.15e+03      |
|    n_updates            | 1250          |
|    policy_gradient_loss | -7.2e-07      |
|    value_loss           | 1.1e+04       |
-------------------------------------------
Eval num_timesteps=257000, episode_reward=-912.74 +/- 177.22
Episode length: 23.24 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-882.77 +/- 180.52
Episode length: 22.08 +/- 8.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -883     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-939.14 +/- 174.77
Episode length: 24.28 +/- 9.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 126      |
|    time_elapsed    | 732      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-936.76 +/- 128.94
Episode length: 24.66 +/- 9.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.7          |
|    mean_reward          | -937          |
| time/                   |               |
|    total_timesteps      | 258500        |
| train/                  |               |
|    approx_kl            | 1.4319085e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000479     |
|    explained_variance   | 0.886         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.74e+03      |
|    n_updates            | 1260          |
|    policy_gradient_loss | 4.43e-08      |
|    value_loss           | 7.56e+03      |
-------------------------------------------
Eval num_timesteps=259000, episode_reward=-936.70 +/- 139.14
Episode length: 28.54 +/- 12.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 28.5     |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-899.56 +/- 168.18
Episode length: 23.92 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -900     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-949.92 +/- 125.44
Episode length: 24.16 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -946     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 127      |
|    time_elapsed    | 738      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=-904.13 +/- 166.48
Episode length: 23.20 +/- 10.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 23.2      |
|    mean_reward          | -904      |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000423 |
|    explained_variance   | 0.902     |
|    learning_rate        | 0.0001    |
|    loss                 | 4.13e+03  |
|    n_updates            | 1270      |
|    policy_gradient_loss | -1.26e-06 |
|    value_loss           | 6.73e+03  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=-921.16 +/- 141.52
Episode length: 24.30 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-898.23 +/- 182.14
Episode length: 23.78 +/- 8.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-901.94 +/- 167.70
Episode length: 23.76 +/- 8.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 128      |
|    time_elapsed    | 743      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-898.29 +/- 162.37
Episode length: 24.84 +/- 9.90
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.8           |
|    mean_reward          | -898           |
| time/                   |                |
|    total_timesteps      | 262500         |
| train/                  |                |
|    approx_kl            | -2.6193447e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000412      |
|    explained_variance   | 0.9            |
|    learning_rate        | 0.0001         |
|    loss                 | 2.44e+03       |
|    n_updates            | 1280           |
|    policy_gradient_loss | 1.2e-07        |
|    value_loss           | 7.27e+03       |
--------------------------------------------
Eval num_timesteps=263000, episode_reward=-940.34 +/- 137.24
Episode length: 23.64 +/- 8.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-912.71 +/- 153.29
Episode length: 23.72 +/- 8.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-923.57 +/- 146.59
Episode length: 25.38 +/- 8.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -907     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 129      |
|    time_elapsed    | 749      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=-924.75 +/- 120.78
Episode length: 24.60 +/- 10.07
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.6           |
|    mean_reward          | -925           |
| time/                   |                |
|    total_timesteps      | 264500         |
| train/                  |                |
|    approx_kl            | -2.2992026e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000319      |
|    explained_variance   | 0.88           |
|    learning_rate        | 0.0001         |
|    loss                 | 6.56e+03       |
|    n_updates            | 1290           |
|    policy_gradient_loss | -3.08e-09      |
|    value_loss           | 8.45e+03       |
--------------------------------------------
Eval num_timesteps=265000, episode_reward=-859.65 +/- 191.38
Episode length: 21.76 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | -860     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
New best mean reward!
Eval num_timesteps=265500, episode_reward=-905.22 +/- 168.38
Episode length: 23.10 +/- 8.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-941.54 +/- 130.10
Episode length: 25.18 +/- 10.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -899     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 130      |
|    time_elapsed    | 755      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-954.68 +/- 120.91
Episode length: 23.72 +/- 8.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.7        |
|    mean_reward          | -955        |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 1.36788e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000299   |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.9e+03     |
|    n_updates            | 1300        |
|    policy_gradient_loss | 3.04e-08    |
|    value_loss           | 7.84e+03    |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=-916.34 +/- 148.76
Episode length: 23.38 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-954.74 +/- 140.71
Episode length: 23.76 +/- 8.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -955     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-917.34 +/- 165.63
Episode length: 23.64 +/- 9.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -917     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 131      |
|    time_elapsed    | 761      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=-929.55 +/- 151.88
Episode length: 23.88 +/- 9.14
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.9           |
|    mean_reward          | -930           |
| time/                   |                |
|    total_timesteps      | 268500         |
| train/                  |                |
|    approx_kl            | -1.9790605e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000297      |
|    explained_variance   | 0.878          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.49e+03       |
|    n_updates            | 1310           |
|    policy_gradient_loss | 9.13e-08       |
|    value_loss           | 8.78e+03       |
--------------------------------------------
Eval num_timesteps=269000, episode_reward=-948.78 +/- 110.41
Episode length: 26.56 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-957.13 +/- 111.04
Episode length: 24.50 +/- 8.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-880.02 +/- 193.24
Episode length: 22.88 +/- 9.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -880     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 132      |
|    time_elapsed    | 766      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-947.55 +/- 124.71
Episode length: 23.72 +/- 8.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.7          |
|    mean_reward          | -948          |
| time/                   |               |
|    total_timesteps      | 270500        |
| train/                  |               |
|    approx_kl            | 6.4028427e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000386     |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.27e+03      |
|    n_updates            | 1320          |
|    policy_gradient_loss | -1.77e-08     |
|    value_loss           | 6.1e+03       |
-------------------------------------------
Eval num_timesteps=271000, episode_reward=-941.56 +/- 110.36
Episode length: 25.48 +/- 10.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-908.93 +/- 180.70
Episode length: 22.62 +/- 9.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-933.12 +/- 157.83
Episode length: 23.02 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -909     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 133      |
|    time_elapsed    | 772      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=-935.55 +/- 138.20
Episode length: 25.24 +/- 9.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.2          |
|    mean_reward          | -936          |
| time/                   |               |
|    total_timesteps      | 272500        |
| train/                  |               |
|    approx_kl            | 1.7840648e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000535     |
|    explained_variance   | 0.906         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.53e+03      |
|    n_updates            | 1330          |
|    policy_gradient_loss | -5.41e-07     |
|    value_loss           | 6.04e+03      |
-------------------------------------------
Eval num_timesteps=273000, episode_reward=-935.57 +/- 152.09
Episode length: 27.20 +/- 9.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.2     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-889.97 +/- 152.87
Episode length: 24.46 +/- 8.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -890     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-961.95 +/- 115.90
Episode length: 26.74 +/- 10.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.7     |
|    mean_reward     | -962     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -918     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 134      |
|    time_elapsed    | 778      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=-901.93 +/- 139.07
Episode length: 25.28 +/- 9.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.3          |
|    mean_reward          | -902          |
| time/                   |               |
|    total_timesteps      | 274500        |
| train/                  |               |
|    approx_kl            | -4.656613e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000529     |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.11e+03      |
|    n_updates            | 1340          |
|    policy_gradient_loss | -1.83e-07     |
|    value_loss           | 5.49e+03      |
-------------------------------------------
Eval num_timesteps=275000, episode_reward=-911.58 +/- 118.68
Episode length: 24.72 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-927.16 +/- 124.26
Episode length: 25.34 +/- 10.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-927.14 +/- 136.41
Episode length: 22.96 +/- 8.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 135      |
|    time_elapsed    | 784      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=-911.35 +/- 149.91
Episode length: 21.84 +/- 8.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.8          |
|    mean_reward          | -911          |
| time/                   |               |
|    total_timesteps      | 276500        |
| train/                  |               |
|    approx_kl            | 2.1536835e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000427     |
|    explained_variance   | 0.893         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.89e+03      |
|    n_updates            | 1350          |
|    policy_gradient_loss | -4.62e-08     |
|    value_loss           | 7.82e+03      |
-------------------------------------------
Eval num_timesteps=277000, episode_reward=-925.95 +/- 144.64
Episode length: 25.26 +/- 7.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-942.77 +/- 121.37
Episode length: 24.48 +/- 8.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-927.08 +/- 125.43
Episode length: 23.76 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-905.55 +/- 145.17
Episode length: 22.66 +/- 9.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -920     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 136      |
|    time_elapsed    | 791      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=-917.56 +/- 167.03
Episode length: 24.62 +/- 8.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | -918          |
| time/                   |               |
|    total_timesteps      | 279000        |
| train/                  |               |
|    approx_kl            | 1.0884833e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000533     |
|    explained_variance   | 0.914         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.8e+03       |
|    n_updates            | 1360          |
|    policy_gradient_loss | 9.26e-07      |
|    value_loss           | 6.34e+03      |
-------------------------------------------
Eval num_timesteps=279500, episode_reward=-930.76 +/- 146.03
Episode length: 24.74 +/- 8.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-928.35 +/- 132.66
Episode length: 22.56 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-943.94 +/- 134.70
Episode length: 24.14 +/- 9.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -941     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 137      |
|    time_elapsed    | 797      |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=-909.16 +/- 167.29
Episode length: 24.46 +/- 9.75
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.5           |
|    mean_reward          | -909           |
| time/                   |                |
|    total_timesteps      | 281000         |
| train/                  |                |
|    approx_kl            | -7.2759576e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000511      |
|    explained_variance   | 0.919          |
|    learning_rate        | 0.0001         |
|    loss                 | 2.71e+03       |
|    n_updates            | 1370           |
|    policy_gradient_loss | 2e-07          |
|    value_loss           | 5.95e+03       |
--------------------------------------------
Eval num_timesteps=281500, episode_reward=-949.95 +/- 128.31
Episode length: 25.44 +/- 8.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-883.97 +/- 166.70
Episode length: 22.38 +/- 7.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -884     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-897.16 +/- 152.49
Episode length: 24.72 +/- 8.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -897     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -904     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 138      |
|    time_elapsed    | 803      |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-886.23 +/- 195.23
Episode length: 22.30 +/- 9.93
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 22.3           |
|    mean_reward          | -886           |
| time/                   |                |
|    total_timesteps      | 283000         |
| train/                  |                |
|    approx_kl            | -5.7043508e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000338      |
|    explained_variance   | 0.92           |
|    learning_rate        | 0.0001         |
|    loss                 | 2.78e+03       |
|    n_updates            | 1380           |
|    policy_gradient_loss | -6.19e-08      |
|    value_loss           | 5.54e+03       |
--------------------------------------------
Eval num_timesteps=283500, episode_reward=-945.14 +/- 133.99
Episode length: 25.54 +/- 9.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -945     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-927.08 +/- 143.35
Episode length: 23.10 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-921.03 +/- 161.84
Episode length: 22.74 +/- 8.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 139      |
|    time_elapsed    | 809      |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-954.74 +/- 117.89
Episode length: 23.92 +/- 9.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.9         |
|    mean_reward          | -955         |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 7.945346e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000456    |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.87e+03     |
|    n_updates            | 1390         |
|    policy_gradient_loss | -5.98e-07    |
|    value_loss           | 5.47e+03     |
------------------------------------------
Eval num_timesteps=285500, episode_reward=-907.96 +/- 126.43
Episode length: 25.14 +/- 8.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-943.97 +/- 131.46
Episode length: 27.32 +/- 9.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.3     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-892.33 +/- 174.87
Episode length: 26.46 +/- 10.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.5     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -931     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 140      |
|    time_elapsed    | 815      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-909.88 +/- 208.12
Episode length: 23.02 +/- 9.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | -910          |
| time/                   |               |
|    total_timesteps      | 287000        |
| train/                  |               |
|    approx_kl            | -2.066372e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000441     |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.27e+03      |
|    n_updates            | 1400          |
|    policy_gradient_loss | -4.04e-08     |
|    value_loss           | 5.83e+03      |
-------------------------------------------
Eval num_timesteps=287500, episode_reward=-887.56 +/- 160.10
Episode length: 21.92 +/- 8.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.9     |
|    mean_reward     | -888     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-936.58 +/- 138.21
Episode length: 24.56 +/- 8.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-942.76 +/- 139.58
Episode length: 25.22 +/- 9.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -931     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 141      |
|    time_elapsed    | 821      |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-923.41 +/- 146.09
Episode length: 23.10 +/- 9.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.1          |
|    mean_reward          | -923          |
| time/                   |               |
|    total_timesteps      | 289000        |
| train/                  |               |
|    approx_kl            | -6.170012e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000327     |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.15e+03      |
|    n_updates            | 1410          |
|    policy_gradient_loss | -2.56e-07     |
|    value_loss           | 5.97e+03      |
-------------------------------------------
Eval num_timesteps=289500, episode_reward=-935.56 +/- 136.62
Episode length: 23.84 +/- 8.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-936.74 +/- 127.83
Episode length: 26.96 +/- 11.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-922.29 +/- 150.43
Episode length: 24.76 +/- 9.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 142      |
|    time_elapsed    | 826      |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-943.95 +/- 118.20
Episode length: 25.88 +/- 9.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.9         |
|    mean_reward          | -944         |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 1.169974e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000318    |
|    explained_variance   | 0.868        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.12e+03     |
|    n_updates            | 1420         |
|    policy_gradient_loss | -2.7e-07     |
|    value_loss           | 8.73e+03     |
------------------------------------------
Eval num_timesteps=291500, episode_reward=-948.76 +/- 116.73
Episode length: 24.50 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-931.96 +/- 146.48
Episode length: 25.64 +/- 8.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-911.55 +/- 143.91
Episode length: 24.04 +/- 8.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -930     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 143      |
|    time_elapsed    | 832      |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-908.93 +/- 170.49
Episode length: 24.64 +/- 10.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | -909          |
| time/                   |               |
|    total_timesteps      | 293000        |
| train/                  |               |
|    approx_kl            | 1.4581019e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000597     |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.54e+03      |
|    n_updates            | 1430          |
|    policy_gradient_loss | 2.53e-07      |
|    value_loss           | 7.93e+03      |
-------------------------------------------
Eval num_timesteps=293500, episode_reward=-925.94 +/- 146.69
Episode length: 24.68 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-921.13 +/- 157.90
Episode length: 22.98 +/- 10.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-913.95 +/- 141.62
Episode length: 22.78 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 144      |
|    time_elapsed    | 838      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-935.56 +/- 119.77
Episode length: 24.28 +/- 8.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.3          |
|    mean_reward          | -936          |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000749     |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.59e+03      |
|    n_updates            | 1440          |
|    policy_gradient_loss | 5.53e-07      |
|    value_loss           | 5.26e+03      |
-------------------------------------------
Eval num_timesteps=295500, episode_reward=-918.39 +/- 180.33
Episode length: 24.02 +/- 9.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-922.18 +/- 139.83
Episode length: 24.70 +/- 8.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-943.95 +/- 120.63
Episode length: 25.00 +/- 9.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 145      |
|    time_elapsed    | 844      |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=-893.49 +/- 148.62
Episode length: 22.94 +/- 9.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.9          |
|    mean_reward          | -893          |
| time/                   |               |
|    total_timesteps      | 297000        |
| train/                  |               |
|    approx_kl            | 2.3283064e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000491     |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.07e+03      |
|    n_updates            | 1450          |
|    policy_gradient_loss | -4.72e-07     |
|    value_loss           | 6.13e+03      |
-------------------------------------------
Eval num_timesteps=297500, episode_reward=-921.16 +/- 149.44
Episode length: 24.86 +/- 7.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-900.67 +/- 166.16
Episode length: 22.96 +/- 9.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-906.75 +/- 159.97
Episode length: 22.92 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-977.55 +/- 121.66
Episode length: 25.82 +/- 10.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -978     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 146      |
|    time_elapsed    | 850      |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=-934.36 +/- 165.73
Episode length: 27.48 +/- 11.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 27.5         |
|    mean_reward          | -934         |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 1.557055e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00094     |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.81e+03     |
|    n_updates            | 1460         |
|    policy_gradient_loss | -1.12e-06    |
|    value_loss           | 7.63e+03     |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-898.35 +/- 170.97
Episode length: 22.42 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-894.74 +/- 166.04
Episode length: 23.80 +/- 9.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-931.93 +/- 147.96
Episode length: 23.98 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -907     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 147      |
|    time_elapsed    | 856      |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=-917.54 +/- 172.55
Episode length: 22.82 +/- 8.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.8          |
|    mean_reward          | -918          |
| time/                   |               |
|    total_timesteps      | 301500        |
| train/                  |               |
|    approx_kl            | 1.3038516e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00092      |
|    explained_variance   | 0.886         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.92e+03      |
|    n_updates            | 1470          |
|    policy_gradient_loss | 7.99e-07      |
|    value_loss           | 7.51e+03      |
-------------------------------------------
Eval num_timesteps=302000, episode_reward=-937.96 +/- 126.58
Episode length: 25.28 +/- 9.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=-925.71 +/- 175.20
Episode length: 22.80 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-919.90 +/- 153.67
Episode length: 25.14 +/- 9.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -938     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 148      |
|    time_elapsed    | 862      |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=-933.17 +/- 145.43
Episode length: 24.72 +/- 8.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.7          |
|    mean_reward          | -933          |
| time/                   |               |
|    total_timesteps      | 303500        |
| train/                  |               |
|    approx_kl            | 2.6862836e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00164      |
|    explained_variance   | 0.919         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.84e+03      |
|    n_updates            | 1480          |
|    policy_gradient_loss | 9.55e-07      |
|    value_loss           | 5.11e+03      |
-------------------------------------------
Eval num_timesteps=304000, episode_reward=-930.76 +/- 127.05
Episode length: 25.18 +/- 9.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-907.97 +/- 168.86
Episode length: 23.74 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-904.36 +/- 145.50
Episode length: 21.40 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -932     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 149      |
|    time_elapsed    | 867      |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=-895.90 +/- 178.10
Episode length: 22.42 +/- 8.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | -896          |
| time/                   |               |
|    total_timesteps      | 305500        |
| train/                  |               |
|    approx_kl            | 0.00027548335 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00112      |
|    explained_variance   | 0.91          |
|    learning_rate        | 0.0001        |
|    loss                 | 3.68e+03      |
|    n_updates            | 1490          |
|    policy_gradient_loss | -0.000155     |
|    value_loss           | 6.04e+03      |
-------------------------------------------
Eval num_timesteps=306000, episode_reward=-899.51 +/- 150.24
Episode length: 24.54 +/- 9.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -900     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-939.15 +/- 137.92
Episode length: 25.66 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=-906.76 +/- 146.32
Episode length: 26.04 +/- 9.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -879     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 150      |
|    time_elapsed    | 873      |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=-949.96 +/- 116.51
Episode length: 23.60 +/- 7.80
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.6           |
|    mean_reward          | -950           |
| time/                   |                |
|    total_timesteps      | 307500         |
| train/                  |                |
|    approx_kl            | -3.6379788e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000906      |
|    explained_variance   | 0.844          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.21e+03       |
|    n_updates            | 1500           |
|    policy_gradient_loss | 1.1e-06        |
|    value_loss           | 1.05e+04       |
--------------------------------------------
Eval num_timesteps=308000, episode_reward=-922.34 +/- 156.98
Episode length: 22.98 +/- 9.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=-934.33 +/- 123.40
Episode length: 22.40 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-948.77 +/- 127.92
Episode length: 24.38 +/- 9.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -931     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 151      |
|    time_elapsed    | 879      |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=-892.10 +/- 201.57
Episode length: 24.92 +/- 9.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.9          |
|    mean_reward          | -892          |
| time/                   |               |
|    total_timesteps      | 309500        |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000343     |
|    explained_variance   | 0.922         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.93e+03      |
|    n_updates            | 1510          |
|    policy_gradient_loss | -5.47e-07     |
|    value_loss           | 5.15e+03      |
-------------------------------------------
Eval num_timesteps=310000, episode_reward=-948.72 +/- 118.61
Episode length: 25.14 +/- 8.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-921.16 +/- 165.00
Episode length: 25.40 +/- 9.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=-931.95 +/- 143.49
Episode length: 25.62 +/- 8.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 152      |
|    time_elapsed    | 884      |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=-881.32 +/- 167.42
Episode length: 23.16 +/- 9.52
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.2           |
|    mean_reward          | -881           |
| time/                   |                |
|    total_timesteps      | 311500         |
| train/                  |                |
|    approx_kl            | -1.8335413e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000295      |
|    explained_variance   | 0.892          |
|    learning_rate        | 0.0001         |
|    loss                 | 6.15e+03       |
|    n_updates            | 1520           |
|    policy_gradient_loss | 9.63e-08       |
|    value_loss           | 7.43e+03       |
--------------------------------------------
Eval num_timesteps=312000, episode_reward=-912.76 +/- 121.21
Episode length: 23.50 +/- 7.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=-913.94 +/- 141.10
Episode length: 24.58 +/- 9.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-918.74 +/- 143.44
Episode length: 24.62 +/- 9.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | -890     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 153      |
|    time_elapsed    | 890      |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=-911.48 +/- 141.63
Episode length: 26.30 +/- 9.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 26.3         |
|    mean_reward          | -911         |
| time/                   |              |
|    total_timesteps      | 313500       |
| train/                  |              |
|    approx_kl            | 1.268927e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000467    |
|    explained_variance   | 0.885        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.03e+03     |
|    n_updates            | 1530         |
|    policy_gradient_loss | 2.56e-07     |
|    value_loss           | 8.46e+03     |
------------------------------------------
Eval num_timesteps=314000, episode_reward=-929.53 +/- 129.96
Episode length: 23.22 +/- 8.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=-947.56 +/- 136.29
Episode length: 25.86 +/- 9.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-951.16 +/- 136.73
Episode length: 24.24 +/- 7.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 154      |
|    time_elapsed    | 896      |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=-900.48 +/- 165.24
Episode length: 23.54 +/- 8.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.5          |
|    mean_reward          | -900          |
| time/                   |               |
|    total_timesteps      | 315500        |
| train/                  |               |
|    approx_kl            | -5.820766e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000429     |
|    explained_variance   | 0.917         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.94e+03      |
|    n_updates            | 1540          |
|    policy_gradient_loss | -2.53e-07     |
|    value_loss           | 5.4e+03       |
-------------------------------------------
Eval num_timesteps=316000, episode_reward=-865.94 +/- 175.27
Episode length: 22.18 +/- 8.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -866     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=-923.56 +/- 133.21
Episode length: 22.82 +/- 7.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-931.96 +/- 124.12
Episode length: 23.68 +/- 8.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -942     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 155      |
|    time_elapsed    | 902      |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=-907.95 +/- 156.92
Episode length: 20.86 +/- 9.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 20.9          |
|    mean_reward          | -908          |
| time/                   |               |
|    total_timesteps      | 317500        |
| train/                  |               |
|    approx_kl            | -9.895302e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000449     |
|    explained_variance   | 0.909         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.58e+03      |
|    n_updates            | 1550          |
|    policy_gradient_loss | -6.32e-09     |
|    value_loss           | 5.74e+03      |
-------------------------------------------
Eval num_timesteps=318000, episode_reward=-905.35 +/- 156.73
Episode length: 24.40 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=-939.01 +/- 161.27
Episode length: 22.00 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=-921.16 +/- 131.50
Episode length: 22.56 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 156      |
|    time_elapsed    | 907      |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=-964.38 +/- 143.50
Episode length: 25.06 +/- 7.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.1         |
|    mean_reward          | -964         |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 9.837095e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000389    |
|    explained_variance   | 0.896        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.75e+03     |
|    n_updates            | 1560         |
|    policy_gradient_loss | -8.73e-07    |
|    value_loss           | 6.77e+03     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-923.56 +/- 143.10
Episode length: 22.66 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=-937.95 +/- 102.02
Episode length: 25.70 +/- 11.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=-885.14 +/- 139.28
Episode length: 22.76 +/- 8.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -885     |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=-949.94 +/- 131.07
Episode length: 22.14 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 157      |
|    time_elapsed    | 914      |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=-913.95 +/- 187.93
Episode length: 23.54 +/- 8.74
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.5           |
|    mean_reward          | -914           |
| time/                   |                |
|    total_timesteps      | 322000         |
| train/                  |                |
|    approx_kl            | -1.2805685e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000342      |
|    explained_variance   | 0.896          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.39e+03       |
|    n_updates            | 1570           |
|    policy_gradient_loss | 6.59e-08       |
|    value_loss           | 6.93e+03       |
--------------------------------------------
Eval num_timesteps=322500, episode_reward=-912.44 +/- 163.04
Episode length: 25.32 +/- 10.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=-924.75 +/- 115.24
Episode length: 23.66 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=-919.97 +/- 136.30
Episode length: 27.32 +/- 10.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.3     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 158      |
|    time_elapsed    | 919      |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=-888.73 +/- 183.94
Episode length: 22.86 +/- 7.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.9          |
|    mean_reward          | -889          |
| time/                   |               |
|    total_timesteps      | 324000        |
| train/                  |               |
|    approx_kl            | 1.0099029e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000486     |
|    explained_variance   | 0.909         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.9e+03       |
|    n_updates            | 1580          |
|    policy_gradient_loss | 8.06e-07      |
|    value_loss           | 6.15e+03      |
-------------------------------------------
Eval num_timesteps=324500, episode_reward=-917.54 +/- 160.02
Episode length: 23.18 +/- 9.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-937.92 +/- 136.45
Episode length: 23.38 +/- 8.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=-889.54 +/- 188.13
Episode length: 21.42 +/- 8.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.4     |
|    mean_reward     | -890     |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -909     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 159      |
|    time_elapsed    | 925      |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=-925.97 +/- 139.05
Episode length: 24.58 +/- 9.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | -926         |
| time/                   |              |
|    total_timesteps      | 326000       |
| train/                  |              |
|    approx_kl            | 7.916242e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000666    |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.41e+03     |
|    n_updates            | 1590         |
|    policy_gradient_loss | 3.52e-07     |
|    value_loss           | 5.43e+03     |
------------------------------------------
Eval num_timesteps=326500, episode_reward=-909.15 +/- 159.32
Episode length: 23.48 +/- 9.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=-905.49 +/- 152.03
Episode length: 23.64 +/- 8.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=-961.76 +/- 124.62
Episode length: 25.16 +/- 8.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -962     |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -921     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 160      |
|    time_elapsed    | 931      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=-933.16 +/- 124.06
Episode length: 25.52 +/- 8.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.5          |
|    mean_reward          | -933          |
| time/                   |               |
|    total_timesteps      | 328000        |
| train/                  |               |
|    approx_kl            | 1.3154931e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00093      |
|    explained_variance   | 0.905         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.83e+03      |
|    n_updates            | 1600          |
|    policy_gradient_loss | -1.49e-06     |
|    value_loss           | 6.07e+03      |
-------------------------------------------
Eval num_timesteps=328500, episode_reward=-909.16 +/- 144.65
Episode length: 23.66 +/- 10.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=-923.52 +/- 152.00
Episode length: 23.76 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=-933.13 +/- 115.04
Episode length: 23.68 +/- 8.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -909     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 161      |
|    time_elapsed    | 936      |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-928.35 +/- 130.47
Episode length: 23.64 +/- 8.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | -928          |
| time/                   |               |
|    total_timesteps      | 330000        |
| train/                  |               |
|    approx_kl            | 1.6880222e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000949     |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.96e+03      |
|    n_updates            | 1610          |
|    policy_gradient_loss | 2.21e-07      |
|    value_loss           | 6.32e+03      |
-------------------------------------------
Eval num_timesteps=330500, episode_reward=-907.95 +/- 162.33
Episode length: 23.94 +/- 9.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=-907.97 +/- 164.97
Episode length: 23.70 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=-911.52 +/- 138.88
Episode length: 24.14 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 162      |
|    time_elapsed    | 942      |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=-958.37 +/- 120.57
Episode length: 27.14 +/- 8.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.1          |
|    mean_reward          | -958          |
| time/                   |               |
|    total_timesteps      | 332000        |
| train/                  |               |
|    approx_kl            | 5.2362448e-06 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00183      |
|    explained_variance   | 0.887         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.55e+03      |
|    n_updates            | 1620          |
|    policy_gradient_loss | 1.47e-05      |
|    value_loss           | 7.76e+03      |
-------------------------------------------
Eval num_timesteps=332500, episode_reward=-933.13 +/- 124.12
Episode length: 24.22 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=-922.36 +/- 139.52
Episode length: 23.58 +/- 8.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=-935.55 +/- 159.03
Episode length: 25.46 +/- 9.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -906     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 163      |
|    time_elapsed    | 948      |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=-903.14 +/- 150.19
Episode length: 23.58 +/- 8.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | -903          |
| time/                   |               |
|    total_timesteps      | 334000        |
| train/                  |               |
|    approx_kl            | 4.3655746e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00102      |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.09e+03      |
|    n_updates            | 1630          |
|    policy_gradient_loss | 6.17e-07      |
|    value_loss           | 7.45e+03      |
-------------------------------------------
Eval num_timesteps=334500, episode_reward=-951.15 +/- 130.81
Episode length: 25.62 +/- 12.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=-935.55 +/- 135.02
Episode length: 23.36 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=-905.27 +/- 200.20
Episode length: 22.32 +/- 8.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 164      |
|    time_elapsed    | 954      |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=-910.34 +/- 160.81
Episode length: 23.68 +/- 9.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.7          |
|    mean_reward          | -910          |
| time/                   |               |
|    total_timesteps      | 336000        |
| train/                  |               |
|    approx_kl            | 1.5308615e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00133      |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.16e+03      |
|    n_updates            | 1640          |
|    policy_gradient_loss | 3.41e-07      |
|    value_loss           | 6.41e+03      |
-------------------------------------------
Eval num_timesteps=336500, episode_reward=-918.59 +/- 130.14
Episode length: 24.02 +/- 7.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=-931.96 +/- 134.17
Episode length: 24.10 +/- 9.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=-901.85 +/- 145.92
Episode length: 25.04 +/- 8.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 165      |
|    time_elapsed    | 959      |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=-866.78 +/- 205.64
Episode length: 23.72 +/- 10.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.7         |
|    mean_reward          | -867         |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 4.947651e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000842    |
|    explained_variance   | 0.903        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.06e+03     |
|    n_updates            | 1650         |
|    policy_gradient_loss | 2.86e-07     |
|    value_loss           | 6.24e+03     |
------------------------------------------
Eval num_timesteps=338500, episode_reward=-928.36 +/- 141.08
Episode length: 25.32 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=-914.96 +/- 159.09
Episode length: 24.52 +/- 9.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=-899.49 +/- 157.33
Episode length: 22.94 +/- 9.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -899     |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 352      |
|    iterations      | 166      |
|    time_elapsed    | 965      |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-913.94 +/- 146.12
Episode length: 23.24 +/- 7.29
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 23.2           |
|    mean_reward          | -914           |
| time/                   |                |
|    total_timesteps      | 340000         |
| train/                  |                |
|    approx_kl            | -2.3574103e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000515      |
|    explained_variance   | 0.919          |
|    learning_rate        | 0.0001         |
|    loss                 | 2.51e+03       |
|    n_updates            | 1660           |
|    policy_gradient_loss | 2.08e-07       |
|    value_loss           | 5.49e+03       |
--------------------------------------------
Eval num_timesteps=340500, episode_reward=-887.56 +/- 169.29
Episode length: 24.92 +/- 10.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -888     |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-906.74 +/- 148.79
Episode length: 23.90 +/- 9.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=-889.90 +/- 124.44
Episode length: 22.66 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -890     |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-877.96 +/- 164.21
Episode length: 24.50 +/- 8.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -878     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -935     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 167      |
|    time_elapsed    | 971      |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=-949.97 +/- 110.15
Episode length: 25.64 +/- 8.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | -950         |
| time/                   |              |
|    total_timesteps      | 342500       |
| train/                  |              |
|    approx_kl            | 8.294592e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000517    |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.36e+03     |
|    n_updates            | 1670         |
|    policy_gradient_loss | 1.64e-07     |
|    value_loss           | 7.28e+03     |
------------------------------------------
Eval num_timesteps=343000, episode_reward=-878.70 +/- 185.98
Episode length: 23.64 +/- 8.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -879     |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=-895.81 +/- 147.65
Episode length: 23.92 +/- 8.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-903.11 +/- 185.51
Episode length: 23.52 +/- 8.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 168      |
|    time_elapsed    | 977      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=-937.92 +/- 150.17
Episode length: 24.66 +/- 6.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.7         |
|    mean_reward          | -938         |
| time/                   |              |
|    total_timesteps      | 344500       |
| train/                  |              |
|    approx_kl            | 9.633368e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000625    |
|    explained_variance   | 0.925        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.82e+03     |
|    n_updates            | 1680         |
|    policy_gradient_loss | 7.74e-07     |
|    value_loss           | 5.68e+03     |
------------------------------------------
Eval num_timesteps=345000, episode_reward=-942.75 +/- 128.28
Episode length: 27.12 +/- 9.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.1     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=-905.25 +/- 164.09
Episode length: 22.04 +/- 8.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=-909.16 +/- 143.66
Episode length: 23.58 +/- 8.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -906     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 169      |
|    time_elapsed    | 983      |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=-943.95 +/- 144.50
Episode length: 23.84 +/- 8.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.8          |
|    mean_reward          | -944          |
| time/                   |               |
|    total_timesteps      | 346500        |
| train/                  |               |
|    approx_kl            | 5.0349627e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000581     |
|    explained_variance   | 0.917         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.61e+03      |
|    n_updates            | 1690          |
|    policy_gradient_loss | -7.44e-08     |
|    value_loss           | 5.3e+03       |
-------------------------------------------
Eval num_timesteps=347000, episode_reward=-934.35 +/- 127.99
Episode length: 25.32 +/- 9.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=-973.95 +/- 91.63
Episode length: 25.86 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -974     |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=-929.47 +/- 162.35
Episode length: 24.76 +/- 8.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -929     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -941     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 170      |
|    time_elapsed    | 989      |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=-901.94 +/- 122.53
Episode length: 25.64 +/- 9.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.6         |
|    mean_reward          | -902         |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 8.614734e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000707    |
|    explained_variance   | 0.898        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.67e+03     |
|    n_updates            | 1700         |
|    policy_gradient_loss | -1.41e-06    |
|    value_loss           | 6.67e+03     |
------------------------------------------
Eval num_timesteps=349000, episode_reward=-929.56 +/- 126.53
Episode length: 25.70 +/- 10.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=-900.77 +/- 183.91
Episode length: 23.70 +/- 7.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-893.41 +/- 154.55
Episode length: 22.58 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -893     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -931     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 171      |
|    time_elapsed    | 995      |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=-934.36 +/- 137.23
Episode length: 25.68 +/- 10.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.7          |
|    mean_reward          | -934          |
| time/                   |               |
|    total_timesteps      | 350500        |
| train/                  |               |
|    approx_kl            | -2.240995e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000538     |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.31e+03      |
|    n_updates            | 1710          |
|    policy_gradient_loss | 4.46e-07      |
|    value_loss           | 6.61e+03      |
-------------------------------------------
Eval num_timesteps=351000, episode_reward=-903.15 +/- 144.81
Episode length: 23.16 +/- 8.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-911.45 +/- 156.71
Episode length: 23.32 +/- 9.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -911     |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-954.75 +/- 108.99
Episode length: 25.44 +/- 10.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -955     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | -934     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 172      |
|    time_elapsed    | 1000     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=-891.16 +/- 168.80
Episode length: 24.52 +/- 10.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.5          |
|    mean_reward          | -891          |
| time/                   |               |
|    total_timesteps      | 352500        |
| train/                  |               |
|    approx_kl            | -3.434252e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000495     |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.95e+03      |
|    n_updates            | 1720          |
|    policy_gradient_loss | 5.19e-08      |
|    value_loss           | 7.26e+03      |
-------------------------------------------
Eval num_timesteps=353000, episode_reward=-939.17 +/- 125.31
Episode length: 25.32 +/- 7.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=-884.86 +/- 186.96
Episode length: 21.96 +/- 9.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | -885     |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-894.77 +/- 153.44
Episode length: 23.44 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -904     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 173      |
|    time_elapsed    | 1006     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=-921.14 +/- 128.19
Episode length: 23.68 +/- 9.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.7          |
|    mean_reward          | -921          |
| time/                   |               |
|    total_timesteps      | 354500        |
| train/                  |               |
|    approx_kl            | 1.2456439e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000569     |
|    explained_variance   | 0.884         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.89e+03      |
|    n_updates            | 1730          |
|    policy_gradient_loss | -1.8e-06      |
|    value_loss           | 7.43e+03      |
-------------------------------------------
Eval num_timesteps=355000, episode_reward=-913.98 +/- 149.53
Episode length: 26.94 +/- 9.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.9     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=-941.54 +/- 127.35
Episode length: 24.12 +/- 8.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=-961.97 +/- 100.57
Episode length: 25.56 +/- 9.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -962     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -910     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 174      |
|    time_elapsed    | 1012     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=-981.16 +/- 123.60
Episode length: 25.88 +/- 8.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.9          |
|    mean_reward          | -981          |
| time/                   |               |
|    total_timesteps      | 356500        |
| train/                  |               |
|    approx_kl            | 4.5457768e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000777     |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.83e+03      |
|    n_updates            | 1740          |
|    policy_gradient_loss | -6.64e-06     |
|    value_loss           | 8.5e+03       |
-------------------------------------------
Eval num_timesteps=357000, episode_reward=-943.96 +/- 143.50
Episode length: 24.56 +/- 7.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=-907.95 +/- 147.46
Episode length: 25.70 +/- 9.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=-917.55 +/- 141.36
Episode length: 24.24 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -926     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 175      |
|    time_elapsed    | 1018     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=-916.25 +/- 161.24
Episode length: 25.40 +/- 7.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | -916          |
| time/                   |               |
|    total_timesteps      | 358500        |
| train/                  |               |
|    approx_kl            | 3.6670826e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000625     |
|    explained_variance   | 0.894         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.81e+03      |
|    n_updates            | 1750          |
|    policy_gradient_loss | 1e-06         |
|    value_loss           | 6.84e+03      |
-------------------------------------------
Eval num_timesteps=359000, episode_reward=-910.36 +/- 163.03
Episode length: 25.28 +/- 10.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=-969.16 +/- 126.58
Episode length: 25.20 +/- 7.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -969     |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-929.47 +/- 149.46
Episode length: 26.66 +/- 11.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.7     |
|    mean_reward     | -929     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -921     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 176      |
|    time_elapsed    | 1024     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=-916.36 +/- 145.30
Episode length: 24.40 +/- 10.18
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.4           |
|    mean_reward          | -916           |
| time/                   |                |
|    total_timesteps      | 360500         |
| train/                  |                |
|    approx_kl            | -4.3655746e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000456      |
|    explained_variance   | 0.883          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.36e+03       |
|    n_updates            | 1760           |
|    policy_gradient_loss | 5.17e-07       |
|    value_loss           | 7.35e+03       |
--------------------------------------------
Eval num_timesteps=361000, episode_reward=-942.75 +/- 133.79
Episode length: 24.04 +/- 9.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=-958.36 +/- 127.54
Episode length: 25.98 +/- 8.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | -958     |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=-952.36 +/- 126.09
Episode length: 24.12 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -952     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -926     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 177      |
|    time_elapsed    | 1030     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=-922.37 +/- 130.99
Episode length: 24.22 +/- 8.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | -922         |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 1.702574e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000879    |
|    explained_variance   | 0.91         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.18e+03     |
|    n_updates            | 1770         |
|    policy_gradient_loss | 7.49e-07     |
|    value_loss           | 5.59e+03     |
------------------------------------------
Eval num_timesteps=363000, episode_reward=-913.94 +/- 134.85
Episode length: 24.62 +/- 9.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-937.77 +/- 144.83
Episode length: 23.00 +/- 8.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=-935.55 +/- 125.10
Episode length: 26.12 +/- 8.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=-954.76 +/- 96.38
Episode length: 25.24 +/- 9.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -955     |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 178      |
|    time_elapsed    | 1036     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=-949.95 +/- 125.42
Episode length: 24.80 +/- 9.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | -950          |
| time/                   |               |
|    total_timesteps      | 365000        |
| train/                  |               |
|    approx_kl            | 1.1059456e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000478     |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.96e+03      |
|    n_updates            | 1780          |
|    policy_gradient_loss | -4.24e-06     |
|    value_loss           | 6.34e+03      |
-------------------------------------------
Eval num_timesteps=365500, episode_reward=-957.15 +/- 114.78
Episode length: 24.06 +/- 8.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=-951.16 +/- 131.38
Episode length: 23.98 +/- 8.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=-922.36 +/- 155.62
Episode length: 26.72 +/- 10.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.7     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -942     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 179      |
|    time_elapsed    | 1042     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=-934.36 +/- 141.37
Episode length: 25.52 +/- 8.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.5         |
|    mean_reward          | -934         |
| time/                   |              |
|    total_timesteps      | 367000       |
| train/                  |              |
|    approx_kl            | 5.918613e-06 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000393    |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.23e+03     |
|    n_updates            | 1790         |
|    policy_gradient_loss | -9.84e-06    |
|    value_loss           | 6e+03        |
------------------------------------------
Eval num_timesteps=367500, episode_reward=-942.74 +/- 117.16
Episode length: 25.36 +/- 9.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-894.55 +/- 158.61
Episode length: 25.16 +/- 9.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=-939.15 +/- 120.02
Episode length: 24.84 +/- 10.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -946     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 180      |
|    time_elapsed    | 1048     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=-925.84 +/- 151.89
Episode length: 24.80 +/- 9.10
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 24.8           |
|    mean_reward          | -926           |
| time/                   |                |
|    total_timesteps      | 369000         |
| train/                  |                |
|    approx_kl            | -2.8230716e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000364      |
|    explained_variance   | 0.889          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.31e+03       |
|    n_updates            | 1800           |
|    policy_gradient_loss | 6.75e-08       |
|    value_loss           | 7.9e+03        |
--------------------------------------------
Eval num_timesteps=369500, episode_reward=-888.77 +/- 132.35
Episode length: 23.12 +/- 8.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -889     |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-910.21 +/- 161.78
Episode length: 25.10 +/- 9.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=-911.54 +/- 126.88
Episode length: 22.38 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -896     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 181      |
|    time_elapsed    | 1054     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=-952.34 +/- 131.71
Episode length: 25.68 +/- 9.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.7         |
|    mean_reward          | -952         |
| time/                   |              |
|    total_timesteps      | 371000       |
| train/                  |              |
|    approx_kl            | 6.490154e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000406    |
|    explained_variance   | 0.867        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.29e+03     |
|    n_updates            | 1810         |
|    policy_gradient_loss | -1.29e-06    |
|    value_loss           | 9.12e+03     |
------------------------------------------
Eval num_timesteps=371500, episode_reward=-941.55 +/- 145.77
Episode length: 24.42 +/- 9.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-919.96 +/- 141.99
Episode length: 24.90 +/- 10.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-922.37 +/- 132.63
Episode length: 23.94 +/- 8.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -914     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 182      |
|    time_elapsed    | 1059     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=-920.96 +/- 146.21
Episode length: 24.66 +/- 9.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.7          |
|    mean_reward          | -921          |
| time/                   |               |
|    total_timesteps      | 373000        |
| train/                  |               |
|    approx_kl            | 1.5745172e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000472     |
|    explained_variance   | 0.888         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.57e+03      |
|    n_updates            | 1820          |
|    policy_gradient_loss | 9.26e-08      |
|    value_loss           | 7.39e+03      |
-------------------------------------------
Eval num_timesteps=373500, episode_reward=-940.26 +/- 152.73
Episode length: 24.92 +/- 8.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=-925.88 +/- 137.68
Episode length: 23.54 +/- 9.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=-902.96 +/- 194.14
Episode length: 24.76 +/- 10.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 183      |
|    time_elapsed    | 1065     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=-887.56 +/- 154.16
Episode length: 22.56 +/- 9.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.6          |
|    mean_reward          | -888          |
| time/                   |               |
|    total_timesteps      | 375000        |
| train/                  |               |
|    approx_kl            | 4.6857167e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000748     |
|    explained_variance   | 0.893         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.78e+03      |
|    n_updates            | 1830          |
|    policy_gradient_loss | -6.85e-07     |
|    value_loss           | 6.95e+03      |
-------------------------------------------
Eval num_timesteps=375500, episode_reward=-906.75 +/- 132.92
Episode length: 22.50 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=-924.61 +/- 167.46
Episode length: 23.06 +/- 8.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=-886.28 +/- 185.50
Episode length: 23.36 +/- 8.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -886     |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -895     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 184      |
|    time_elapsed    | 1071     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=-942.49 +/- 184.89
Episode length: 24.98 +/- 8.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | -942          |
| time/                   |               |
|    total_timesteps      | 377000        |
| train/                  |               |
|    approx_kl            | 1.0972144e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000903     |
|    explained_variance   | 0.887         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.01e+03      |
|    n_updates            | 1840          |
|    policy_gradient_loss | 1.22e-06      |
|    value_loss           | 7.47e+03      |
-------------------------------------------
Eval num_timesteps=377500, episode_reward=-917.49 +/- 150.44
Episode length: 22.50 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -917     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-911.55 +/- 143.39
Episode length: 25.00 +/- 8.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-931.95 +/- 156.91
Episode length: 25.10 +/- 9.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | -904     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 185      |
|    time_elapsed    | 1076     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=-947.54 +/- 135.24
Episode length: 23.00 +/- 8.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23          |
|    mean_reward          | -948        |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 5.84987e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000845   |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.46e+03    |
|    n_updates            | 1850        |
|    policy_gradient_loss | 1.35e-06    |
|    value_loss           | 5.87e+03    |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=-937.77 +/- 131.76
Episode length: 24.06 +/- 8.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-940.33 +/- 120.51
Episode length: 24.04 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=-900.77 +/- 135.16
Episode length: 24.64 +/- 8.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 186      |
|    time_elapsed    | 1082     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=-946.36 +/- 124.34
Episode length: 28.16 +/- 10.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 28.2          |
|    mean_reward          | -946          |
| time/                   |               |
|    total_timesteps      | 381000        |
| train/                  |               |
|    approx_kl            | 4.5110937e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000653     |
|    explained_variance   | 0.893         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.63e+03      |
|    n_updates            | 1860          |
|    policy_gradient_loss | 1.03e-06      |
|    value_loss           | 7.05e+03      |
-------------------------------------------
Eval num_timesteps=381500, episode_reward=-945.17 +/- 129.62
Episode length: 25.10 +/- 10.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -945     |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=-879.08 +/- 161.69
Episode length: 24.04 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -879     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-918.76 +/- 149.83
Episode length: 23.48 +/- 9.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -931     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 187      |
|    time_elapsed    | 1088     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=-941.55 +/- 135.00
Episode length: 25.64 +/- 10.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | -942          |
| time/                   |               |
|    total_timesteps      | 383000        |
| train/                  |               |
|    approx_kl            | 5.8311358e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00115      |
|    explained_variance   | 0.907         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.88e+03      |
|    n_updates            | 1870          |
|    policy_gradient_loss | -3.22e-05     |
|    value_loss           | 5.65e+03      |
-------------------------------------------
Eval num_timesteps=383500, episode_reward=-940.27 +/- 125.59
Episode length: 25.22 +/- 9.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-929.57 +/- 136.89
Episode length: 22.80 +/- 8.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-941.55 +/- 118.53
Episode length: 25.72 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-943.95 +/- 138.39
Episode length: 25.16 +/- 10.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -920     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 188      |
|    time_elapsed    | 1095     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=-921.15 +/- 132.07
Episode length: 25.58 +/- 11.15
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 25.6           |
|    mean_reward          | -921           |
| time/                   |                |
|    total_timesteps      | 385500         |
| train/                  |                |
|    approx_kl            | -2.0372681e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000716      |
|    explained_variance   | 0.909          |
|    learning_rate        | 0.0001         |
|    loss                 | 5.01e+03       |
|    n_updates            | 1880           |
|    policy_gradient_loss | 8.26e-07       |
|    value_loss           | 6.54e+03       |
--------------------------------------------
Eval num_timesteps=386000, episode_reward=-912.75 +/- 158.34
Episode length: 24.18 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=-914.79 +/- 132.15
Episode length: 22.80 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-901.94 +/- 158.87
Episode length: 22.38 +/- 8.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -946     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 189      |
|    time_elapsed    | 1100     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=-904.33 +/- 168.00
Episode length: 21.74 +/- 8.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.7          |
|    mean_reward          | -904          |
| time/                   |               |
|    total_timesteps      | 387500        |
| train/                  |               |
|    approx_kl            | 7.1013346e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000845     |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.19e+03      |
|    n_updates            | 1890          |
|    policy_gradient_loss | 1.43e-06      |
|    value_loss           | 5.65e+03      |
-------------------------------------------
Eval num_timesteps=388000, episode_reward=-939.13 +/- 123.56
Episode length: 23.54 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-909.14 +/- 160.68
Episode length: 23.72 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-933.16 +/- 147.41
Episode length: 23.48 +/- 8.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 190      |
|    time_elapsed    | 1106     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=-883.95 +/- 183.58
Episode length: 20.36 +/- 7.99
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 20.4           |
|    mean_reward          | -884           |
| time/                   |                |
|    total_timesteps      | 389500         |
| train/                  |                |
|    approx_kl            | -1.7753337e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000611      |
|    explained_variance   | 0.903          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.64e+03       |
|    n_updates            | 1900           |
|    policy_gradient_loss | -4.15e-07      |
|    value_loss           | 6.37e+03       |
--------------------------------------------
Eval num_timesteps=390000, episode_reward=-911.51 +/- 160.11
Episode length: 24.16 +/- 8.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=-901.97 +/- 147.57
Episode length: 24.98 +/- 9.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-906.76 +/- 153.52
Episode length: 22.88 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -945     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 191      |
|    time_elapsed    | 1112     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=-925.94 +/- 135.37
Episode length: 23.14 +/- 9.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.1         |
|    mean_reward          | -926         |
| time/                   |              |
|    total_timesteps      | 391500       |
| train/                  |              |
|    approx_kl            | 9.400537e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000538    |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.03e+03     |
|    n_updates            | 1910         |
|    policy_gradient_loss | 6.46e-07     |
|    value_loss           | 6.13e+03     |
------------------------------------------
Eval num_timesteps=392000, episode_reward=-948.74 +/- 134.51
Episode length: 24.66 +/- 8.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-954.74 +/- 125.58
Episode length: 22.34 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -955     |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-912.76 +/- 143.52
Episode length: 25.58 +/- 8.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | -909     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 192      |
|    time_elapsed    | 1117     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=-893.38 +/- 160.14
Episode length: 21.46 +/- 8.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.5          |
|    mean_reward          | -893          |
| time/                   |               |
|    total_timesteps      | 393500        |
| train/                  |               |
|    approx_kl            | 1.7200364e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00117      |
|    explained_variance   | 0.913         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.84e+03      |
|    n_updates            | 1920          |
|    policy_gradient_loss | -1.61e-06     |
|    value_loss           | 6.16e+03      |
-------------------------------------------
Eval num_timesteps=394000, episode_reward=-941.56 +/- 138.16
Episode length: 23.62 +/- 8.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=-928.29 +/- 165.71
Episode length: 24.18 +/- 8.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-935.55 +/- 144.80
Episode length: 25.56 +/- 10.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 193      |
|    time_elapsed    | 1123     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=-922.37 +/- 143.58
Episode length: 22.04 +/- 6.72
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 22             |
|    mean_reward          | -922           |
| time/                   |                |
|    total_timesteps      | 395500         |
| train/                  |                |
|    approx_kl            | 1.14087015e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0013        |
|    explained_variance   | 0.906          |
|    learning_rate        | 0.0001         |
|    loss                 | 3.47e+03       |
|    n_updates            | 1930           |
|    policy_gradient_loss | -1.6e-06       |
|    value_loss           | 6.8e+03        |
--------------------------------------------
Eval num_timesteps=396000, episode_reward=-911.54 +/- 159.17
Episode length: 24.46 +/- 9.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-940.36 +/- 125.76
Episode length: 24.38 +/- 11.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=-935.55 +/- 120.95
Episode length: 22.62 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 194      |
|    time_elapsed    | 1129     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=-927.15 +/- 154.72
Episode length: 24.66 +/- 8.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.7         |
|    mean_reward          | -927         |
| time/                   |              |
|    total_timesteps      | 397500       |
| train/                  |              |
|    approx_kl            | 9.284122e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00143     |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.4e+03      |
|    n_updates            | 1940         |
|    policy_gradient_loss | -6.08e-06    |
|    value_loss           | 5.78e+03     |
------------------------------------------
Eval num_timesteps=398000, episode_reward=-971.57 +/- 110.52
Episode length: 24.76 +/- 8.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -972     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-931.97 +/- 138.39
Episode length: 24.38 +/- 8.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-937.96 +/- 140.58
Episode length: 24.50 +/- 8.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -929     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 195      |
|    time_elapsed    | 1134     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=-921.11 +/- 135.43
Episode length: 23.28 +/- 8.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.3          |
|    mean_reward          | -921          |
| time/                   |               |
|    total_timesteps      | 399500        |
| train/                  |               |
|    approx_kl            | 2.0343577e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00185      |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.86e+03      |
|    n_updates            | 1950          |
|    policy_gradient_loss | 2.5e-08       |
|    value_loss           | 4.99e+03      |
-------------------------------------------
Eval num_timesteps=400000, episode_reward=-931.91 +/- 130.96
Episode length: 25.04 +/- 9.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=-902.72 +/- 184.64
Episode length: 22.56 +/- 8.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-916.37 +/- 146.78
Episode length: 25.52 +/- 9.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 196      |
|    time_elapsed    | 1140     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=-937.95 +/- 129.93
Episode length: 23.92 +/- 7.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.9          |
|    mean_reward          | -938          |
| time/                   |               |
|    total_timesteps      | 401500        |
| train/                  |               |
|    approx_kl            | 5.4183998e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00294      |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.83e+03      |
|    n_updates            | 1960          |
|    policy_gradient_loss | -1.43e-05     |
|    value_loss           | 6.48e+03      |
-------------------------------------------
Eval num_timesteps=402000, episode_reward=-918.75 +/- 156.87
Episode length: 23.80 +/- 8.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=-940.37 +/- 129.14
Episode length: 25.02 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=-946.34 +/- 113.44
Episode length: 23.10 +/- 9.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -908     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 197      |
|    time_elapsed    | 1146     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=-918.73 +/- 150.38
Episode length: 25.92 +/- 9.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.9          |
|    mean_reward          | -919          |
| time/                   |               |
|    total_timesteps      | 403500        |
| train/                  |               |
|    approx_kl            | 4.3655746e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00133      |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.51e+03      |
|    n_updates            | 1970          |
|    policy_gradient_loss | 4.41e-06      |
|    value_loss           | 7.76e+03      |
-------------------------------------------
Eval num_timesteps=404000, episode_reward=-954.78 +/- 130.62
Episode length: 26.22 +/- 8.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -955     |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=-924.69 +/- 160.86
Episode length: 24.14 +/- 8.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-910.35 +/- 149.66
Episode length: 23.92 +/- 8.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=-919.95 +/- 167.14
Episode length: 23.02 +/- 8.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 198      |
|    time_elapsed    | 1153     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=-958.36 +/- 108.64
Episode length: 25.40 +/- 9.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.4          |
|    mean_reward          | -958          |
| time/                   |               |
|    total_timesteps      | 406000        |
| train/                  |               |
|    approx_kl            | 4.2044412e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00184      |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.0001        |
|    loss                 | 4.27e+03      |
|    n_updates            | 1980          |
|    policy_gradient_loss | 1e-06         |
|    value_loss           | 7.9e+03       |
-------------------------------------------
Eval num_timesteps=406500, episode_reward=-892.28 +/- 177.11
Episode length: 23.76 +/- 8.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=-936.77 +/- 123.82
Episode length: 24.90 +/- 9.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -937     |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=-922.29 +/- 150.59
Episode length: 24.72 +/- 9.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 199      |
|    time_elapsed    | 1158     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=-930.77 +/- 132.60
Episode length: 25.50 +/- 8.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.5          |
|    mean_reward          | -931          |
| time/                   |               |
|    total_timesteps      | 408000        |
| train/                  |               |
|    approx_kl            | 1.3940735e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00146      |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.36e+03      |
|    n_updates            | 1990          |
|    policy_gradient_loss | -8.18e-07     |
|    value_loss           | 6.72e+03      |
-------------------------------------------
Eval num_timesteps=408500, episode_reward=-924.77 +/- 159.31
Episode length: 24.12 +/- 10.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-941.46 +/- 137.48
Episode length: 22.30 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -941     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=-872.97 +/- 195.01
Episode length: 23.70 +/- 9.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -873     |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -910     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 200      |
|    time_elapsed    | 1164     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-948.74 +/- 127.37
Episode length: 25.48 +/- 8.27
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 25.5           |
|    mean_reward          | -949           |
| time/                   |                |
|    total_timesteps      | 410000         |
| train/                  |                |
|    approx_kl            | -3.2014214e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00137       |
|    explained_variance   | 0.878          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.07e+03       |
|    n_updates            | 2000           |
|    policy_gradient_loss | 1.48e-06       |
|    value_loss           | 8.44e+03       |
--------------------------------------------
Eval num_timesteps=410500, episode_reward=-918.43 +/- 170.33
Episode length: 24.48 +/- 10.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=-894.74 +/- 159.91
Episode length: 22.72 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=-925.94 +/- 129.94
Episode length: 21.78 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | -956     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 201      |
|    time_elapsed    | 1170     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=-930.75 +/- 141.53
Episode length: 25.22 +/- 8.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.2         |
|    mean_reward          | -931         |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 8.844654e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00274     |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.94e+03     |
|    n_updates            | 2010         |
|    policy_gradient_loss | 1.09e-05     |
|    value_loss           | 5.4e+03      |
------------------------------------------
Eval num_timesteps=412500, episode_reward=-881.40 +/- 192.33
Episode length: 24.80 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -881     |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=-913.96 +/- 150.99
Episode length: 25.74 +/- 7.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=-925.89 +/- 145.26
Episode length: 25.82 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -895     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 202      |
|    time_elapsed    | 1176     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=-947.53 +/- 128.12
Episode length: 23.50 +/- 7.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.5          |
|    mean_reward          | -948          |
| time/                   |               |
|    total_timesteps      | 414000        |
| train/                  |               |
|    approx_kl            | 6.4319465e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00256      |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.19e+03      |
|    n_updates            | 2020          |
|    policy_gradient_loss | 2.87e-06      |
|    value_loss           | 7.97e+03      |
-------------------------------------------
Eval num_timesteps=414500, episode_reward=-961.96 +/- 119.55
Episode length: 25.32 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -962     |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=-919.66 +/- 190.82
Episode length: 23.62 +/- 8.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=-908.98 +/- 151.62
Episode length: 24.10 +/- 8.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -902     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 203      |
|    time_elapsed    | 1181     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=-941.55 +/- 132.84
Episode length: 22.32 +/- 10.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.3         |
|    mean_reward          | -942         |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 9.390077e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00217     |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.81e+03     |
|    n_updates            | 2030         |
|    policy_gradient_loss | 2.51e-05     |
|    value_loss           | 6.88e+03     |
------------------------------------------
Eval num_timesteps=416500, episode_reward=-932.95 +/- 166.39
Episode length: 22.38 +/- 8.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=-905.52 +/- 151.99
Episode length: 21.66 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.7     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=-924.76 +/- 143.12
Episode length: 24.56 +/- 8.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -926     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 204      |
|    time_elapsed    | 1187     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=-947.55 +/- 121.79
Episode length: 24.82 +/- 10.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.8          |
|    mean_reward          | -948          |
| time/                   |               |
|    total_timesteps      | 418000        |
| train/                  |               |
|    approx_kl            | 1.3620011e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0026       |
|    explained_variance   | 0.902         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.4e+03       |
|    n_updates            | 2040          |
|    policy_gradient_loss | -2.03e-05     |
|    value_loss           | 6.42e+03      |
-------------------------------------------
Eval num_timesteps=418500, episode_reward=-918.75 +/- 168.37
Episode length: 25.16 +/- 11.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=-922.23 +/- 149.96
Episode length: 23.66 +/- 8.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=-947.55 +/- 112.59
Episode length: 24.26 +/- 8.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -911     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 205      |
|    time_elapsed    | 1193     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-904.28 +/- 171.22
Episode length: 23.18 +/- 8.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.2          |
|    mean_reward          | -904          |
| time/                   |               |
|    total_timesteps      | 420000        |
| train/                  |               |
|    approx_kl            | 6.1409082e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0015       |
|    explained_variance   | 0.878         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.49e+03      |
|    n_updates            | 2050          |
|    policy_gradient_loss | 4.24e-06      |
|    value_loss           | 7.91e+03      |
-------------------------------------------
Eval num_timesteps=420500, episode_reward=-939.14 +/- 148.00
Episode length: 24.46 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=-933.15 +/- 134.10
Episode length: 25.50 +/- 9.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=-931.94 +/- 156.49
Episode length: 24.40 +/- 10.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.8     |
|    ep_rew_mean     | -917     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 206      |
|    time_elapsed    | 1198     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=-948.74 +/- 126.24
Episode length: 24.90 +/- 10.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.9          |
|    mean_reward          | -949          |
| time/                   |               |
|    total_timesteps      | 422000        |
| train/                  |               |
|    approx_kl            | 3.5215635e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00138      |
|    explained_variance   | 0.899         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.11e+03      |
|    n_updates            | 2060          |
|    policy_gradient_loss | 1.93e-06      |
|    value_loss           | 6.63e+03      |
-------------------------------------------
Eval num_timesteps=422500, episode_reward=-948.70 +/- 131.32
Episode length: 26.88 +/- 12.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.9     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=-899.52 +/- 164.34
Episode length: 22.50 +/- 8.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -900     |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=-947.57 +/- 119.41
Episode length: 25.04 +/- 8.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 207      |
|    time_elapsed    | 1204     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=-954.75 +/- 130.63
Episode length: 24.96 +/- 8.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | -955          |
| time/                   |               |
|    total_timesteps      | 424000        |
| train/                  |               |
|    approx_kl            | 2.3894245e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00237      |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.31e+03      |
|    n_updates            | 2070          |
|    policy_gradient_loss | -2.28e-06     |
|    value_loss           | 7.03e+03      |
-------------------------------------------
Eval num_timesteps=424500, episode_reward=-941.57 +/- 139.72
Episode length: 23.90 +/- 8.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=-927.16 +/- 135.89
Episode length: 24.16 +/- 9.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=-903.06 +/- 140.51
Episode length: 23.90 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -932     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 208      |
|    time_elapsed    | 1210     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=-924.75 +/- 126.57
Episode length: 23.90 +/- 9.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.9          |
|    mean_reward          | -925          |
| time/                   |               |
|    total_timesteps      | 426000        |
| train/                  |               |
|    approx_kl            | 2.6193447e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00103      |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.7e+03       |
|    n_updates            | 2080          |
|    policy_gradient_loss | -7.54e-06     |
|    value_loss           | 6.84e+03      |
-------------------------------------------
Eval num_timesteps=426500, episode_reward=-913.94 +/- 144.65
Episode length: 23.32 +/- 10.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=-927.16 +/- 132.13
Episode length: 23.72 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=-939.15 +/- 127.58
Episode length: 24.38 +/- 9.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=-906.72 +/- 135.20
Episode length: 25.52 +/- 11.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 209      |
|    time_elapsed    | 1217     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=-909.14 +/- 143.14
Episode length: 22.94 +/- 7.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.9         |
|    mean_reward          | -909         |
| time/                   |              |
|    total_timesteps      | 428500       |
| train/                  |              |
|    approx_kl            | 4.520573e-05 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00105     |
|    explained_variance   | 0.869        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.97e+03     |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.000208    |
|    value_loss           | 1.01e+04     |
------------------------------------------
Eval num_timesteps=429000, episode_reward=-911.22 +/- 136.26
Episode length: 21.68 +/- 8.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.7     |
|    mean_reward     | -911     |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=-983.56 +/- 96.89
Episode length: 26.28 +/- 10.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.3     |
|    mean_reward     | -984     |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-919.94 +/- 143.01
Episode length: 23.44 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -901     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 210      |
|    time_elapsed    | 1222     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=-922.35 +/- 164.19
Episode length: 23.30 +/- 9.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.3         |
|    mean_reward          | -922         |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 7.779454e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00298     |
|    explained_variance   | 0.894        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.71e+03     |
|    n_updates            | 2100         |
|    policy_gradient_loss | 1.42e-05     |
|    value_loss           | 7.47e+03     |
------------------------------------------
Eval num_timesteps=431000, episode_reward=-899.45 +/- 168.50
Episode length: 24.58 +/- 8.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -899     |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=-946.30 +/- 123.11
Episode length: 26.22 +/- 8.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -946     |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=-896.88 +/- 190.27
Episode length: 23.66 +/- 8.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -897     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 211      |
|    time_elapsed    | 1228     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=-931.93 +/- 145.50
Episode length: 22.32 +/- 8.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.3          |
|    mean_reward          | -932          |
| time/                   |               |
|    total_timesteps      | 432500        |
| train/                  |               |
|    approx_kl            | 1.7462298e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0025       |
|    explained_variance   | 0.905         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.43e+03      |
|    n_updates            | 2110          |
|    policy_gradient_loss | 6.59e-06      |
|    value_loss           | 6.5e+03       |
-------------------------------------------
Eval num_timesteps=433000, episode_reward=-911.42 +/- 187.62
Episode length: 22.62 +/- 8.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -911     |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=-948.73 +/- 104.38
Episode length: 22.78 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=-947.54 +/- 111.92
Episode length: 24.16 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 212      |
|    time_elapsed    | 1234     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=-883.89 +/- 172.87
Episode length: 24.24 +/- 9.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | -884          |
| time/                   |               |
|    total_timesteps      | 434500        |
| train/                  |               |
|    approx_kl            | 0.00042713014 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00491      |
|    explained_variance   | 0.893         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.11e+03      |
|    n_updates            | 2120          |
|    policy_gradient_loss | 0.00015       |
|    value_loss           | 7.29e+03      |
-------------------------------------------
Eval num_timesteps=435000, episode_reward=-940.37 +/- 146.38
Episode length: 25.56 +/- 9.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=-915.15 +/- 144.27
Episode length: 24.10 +/- 8.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=-905.53 +/- 137.01
Episode length: 22.44 +/- 8.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -885     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 213      |
|    time_elapsed    | 1239     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=-930.76 +/- 144.05
Episode length: 23.12 +/- 9.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.1          |
|    mean_reward          | -931          |
| time/                   |               |
|    total_timesteps      | 436500        |
| train/                  |               |
|    approx_kl            | 1.7113052e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00322      |
|    explained_variance   | 0.871         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.28e+03      |
|    n_updates            | 2130          |
|    policy_gradient_loss | 3.16e-05      |
|    value_loss           | 8.69e+03      |
-------------------------------------------
Eval num_timesteps=437000, episode_reward=-924.76 +/- 118.97
Episode length: 23.92 +/- 8.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=-923.29 +/- 180.09
Episode length: 26.06 +/- 11.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -923     |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=-918.49 +/- 188.10
Episode length: 23.62 +/- 9.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 214      |
|    time_elapsed    | 1245     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=-934.35 +/- 139.82
Episode length: 25.28 +/- 9.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.3          |
|    mean_reward          | -934          |
| time/                   |               |
|    total_timesteps      | 438500        |
| train/                  |               |
|    approx_kl            | 0.00016026018 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00131      |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.39e+03      |
|    n_updates            | 2140          |
|    policy_gradient_loss | 2.74e-05      |
|    value_loss           | 7.79e+03      |
-------------------------------------------
Eval num_timesteps=439000, episode_reward=-918.75 +/- 164.50
Episode length: 23.28 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=-934.35 +/- 158.18
Episode length: 24.54 +/- 9.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-952.36 +/- 130.03
Episode length: 25.24 +/- 9.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -952     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 215      |
|    time_elapsed    | 1251     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=-954.75 +/- 114.15
Episode length: 23.88 +/- 9.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.9         |
|    mean_reward          | -955         |
| time/                   |              |
|    total_timesteps      | 440500       |
| train/                  |              |
|    approx_kl            | 8.440111e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000734    |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.93e+03     |
|    n_updates            | 2150         |
|    policy_gradient_loss | 9.55e-07     |
|    value_loss           | 5.49e+03     |
------------------------------------------
Eval num_timesteps=441000, episode_reward=-909.15 +/- 168.98
Episode length: 23.74 +/- 9.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=-915.15 +/- 143.73
Episode length: 22.18 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=-900.77 +/- 167.08
Episode length: 24.06 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -933     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 216      |
|    time_elapsed    | 1257     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=-930.64 +/- 135.33
Episode length: 21.86 +/- 8.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.9          |
|    mean_reward          | -931          |
| time/                   |               |
|    total_timesteps      | 442500        |
| train/                  |               |
|    approx_kl            | 1.9063009e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0014       |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.53e+03      |
|    n_updates            | 2160          |
|    policy_gradient_loss | 5.78e-06      |
|    value_loss           | 6.97e+03      |
-------------------------------------------
Eval num_timesteps=443000, episode_reward=-967.96 +/- 111.28
Episode length: 26.70 +/- 11.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.7     |
|    mean_reward     | -968     |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=-955.94 +/- 125.90
Episode length: 25.78 +/- 10.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -956     |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=-933.15 +/- 126.92
Episode length: 23.26 +/- 8.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -908     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 217      |
|    time_elapsed    | 1262     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=-912.74 +/- 162.34
Episode length: 23.70 +/- 9.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.7         |
|    mean_reward          | -913         |
| time/                   |              |
|    total_timesteps      | 444500       |
| train/                  |              |
|    approx_kl            | 8.614734e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00128     |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.73e+03     |
|    n_updates            | 2170         |
|    policy_gradient_loss | -3.72e-08    |
|    value_loss           | 6.53e+03     |
------------------------------------------
Eval num_timesteps=445000, episode_reward=-931.96 +/- 151.31
Episode length: 25.22 +/- 9.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=-911.55 +/- 153.58
Episode length: 22.86 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=-928.36 +/- 147.58
Episode length: 25.20 +/- 9.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -911     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 218      |
|    time_elapsed    | 1268     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=-928.15 +/- 160.05
Episode length: 24.42 +/- 8.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | -928          |
| time/                   |               |
|    total_timesteps      | 446500        |
| train/                  |               |
|    approx_kl            | 4.3934415e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00384      |
|    explained_variance   | 0.891         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.45e+03      |
|    n_updates            | 2180          |
|    policy_gradient_loss | 0.000484      |
|    value_loss           | 6.01e+03      |
-------------------------------------------
Eval num_timesteps=447000, episode_reward=-910.36 +/- 170.78
Episode length: 22.62 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=-921.18 +/- 162.81
Episode length: 26.34 +/- 9.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.3     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=-919.96 +/- 151.79
Episode length: 23.72 +/- 7.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=-912.74 +/- 146.01
Episode length: 21.86 +/- 8.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.9     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -907     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 219      |
|    time_elapsed    | 1275     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=-943.96 +/- 138.39
Episode length: 27.28 +/- 11.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.3          |
|    mean_reward          | -944          |
| time/                   |               |
|    total_timesteps      | 449000        |
| train/                  |               |
|    approx_kl            | 3.7762104e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0023       |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.0001        |
|    loss                 | 5.08e+03      |
|    n_updates            | 2190          |
|    policy_gradient_loss | 0.000116      |
|    value_loss           | 6.56e+03      |
-------------------------------------------
Eval num_timesteps=449500, episode_reward=-937.71 +/- 141.33
Episode length: 24.44 +/- 9.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-905.20 +/- 203.04
Episode length: 23.08 +/- 9.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=-964.35 +/- 113.85
Episode length: 24.50 +/- 8.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -964     |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 220      |
|    time_elapsed    | 1281     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=-940.32 +/- 121.10
Episode length: 22.24 +/- 8.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | -940          |
| time/                   |               |
|    total_timesteps      | 451000        |
| train/                  |               |
|    approx_kl            | 6.0666032e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.001        |
|    explained_variance   | 0.902         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.11e+03      |
|    n_updates            | 2200          |
|    policy_gradient_loss | 5.57e-05      |
|    value_loss           | 6.35e+03      |
-------------------------------------------
Eval num_timesteps=451500, episode_reward=-874.26 +/- 191.47
Episode length: 23.90 +/- 9.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -874     |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=-930.75 +/- 147.99
Episode length: 24.12 +/- 8.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=-913.90 +/- 131.12
Episode length: 24.58 +/- 8.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -908     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 221      |
|    time_elapsed    | 1286     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=-886.32 +/- 167.40
Episode length: 21.70 +/- 8.66
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 21.7           |
|    mean_reward          | -886           |
| time/                   |                |
|    total_timesteps      | 453000         |
| train/                  |                |
|    approx_kl            | 1.36496965e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00118       |
|    explained_variance   | 0.883          |
|    learning_rate        | 0.0001         |
|    loss                 | 4.19e+03       |
|    n_updates            | 2210           |
|    policy_gradient_loss | 4.44e-08       |
|    value_loss           | 8.29e+03       |
--------------------------------------------
Eval num_timesteps=453500, episode_reward=-904.20 +/- 150.60
Episode length: 26.16 +/- 9.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=-939.15 +/- 121.22
Episode length: 23.10 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=-919.95 +/- 125.28
Episode length: 23.74 +/- 8.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -911     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 222      |
|    time_elapsed    | 1292     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=-918.74 +/- 177.60
Episode length: 24.00 +/- 8.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | -919          |
| time/                   |               |
|    total_timesteps      | 455000        |
| train/                  |               |
|    approx_kl            | 2.9104413e-06 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00118      |
|    explained_variance   | 0.885         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.36e+03      |
|    n_updates            | 2220          |
|    policy_gradient_loss | 2.13e-05      |
|    value_loss           | 6.47e+03      |
-------------------------------------------
Eval num_timesteps=455500, episode_reward=-945.16 +/- 108.47
Episode length: 27.10 +/- 12.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.1     |
|    mean_reward     | -945     |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=-945.16 +/- 113.65
Episode length: 26.30 +/- 9.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.3     |
|    mean_reward     | -945     |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=-912.75 +/- 116.37
Episode length: 24.56 +/- 8.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | -940     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 223      |
|    time_elapsed    | 1298     |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=-901.84 +/- 146.86
Episode length: 22.70 +/- 9.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.7          |
|    mean_reward          | -902          |
| time/                   |               |
|    total_timesteps      | 457000        |
| train/                  |               |
|    approx_kl            | 9.0367394e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00324      |
|    explained_variance   | 0.923         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.84e+03      |
|    n_updates            | 2230          |
|    policy_gradient_loss | 1.9e-05       |
|    value_loss           | 4.96e+03      |
-------------------------------------------
Eval num_timesteps=457500, episode_reward=-916.35 +/- 171.23
Episode length: 24.24 +/- 10.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=-931.77 +/- 146.17
Episode length: 24.42 +/- 8.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=-886.29 +/- 176.68
Episode length: 24.02 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -886     |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -919     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 224      |
|    time_elapsed    | 1304     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=-930.70 +/- 153.46
Episode length: 26.04 +/- 9.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26            |
|    mean_reward          | -931          |
| time/                   |               |
|    total_timesteps      | 459000        |
| train/                  |               |
|    approx_kl            | 7.0402166e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00282      |
|    explained_variance   | 0.864         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.65e+03      |
|    n_updates            | 2240          |
|    policy_gradient_loss | 1.15e-05      |
|    value_loss           | 8.87e+03      |
-------------------------------------------
Eval num_timesteps=459500, episode_reward=-912.75 +/- 140.98
Episode length: 22.98 +/- 9.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -913     |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-933.01 +/- 160.06
Episode length: 24.70 +/- 8.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=-922.37 +/- 156.99
Episode length: 26.08 +/- 10.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | -921     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 225      |
|    time_elapsed    | 1310     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=-906.75 +/- 151.64
Episode length: 21.14 +/- 7.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 21.1          |
|    mean_reward          | -907          |
| time/                   |               |
|    total_timesteps      | 461000        |
| train/                  |               |
|    approx_kl            | 4.4849003e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00239      |
|    explained_variance   | 0.873         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.95e+03      |
|    n_updates            | 2250          |
|    policy_gradient_loss | -1.35e-05     |
|    value_loss           | 8.77e+03      |
-------------------------------------------
Eval num_timesteps=461500, episode_reward=-940.36 +/- 132.97
Episode length: 25.80 +/- 10.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=-969.11 +/- 115.28
Episode length: 23.72 +/- 8.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -969     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=-915.16 +/- 146.22
Episode length: 23.86 +/- 8.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 226      |
|    time_elapsed    | 1315     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=-907.95 +/- 171.45
Episode length: 24.96 +/- 10.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25           |
|    mean_reward          | -908         |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 6.449409e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00122     |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.02e+03     |
|    n_updates            | 2260         |
|    policy_gradient_loss | -3.99e-05    |
|    value_loss           | 5.36e+03     |
------------------------------------------
Eval num_timesteps=463500, episode_reward=-924.75 +/- 157.95
Episode length: 22.48 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=-899.37 +/- 166.14
Episode length: 23.48 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -899     |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=-901.95 +/- 150.95
Episode length: 23.36 +/- 8.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | -912     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 227      |
|    time_elapsed    | 1321     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=-933.17 +/- 120.54
Episode length: 25.02 +/- 8.17
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 25             |
|    mean_reward          | -933           |
| time/                   |                |
|    total_timesteps      | 465000         |
| train/                  |                |
|    approx_kl            | -1.4551915e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00051       |
|    explained_variance   | 0.913          |
|    learning_rate        | 0.0001         |
|    loss                 | 1.87e+03       |
|    n_updates            | 2270           |
|    policy_gradient_loss | 1.66e-06       |
|    value_loss           | 6.25e+03       |
--------------------------------------------
Eval num_timesteps=465500, episode_reward=-933.08 +/- 140.97
Episode length: 24.48 +/- 8.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=-909.17 +/- 168.97
Episode length: 25.04 +/- 9.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=-868.03 +/- 185.74
Episode length: 20.96 +/- 8.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21       |
|    mean_reward     | -868     |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -919     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 228      |
|    time_elapsed    | 1327     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=-917.55 +/- 174.21
Episode length: 24.18 +/- 9.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | -918          |
| time/                   |               |
|    total_timesteps      | 467000        |
| train/                  |               |
|    approx_kl            | 5.2677933e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000557     |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.09e+03      |
|    n_updates            | 2280          |
|    policy_gradient_loss | 5.59e-08      |
|    value_loss           | 7.26e+03      |
-------------------------------------------
Eval num_timesteps=467500, episode_reward=-965.46 +/- 112.86
Episode length: 26.14 +/- 9.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -965     |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=-953.50 +/- 134.12
Episode length: 25.12 +/- 10.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -954     |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=-935.55 +/- 137.67
Episode length: 22.84 +/- 10.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 229      |
|    time_elapsed    | 1333     |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=-941.51 +/- 143.87
Episode length: 23.08 +/- 8.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.1          |
|    mean_reward          | -942          |
| time/                   |               |
|    total_timesteps      | 469000        |
| train/                  |               |
|    approx_kl            | 3.0413503e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00133      |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.82e+03      |
|    n_updates            | 2290          |
|    policy_gradient_loss | -5.19e-07     |
|    value_loss           | 5.66e+03      |
-------------------------------------------
Eval num_timesteps=469500, episode_reward=-966.74 +/- 107.80
Episode length: 22.38 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-918.73 +/- 142.96
Episode length: 24.24 +/- 10.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=-889.93 +/- 149.10
Episode length: 22.52 +/- 7.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -890     |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=-889.96 +/- 147.10
Episode length: 24.20 +/- 10.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -890     |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -912     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 230      |
|    time_elapsed    | 1339     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=-933.15 +/- 125.81
Episode length: 24.64 +/- 9.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | -933          |
| time/                   |               |
|    total_timesteps      | 471500        |
| train/                  |               |
|    approx_kl            | 1.5133992e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000759     |
|    explained_variance   | 0.875         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.75e+03      |
|    n_updates            | 2300          |
|    policy_gradient_loss | 1.68e-05      |
|    value_loss           | 8.69e+03      |
-------------------------------------------
Eval num_timesteps=472000, episode_reward=-888.75 +/- 153.50
Episode length: 24.46 +/- 9.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -889     |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=-961.85 +/- 119.31
Episode length: 23.24 +/- 8.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -962     |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=-923.56 +/- 130.47
Episode length: 25.00 +/- 8.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -906     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 231      |
|    time_elapsed    | 1345     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=-957.15 +/- 112.88
Episode length: 25.76 +/- 8.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.8         |
|    mean_reward          | -957         |
| time/                   |              |
|    total_timesteps      | 473500       |
| train/                  |              |
|    approx_kl            | 1.071021e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00123     |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.57e+03     |
|    n_updates            | 2310         |
|    policy_gradient_loss | -1.06e-06    |
|    value_loss           | 8.46e+03     |
------------------------------------------
Eval num_timesteps=474000, episode_reward=-887.53 +/- 177.99
Episode length: 20.68 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.7     |
|    mean_reward     | -888     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=-885.13 +/- 200.39
Episode length: 26.16 +/- 9.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -885     |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=-942.60 +/- 145.79
Episode length: 26.16 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.6     |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 232      |
|    time_elapsed    | 1350     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=-939.12 +/- 123.01
Episode length: 24.14 +/- 8.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.1         |
|    mean_reward          | -939         |
| time/                   |              |
|    total_timesteps      | 475500       |
| train/                  |              |
|    approx_kl            | 3.783498e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00118     |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.23e+03     |
|    n_updates            | 2320         |
|    policy_gradient_loss | 2.02e-06     |
|    value_loss           | 6.48e+03     |
------------------------------------------
Eval num_timesteps=476000, episode_reward=-900.76 +/- 138.83
Episode length: 24.56 +/- 9.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=-927.01 +/- 162.50
Episode length: 23.80 +/- 8.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=-954.75 +/- 148.19
Episode length: 24.30 +/- 9.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -955     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -941     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 233      |
|    time_elapsed    | 1356     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=-898.24 +/- 152.49
Episode length: 22.78 +/- 8.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.8          |
|    mean_reward          | -898          |
| time/                   |               |
|    total_timesteps      | 477500        |
| train/                  |               |
|    approx_kl            | 1.9208528e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00147      |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.05e+03      |
|    n_updates            | 2330          |
|    policy_gradient_loss | 3.37e-06      |
|    value_loss           | 6.44e+03      |
-------------------------------------------
Eval num_timesteps=478000, episode_reward=-928.17 +/- 120.27
Episode length: 23.60 +/- 8.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=-919.67 +/- 164.57
Episode length: 25.64 +/- 10.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=-933.14 +/- 169.24
Episode length: 24.10 +/- 9.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | -929     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 234      |
|    time_elapsed    | 1362     |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=-927.08 +/- 153.40
Episode length: 22.42 +/- 8.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | -927          |
| time/                   |               |
|    total_timesteps      | 479500        |
| train/                  |               |
|    approx_kl            | 2.1682354e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0018       |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.67e+03      |
|    n_updates            | 2340          |
|    policy_gradient_loss | 3.68e-06      |
|    value_loss           | 5.79e+03      |
-------------------------------------------
Eval num_timesteps=480000, episode_reward=-924.71 +/- 158.82
Episode length: 24.58 +/- 9.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=-956.94 +/- 136.99
Episode length: 25.66 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=-919.95 +/- 133.10
Episode length: 22.30 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 235      |
|    time_elapsed    | 1368     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=-918.75 +/- 138.90
Episode length: 23.84 +/- 9.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.8         |
|    mean_reward          | -919         |
| time/                   |              |
|    total_timesteps      | 481500       |
| train/                  |              |
|    approx_kl            | 3.608875e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00388     |
|    explained_variance   | 0.895        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.9e+03      |
|    n_updates            | 2350         |
|    policy_gradient_loss | 1.59e-05     |
|    value_loss           | 6.75e+03     |
------------------------------------------
Eval num_timesteps=482000, episode_reward=-924.72 +/- 123.75
Episode length: 25.88 +/- 9.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=-965.56 +/- 103.49
Episode length: 24.80 +/- 7.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -966     |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=-919.97 +/- 177.58
Episode length: 23.84 +/- 10.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -897     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 236      |
|    time_elapsed    | 1373     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=-897.16 +/- 163.90
Episode length: 23.72 +/- 9.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.7         |
|    mean_reward          | -897         |
| time/                   |              |
|    total_timesteps      | 483500       |
| train/                  |              |
|    approx_kl            | 1.132139e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00348     |
|    explained_variance   | 0.881        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.78e+03     |
|    n_updates            | 2360         |
|    policy_gradient_loss | -4.44e-07    |
|    value_loss           | 7.66e+03     |
------------------------------------------
Eval num_timesteps=484000, episode_reward=-928.37 +/- 125.39
Episode length: 24.96 +/- 8.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=-930.52 +/- 192.45
Episode length: 24.36 +/- 10.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-911.39 +/- 161.23
Episode length: 25.44 +/- 11.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -911     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -914     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 237      |
|    time_elapsed    | 1379     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=-913.95 +/- 129.97
Episode length: 24.58 +/- 8.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | -914          |
| time/                   |               |
|    total_timesteps      | 485500        |
| train/                  |               |
|    approx_kl            | 0.00036549856 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000771     |
|    explained_variance   | 0.91          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.83e+03      |
|    n_updates            | 2370          |
|    policy_gradient_loss | 9.93e-06      |
|    value_loss           | 5.6e+03       |
-------------------------------------------
Eval num_timesteps=486000, episode_reward=-909.15 +/- 144.16
Episode length: 23.18 +/- 8.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=-877.98 +/- 156.12
Episode length: 24.28 +/- 8.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -878     |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=-858.43 +/- 207.83
Episode length: 22.42 +/- 9.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -858     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -912     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 238      |
|    time_elapsed    | 1385     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=-930.75 +/- 134.22
Episode length: 23.36 +/- 8.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | -931          |
| time/                   |               |
|    total_timesteps      | 487500        |
| train/                  |               |
|    approx_kl            | 1.3707904e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000607     |
|    explained_variance   | 0.889         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.82e+03      |
|    n_updates            | 2380          |
|    policy_gradient_loss | 1.12e-06      |
|    value_loss           | 7.05e+03      |
-------------------------------------------
Eval num_timesteps=488000, episode_reward=-913.93 +/- 148.06
Episode length: 22.08 +/- 8.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=-892.20 +/- 145.60
Episode length: 22.78 +/- 8.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=-947.56 +/- 124.14
Episode length: 24.62 +/- 8.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 239      |
|    time_elapsed    | 1391     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=-917.46 +/- 119.78
Episode length: 23.98 +/- 9.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | -917          |
| time/                   |               |
|    total_timesteps      | 489500        |
| train/                  |               |
|    approx_kl            | 2.3515895e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00148      |
|    explained_variance   | 0.905         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.91e+03      |
|    n_updates            | 2390          |
|    policy_gradient_loss | 2.31e-06      |
|    value_loss           | 6.16e+03      |
-------------------------------------------
Eval num_timesteps=490000, episode_reward=-910.28 +/- 167.74
Episode length: 23.64 +/- 9.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=-927.14 +/- 150.00
Episode length: 24.14 +/- 9.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=-897.15 +/- 164.32
Episode length: 22.00 +/- 8.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22       |
|    mean_reward     | -897     |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=-899.41 +/- 164.57
Episode length: 24.34 +/- 9.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -899     |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | -939     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 240      |
|    time_elapsed    | 1397     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=-943.88 +/- 136.43
Episode length: 24.58 +/- 8.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | -944          |
| time/                   |               |
|    total_timesteps      | 492000        |
| train/                  |               |
|    approx_kl            | 0.00027114508 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00119      |
|    explained_variance   | 0.916         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.63e+03      |
|    n_updates            | 2400          |
|    policy_gradient_loss | -0.000104     |
|    value_loss           | 5.62e+03      |
-------------------------------------------
Eval num_timesteps=492500, episode_reward=-898.19 +/- 151.60
Episode length: 23.02 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-905.50 +/- 164.88
Episode length: 23.30 +/- 8.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=-925.97 +/- 148.07
Episode length: 25.74 +/- 10.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -930     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 241      |
|    time_elapsed    | 1403     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=-937.84 +/- 157.85
Episode length: 25.02 +/- 8.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | -938          |
| time/                   |               |
|    total_timesteps      | 494000        |
| train/                  |               |
|    approx_kl            | 1.9615982e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0014       |
|    explained_variance   | 0.907         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.1e+03       |
|    n_updates            | 2410          |
|    policy_gradient_loss | 2.07e-06      |
|    value_loss           | 5.91e+03      |
-------------------------------------------
Eval num_timesteps=494500, episode_reward=-915.08 +/- 151.26
Episode length: 22.42 +/- 8.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=-953.55 +/- 137.32
Episode length: 23.82 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -954     |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=-892.29 +/- 180.36
Episode length: 23.94 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 242      |
|    time_elapsed    | 1409     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=-919.97 +/- 146.00
Episode length: 25.88 +/- 8.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.9          |
|    mean_reward          | -920          |
| time/                   |               |
|    total_timesteps      | 496000        |
| train/                  |               |
|    approx_kl            | 4.0163286e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00139      |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.59e+03      |
|    n_updates            | 2420          |
|    policy_gradient_loss | 2.02e-06      |
|    value_loss           | 5.88e+03      |
-------------------------------------------
Eval num_timesteps=496500, episode_reward=-907.96 +/- 158.29
Episode length: 23.86 +/- 8.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -908     |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=-917.56 +/- 146.86
Episode length: 23.54 +/- 9.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=-935.54 +/- 127.92
Episode length: 24.12 +/- 7.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | -919     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 243      |
|    time_elapsed    | 1414     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=-925.96 +/- 128.83
Episode length: 23.44 +/- 8.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.4          |
|    mean_reward          | -926          |
| time/                   |               |
|    total_timesteps      | 498000        |
| train/                  |               |
|    approx_kl            | 2.7095666e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00216      |
|    explained_variance   | 0.932         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.8e+03       |
|    n_updates            | 2430          |
|    policy_gradient_loss | 8.65e-06      |
|    value_loss           | 4.95e+03      |
-------------------------------------------
Eval num_timesteps=498500, episode_reward=-925.94 +/- 136.42
Episode length: 25.52 +/- 10.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=-902.95 +/- 169.19
Episode length: 22.28 +/- 8.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=-931.77 +/- 139.60
Episode length: 24.88 +/- 9.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -915     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 244      |
|    time_elapsed    | 1420     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-925.88 +/- 106.22
Episode length: 23.86 +/- 8.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.9          |
|    mean_reward          | -926          |
| time/                   |               |
|    total_timesteps      | 500000        |
| train/                  |               |
|    approx_kl            | 6.8726076e-05 |
|    clip_fraction        | 0.000342      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00182      |
|    explained_variance   | 0.888         |
|    learning_rate        | 0.0001        |
|    loss                 | 4e+03         |
|    n_updates            | 2440          |
|    policy_gradient_loss | 3.57e-06      |
|    value_loss           | 7.53e+03      |
-------------------------------------------
Eval num_timesteps=500500, episode_reward=-893.35 +/- 188.29
Episode length: 22.08 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -893     |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=-911.35 +/- 164.15
Episode length: 22.18 +/- 9.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.2     |
|    mean_reward     | -911     |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=-917.53 +/- 143.45
Episode length: 24.32 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -914     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 245      |
|    time_elapsed    | 1426     |
|    total_timesteps | 501760   |
---------------------------------
Eval num_timesteps=502000, episode_reward=-918.74 +/- 137.32
Episode length: 23.00 +/- 8.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | -919          |
| time/                   |               |
|    total_timesteps      | 502000        |
| train/                  |               |
|    approx_kl            | 5.9371814e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000609     |
|    explained_variance   | 0.881         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.66e+03      |
|    n_updates            | 2450          |
|    policy_gradient_loss | 6.27e-06      |
|    value_loss           | 7.26e+03      |
-------------------------------------------
Eval num_timesteps=502500, episode_reward=-959.52 +/- 119.06
Episode length: 24.18 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -960     |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=-935.55 +/- 132.35
Episode length: 22.14 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=-916.36 +/- 143.31
Episode length: 23.32 +/- 9.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.1     |
|    ep_rew_mean     | -939     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 246      |
|    time_elapsed    | 1431     |
|    total_timesteps | 503808   |
---------------------------------
Eval num_timesteps=504000, episode_reward=-922.30 +/- 144.25
Episode length: 24.04 +/- 9.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24          |
|    mean_reward          | -922        |
| time/                   |             |
|    total_timesteps      | 504000      |
| train/                  |             |
|    approx_kl            | 2.05182e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000933   |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.35e+03    |
|    n_updates            | 2460        |
|    policy_gradient_loss | 3.58e-06    |
|    value_loss           | 5.57e+03    |
-----------------------------------------
Eval num_timesteps=504500, episode_reward=-924.75 +/- 141.11
Episode length: 24.58 +/- 10.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=-931.96 +/- 153.68
Episode length: 27.04 +/- 8.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27       |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=-939.17 +/- 134.74
Episode length: 24.16 +/- 8.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.7     |
|    ep_rew_mean     | -931     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 247      |
|    time_elapsed    | 1437     |
|    total_timesteps | 505856   |
---------------------------------
Eval num_timesteps=506000, episode_reward=-931.75 +/- 180.72
Episode length: 22.20 +/- 8.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.2         |
|    mean_reward          | -932         |
| time/                   |              |
|    total_timesteps      | 506000       |
| train/                  |              |
|    approx_kl            | 9.487849e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00102     |
|    explained_variance   | 0.913        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.18e+03     |
|    n_updates            | 2470         |
|    policy_gradient_loss | 2.58e-06     |
|    value_loss           | 5.74e+03     |
------------------------------------------
Eval num_timesteps=506500, episode_reward=-924.75 +/- 132.14
Episode length: 26.88 +/- 11.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.9     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=-880.27 +/- 168.91
Episode length: 23.04 +/- 8.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -880     |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=-918.60 +/- 154.90
Episode length: 23.48 +/- 8.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -922     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 248      |
|    time_elapsed    | 1443     |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=508000, episode_reward=-942.77 +/- 130.51
Episode length: 23.56 +/- 7.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.6          |
|    mean_reward          | -943          |
| time/                   |               |
|    total_timesteps      | 508000        |
| train/                  |               |
|    approx_kl            | 2.8230716e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00152      |
|    explained_variance   | 0.913         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.96e+03      |
|    n_updates            | 2480          |
|    policy_gradient_loss | 3.98e-06      |
|    value_loss           | 5.81e+03      |
-------------------------------------------
Eval num_timesteps=508500, episode_reward=-897.11 +/- 144.77
Episode length: 22.96 +/- 9.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23       |
|    mean_reward     | -897     |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=-916.35 +/- 148.24
Episode length: 24.50 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=-935.56 +/- 135.57
Episode length: 23.52 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -919     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 249      |
|    time_elapsed    | 1448     |
|    total_timesteps | 509952   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-913.95 +/- 139.03
Episode length: 24.62 +/- 9.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.6          |
|    mean_reward          | -914          |
| time/                   |               |
|    total_timesteps      | 510000        |
| train/                  |               |
|    approx_kl            | 1.4639227e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000951     |
|    explained_variance   | 0.898         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.23e+03      |
|    n_updates            | 2490          |
|    policy_gradient_loss | 2.99e-06      |
|    value_loss           | 7.46e+03      |
-------------------------------------------
Eval num_timesteps=510500, episode_reward=-937.96 +/- 140.58
Episode length: 27.36 +/- 9.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.4     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=-883.79 +/- 195.58
Episode length: 23.68 +/- 9.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -884     |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=-930.76 +/- 138.44
Episode length: 23.88 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=-928.35 +/- 130.47
Episode length: 25.14 +/- 9.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.5     |
|    ep_rew_mean     | -929     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 250      |
|    time_elapsed    | 1455     |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=512500, episode_reward=-946.37 +/- 132.18
Episode length: 25.28 +/- 9.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.3          |
|    mean_reward          | -946          |
| time/                   |               |
|    total_timesteps      | 512500        |
| train/                  |               |
|    approx_kl            | 1.7636921e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00146      |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.66e+03      |
|    n_updates            | 2500          |
|    policy_gradient_loss | 6.57e-06      |
|    value_loss           | 4.87e+03      |
-------------------------------------------
Eval num_timesteps=513000, episode_reward=-927.15 +/- 141.09
Episode length: 25.88 +/- 9.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=-948.77 +/- 129.61
Episode length: 26.74 +/- 9.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.7     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=-910.37 +/- 158.09
Episode length: 24.66 +/- 10.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -910     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 251      |
|    time_elapsed    | 1461     |
|    total_timesteps | 514048   |
---------------------------------
Eval num_timesteps=514500, episode_reward=-937.95 +/- 127.13
Episode length: 23.26 +/- 9.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.3         |
|    mean_reward          | -938         |
| time/                   |              |
|    total_timesteps      | 514500       |
| train/                  |              |
|    approx_kl            | 6.868504e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00126     |
|    explained_variance   | 0.892        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.37e+03     |
|    n_updates            | 2510         |
|    policy_gradient_loss | -4.89e-07    |
|    value_loss           | 6.68e+03     |
------------------------------------------
Eval num_timesteps=515000, episode_reward=-929.56 +/- 154.21
Episode length: 22.54 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=-927.08 +/- 144.16
Episode length: 22.68 +/- 8.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=-921.15 +/- 170.16
Episode length: 23.56 +/- 9.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -925     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 252      |
|    time_elapsed    | 1467     |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=516500, episode_reward=-894.66 +/- 171.44
Episode length: 23.54 +/- 9.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.5          |
|    mean_reward          | -895          |
| time/                   |               |
|    total_timesteps      | 516500        |
| train/                  |               |
|    approx_kl            | 1.3969839e-09 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00132      |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.21e+03      |
|    n_updates            | 2520          |
|    policy_gradient_loss | 1.6e-06       |
|    value_loss           | 6.55e+03      |
-------------------------------------------
Eval num_timesteps=517000, episode_reward=-900.70 +/- 158.74
Episode length: 22.50 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -901     |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=-917.45 +/- 153.86
Episode length: 26.14 +/- 11.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -917     |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=-933.14 +/- 163.17
Episode length: 22.52 +/- 8.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.5     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -914     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 253      |
|    time_elapsed    | 1472     |
|    total_timesteps | 518144   |
---------------------------------
Eval num_timesteps=518500, episode_reward=-916.30 +/- 159.44
Episode length: 22.38 +/- 8.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 22.4         |
|    mean_reward          | -916         |
| time/                   |              |
|    total_timesteps      | 518500       |
| train/                  |              |
|    approx_kl            | 7.916242e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00118     |
|    explained_variance   | 0.907        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.11e+03     |
|    n_updates            | 2530         |
|    policy_gradient_loss | 1.2e-06      |
|    value_loss           | 6.67e+03     |
------------------------------------------
Eval num_timesteps=519000, episode_reward=-909.09 +/- 157.71
Episode length: 23.78 +/- 7.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=-925.94 +/- 173.22
Episode length: 23.30 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-945.16 +/- 153.54
Episode length: 24.80 +/- 9.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -945     |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -905     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 254      |
|    time_elapsed    | 1478     |
|    total_timesteps | 520192   |
---------------------------------
Eval num_timesteps=520500, episode_reward=-913.95 +/- 132.14
Episode length: 25.12 +/- 10.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 25.1        |
|    mean_reward          | -914        |
| time/                   |             |
|    total_timesteps      | 520500      |
| train/                  |             |
|    approx_kl            | 8.20728e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00142    |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.3e+03     |
|    n_updates            | 2540        |
|    policy_gradient_loss | 3.03e-07    |
|    value_loss           | 9.89e+03    |
-----------------------------------------
Eval num_timesteps=521000, episode_reward=-928.27 +/- 150.30
Episode length: 23.44 +/- 8.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=-949.89 +/- 136.16
Episode length: 25.18 +/- 10.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=-947.33 +/- 139.20
Episode length: 24.44 +/- 8.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -947     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -926     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 255      |
|    time_elapsed    | 1484     |
|    total_timesteps | 522240   |
---------------------------------
Eval num_timesteps=522500, episode_reward=-901.70 +/- 165.65
Episode length: 22.28 +/- 9.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.3          |
|    mean_reward          | -902          |
| time/                   |               |
|    total_timesteps      | 522500        |
| train/                  |               |
|    approx_kl            | 1.6506936e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00203      |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.27e+03      |
|    n_updates            | 2550          |
|    policy_gradient_loss | 2e-05         |
|    value_loss           | 5.25e+03      |
-------------------------------------------
Eval num_timesteps=523000, episode_reward=-909.14 +/- 129.39
Episode length: 24.60 +/- 9.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=-942.78 +/- 134.32
Episode length: 24.40 +/- 10.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=-905.54 +/- 176.52
Episode length: 24.66 +/- 10.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -919     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 256      |
|    time_elapsed    | 1490     |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=524500, episode_reward=-954.77 +/- 126.71
Episode length: 26.88 +/- 8.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 26.9          |
|    mean_reward          | -955          |
| time/                   |               |
|    total_timesteps      | 524500        |
| train/                  |               |
|    approx_kl            | 1.2776582e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00097      |
|    explained_variance   | 0.883         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.99e+03      |
|    n_updates            | 2560          |
|    policy_gradient_loss | 3.2e-06       |
|    value_loss           | 7.41e+03      |
-------------------------------------------
Eval num_timesteps=525000, episode_reward=-931.95 +/- 114.47
Episode length: 25.94 +/- 10.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=-943.95 +/- 138.90
Episode length: 27.12 +/- 10.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.1     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=-921.15 +/- 148.47
Episode length: 23.52 +/- 9.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 257      |
|    time_elapsed    | 1496     |
|    total_timesteps | 526336   |
---------------------------------
Eval num_timesteps=526500, episode_reward=-941.56 +/- 129.01
Episode length: 24.86 +/- 9.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.9         |
|    mean_reward          | -942         |
| time/                   |              |
|    total_timesteps      | 526500       |
| train/                  |              |
|    approx_kl            | 3.882451e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00142     |
|    explained_variance   | 0.899        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.9e+03      |
|    n_updates            | 2570         |
|    policy_gradient_loss | 3.2e-06      |
|    value_loss           | 6.44e+03     |
------------------------------------------
Eval num_timesteps=527000, episode_reward=-928.37 +/- 138.51
Episode length: 24.90 +/- 8.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=-964.35 +/- 124.15
Episode length: 23.94 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -964     |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=-901.97 +/- 154.73
Episode length: 23.68 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -902     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -932     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 258      |
|    time_elapsed    | 1501     |
|    total_timesteps | 528384   |
---------------------------------
Eval num_timesteps=528500, episode_reward=-941.55 +/- 129.00
Episode length: 23.48 +/- 8.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.5          |
|    mean_reward          | -942          |
| time/                   |               |
|    total_timesteps      | 528500        |
| train/                  |               |
|    approx_kl            | 4.9900584e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00125      |
|    explained_variance   | 0.921         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.82e+03      |
|    n_updates            | 2580          |
|    policy_gradient_loss | 1.87e-05      |
|    value_loss           | 5.55e+03      |
-------------------------------------------
Eval num_timesteps=529000, episode_reward=-934.35 +/- 126.29
Episode length: 23.24 +/- 8.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=-947.57 +/- 121.79
Episode length: 23.54 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -948     |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-933.13 +/- 139.88
Episode length: 22.80 +/- 8.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.8     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | -935     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 259      |
|    time_elapsed    | 1507     |
|    total_timesteps | 530432   |
---------------------------------
Eval num_timesteps=530500, episode_reward=-953.56 +/- 107.30
Episode length: 24.98 +/- 8.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | -954          |
| time/                   |               |
|    total_timesteps      | 530500        |
| train/                  |               |
|    approx_kl            | 1.4173565e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00138      |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.86e+03      |
|    n_updates            | 2590          |
|    policy_gradient_loss | -8.43e-07     |
|    value_loss           | 6.11e+03      |
-------------------------------------------
Eval num_timesteps=531000, episode_reward=-913.97 +/- 157.95
Episode length: 25.38 +/- 8.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=-941.55 +/- 116.69
Episode length: 24.06 +/- 8.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=-911.55 +/- 135.13
Episode length: 23.32 +/- 8.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -935     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 260      |
|    time_elapsed    | 1513     |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=532500, episode_reward=-931.97 +/- 127.00
Episode length: 24.18 +/- 9.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.2          |
|    mean_reward          | -932          |
| time/                   |               |
|    total_timesteps      | 532500        |
| train/                  |               |
|    approx_kl            | 2.3873872e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00512      |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.8e+03       |
|    n_updates            | 2600          |
|    policy_gradient_loss | 3.89e-05      |
|    value_loss           | 6.36e+03      |
-------------------------------------------
Eval num_timesteps=533000, episode_reward=-928.18 +/- 144.22
Episode length: 25.26 +/- 9.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=-951.17 +/- 130.81
Episode length: 26.82 +/- 8.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.8     |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=-918.75 +/- 125.22
Episode length: 23.48 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=-931.89 +/- 174.93
Episode length: 24.18 +/- 9.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25       |
|    ep_rew_mean     | -937     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 261      |
|    time_elapsed    | 1519     |
|    total_timesteps | 534528   |
---------------------------------
Eval num_timesteps=535000, episode_reward=-939.16 +/- 146.99
Episode length: 24.22 +/- 8.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.2         |
|    mean_reward          | -939         |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 8.810975e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00342     |
|    explained_variance   | 0.914        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.3e+03      |
|    n_updates            | 2610         |
|    policy_gradient_loss | 1.57e-05     |
|    value_loss           | 5.37e+03     |
------------------------------------------
Eval num_timesteps=535500, episode_reward=-897.16 +/- 163.43
Episode length: 25.94 +/- 13.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -897     |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=-939.16 +/- 167.15
Episode length: 25.56 +/- 8.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=-928.34 +/- 152.89
Episode length: 24.92 +/- 8.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -915     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 262      |
|    time_elapsed    | 1525     |
|    total_timesteps | 536576   |
---------------------------------
Eval num_timesteps=537000, episode_reward=-897.15 +/- 145.26
Episode length: 25.04 +/- 10.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25            |
|    mean_reward          | -897          |
| time/                   |               |
|    total_timesteps      | 537000        |
| train/                  |               |
|    approx_kl            | 0.00036299042 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0016       |
|    explained_variance   | 0.887         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.04e+03      |
|    n_updates            | 2620          |
|    policy_gradient_loss | -5.21e-05     |
|    value_loss           | 7.49e+03      |
-------------------------------------------
Eval num_timesteps=537500, episode_reward=-941.55 +/- 120.94
Episode length: 25.92 +/- 10.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=-935.56 +/- 139.74
Episode length: 23.50 +/- 7.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=-931.77 +/- 156.87
Episode length: 24.60 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | -890     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 263      |
|    time_elapsed    | 1531     |
|    total_timesteps | 538624   |
---------------------------------
Eval num_timesteps=539000, episode_reward=-880.35 +/- 185.55
Episode length: 24.52 +/- 8.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.5         |
|    mean_reward          | -880         |
| time/                   |              |
|    total_timesteps      | 539000       |
| train/                  |              |
|    approx_kl            | 6.947084e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00235     |
|    explained_variance   | 0.858        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.84e+03     |
|    n_updates            | 2630         |
|    policy_gradient_loss | -1.05e-05    |
|    value_loss           | 8.91e+03     |
------------------------------------------
Eval num_timesteps=539500, episode_reward=-909.15 +/- 121.37
Episode length: 23.24 +/- 7.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -909     |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-942.75 +/- 140.61
Episode length: 24.22 +/- 8.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=-921.15 +/- 155.11
Episode length: 23.38 +/- 8.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -921     |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -904     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 264      |
|    time_elapsed    | 1537     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=541000, episode_reward=-941.56 +/- 149.68
Episode length: 24.40 +/- 9.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.4          |
|    mean_reward          | -942          |
| time/                   |               |
|    total_timesteps      | 541000        |
| train/                  |               |
|    approx_kl            | 1.8387247e-05 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00246      |
|    explained_variance   | 0.903         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.65e+03      |
|    n_updates            | 2640          |
|    policy_gradient_loss | 4.03e-05      |
|    value_loss           | 6.48e+03      |
-------------------------------------------
Eval num_timesteps=541500, episode_reward=-909.98 +/- 185.73
Episode length: 24.46 +/- 10.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=-897.12 +/- 123.88
Episode length: 23.52 +/- 9.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -897     |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=-939.15 +/- 130.91
Episode length: 24.76 +/- 8.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.5     |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 265      |
|    time_elapsed    | 1543     |
|    total_timesteps | 542720   |
---------------------------------
Eval num_timesteps=543000, episode_reward=-922.35 +/- 146.54
Episode length: 22.24 +/- 6.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | -922          |
| time/                   |               |
|    total_timesteps      | 543000        |
| train/                  |               |
|    approx_kl            | 2.9685907e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00229      |
|    explained_variance   | 0.881         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.39e+03      |
|    n_updates            | 2650          |
|    policy_gradient_loss | 4.85e-06      |
|    value_loss           | 7.74e+03      |
-------------------------------------------
Eval num_timesteps=543500, episode_reward=-931.85 +/- 147.38
Episode length: 22.66 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=-893.57 +/- 148.89
Episode length: 22.28 +/- 8.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.3     |
|    mean_reward     | -894     |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=-953.56 +/- 126.41
Episode length: 25.96 +/- 9.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | -954     |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.5     |
|    ep_rew_mean     | -918     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 266      |
|    time_elapsed    | 1548     |
|    total_timesteps | 544768   |
---------------------------------
Eval num_timesteps=545000, episode_reward=-912.76 +/- 163.68
Episode length: 23.66 +/- 10.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.7         |
|    mean_reward          | -913         |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 8.025067e-05 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00167     |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.3e+03      |
|    n_updates            | 2660         |
|    policy_gradient_loss | -1.57e-05    |
|    value_loss           | 7.13e+03     |
------------------------------------------
Eval num_timesteps=545500, episode_reward=-911.55 +/- 162.24
Episode length: 25.04 +/- 10.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=-911.40 +/- 161.05
Episode length: 23.84 +/- 10.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -911     |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=-922.37 +/- 148.02
Episode length: 23.10 +/- 8.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -937     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 267      |
|    time_elapsed    | 1554     |
|    total_timesteps | 546816   |
---------------------------------
Eval num_timesteps=547000, episode_reward=-922.30 +/- 163.41
Episode length: 23.72 +/- 7.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.7         |
|    mean_reward          | -922         |
| time/                   |              |
|    total_timesteps      | 547000       |
| train/                  |              |
|    approx_kl            | 7.381293e-05 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0022      |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.74e+03     |
|    n_updates            | 2670         |
|    policy_gradient_loss | 4.76e-06     |
|    value_loss           | 5.57e+03     |
------------------------------------------
Eval num_timesteps=547500, episode_reward=-928.35 +/- 135.88
Episode length: 24.38 +/- 9.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=-919.96 +/- 142.50
Episode length: 24.36 +/- 10.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=-942.61 +/- 137.73
Episode length: 24.88 +/- 9.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -943     |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -907     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 268      |
|    time_elapsed    | 1560     |
|    total_timesteps | 548864   |
---------------------------------
Eval num_timesteps=549000, episode_reward=-889.76 +/- 176.91
Episode length: 22.24 +/- 8.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.2          |
|    mean_reward          | -890          |
| time/                   |               |
|    total_timesteps      | 549000        |
| train/                  |               |
|    approx_kl            | 0.00025380825 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00151      |
|    explained_variance   | 0.896         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.37e+03      |
|    n_updates            | 2680          |
|    policy_gradient_loss | 2.65e-05      |
|    value_loss           | 7.09e+03      |
-------------------------------------------
Eval num_timesteps=549500, episode_reward=-924.72 +/- 150.14
Episode length: 23.80 +/- 8.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-910.37 +/- 155.34
Episode length: 22.14 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=-915.15 +/- 154.82
Episode length: 26.14 +/- 9.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.4     |
|    ep_rew_mean     | -921     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 269      |
|    time_elapsed    | 1565     |
|    total_timesteps | 550912   |
---------------------------------
Eval num_timesteps=551000, episode_reward=-917.56 +/- 149.76
Episode length: 24.50 +/- 9.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.5          |
|    mean_reward          | -918          |
| time/                   |               |
|    total_timesteps      | 551000        |
| train/                  |               |
|    approx_kl            | 6.5076165e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00267      |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.01e+03      |
|    n_updates            | 2690          |
|    policy_gradient_loss | 1.09e-05      |
|    value_loss           | 4.94e+03      |
-------------------------------------------
Eval num_timesteps=551500, episode_reward=-898.34 +/- 151.77
Episode length: 23.40 +/- 10.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=-934.29 +/- 118.31
Episode length: 25.16 +/- 8.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -934     |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=-922.36 +/- 149.48
Episode length: 25.50 +/- 10.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | -941     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 270      |
|    time_elapsed    | 1571     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=553000, episode_reward=-922.35 +/- 157.46
Episode length: 22.98 +/- 8.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23            |
|    mean_reward          | -922          |
| time/                   |               |
|    total_timesteps      | 553000        |
| train/                  |               |
|    approx_kl            | 8.5905805e-05 |
|    clip_fraction        | 0.000635      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0029       |
|    explained_variance   | 0.92          |
|    learning_rate        | 0.0001        |
|    loss                 | 1.83e+03      |
|    n_updates            | 2700          |
|    policy_gradient_loss | -7.99e-05     |
|    value_loss           | 5.2e+03       |
-------------------------------------------
Eval num_timesteps=553500, episode_reward=-923.55 +/- 136.41
Episode length: 23.94 +/- 8.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.9     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=-916.35 +/- 136.09
Episode length: 24.60 +/- 10.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=-919.95 +/- 173.89
Episode length: 22.64 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -920     |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=-930.70 +/- 146.76
Episode length: 25.08 +/- 9.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -940     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 271      |
|    time_elapsed    | 1577     |
|    total_timesteps | 555008   |
---------------------------------
Eval num_timesteps=555500, episode_reward=-880.17 +/- 188.55
Episode length: 24.04 +/- 9.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24            |
|    mean_reward          | -880          |
| time/                   |               |
|    total_timesteps      | 555500        |
| train/                  |               |
|    approx_kl            | 4.2523607e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0078       |
|    explained_variance   | 0.869         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.82e+03      |
|    n_updates            | 2710          |
|    policy_gradient_loss | 1.02e-05      |
|    value_loss           | 7.86e+03      |
-------------------------------------------
Eval num_timesteps=556000, episode_reward=-912.38 +/- 165.30
Episode length: 23.82 +/- 9.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=-905.35 +/- 165.84
Episode length: 23.18 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -905     |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=-918.76 +/- 122.32
Episode length: 25.68 +/- 11.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.5     |
|    ep_rew_mean     | -957     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 272      |
|    time_elapsed    | 1583     |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=557500, episode_reward=-940.36 +/- 124.61
Episode length: 23.86 +/- 9.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.9          |
|    mean_reward          | -940          |
| time/                   |               |
|    total_timesteps      | 557500        |
| train/                  |               |
|    approx_kl            | 0.00027443454 |
|    clip_fraction        | 0.000732      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00588      |
|    explained_variance   | 0.923         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.37e+03      |
|    n_updates            | 2720          |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 4.8e+03       |
-------------------------------------------
Eval num_timesteps=558000, episode_reward=-923.56 +/- 124.83
Episode length: 24.12 +/- 9.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=-941.50 +/- 151.80
Episode length: 25.56 +/- 8.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -941     |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=-877.96 +/- 198.36
Episode length: 22.56 +/- 8.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -878     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 273      |
|    time_elapsed    | 1589     |
|    total_timesteps | 559104   |
---------------------------------
Eval num_timesteps=559500, episode_reward=-937.95 +/- 138.03
Episode length: 25.10 +/- 8.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.1          |
|    mean_reward          | -938          |
| time/                   |               |
|    total_timesteps      | 559500        |
| train/                  |               |
|    approx_kl            | 0.00077198446 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00394      |
|    explained_variance   | 0.89          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.68e+03      |
|    n_updates            | 2730          |
|    policy_gradient_loss | 3.53e-05      |
|    value_loss           | 7.39e+03      |
-------------------------------------------
Eval num_timesteps=560000, episode_reward=-935.56 +/- 116.11
Episode length: 24.44 +/- 9.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=-869.31 +/- 189.74
Episode length: 23.06 +/- 9.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -869     |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=-951.16 +/- 139.87
Episode length: 23.44 +/- 8.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -951     |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.2     |
|    ep_rew_mean     | -946     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 274      |
|    time_elapsed    | 1595     |
|    total_timesteps | 561152   |
---------------------------------
Eval num_timesteps=561500, episode_reward=-930.76 +/- 120.65
Episode length: 25.74 +/- 8.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.7          |
|    mean_reward          | -931          |
| time/                   |               |
|    total_timesteps      | 561500        |
| train/                  |               |
|    approx_kl            | 4.8370566e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00383      |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.9e+03       |
|    n_updates            | 2740          |
|    policy_gradient_loss | 3.48e-06      |
|    value_loss           | 5.21e+03      |
-------------------------------------------
Eval num_timesteps=562000, episode_reward=-922.33 +/- 127.68
Episode length: 23.10 +/- 8.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.1     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=-905.55 +/- 134.90
Episode length: 24.58 +/- 10.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.6     |
|    mean_reward     | -906     |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=-918.77 +/- 163.61
Episode length: 25.90 +/- 10.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -919     |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.1     |
|    ep_rew_mean     | -947     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 275      |
|    time_elapsed    | 1600     |
|    total_timesteps | 563200   |
---------------------------------
Eval num_timesteps=563500, episode_reward=-881.54 +/- 171.68
Episode length: 22.74 +/- 8.51
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 22.7           |
|    mean_reward          | -882           |
| time/                   |                |
|    total_timesteps      | 563500         |
| train/                  |                |
|    approx_kl            | 1.15600415e-07 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0072        |
|    explained_variance   | 0.907          |
|    learning_rate        | 0.0001         |
|    loss                 | 2.72e+03       |
|    n_updates            | 2750           |
|    policy_gradient_loss | 7.21e-06       |
|    value_loss           | 6.69e+03       |
--------------------------------------------
Eval num_timesteps=564000, episode_reward=-925.96 +/- 129.93
Episode length: 24.76 +/- 9.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -926     |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=-904.34 +/- 137.36
Episode length: 23.34 +/- 8.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=-922.36 +/- 148.03
Episode length: 25.42 +/- 10.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -942     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 276      |
|    time_elapsed    | 1606     |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=565500, episode_reward=-903.05 +/- 187.24
Episode length: 25.60 +/- 10.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.6          |
|    mean_reward          | -903          |
| time/                   |               |
|    total_timesteps      | 565500        |
| train/                  |               |
|    approx_kl            | 0.00017029769 |
|    clip_fraction        | 0.00122       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0095       |
|    explained_variance   | 0.914         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.5e+03       |
|    n_updates            | 2760          |
|    policy_gradient_loss | 0.000111      |
|    value_loss           | 5.62e+03      |
-------------------------------------------
Eval num_timesteps=566000, episode_reward=-933.15 +/- 128.07
Episode length: 24.32 +/- 10.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=-952.36 +/- 131.14
Episode length: 26.86 +/- 7.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.9     |
|    mean_reward     | -952     |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=-965.52 +/- 114.03
Episode length: 25.64 +/- 9.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -966     |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -883     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 277      |
|    time_elapsed    | 1612     |
|    total_timesteps | 567296   |
---------------------------------
Eval num_timesteps=567500, episode_reward=-905.55 +/- 172.81
Episode length: 27.70 +/- 10.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.7          |
|    mean_reward          | -906          |
| time/                   |               |
|    total_timesteps      | 567500        |
| train/                  |               |
|    approx_kl            | 3.0679774e-05 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00932      |
|    explained_variance   | 0.867         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.23e+03      |
|    n_updates            | 2770          |
|    policy_gradient_loss | -8.12e-06     |
|    value_loss           | 9.08e+03      |
-------------------------------------------
Eval num_timesteps=568000, episode_reward=-924.54 +/- 154.95
Episode length: 21.80 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.8     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=-893.56 +/- 163.20
Episode length: 22.68 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.7     |
|    mean_reward     | -894     |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=-913.96 +/- 129.39
Episode length: 25.90 +/- 8.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -914     |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | -918     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 278      |
|    time_elapsed    | 1618     |
|    total_timesteps | 569344   |
---------------------------------
Eval num_timesteps=569500, episode_reward=-904.22 +/- 157.56
Episode length: 25.80 +/- 7.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.8          |
|    mean_reward          | -904          |
| time/                   |               |
|    total_timesteps      | 569500        |
| train/                  |               |
|    approx_kl            | 3.6929938e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0108       |
|    explained_variance   | 0.899         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.74e+03      |
|    n_updates            | 2780          |
|    policy_gradient_loss | -3.31e-05     |
|    value_loss           | 6.83e+03      |
-------------------------------------------
Eval num_timesteps=570000, episode_reward=-911.56 +/- 146.88
Episode length: 25.18 +/- 11.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=-910.02 +/- 180.85
Episode length: 23.74 +/- 10.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -910     |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=-887.32 +/- 155.67
Episode length: 25.76 +/- 10.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -887     |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.8     |
|    ep_rew_mean     | -934     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 279      |
|    time_elapsed    | 1624     |
|    total_timesteps | 571392   |
---------------------------------
Eval num_timesteps=571500, episode_reward=-957.13 +/- 114.18
Episode length: 24.56 +/- 8.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.6         |
|    mean_reward          | -957         |
| time/                   |              |
|    total_timesteps      | 571500       |
| train/                  |              |
|    approx_kl            | 0.0004046323 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0114      |
|    explained_variance   | 0.915        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.66e+03     |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 5.6e+03      |
------------------------------------------
Eval num_timesteps=572000, episode_reward=-906.77 +/- 158.15
Episode length: 26.34 +/- 11.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.3     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=-922.08 +/- 170.97
Episode length: 20.88 +/- 8.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 20.9     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=-959.56 +/- 119.65
Episode length: 24.92 +/- 8.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.9     |
|    mean_reward     | -960     |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.4     |
|    ep_rew_mean     | -909     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 280      |
|    time_elapsed    | 1629     |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=573500, episode_reward=-907.94 +/- 134.17
Episode length: 22.10 +/- 6.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.1          |
|    mean_reward          | -908          |
| time/                   |               |
|    total_timesteps      | 573500        |
| train/                  |               |
|    approx_kl            | 0.00016010675 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.014        |
|    explained_variance   | 0.897         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.62e+03      |
|    n_updates            | 2800          |
|    policy_gradient_loss | -8.08e-05     |
|    value_loss           | 7.06e+03      |
-------------------------------------------
Eval num_timesteps=574000, episode_reward=-928.37 +/- 143.11
Episode length: 25.64 +/- 9.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=-957.10 +/- 144.98
Episode length: 24.08 +/- 7.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -957     |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=-952.35 +/- 115.36
Episode length: 24.50 +/- 9.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -952     |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 281      |
|    time_elapsed    | 1636     |
|    total_timesteps | 575488   |
---------------------------------
Eval num_timesteps=575500, episode_reward=-931.95 +/- 144.00
Episode length: 25.88 +/- 10.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.9          |
|    mean_reward          | -932          |
| time/                   |               |
|    total_timesteps      | 575500        |
| train/                  |               |
|    approx_kl            | 0.00020739003 |
|    clip_fraction        | 0.00103       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00977      |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.97e+03      |
|    n_updates            | 2810          |
|    policy_gradient_loss | 6.24e-06      |
|    value_loss           | 6.23e+03      |
-------------------------------------------
Eval num_timesteps=576000, episode_reward=-916.36 +/- 161.70
Episode length: 24.20 +/- 9.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -916     |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=-948.74 +/- 108.42
Episode length: 23.60 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -949     |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=-939.15 +/- 142.01
Episode length: 25.54 +/- 9.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -939     |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=-924.74 +/- 152.93
Episode length: 24.76 +/- 10.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -925     |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.7     |
|    ep_rew_mean     | -935     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 282      |
|    time_elapsed    | 1643     |
|    total_timesteps | 577536   |
---------------------------------
Eval num_timesteps=578000, episode_reward=-901.96 +/- 168.52
Episode length: 23.08 +/- 7.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.1        |
|    mean_reward          | -902        |
| time/                   |             |
|    total_timesteps      | 578000      |
| train/                  |             |
|    approx_kl            | 4.64872e-05 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00966    |
|    explained_variance   | 0.891       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.67e+03    |
|    n_updates            | 2820        |
|    policy_gradient_loss | 6.9e-05     |
|    value_loss           | 6.93e+03    |
-----------------------------------------
Eval num_timesteps=578500, episode_reward=-904.35 +/- 140.46
Episode length: 23.74 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=-941.57 +/- 118.54
Episode length: 26.52 +/- 8.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.5     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=-933.17 +/- 142.43
Episode length: 24.80 +/- 7.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -933     |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -910     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 283      |
|    time_elapsed    | 1649     |
|    total_timesteps | 579584   |
---------------------------------
Eval num_timesteps=580000, episode_reward=-958.36 +/- 107.30
Episode length: 25.72 +/- 8.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 25.7          |
|    mean_reward          | -958          |
| time/                   |               |
|    total_timesteps      | 580000        |
| train/                  |               |
|    approx_kl            | 0.00026788615 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00516      |
|    explained_variance   | 0.88          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.61e+03      |
|    n_updates            | 2830          |
|    policy_gradient_loss | 0.000156      |
|    value_loss           | 7.12e+03      |
-------------------------------------------
Eval num_timesteps=580500, episode_reward=-930.77 +/- 151.35
Episode length: 23.38 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -931     |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=-917.57 +/- 160.90
Episode length: 25.12 +/- 9.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -918     |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=-965.55 +/- 131.65
Episode length: 26.56 +/- 10.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.6     |
|    mean_reward     | -966     |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.3     |
|    ep_rew_mean     | -916     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 284      |
|    time_elapsed    | 1654     |
|    total_timesteps | 581632   |
---------------------------------
Eval num_timesteps=582000, episode_reward=-947.44 +/- 135.80
Episode length: 25.52 +/- 8.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.5         |
|    mean_reward          | -947         |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 5.127577e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00732     |
|    explained_variance   | 0.89         |
|    learning_rate        | 0.0001       |
|    loss                 | 4.25e+03     |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.000131    |
|    value_loss           | 7.28e+03     |
------------------------------------------
Eval num_timesteps=582500, episode_reward=-945.16 +/- 117.40
Episode length: 26.10 +/- 7.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -945     |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=-960.77 +/- 128.59
Episode length: 23.50 +/- 9.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -961     |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=-931.94 +/- 143.99
Episode length: 22.44 +/- 9.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -932     |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 285      |
|    time_elapsed    | 1660     |
|    total_timesteps | 583680   |
---------------------------------
Eval num_timesteps=584000, episode_reward=-940.28 +/- 159.47
Episode length: 27.56 +/- 12.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 27.6          |
|    mean_reward          | -940          |
| time/                   |               |
|    total_timesteps      | 584000        |
| train/                  |               |
|    approx_kl            | 0.00016722729 |
|    clip_fraction        | 0.00083       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | 0.904         |
|    learning_rate        | 0.0001        |
|    loss                 | 5.98e+03      |
|    n_updates            | 2850          |
|    policy_gradient_loss | -6.06e-05     |
|    value_loss           | 6.24e+03      |
-------------------------------------------
Eval num_timesteps=584500, episode_reward=-942.47 +/- 140.20
Episode length: 23.80 +/- 7.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=-937.97 +/- 126.56
Episode length: 26.44 +/- 8.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.4     |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=-895.93 +/- 160.09
Episode length: 25.02 +/- 9.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25       |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.2     |
|    ep_rew_mean     | -898     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 286      |
|    time_elapsed    | 1666     |
|    total_timesteps | 585728   |
---------------------------------
Eval num_timesteps=586000, episode_reward=-922.04 +/- 188.89
Episode length: 23.72 +/- 8.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 23.7         |
|    mean_reward          | -922         |
| time/                   |              |
|    total_timesteps      | 586000       |
| train/                  |              |
|    approx_kl            | 7.055636e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00717     |
|    explained_variance   | 0.891        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.31e+03     |
|    n_updates            | 2860         |
|    policy_gradient_loss | -7.36e-05    |
|    value_loss           | 7.67e+03     |
------------------------------------------
Eval num_timesteps=586500, episode_reward=-902.60 +/- 207.30
Episode length: 23.98 +/- 8.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -903     |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=-929.55 +/- 128.21
Episode length: 21.62 +/- 8.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.6     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=-894.77 +/- 161.22
Episode length: 24.80 +/- 8.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.8     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.9     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 287      |
|    time_elapsed    | 1672     |
|    total_timesteps | 587776   |
---------------------------------
Eval num_timesteps=588000, episode_reward=-923.51 +/- 148.69
Episode length: 24.08 +/- 7.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.1          |
|    mean_reward          | -924          |
| time/                   |               |
|    total_timesteps      | 588000        |
| train/                  |               |
|    approx_kl            | 0.00013978436 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00961      |
|    explained_variance   | 0.915         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.5e+03       |
|    n_updates            | 2870          |
|    policy_gradient_loss | -5.87e-05     |
|    value_loss           | 5.95e+03      |
-------------------------------------------
Eval num_timesteps=588500, episode_reward=-937.97 +/- 131.58
Episode length: 24.02 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24       |
|    mean_reward     | -938     |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=-877.47 +/- 189.60
Episode length: 23.50 +/- 7.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.5     |
|    mean_reward     | -877     |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=-940.33 +/- 159.68
Episode length: 25.16 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -940     |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | -881     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 288      |
|    time_elapsed    | 1678     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=590000, episode_reward=-937.94 +/- 120.16
Episode length: 21.84 +/- 7.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 21.8         |
|    mean_reward          | -938         |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0005082105 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0062      |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.48e+03     |
|    n_updates            | 2880         |
|    policy_gradient_loss | 6.58e-05     |
|    value_loss           | 8.85e+03     |
------------------------------------------
Eval num_timesteps=590500, episode_reward=-926.84 +/- 168.38
Episode length: 24.10 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -927     |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=-892.37 +/- 158.48
Episode length: 25.62 +/- 11.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -892     |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=-949.73 +/- 151.25
Episode length: 24.42 +/- 9.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -950     |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24       |
|    ep_rew_mean     | -936     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 289      |
|    time_elapsed    | 1683     |
|    total_timesteps | 591872   |
---------------------------------
Eval num_timesteps=592000, episode_reward=-897.08 +/- 160.97
Episode length: 22.44 +/- 9.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 22.4          |
|    mean_reward          | -897          |
| time/                   |               |
|    total_timesteps      | 592000        |
| train/                  |               |
|    approx_kl            | 0.00010110895 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00703      |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.52e+03      |
|    n_updates            | 2890          |
|    policy_gradient_loss | -6.08e-06     |
|    value_loss           | 5.17e+03      |
-------------------------------------------
Eval num_timesteps=592500, episode_reward=-935.55 +/- 143.82
Episode length: 23.62 +/- 9.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=-895.85 +/- 197.91
Episode length: 24.22 +/- 9.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=-923.55 +/- 154.70
Episode length: 22.88 +/- 8.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.9     |
|    mean_reward     | -924     |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | -947     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 290      |
|    time_elapsed    | 1689     |
|    total_timesteps | 593920   |
---------------------------------
Eval num_timesteps=594000, episode_reward=-899.56 +/- 173.25
Episode length: 24.84 +/- 9.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.8         |
|    mean_reward          | -900         |
| time/                   |              |
|    total_timesteps      | 594000       |
| train/                  |              |
|    approx_kl            | 4.305056e-05 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00959     |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.37e+03     |
|    n_updates            | 2900         |
|    policy_gradient_loss | -4.64e-06    |
|    value_loss           | 3.67e+03     |
------------------------------------------
Eval num_timesteps=594500, episode_reward=-935.52 +/- 152.19
Episode length: 26.12 +/- 9.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -936     |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=-929.56 +/- 124.22
Episode length: 24.72 +/- 9.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.7     |
|    mean_reward     | -930     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=-922.35 +/- 142.05
Episode length: 27.46 +/- 11.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 27.5     |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -935     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 291      |
|    time_elapsed    | 1695     |
|    total_timesteps | 595968   |
---------------------------------
Eval num_timesteps=596000, episode_reward=-912.60 +/- 154.19
Episode length: 24.26 +/- 10.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 24.3          |
|    mean_reward          | -913          |
| time/                   |               |
|    total_timesteps      | 596000        |
| train/                  |               |
|    approx_kl            | 0.00021254824 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0057       |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.85e+03      |
|    n_updates            | 2910          |
|    policy_gradient_loss | -0.000121     |
|    value_loss           | 4.13e+03      |
-------------------------------------------
Eval num_timesteps=596500, episode_reward=-887.56 +/- 175.14
Episode length: 26.12 +/- 10.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.1     |
|    mean_reward     | -888     |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=-911.56 +/- 142.90
Episode length: 26.16 +/- 8.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=-928.35 +/- 112.70
Episode length: 23.80 +/- 9.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -928     |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=-906.57 +/- 164.05
Episode length: 25.16 +/- 10.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -907     |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.3     |
|    ep_rew_mean     | -934     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 292      |
|    time_elapsed    | 1702     |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=598500, episode_reward=-921.09 +/- 143.64
Episode length: 23.34 +/- 8.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 23.3          |
|    mean_reward          | -921          |
| time/                   |               |
|    total_timesteps      | 598500        |
| train/                  |               |
|    approx_kl            | 0.00010826677 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00361      |
|    explained_variance   | 0.901         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.08e+03      |
|    n_updates            | 2920          |
|    policy_gradient_loss | 4.32e-05      |
|    value_loss           | 6.98e+03      |
-------------------------------------------
Eval num_timesteps=599000, episode_reward=-961.95 +/- 104.11
Episode length: 24.42 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.4     |
|    mean_reward     | -962     |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=-895.71 +/- 163.20
Episode length: 22.62 +/- 9.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -896     |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-904.34 +/- 138.39
Episode length: 24.34 +/- 8.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -904     |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.1     |
|    ep_rew_mean     | -928     |
| time/              |          |
|    fps             | 351      |
|    iterations      | 293      |
|    time_elapsed    | 1707     |
|    total_timesteps | 600064   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 600000
Frame skip: 4
